.
.

of
Matrix Algebra
.

Fundamentals
y

.

Third Edition

θ
.

x

Gregory Hartman

F

M

A

Third Edi on, Version 3.1110

Gregory Hartman, Ph.D.
Department of Mathema cs and Computer Science
Virginia Military Ins tute

Copyright © 2011 Gregory Hartman
Licensed to the public under Crea ve Commons
A ribu on-Noncommercial 3.0 United States License

T

This text took a great deal of eﬀort to accomplish and I owe a great many people
thanks.

I owe Michelle (and Sydney and Alex) much for their support at home. Michelle
puts up with much as I con nually read LATEX manuals, sketch outlines of the text, write
exercises, and draw illustra ons.
My thanks to the Department of Mathema cs and Computer Science at Virginia
Military Ins tute for their support of this project. Lee Dewald and Troy Siemers, my
department heads, deserve special thanks for their special encouragement and recogni on that this eﬀort has been a worthwhile endeavor.
My thanks to all who informed me of errors in the text or provided ideas for improvement. Special thanks to Michelle Feole and Dan Joseph who each caught a number of errors.
This whole project would have been impossible save for the eﬀorts of the LATEX
community. This text makes use of about 15 diﬀerent packages, was compiled using MiKTEX, and edited using TEXnicCenter, all of which was provided free of charge.
This generosity helped convince me that this text should be made freely available as
well.

iii

P
A Note to Students, Teachers, and other Readers

Thank you for reading this short preface. Allow me to share a few key points about
the text so that you may be er understand what you will ﬁnd beyond this page.
This text deals with matrix algebra, as opposed to linear algebra. Without arguing seman cs, I view matrix algebra as a subset of linear algebra, focused primarily
on basic concepts and solu on techniques. There is li le formal development of theory and abstract concepts are avoided. This is akin to the master carpenter teaching
his appren ce how to use a hammer, saw and plane before teaching how to make a
cabinet.
This book is intended to be read. Each sec on starts with “AS YOU READ” ques ons
that the reader should be able to answer a er a careful reading of the sec on even if
all the concepts of the sec on are not fully understood. I use these ques ons as a daily
reading quiz for my students. The text is wri en in a conversa onal manner, hopefully
resul ng in a text that is easy (and even enjoyable) to read.
Many examples are given to illustrate concepts. When a concept is ﬁrst learned,
I try to demonstrate all the necessary steps so mastery can be obtained. Later, when
this concept is now a tool to study another idea, certain steps are glossed over to focus
on the new material at hand. I would suggest that technology be employed in a similar
fashion.
This text is “open.” If it nearly suits your needs as an instructor, but falls short in
any way, feel free to make changes. I will readily share the source ﬁles (and help you
understand them) and you can do with them as you wish. I would ﬁnd such a process
very rewarding on my own end, and I would enjoy seeing this text become be er and
even eventually grow into a separate linear algebra text. I do ask that the Crea ve
Commons copyright be honored, in that any changes acknowledge this as a source
and that it only be used non commercially.
This is the third edi on of the Fundamentals of Matrix Algebra text. I had not
intended a third edi on, but it proved necessary given the number of errors found in
the second edi on and the other opportuni es found to improve the text. It varies
from the ﬁrst and second edi ons in mostly minor ways. I hope this edi on is “stable;”
I do not want a fourth edi on any me soon.
Finally, I welcome any and all feedback. Please contact me with sugges ons, correc ons, etc.
Sincerely,
Gregory Hartman

v

Contents
Thanks

iii

Preface

v

Table of Contents

vii

1 Systems of Linear Equa ons
1.1 Introduc on to Linear Equa ons . . . . . . . . . . . .
1.2 Using Matrices To Solve Systems of Linear Equa ons .
1.3 Elementary Row Opera ons and Gaussian Elimina on .
1.4 Existence and Uniqueness of Solu ons . . . . . . . . .
1.5 Applica ons of Linear Systems . . . . . . . . . . . . .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

1
1
5
12
22
34

2 Matrix Arithme c
2.1 Matrix Addi on and Scalar Mul plica on
2.2 Matrix Mul plica on . . . . . . . . . . .
2.3 Visualizing Matrix Arithme c in 2D . . . .
2.4 Vector Solu ons to Linear Systems . . . .
2.5 Solving Matrix Equa ons AX = B . . . . .
2.6 The Matrix Inverse . . . . . . . . . . . .
2.7 Proper es of the Matrix Inverse . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

45
. 45
. 51
. 66
. 80
. 97
. 103
. 112

3 Opera ons on Matrices
3.1 The Matrix Transpose . . . . .
3.2 The Matrix Trace . . . . . . .
3.3 The Determinant . . . . . . .
3.4 Proper es of the Determinant
3.5 Cramer’s Rule . . . . . . . . .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

121
121
131
135
146
159

4 Eigenvalues and Eigenvectors
163
4.1 Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . 163
4.2 Proper es of Eigenvalues and Eigenvectors . . . . . . . . . . . . . . 177

Contents
5 Graphical Explora ons of Vectors
187
5.1 Transforma ons of the Cartesian Plane . . . . . . . . . . . . . . . . . 187
5.2 Proper es of Linear Transforma ons . . . . . . . . . . . . . . . . . . 202
5.3 Visualizing Vectors: Vectors in Three Dimensions . . . . . . . . . . . 215

viii

A Solu ons To Selected Problems

227

Index

237

1
S

.

L

E

You have probably encountered systems of linear equa ons before; you can probably remember solving systems of equa ons where you had three equa ons, three
unknowns, and you tried to ﬁnd the value of the unknowns. In this chapter we will
uncover some of the fundamental principles guiding the solu on to such problems.
Solving such systems was a bit me consuming, but not terribly diﬃcult. So why
bother? We bother because linear equa ons have many, many, many applica ons,
from business to engineering to computer graphics to understanding more mathematics. And not only are there many applica ons of systems of linear equa ons, on most
occasions where these systems arise we are using far more than three variables. (Engineering applica ons, for instance, o en require thousands of variables.) So ge ng
a good understanding of how to solve these systems eﬀec vely is important.
But don’t worry; we’ll start at the beginning.

1.1

Introduc on to Linear Equa ons

.

.
AS YOU READ
...
1. What is one of the annoying habits of mathema cians?
2. What is the diﬀerence between constants and coeﬃcients?
3. Can a coeﬃcient in a linear equa on be 0?

We’ll begin this sec on by examining a problem you probably already know how
to solve.
. Example 1
.Suppose a jar contains red, blue and green marbles. You are told
that there are a total of 30 marbles in the jar; there are twice as many red marbles as

Chapter 1 Systems of Linear Equa ons
green ones; the number of blue marbles is the same as the sum of the red and green
marbles. How many marbles of each color are there?
We could a empt to solve this with some trial and error, and we’d
S
probably get the correct answer without too much work. However, this won’t lend
itself towards learning a good technique for solving larger problems, so let’s be more
mathema cal about it.
Let’s let r represent the number of red marbles, and let b and g denote the number
of blue and green marbles, respec vely. We can use the given statements about the
marbles in the jar to create some equa ons.
Since we know there are 30 marbles in the jar, we know that
r + b + g = 30.

(1.1)

Also, we are told that there are twice as many red marbles as green ones, so we know
that
r = 2g.
(1.2)
Finally, we know that the number of blue marbles is the same as the sum of the red
and green marbles, so we have
b = r + g.
(1.3)
From this stage, there isn’t one “right” way of proceeding. Rather, there are many
ways to use this informa on to ﬁnd the solu on. One way is to combine ideas from
equa ons 1.2 and 1.3; in 1.3 replace r with 2g. This gives us
b = 2g + g = 3g.

(1.4)

We can then combine equa ons 1.1, 1.2 and 1.4 by replacing r in 1.1 with 2g as we did
before, and replacing b with 3g to get
r + b + g = 30
2g + 3g + g = 30
6g = 30
g=5

(1.5)

We can now use equa on 1.5 to ﬁnd r and b; we know from 1.2 that r = 2g = 10
and then since r + b + g = 30, we easily ﬁnd that b = 15. .
Mathema cians o en see solu ons to given problems and then ask “What if. . .?”
It’s an annoying habit that we would do well to develop – we should learn to think
like a mathema cian. What are the right kinds of “what if” ques ons to ask? Here’s
another annoying habit of mathema cians: they o en ask “wrong” ques ons. That
is, they o en ask ques ons and ﬁnd that the answer isn’t par cularly interes ng. But
asking enough ques ons o en leads to some good “right” ques ons. So don’t be afraid
of doing something “wrong;” we mathema cians do it all the me.
So what is a good ques on to ask a er seeing Example 1? Here are two possible
ques ons:
2

1.1 Introduc on to Linear Equa ons
1. Did we really have to call the red balls “r”? Could we call them “q”?
2. What if we had 60 balls at the start instead of 30?
Let’s look at the ﬁrst ques on. Would the solu on to our problem change if we
called the red balls q? Of course not. At the end, we’d ﬁnd that q = 10, and we would
know that this meant that we had 10 red balls.
Now let’s look at the second ques on. Suppose we had 60 balls, but the other
rela onships stayed the same. How would the situa on and solu on change? Let’s
compare the “orginal” equa ons to the “new” equa ons.
Original
r + b + g = 30
r = 2g
b=r+g

New
r + b + g = 60
r = 2g
b=r+g

By examining these equa ons, we see that nothing has changed except the ﬁrst
equa on. It isn’t too much of a stretch of the imagina on to see that we would solve
this new problem exactly the same way that we solved the original one, except that
we’d have twice as many of each type of ball.
A conclusion from answering these two ques ons is this: it doesn’t ma er what we
call our variables, and while changing constants in the equa ons changes the solu on,
they don’t really change the method of how we solve these equa ons.
In fact, it is a great discovery to realize that all we care about are the constants and
the coeﬃcients of the equa ons. By systema cally handling these, we can solve any
set of linear equa ons in a very nice way. Before we go on, we must ﬁrst deﬁne what
a linear equa on is.
.
Deﬁni on 1

Linear Equa on
A linear equa on is an equa on that can be wri en in the
form
a1 x1 + a2 x2 + · · · + an xn = c

.

where the xi are variables (the unknowns), the ai are
coeﬃcients, and c is a constant.
A system of linear equa ons is a set of linear equa ons that
involve the same variables.
A solu on to a system of linear equa ons is a set of values
for the variables xi such that each equa on in the system is
sa sﬁed.

So in Example 1, when we answered “how many marbles of each color are there?,”
we were also answering “ﬁnd a solu on to a certain system of linear equa ons.”
3

Chapter 1 Systems of Linear Equa ons
The following are examples of linear equa ons:
2x + 3y − 7z = 29
√
7
x1 + x2 + x3 − x4 + 17x5 = 3 −10
2
y1 + 142 y4 + 4 = y2 + 13 − y1
√
3t
7r + πs +
= cos(45◦ )
5
No √
ce that the coeﬃcients and constants can be frac ons and irra onal numbers
(like π, 3 −10 and cos(45◦ )). The variables only come in the form of ai xi ; that is, just
one variable mul plied by a coeﬃcient. (Note that 3t5 = 35 t, just a variable mul plied
by a coeﬃcient.) Also, it doesn’t really ma er what side of the equa on we put the
variables and the constants, although most of the me we write them with the variables on the le and the constants on the right.
We would not regard the above collec on of equa ons to cons tute a system of
equa ons, since each equa on uses diﬀerently named variables. An example of a system of linear equa ons is
x1 − x2 + x3 + x4 = 1
2x1 + 3x2 + x4 = 25
x2 + x3 = 10
It is important to no ce that not all equa ons used all of the variables (it is more
accurate to say that the coeﬃcients can be 0, so the last equa on could have been
wri en as 0x1 + x2 + x3 + 0x4 = 10). Also, just because we have four unknowns does
not mean we have to have four equa ons. We could have had fewer, even just one,
and we could have had more.
To get a be er feel for what a linear equa on is, we point out some examples of
what are not linear equa ons.
2xy + z = 1
5x2 + 2y5 = 100
1 √
+ y + 24z = 3
x
2
sin x1 + cos2 x2 = 29
2x1 + ln x2 = 13
The ﬁrst example is not a linear equa on since the variables x and y are mul plied together. The second is not a linear equa on because the variables are raised
to powers other√than 1; that is also a problem in the third equa on (remember that
1/x = x−1 and x = x1/2 ). Our variables cannot be the argument of func on like sin,
cos or ln, nor can our variables be raised as an exponent.
4

1.2 Using Matrices To Solve Systems of Linear Equa ons
At this stage, we have yet to discuss how to eﬃciently ﬁnd a solu on to a system
of linear equa ons. That is a goal for the upcoming sec ons. Right now we focus on
iden fying linear equa ons. It is also useful to “limber” up by solving a few systems of
equa ons using any method we have at hand to refresh our memory about the basic
process.

Exercises 1.1
In Exercises 1 – 10, state whether or not the
given equa on is linear.

12.

2x
3x

−
+

3y
6y

=
=

3
8

13.

x
2x
4x

−
+
−

y
6y
5y

+
−
+

z
z
2z

x
2x

+
+

y
y
y

1. x + y + z = 10
2. xy + yz + xz = 1
3. −3x + 9 = 3y − 5z + x − 7
√
4. 5y + πx = −1
5. (x − 1)(x + 1) = 0
√
6.
x21 + x22 = 25
7. x1 + y + t = 1
8.

+ 9 = 3 cos(y) − 5z

1
x

9. cos(15)y +
x

x
4

= −1

In Exercises 11 – 14, solve the system of linear
equa ons.

1.2

−

z

+

2z

=
=
=

1
−4
0
1
2
0

15. A farmer looks out his window at his
chickens and pigs. He tells his daughter that he sees 62 heads and 190 legs.
How many chickens and pigs does the
farmer have?

y

10. 2 + 2 = 16

11.

14.

=
=
=

x
2x

+
−

y
3y

=
=

−1
8

16. A lady buys 20 trinkets at a yard sale.
The cost of each trinket is either $0.30
or $0.65. If she spends $8.80, how
many of each type of trinket does she
buy?

Using Matrices To Solve Systems of Linear Equa ons

.

.
AS YOU READ
...
1. What is remarkable about the deﬁni on of a matrix?
2. Ver cal lines of numbers in a matrix are called what?
3. In a matrix A, the entry a53 refers to which entry?
4. What is an augmented matrix?

In Sec on 1.1 we solved a linear system using familiar techniques. Later, we commented that in the linear equa ons we formed, the most important informa on was
5

Chapter 1 Systems of Linear Equa ons
the coeﬃcients and the constants; the names of the variables really didn’t ma er. In
Example 1 we had the following three equa ons:
r + b + g = 30
r = 2g
b=r+g
Let’s rewrite these equa ons so that all variables are on the le of the equal sign
and all constants are on the right. Also, for a bit more consistency, let’s list the variables
in alphabe cal order in each equa on. Therefore we can write the equa ons as
b

+ g + r = 30
− 2g + r = 0 .
+ g + r = 0

−b

(1.6)

As we men oned before, there isn’t just one “right” way of ﬁnding the solu on to
this system of equa ons. Here is another way to do it, a way that is a bit diﬀerent from
our method in Sec on 1.1.
First, lets add the ﬁrst and last equa ons together, and write the result as a new
third equa on. This gives us:
b

+ g + r = 30
− 2g + r = 0 .
2g + 2r = 30

A nice feature of this is that the only equa on with a b in it is the ﬁrst equa on.
Now let’s mul ply the second equa on by − 21 . This gives
b +

g
g
2g

+
r
= 30
− 1/2r = 0 .
+
2r
= 30

Let’s now do two steps in a row; our goal is to get rid of the g’s in the ﬁrst and third
equa ons. In order to remove the g in the ﬁrst equa on, let’s mul ply the second
equa on by −1 and add that to the ﬁrst equa on, replacing the ﬁrst equa on with
that sum. To remove the g in the third equa on, let’s mul ply the second equa on by
−2 and add that to the third equa on, replacing the third equa on. Our new system
of equa ons now becomes
b

+

3/2r = 30
g − 1/2r = 0 .
3r
= 30

Clearly we can mul ply the third equa on by
this our new third equa on, giving
b

6

+

1
3

and ﬁnd that r = 10; let’s make

3/2r = 30
g − 1/2r = 0 .
r
= 10

1.2 Using Matrices To Solve Systems of Linear Equa ons
Now let’s get rid of the r’s in the ﬁrst and second equa on. To remove the r in the
ﬁrst equa on, let’s mul ply the third equa on by − 32 and add the result to the ﬁrst
equa on, replacing the ﬁrst equa on with that sum. To remove the r in the second
equa on, we can mul ply the third equa on by 12 and add that to the second equa on,
replacing the second equa on with that sum. This gives us:
b
g

= 15
= 5 .
r = 10

Clearly we have discovered the same result as when we solved this problem in Sec on
1.1.
Now again revisit the idea that all that really ma ers are the coeﬃcients and the
constants. There is nothing special about the le ers b, g and r; we could have used x,
y and z or x1 , x2 and x3 . And even then, since we wrote our equa ons so carefully, we
really didn’t need to write the variable names at all as long as we put things “in the
right place.”
Let’s look again at our system of equa ons in (1.6) and write the coeﬃcients and
the constants in a rectangular array. This me we won’t ignore the zeros, but rather
write them out.


1
1 1 30
b + g + r = 30
− 2g + r = 0
⇔  0 −2 1 0 
−1 1 1 0
−b + g + r = 0
No ce how even the equal signs are gone; we don’t need them, for we know that the
last column contains the coeﬃcients.
We have just created a matrix. The deﬁni on of matrix is remarkable only in how
unremarkable it seems.
.
Deﬁni on 2

Matrix
A matrix is a rectangular array of numbers.
The horizontal lines of numbers form rows and the ver cal
lines of numbers form columns. A matrix with m rows and
n columns is said to be an m × n matrix (“an m by n matrix”).

.

The entries of an m × n matrix are indexed as follows:


a11 a12 a13 · · · a1n
 a21 a22 a23 · · · a2n 


 a31 a32 a33 · · · a3n 

.
 ..
..
..
.. 
..
 .
.
.
.
. 
am1

am2

am3

···

amn

That is, a32 means “the number in the third row and second
column.”
7

Chapter 1 Systems of Linear Equa ons

In the future, we’ll want to create matrices with just the coeﬃcients of a system
of linear equa ons and leave out the constants. Therefore, when we include the constants, we o en refer to the resul ng matrix as an augmented matrix.
We can use augmented matrices to ﬁnd solu ons to linear equa ons by using essen ally the same steps we used above. Every me we used the word “equa on”
above, subs tute the word “row,” as we show below. The comments explain how we
get from the current set of equa ons (or matrix) to the one on the next line.
We can use a shorthand to describe matrix opera ons; let R1 , R2 represent “row 1”
and “row 2,” respec vely. We can write “add row 1 to row 3, and replace row 3 with
that sum” as “R1 + R3 → R3 .” The expression “R1 ↔ R2 ” means “interchange row 1
and row 2.”

b

+ g + r = 30
− 2g + r = 0
+ g + r = 0

−b

Replace equa on 3 with the sum
of equa ons 1 and 3

b

+ g + r = 30
− 2g + r = 0
2g + 2r = 30
Mul ply equa on 2 by − 21

b +

g
g
2g

+
r
= 30
+ −1/2r = 0
+
2r
= 30

Replace equa on 1 with the sum
of (−1) mes equa on 2 plus
equa on 1;
Replace equa on 3 with the sum
of (−2) mes equa on 2 plus
equa on 3

b

+

3/2r = 30
g − 1/2r = 0
3r
= 30

Mul ply equa on 3 by

8

1
3




1
1 1 30
 0 −2 1 0 
−1 1 1 0
Replace row 3 with the sum of
rows 1 and 3.
(R1 + R3 → R3 )




1 1 1 30
 0 −2 1 0 
0 2 2 30
Mul ply row 2 by − 12
(− 12 R2 → R2 )



1 1
0 1
0 2


30
0 
30

1
− 12
2

Replace row 1 with the sum of
(−1) mes row 2 plus row 1
(−R2 + R1 → R1 );
Replace row 3 with the sum of
(−2) mes row 2 plus row 3
(−2R2 + R3 → R3 )



1 0
0 1
0 0

3
2

− 12
3


30
0 
30

Mul ply row 3 by
( 13 R3 → R3 )

1
3

1.2 Using Matrices To Solve Systems of Linear Equa ons
b



3/2r = 30
g − 1/2r = 0
r
= 10

+

Replace equa on 2 with the sum
of 12 mes equa on 3 plus
equa on 2;
Replace equa on 1 with the sum
of − 32 mes equa on 3 plus
equa on 1

b
g

= 15
= 5
r = 10

1 0
0 1
0 0

3
2

− 12
1


30
0 
10

Replace row 2 with the sum of 12
mes row 3 plus row 2
( 12 R3 + R2 → R2 );
Replace row 1 with the sum of
− 32 mes row 3 plus row 1
(− 32 R3 + R1 → R1 )



1 0 0
0 1 0
0 0 1


15
5 
10

The ﬁnal matrix contains the same solu on informa on as we have on the le in
the form of equa ons. Recall that the ﬁrst column of our matrices held the coeﬃcients
of the b variable; the second and third columns held the coeﬃcients of the g and r
variables, respec vely. Therefore, the ﬁrst row of the matrix can be interpreted as
“b + 0g + 0r = 15,” or more concisely, “b = 15.”
Let’s prac ce this manipula on again.
. Example 2
.Find a solu on to the following system of linear equa ons by simultaneously manipula ng the equa ons and the corresponding augmented matrices.
x1
+ x2 + x3 = 0
2x1 + 2x2 + x3 = 0
−1x1 + x2 − 2x3 = 2
We’ll ﬁrst convert this system of equa ons into a matrix, then we’ll
S
proceed by manipula ng the system of equa ons (and hence the matrix) to ﬁnd a
solu on. Again, there is not just one “right” way of proceeding; we’ll choose a method
that is pre y eﬃcient, but other methods certainly exist (and may be “be er”!). The
method use here, though, is a good one, and it is the method that we will be learning
in the future.
The given system and its corresponding augmented matrix are seen below.
Original system of equa ons

x1
2x1
−1x1

+
+
+

x2
2x2
x2

+ x3
+ x3
− 2x3

= 0
= 0
= 2

Corresponding matrix




1 1 1 0
 2 2 1 0
−1 1 −2 2

We’ll proceed by trying to get the x1 out of the second and third equa on.
9

Chapter 1 Systems of Linear Equa ons
Replace equa on 2 with the sum
of (−2) mes equa on 1 plus
equa on 2;
Replace equa on 3 with the sum
of equa on 1 and equa on 3

x1

+

x2
2x2

x3
−x3
− x3

= 0
= 0
= 2

+

Replace row 2 with the sum of
(−2) mes row 1 plus row 2
(−2R1 + R2 → R2 );
Replace row 3 with the sum of
row 1 and row 3
(R1 + R3 → R3 )



1 1
0 0
0 2


1 0
−1 0 
−1 2

.
No ce that the second equa on no longer contains x2 . We’ll exchange the order of
the equa ons so that we can follow the conven on of solving for the second variable
in the second equa on.
Interchange equa ons 2 and 3

x1

+

x2
2x2

+
−

x3
x3
−x3

= 0
= 2
= 0

Mul ply equa on 2 by

x1

+ x2
x2

+
−

x3
1
2 x3
−x3

1
2

= 0
= 1
= 0

Mul ply equa on 3 by −1

x1

+ x2
x2

+
−

x3
1
2 x3
x3

= 0
= 1
= 0

Interchange rows 2 and 3
R2 ↔ R3



1
0
0

1
2
0


1 0
−1 2 
−1 0

Mul ply row 2 by
( 21 R2 → R2 )



1
0
0

1
1
0

1
− 12
−1

1
2


0
1
0

Mul ply row 3 by −1
(−1R3 → R3 )



1
0
0

1
1
0

1
− 12
1


0
1
0

No ce that the last equa on (and also the last row of the matrix) show that x3 = 0.
Knowing this would allow us to simply eliminate the x3 from the ﬁrst two equa ons.
However, we will formally do this by manipula ng the equa ons (and rows) as we have
previously.
Replace equa on 1 with the sum
of (−1) mes equa on 3 plus
equa on 1;
Replace equa on 2 with the sum
of 21 mes equa on 3 plus
equa on 2

x1
10

+ x2
x2
x3

= 0
= 1
= 0

Replace row 1 with the sum of
(−1) mes row 3 plus row 1
(−R3 + R1 → R1 );
Replace row 2 with the sum of 12
mes row 3 plus row 2
( 12 R3 + R2 → R2 )



1 1
0 1
0 0

0
0
1


0
1
0

1.2 Using Matrices To Solve Systems of Linear Equa ons
No ce how the second equa on shows that x2 = 1. All that remains to do is to
solve for x1 .
Replace row 1 with the sum of
(−1) mes row 2 plus row 1
(−R2 + R1 → R1 )

Replace equa on 1 with the sum
of (−1) mes equa on 2 plus
equa on 1



= −1
= 1
= 0

x1
x2
x3


0 −1
0 1 
1 0

1 0
0 1
0 0

Obviously the equa ons on the le tell us that x1 = −1, x2 = 1 and x3 = 0, and
no ce how the matrix on the right tells us the same informa on. .

Exercises 1.2
In Exercises 1 – 4, convert the given system of
linear equa ons into an augmented matrix.
1.

3x
−x
2x

2.

2x
9x
−2x

+
+
−

4y
y
2y

+

5y

+

4y

+
−
+
−
−
+

5z
3z
3z
6z
8z
z

3.

x1 + 3x2 − 4x3 + 5x4 =
−x1 + 4x3 + 8x4 =
2x1 + 3x2 + 4x3 + 5x4 =

4.

3x1 − 2x2
2x1
−x1 + 9x2
5x1 − 7x2

=
=
=
=

7
1
5

=
=
=
=
=
=

2
10
−7

17
1
6

4
3
8
13

In Exercises 5 – 9, convert the given augmented matrix into a system of linear equaons. Use the variables x1 , x2 , etc.
]
[
1
2 3
5.
−1 3 9
[
]
−3 4
7
6.
0
1 −2
]
[
1 1 −1 −1 2
7.
2 1
3
5
7


1 0 0 0
2
 0 1 0 0 −1 

8. 
0 0 1 0
5 
0 0 0 1
3

[
9.

1
0

0
1

1
3

0
2

7
0

2
5

]

In Exercises 10 – 15, perform the given row
opera ons on A, where


2 −1
7
4
−2  .
A=0
5
0
3
10. −1R1 → R1
11. R2 ↔ R3
12. R1 + R2 → R2
13. 2R2 + R3 → R3
14.
15.

1
R → R2
2 2
− 52 R1 + R3

→ R3

A matrix A is given below. In Exercises 16 –
20, a matrix B is given. Give the row operaon that transforms A into B.


1 1 1
A=1 0 1
1 2 3


1
16. B =  2
1

1
17. B =  2
1

3
18. B =  1
1

1
0
2
1
1
2
5
0
2


1
2
3

1
2
3

7
1
3

11

Chapter 1 Systems of Linear Equa ons


1
19. B =  1
1

1
20. B =  1
0

0
1
2
1
0
2


1
1
3

1
1
2

In Exercises 21 – 26, rewrite the system of
equa ons in matrix form. Find the solu on to
the linear system by simultaneously manipula ng the equa ons and the matrix.
21.

x
2x

+
−

y
3y

=
=

23.

−2x
−x

+
+

3y
y

=
=

2
1

24.

2x
−2x

+
+

3y
6y

=
=

2
1

−5x1
25.
−3x1

3
1
26.

22.

2x
−x

+
+

4y
y

=
=

x1

10
4

+

2x3
x3

=
=
=

14
1
8

+
+
+
+

2x3
2x3
x3

=
=
=

−11
15
−8

x2

−

5x2

−

3x2

1.3 Elementary Row Opera ons and Gaussian Eliminaon

.

.
AS YOU READ
...
1. Give two reasons why the Elementary Row Opera ons are called “Elementary.”
2. T/F: Assuming a solu on exists, all linear systems of equa ons can be solved
using only elementary row opera ons.
3. Give one reason why one might not be interested in pu ng a matrix into reduced row echelon form.
4. Iden fy the leading 1s in the following matrix:


1 0 0 1
0 1 1 0


0 0 1 1
0 0 0 0
5. Using the “forward” and “backward” steps of Gaussian elimina on creates lots
of
making computa ons easier.

In our examples thus far, we have essen ally used just three types of manipula ons
in order to ﬁnd solu ons to our systems of equa ons. These three manipula ons are:
1. Add a scalar mul ple of one equa on to a second equa on, and replace the
second equa on with that sum
12

1.3 Elementary Row Opera ons and Gaussian Elimina on
2. Mul ply one equa on by a nonzero scalar
3. Swap the posi on of two equa ons in our list
We saw earlier how we could write all the informa on of a system of equa ons in
a matrix, so it makes sense that we can perform similar opera ons on matrices (as we
have done before). Again, simply replace the word “equa on” above with the word
“row.”
We didn’t jus fy our ability to manipulate our equa ons in the above three ways;
it seems rather obvious that we should be able to do that. In that sense, these opera ons are “elementary.” These opera ons are elementary in another sense; they are
fundamental – they form the basis for much of what we will do in matrix algebra. Since
these opera ons are so important, we list them again here in the context of matrices.
.
Key Idea 1

Elementary Row Opera ons
1. Add a scalar mul ple of one row to another row, and
replace the la er row with that sum

.

2. Mul ply one row by a nonzero scalar
3. Swap the posi on of two rows

Given any system of linear equa ons, we can ﬁnd a solu on (if one exists) by using
these three row opera ons. Elementary row opera ons give us a new linear system,
but the solu on to the new system is the same as the old. We can use these operaons as much as we want and not change the solu on. This brings to mind two good
ques ons:
1. Since we can use these opera ons as much as we want, how do we know when
to stop? (Where are we supposed to “go” with these opera ons?)
2. Is there an eﬃcient way of using these opera ons? (How do we get “there” the
fastest?)
We’ll answer the ﬁrst ques on ﬁrst. Most of the me1 we will want to take our
original matrix and, using the elementary row opera ons, put it into something called
reduced row echelon form.2 This is our “des na on,” for this form allows us to readily
iden fy whether or not a solu on exists, and in the case that it does, what that solu on
is.
In the previous sec on, when we manipulated matrices to ﬁnd solu ons, we were
unwi ngly pu ng the matrix into reduced row echelon form. However, not all soluons come in such a simple manner as we’ve seen so far. Pu ng a matrix into reduced
1 unless one
2 Some texts

prefers obfusca on to clariﬁca on
use the term reduced echelon form instead.

13

Chapter 1 Systems of Linear Equa ons
row echelon form helps us iden fy all types of solu ons. We’ll explore the topic of understanding what the reduced row echelon form of a matrix tells us in the following
sec ons; in this sec on we focus on ﬁnding it.
.
Deﬁni on 3

Reduced Row Echelon Form
A matrix is in reduced row echelon form if its entries sa sfy
the following condi ons.
1. The ﬁrst nonzero entry in each row is a 1 (called a leading 1).

.

2. Each leading 1 comes in a column to the right of the
leading 1s in rows above it.
3. All rows of all 0s come at the bo om of the matrix.
4. If a column contains a leading 1, then all other entries
in that column are 0.
A matrix that sa sﬁes the ﬁrst three condi ons is said to be
in row echelon form.

. Example 3

.Which of the following matrices is in reduced row echelon form?
]
[
]
1 0
1 0 1
a)
b)
0 1
0 1 2
[

[

0 0
c)
0 0


]

[

1
d)
0

0
0
0

0
0
1


1
0
3

0 1
g)  0 0
0 0

2
0
0

3
0
0

1
e)  0
0


1
0

0
1

1 2
f)  0 0
0 0

0
3
0



0
1
0


4
5
0



1 1
h)  0 1
0 0

]

0
0
4


0
0
1

The matrices in a), b), c), d) and g) are all in reduced row echelon
S
form. Check to see that each sa sﬁes the necessary condi ons. If your ins ncts were
wrong on some of these, correct your thinking accordingly.
The matrix in e) is not in reduced row echelon form since the row of all zeros is
not at the bo om. The matrix in f) is not in reduced row echelon form since the ﬁrst
14

1.3 Elementary Row Opera ons and Gaussian Elimina on
nonzero entries in rows 2 and 3 are not 1. Finally, the matrix in h) is not in reduced
row echelon form since the ﬁrst entry in column 2 is not zero; the second 1 in column
2 is a leading one, hence all other entries in that column should be 0.
We end this example with a preview of what we’ll learn in the future. Consider
the matrix in b). If this matrix came from the augmented matrix of a system of linear
equa ons, then we can readily recognize that the solu on of the system is x1 = 1 and
x2 = 2. Again, in previous examples, when we found the solu on to a linear system,
we were unwi ngly pu ng our matrices into reduced row echelon form. .
We began this sec on discussing how we can manipulate the entries in a matrix
with elementary row opera ons. This led to two ques ons, “Where do we go?” and
“How do we get there quickly?” We’ve just answered the ﬁrst ques on: most of the
me we are “going to” reduced row echelon form. We now address the second queson.
There is no one “right” way of using these opera ons to transform a matrix into
reduced row echelon form. However, there is a general technique that works very well
in that it is very eﬃcient (so we don’t waste me on unnecessary steps). This technique
is called Gaussian elimina on. It is named in honor of the great mathema cian Karl
Friedrich Gauss.
While this technique isn’t very diﬃcult to use, it is one of those things that is easier understood by watching it being used than explained as a series of steps. With this
in mind, we will go through one more example highligh ng important steps and then
we’ll explain the procedure in detail.
. Example 4
.Put the augmented matrix of the following system of linear equaons into reduced row echelon form.
−3x1
2x1

S

−
+

3x2
2x2
−2x2

+ 9x3
− 4x3
− 4x3

= 12
= −2
= −8

We start by conver ng the linear system into an augmented ma-

trix.


−3
 2
0


−3 9
12
2 −4 −2 
−2 −4 −8

Our next step is to change the entry in the box to a 1. To do this, let’s mul ply row
1 by − 13 .

− 13 R1 → R1


1 1 −3 −4
 2 2 −4 −2 
0 −2 −4 −8
15

Chapter 1 Systems of Linear Equa ons
We have now created a leading 1; that is, the ﬁrst entry in the ﬁrst row is a 1.
Our next step is to put zeros under this 1. To do this, we’ll use the elementary row
opera on given below.


1 1 −3 −4
0 0
−2R1 + R2 → R2
2
6 
0 −2 −4 −8
Once this is accomplished, we shi our focus from the leading one down one row,
and to the right one column, to the posi on that is boxed. We again want to put a 1
in this posi on. We can use any elementary row opera ons, but we need to restrict
ourselves to using only the second row and any rows below it. Probably the simplest
thing we can do is interchange rows 2 and 3, and then scale the new second row so
that there is a 1 in the desired posi on.


1
1
−3 −4
 0 −2 −4 −8 
R2 ↔ R3
0

− 12 R2 → R2

1 1
0 1
0 0

0

2

6


−3 −4
2
4 
6
2

We have now created another leading 1, this me in the second row. Our next
desire is to put zeros underneath it, but this has already been accomplished by our
previous steps. Therefore we again shi our a en on to the right one column and
down one row, to the next posi on put in the box. We want that to be a 1. A simple
scaling will accomplish this.


1 1 −3 −4
1
0 1 2
R → R3
4 
2 3
0 0 1
3
This ends what we will refer to as the forward steps. Our next task is to use the
elementary row opera ons and go back and put zeros above our leading 1s. This is
referred to as the backward steps. These steps are given below.


1 1 0 5
3R3 + R1 → R1
 0 1 0 −2 
−2R3 + R2 → R2
0 0 1 3

−R2 + R1 → R1

1
0
0


0 0 7
1 0 −2 
0 1 3

It is now easy to read oﬀ the solu on as x1 = 7, x2 = −2 and x3 = 3. .

16

1.3 Elementary Row Opera ons and Gaussian Elimina on
We now formally explain the procedure used to ﬁnd the solu on above. As you
read through the procedure, follow along with the example above so that the explana on makes more sense.
Forward Steps
1. Working from le to right, consider the ﬁrst column that isn’t all zeros that hasn’t
already been worked on. Then working from top to bo om, consider the ﬁrst
row that hasn’t been worked on.
2. If the entry in the row and column that we are considering is zero, interchange
rows with a row below the current row so that that entry is nonzero. If all entries
below are zero, we are done with this column; start again at step 1.
3. Mul ply the current row by a scalar to make its ﬁrst entry a 1 (a leading 1).
4. Repeatedly use Elementary Row Opera on 1 to put zeros underneath the leading one.
5. Go back to step 1 and work on the new rows and columns un l either all rows
or columns have been worked on.
If the above steps have been followed properly, then the following should be true
about the current state of the matrix:
1. The ﬁrst nonzero entry in each row is a 1 (a leading 1).
2. Each leading 1 is in a column to the right of the leading 1s above it.
3. All rows of all zeros come at the bo om of the matrix.
Note that this means we have just put a matrix into row echelon form. The next
steps ﬁnish the conversion into reduced row echelon form. These next steps are referred to as the backward steps. These are much easier to state.
Backward Steps
1. Star ng from the right and working le , use Elementary Row Opera on 1 repeatedly to put zeros above each leading 1.
The basic method of Gaussian elimina on is this: create leading ones and then use
elementary row opera ons to put zeros above and below these leading ones. We can
do this in any order we please, but by following the “Forward Steps” and “Backward
Steps,” we make use of the presence of zeros to make the overall computa ons easier.
This method is very eﬃcient, so it gets its own name (which we’ve already been using).
17

Chapter 1 Systems of Linear Equa ons

.
Deﬁni on 4

Gaussian Elimina on
Gaussian elimina on is the technique for ﬁnding the reduced row echelon form of a matrix using the above procedure. It can be abbreviated to:
1. Create a leading 1.

.

2. Use this leading 1 to put zeros underneath it.
3. Repeat the above steps un l all possible rows have
leading 1s.
4. Put zeros above these leading 1s.

Let’s prac ce some more.
. Example 5
.Use Gaussian elimina on to put the matrix A into reduced row echelon form, where


−2 −4 −2 −10 0
4
1
9
−2  .
A= 2
3
6
1
13 −4
We start by wan ng to make the entry in the ﬁrst column and ﬁrst
S
row a 1 (a leading 1). To do this we’ll scale the ﬁrst row by a factor of − 12 .


1 2 1 5
0
 2 4 1 9 −2 
− 21 R1 → R1
3 6 1 13 −4
Next we need to put zeros in the column below this newly formed leading 1.


1 2
1
5
0
−2R1 + R2 → R2
 0 0 −1 −1 −2 
−3R1 + R3 → R3
0 0 −2 −2 −4
Our a en on now shi s to the right one column and down one row to the posi on
indicated by the box. We want to put a 1 in that posi on. Our only op ons are to either
scale the current row or to interchange rows with a row below it. However, in this case
neither of these op ons will accomplish our goal. Therefore, we shi our a en on to
the right one more column.
We want to put a 1 where there is a –1. A simple scaling will accomplish this; once
done, we will put a 0 underneath this leading one.


1 2 1
5
0
0 0 1
−R2 → R2
1
2 
0 0 −2 −2 −4
18

1.3 Elementary Row Opera ons and Gaussian Elimina on


1 2
0 0
0 0

2R2 + R3 → R3

1
1
0

5
1
0


0
2
0

Our a en on now shi s over one more column and down one row to the posi on
indicated by the box; we wish to make this a 1. Of course, there is no way to do this,
so we are done with the forward steps.
Our next goal is to put a 0 above each of the leading 1s (in this case there is only
one leading 1 to deal with).


1 2 0 4 −2
0 0 1 1 2 
−R2 + R1 → R1
0 0 0 0 0
This ﬁnal matrix is in reduced row echelon form. .
. Example 6

Put the matrix



1
2
3

2
1
3

1
1
2


3
1
1

into reduced row echelon form.
S

Here we will show all steps without explaining each one.


1 2
1
3
−2R1 + R2 → R2
 0 −3 −1 −5 
−3R1 + R3 → R3
0 −3 −1 −8


1 2
1
3
 0 1 1/3 5/3 
− 13 R2 → R2
0 −3 −1 −8


1 2
1
3
 0 1 1/3 5/3 
3R2 + R3 → R3
0 0
0
−3


1 2
1
3
 0 1 1/3 5/3 
− 13 R3 → R3
0 0
0
1


1 2
1
0
−3R3 + R1 → R1
 0 1 1/3 0 
− 53 R3 + R2 → R2
0 0
0
1


1 0 1/3 0
 0 1 1/3 0 
−2R2 + R1 → R1
0 0
0
1

.

19

Chapter 1 Systems of Linear Equa ons
The last matrix in the above example is in reduced row echelon form. If one thinks
of the original matrix as represen ng the augmented matrix of a system of linear equaons, this ﬁnal result is interes ng. What does it mean to have a leading one in the
last column? We’ll ﬁgure this out in the next sec on.
. Example 7

.Put the matrix A into reduced row echelon form, where


2 1 −1 4
A =  1 −1 2 12  .
2 2 −1 9

We’ll again show the steps without explana on, although we will
S
stop at the end of the forward steps and make a comment.


1 1/2 −1/2 2
1
 1 −1
R → R1
2
12 
2 1
2
2
−1
9


1 1/2 −1/2 2
−R1 + R2 → R2
 0 −3/2 5/2 10 
−2R1 + R3 → R3
0
1
0
5


1 1/2 −1/2
2
0
− 23 R2 → R2
1
−5/3 −20/3 
0
1
0
5


1 1/2 −1/2
2
0
−R2 + R3 → R3
1
−5/3 −20/3 
0
0
5/3
35/3


1 1/2 −1/2
2
3
0
R → R3
1
−5/3 −20/3 
5 3
0
0
1
7
Let’s take a break here and think about the state of our linear system at this moment. Conver ng back to linear equa ons, we now know
x1 + 1/2x2 − 1/2x3
x2 − 5/3x3
x3

= 2
= −20/3 .
= 7

Since we know that x3 = 7, the second equa on turns into
x2 − (5/3)(7) = −20/3,
telling us that x2 = 5.
Finally, knowing values for x2 and x3 lets us subs tute in the ﬁrst equa on and ﬁnd
x1 + (1/2)(5) − (1/2)(7) = 2,
20

1.3 Elementary Row Opera ons and Gaussian Elimina on
so x1 = 3.
This process of subs tu ng known values back into other equa ons is called back
subs tu on. This process is essen ally what happens when we perform the backward
steps of Gaussian elimina on. We make note of this below as we ﬁnish out ﬁnding the
reduced row echelon form of our matrix.


5
1 1/2 −1/2 2
R + R2 → R2
3 3
0
1
0
5
(knowing x3 = 7 allows us
to ﬁnd x2 = 5)
0
0
1
7


1
R + R1 → R1
2 3
− 12 R2 + R1 → R1
(knowing x2 = 5 and x3

1 0
0 1
0 0

=7
allows us to ﬁnd x1 = 3)

0
0
1


3
5
7

We did our opera ons slightly “out of order” in that we didn’t put the zeros above
our leading 1 in the third column in the same step, highligh ng how back subs tu on
works. .
In all of our prac ce, we’ve only encountered systems of linear equa ons with exactly one solu on. Is this always going to be the case? Could we ever have systems
with more than one solu on? If so, how many solu ons could there be? Could we
have systems without a solu on? These are some of the ques ons we’ll address in
the next sec on.

Exercises 1.3



1
(c)  0
0

1
(d)  0
0

In Exercises 1 – 4, state whether or not the
given matrices are in reduced row echelon
form. If it is not, state why.
[
1.

0
1

0
1

1
0

1
0

0
0

0
1

1
0

0
1

1
1

1
(a)  0
0

1
(b)  0
0

1
1
0

[
[
(a)
[
(b)


3.

[

1
0

(a)
(b)

2.

]

0
1
0

(c)
]

[
(d)
]

[
(c)

]


1
1
1

0
0
0

[
(d)

]

1
1

1
1

1
0

0
1

1
2

0
1

0
0

0
0

0
0

0
0

0
0

]
]


4.

(a)

]
(b)

(c)

(d)

2
0
0

0
0
0

0
0
0

1
0
0

0
0
0


0
1
0

0
1
0

0
0
1

0
2
0

0
0
2

1
0
0

0
1
0


2
2
2

0
0
0

0
0
0

1
0
0


−5
0 
0

1
0
0

0
1
0

0
0
1


−5
7 
3

1
1
0


1
1
0

21

Chapter 1 Systems of Linear Equa ons
In Exercises 5 – 22, use Gaussian Elimina on
to put the given matrix into reduced row echelon form.
[
]
1
2
5.
−3 −5
[
]
2 −2
6.
3 −2
]
[
4
12
7.
−2 −6
[
]
−5 7
8.
10 14
]
[
−1 1 4
9.
−2 1 1
[
]
7 2 3
10.
3 1 2
[
]
3
−3
6
11.
−1
1
−2
[
]
4
5
−6
12.
−12 −15 18


−2 −4 −8
13.  −2 −3 −5 
2
3
6


2 1 1
14.  1 1 1 
2 1 2



1
15.  1
−1


1
16.  0
1
1
17.  2
−1

1
1
2

−1
0
−1


7
10 
17

1
1
1

8
2
5


15
7 
11

2
1

2
1

1
1

3
3

1
2

−1
−2

1
19.  2
3


4
20.  1
3

[
22.


2
1
0

1
−1
1


5
−1 
0



[

1
−1
1
1
6
5

2
18.  3
3

21.


3
5
9

2
4
6






1
1
0

2
3
−3

−1
1
0

3
6

1
1
1
1

4
4
−2
−2

]

9
13

]

1.4 Existence and Uniqueness of Solu ons

.

.
AS YOU READ
...
1. T/F: It is possible for a linear system to have exactly 5 solu ons.
2. T/F: A variable that corresponds to a leading 1 is “free.”
3. How can one tell what kind of solu on a linear system of equa ons has?
4. Give an example (diﬀerent from those given in the text) of a 2 equa on, 2 unknown linear system that is not consistent.
5. T/F: A par cular solu on for a linear system with inﬁnite solu ons can be found
by arbitrarily picking values for the free variables.
22

1.4 Existence and Uniqueness of Solu ons
So far, whenever we have solved a system of linear equa ons, we have always
found exactly one solu on. This is not always the case; we will ﬁnd in this sec on that
some systems do not have a solu on, and others have more than one.
We start with a very simple example. Consider the following linear system:
x − y = 0.
There are obviously inﬁnite solu ons to this system; as long as x = y, we have a solu on. We can picture all of these solu ons by thinking of the graph of the equa on
y = x on the tradi onal x, y coordinate plane.
Let’s con nue this visual aspect of considering solu ons to linear systems. Consider the system
x+y=2
x − y = 0.
Each of these equa ons can be viewed as lines in the coordinate plane, and since their
slopes are diﬀerent, we know they will intersect somewhere (see Figure 1.1 (a)). In
this example, they intersect at the point (1, 1) – that is, when x = 1 and y = 1, both
equa ons are sa sﬁed and we have a solu on to our linear system. Since this is the
only place the two lines intersect, this is the only solu on.
Now consider the linear system
x+y=1
2x + 2y = 2.
It is clear that while we have two equa ons, they are essen ally the same equa on;
the second is just a mul ple of the ﬁrst. Therefore, when we graph the two equa ons,
we are graphing the same line twice (see Figure 1.1 (b); the thicker line is used to
represent drawing the line twice). In this case, we have an inﬁnite solu on set, just as
if we only had the one equa on x + y = 1. We o en write the solu on as x = 1 − y to
demonstrate that y can be any real number, and x is determined once we pick a value
for y.

.
(a)

(b)

(c)

. linear equa ons with two unknowns.
Figure 1.1: The three possibili es for two

23

Chapter 1 Systems of Linear Equa ons
Finally, consider the linear system
x+y=1
x + y = 2.
We should immediately spot a problem with this system; if the sum of x and y is 1,
how can it also be 2? There is no solu on to such a problem; this linear system has no
solu on. We can visualize this situa on in Figure 1.1 (c); the two lines are parallel and
never intersect.
If we were to consider a linear system with three equa ons and two unknowns, we
could visualize the solu on by graphing the corresponding three lines. We can picture
that perhaps all three lines would meet at one point, giving exactly 1 solu on; perhaps all three equa ons describe the same line, giving an inﬁnite number of solu ons;
perhaps we have diﬀerent lines, but they do not all meet at the same point, giving
no solu on. We further visualize similar situa ons with, say, 20 equa ons with two
variables.
While it becomes harder to visualize when we add variables, no ma er how many
equa ons and variables we have, solu ons to linear equa ons always come in one of
three forms: exactly one solu on, inﬁnite solu ons, or no solu on. This is a fact that
we will not prove here, but it deserves to be stated.
.
Theorem 1

Solu on Forms of Linear Systems

.

Every linear system of equa ons has exactly one solu on,
inﬁnite solu ons, or no solu on.

This leads us to a deﬁni on. Here we don’t diﬀeren ate between having one solu on and inﬁnite solu ons, but rather just whether or not a solu on exists.
.
Deﬁni on 5

Consistent and Inconsistent Linear Systems

.

A system of linear equa ons is consistent if it has a solu on
(perhaps more than one). A linear system is inconsistent if
it does not have a solu on.

How can we tell what kind of solu on (if one exists) a given system of linear equaons has? The answer to this ques on lies with properly understanding the reduced
row echelon form of a matrix. To discover what the solu on is to a linear system, we
ﬁrst put the matrix into reduced row echelon form and then interpret that form properly.
Before we start with a simple example, let us make a note about ﬁnding the re24

1.4 Existence and Uniqueness of Solu ons
duced row echelon form of a matrix.
Technology Note: In the previous sec on, we learned how to ﬁnd the reduced row
echelon form of a matrix using Gaussian elimina on – by hand. We need to know how
to do this; understanding the process has beneﬁts. However, actually execu ng the
process by hand for every problem is not usually beneﬁcial. In fact, with large systems,
compu ng the reduced row echelon form by hand is eﬀec vely impossible. Our main
concern is what “the rref” is, not what exact steps were used to arrive there. Therefore,
the reader is encouraged to employ some form of technology to ﬁnd the reduced row
echelon form. Computer programs such as Mathema ca, MATLAB, Maple, and Derive
can be used; many handheld calculators (such as Texas Instruments calculators) will
perform these calcula ons very quickly.
As a general rule, when we are learning a new technique, it is best to not use
technology to aid us. This helps us learn not only the technique but some of its “inner
workings.” We can then use technology once we have mastered the technique and are
now learning how to use it to solve problems.
From here on out, in our examples, when we need the reduced row echelon form
of a matrix, we will not show the steps involved. Rather, we will give the ini al matrix,
then immediately give the reduced row echelon form of the matrix. We trust that the
reader can verify the accuracy of this form by both performing the necessary steps by
hand or u lizing some technology to do it for them.
Our ﬁrst example explores oﬃcially a quick example used in the introduc on of
this sec on.
. Example 8

.Find the solu on to the linear system
x1
2x1

+ x2
+ 2x2

= 1
.
= 2

Create the corresponding augmented matrix, and then put the maS
trix into reduced row echelon form.
[
]
[
]
−→
1 1 1
1 1 1
rref
2 2 2
0 0 0
Now convert the reduced matrix back into equa ons. In this case, we only have
one equa on,
x1 + x2 = 1
or, equivalently,
x1 = 1 − x2
x2 is free.
We have just introduced a new term, the word free. It is used to stress that idea
that x2 can take on any value; we are “free” to choose any value for x2 . Once this value
25

Chapter 1 Systems of Linear Equa ons
is chosen, the value of x1 is determined. We have inﬁnite choices for the value of x2 ,
so therefore we have inﬁnite solu ons.
For example, if we set x2 = 0, then x1 = 1; if we set x2 = 5, then x1 = −4. .
Let’s try another example, one that uses more variables.
. Example 9

Find the solu on to the linear system
x2
x1

To ﬁnd the
S
row echelon form.

0 1 −1
1 0
2
0 −3 3

−3x2

− x3
+ 2x3
+ 3x3

= 3
= 2 .
= −9

solu on, put the corresponding matrix into reduced

3
2 
−9


−→
rref

1 0
0 1
0 0


2 2
−1 3 
0 0

Now convert this reduced matrix back into equa ons. We have
x1 + 2x3 = 2
x2 − x3 = 3
or, equivalently,
x1 = 2 − 2x3
x2 = 3 + x3
x3 is free.
These two equa ons tell us that the values of x1 and x2 depend on what x3 is. As
we saw before, there is no restric on on what x3 must be; it is “free” to take on the
value of any real number. Once x3 is chosen, we have a solu on. Since we have inﬁnite
choices for the value of x3 , we have inﬁnite solu ons.
As examples, x1 = 2, x2 = 3, x3 = 0 is one solu on; x1 = −2, x2 = 5, x3 = 2 is
another solu on. Try plugging these values back into the original equa ons to verify
that these indeed are solu ons. (By the way, since inﬁnite solu ons exist, this system
of equa ons is consistent.) .
In the two previous examples we have used the word “free” to describe certain
variables. What exactly is a free variable? How do we recognize which variables are
free and which are not?
Look back to the reduced matrix in Example 8. No ce that there is only one leading
1 in that matrix, and that leading 1 corresponded to the x1 variable. That told us that
x1 was not a free variable; since x2 did not correspond to a leading 1, it was a free
variable.
26

1.4 Existence and Uniqueness of Solu ons
Look also at the reduced matrix in Example 9. There were two leading 1s in that
matrix; one corresponded to x1 and the other to x2 . This meant that x1 and x2 were
not free variables; since there was not a leading 1 that corresponded to x3 , it was a
free variable.
We formally deﬁne this and a few other terms in this following deﬁni on.
.
Deﬁni on 6

Dependent and Independent Variables
Consider the reduced row echelon form of an augmented
matrix of a linear system of equa ons. Then:

.

a variable that corresponds to a leading 1 is a basic, or
dependent, variable, and
a variable that does not correspond to a leading 1 is a free,
or independent, variable.

One can probably see that “free” and “independent” are rela vely synonymous. It
follows that if a variable is not independent, it must be dependent; the word “basic”
comes from connec ons to other areas of mathema cs that we won’t explore here.
These deﬁni ons help us understand when a consistent system of linear equa ons
will have inﬁnite solu ons. If there are no free variables, then there is exactly one
solu on; if there are any free variables, there are inﬁnite solu ons.
.
Key Idea 2

Consistent Solu on Types
A consistent linear system of equa ons will have exactly
one solu on if and only if there is a leading 1 for each
variable in the system.

.

If a consistent linear system of equa ons has a free variable,
it has inﬁnite solu ons.
If a consistent linear system has more variables than leading
1s, then the system will have inﬁnite solu ons.
A consistent linear system with more variables than equaons will always have inﬁnite solu ons.

Note: Key Idea 2 applies only to consistent systems. If a system is inconsistent,
27

Chapter 1 Systems of Linear Equa ons
then no solu on exists and talking about free and basic variables is meaningless.
When a consistent system has only one solu on, each equa on that comes from
the reduced row echelon form of the corresponding augmented matrix will contain
exactly one variable. If the consistent system has inﬁnite solu ons, then there will be
at least one equa on coming from the reduced row echelon form that contains more
than one variable. The “ﬁrst” variable will be the basic (or dependent) variable; all
others will be free variables.
We have now seen examples of consistent systems with exactly one solu on and
others with inﬁnite solu ons. How will we recognize that a system is inconsistent?
Let’s ﬁnd out through an example.
. Example 10

Find the solu on to the linear system
x1
x1
2x1

S

+ x2
+ 2x2
+ 3x2

+ x3
+ x3
+ 2x3

= 1
= 2 .
= 0

We start by pu ng the corresponding matrix into reduced row

echelon form.


1 1
1 2
2 3

1
1
2


1
2
0


−→
rref

1 0 1
0 1 0
0 0 0


0
0
1

Now let us take the reduced matrix and write out the corresponding equa ons.
The ﬁrst two rows give us the equa ons
x1 + x3 = 0
x2 = 0.

So far, so good. However the last row gives us the equa on
0x1 + 0x2 + 0x3 = 1
or, more concisely, 0 = 1. Obviously, this is not true; we have reached a contradic on.
Therefore, no solu on exists; this system is inconsistent. .
In previous sec ons we have only encountered linear systems with unique soluons (exactly one solu on). Now we have seen three more examples with diﬀerent
solu on types. The ﬁrst two examples in this sec on had inﬁnite solu ons, and the
third had no solu on. How can we tell if a system is inconsistent?
A linear system will be inconsistent only when it implies that 0 equals 1. We can
tell if a linear system implies this by pu ng its corresponding augmented matrix into
reduced row echelon form. If we have any row where all entries are 0 except for the
entry in the last column, then the system implies 0=1. More succinctly, if we have a
28

1.4 Existence and Uniqueness of Solu ons
leading 1 in the last column of an augmented matrix, then the linear system has no
solu on.
.
Key Idea 3

Inconsistent Systems of Linear Equa ons

.

A system of linear equa ons is inconsistent if the reduced
row echelon form of its corresponding augmented matrix
has a leading 1 in the last column.

. Example 11

Conﬁrm that the linear system
x
2x

+ y
+ 2y

= 0
= 4

has no solu on.
We can verify that this system has no solu on in two ways. First,
S
let’s just think about it. If x + y = 0, then it stands to reason, by mul plying both sides
of this equa on by 2, that 2x + 2y = 0. However, the second equa on of our system
says that 2x + 2y = 4. Since 0 ̸= 4, we have a contradic on and hence our system has
no solu on. (We cannot possibly pick values for x and y so that 2x + 2y equals both 0
and 4.)
Now let us conﬁrm this using the prescribed technique from above. The reduced
row echelon form of the corresponding augmented matrix is
[

1
0

1
0

]
0
.
1

We have a leading 1 in the last column, so therefore the system is inconsistent. .
Let’s summarize what we have learned up to this point. Consider the reduced row
echelon form of the augmented matrix of a system of linear equa ons.3 If there is a
leading 1 in the last column, the system has no solu on. Otherwise, if there is a leading
1 for each variable, then there is exactly one solu on; otherwise (i.e., there are free
variables) there are inﬁnite solu ons.
Systems with exactly one solu on or no solu on are the easiest to deal with; systems with inﬁnite solu ons are a bit harder to deal with. Therefore, we’ll do a li le
more prac ce. First, a deﬁni on: if there are inﬁnite solu ons, what do we call one of
those inﬁnite solu ons?
3 That sure seems like a mouthful in and of itself. However, it boils down to “look at the reduced form of
the usual matrix.”

29

Chapter 1 Systems of Linear Equa ons

.
Deﬁni on 7

Par cular Solu on

.

Consider a linear system of equa ons with inﬁnite solu ons.
A par cular solu on is one solu on out of the inﬁnite set of
possible solu ons.

The easiest way to ﬁnd a par cular solu on is to pick values for the free variables
which then determines the values of the dependent variables. Again, more prac ce is
called for.
. Example 12
.Give the solu on to a linear system whose augmented matrix in
reduced row echelon form is


1 −1 0 2 4
 0 0 1 −3 7 
0 0 0 0 0
and give two par cular solu ons.
We can essen ally ignore the third row; it does not divulge any
S
informa on about the solu on.4 The ﬁrst and second rows can be rewri en as the
following equa ons:
x1 − x2 + 2x4 = 4
x3 − 3x4 = 7.
No ce how the variables x1 and x3 correspond to the leading 1s of the given matrix.
Therefore x1 and x3 are dependent variables; all other variables (in this case, x2 and
x4 ) are free variables.
We generally write our solu on with the dependent variables on the le and independent variables and constants on the right. It is also a good prac ce to acknowledge
the fact that our free variables are, in fact, free. So our ﬁnal solu on would look something like
x1 = 4 + x2 − 2x4
x2 is free
x3 = 7 + 3x4
x4 is free.
To ﬁnd par cular solu ons, choose values for our free variables. There is no “right”
way of doing this; we are “free” to choose whatever we wish.
4 Then why include it? Rows of zeros some mes appear “unexpectedly” in matrices a er they have been
put in reduced row echelon form. When this happens, we do learn something; it means that at least one
equa on was a combina on of some of the others.

30

1.4 Existence and Uniqueness of Solu ons
By se ng x2 = 0 = x4 , we have the solu on x1 = 4, x2 = 0, x3 = 7, x4 = 0.
By se ng x2 = 1 and x4 = −5, we have the solu on x1 = 15, x2 = 1, x3 = −8,
x4 = −5. It is easier to read this when are variables are listed ver cally, so we repeat
these solu ons:
Another par cular solu on is:
One par cular solu on is:

.

x1 = 4
x2 = 0
x3 = 7

x1 = 15
x2 = 1
x3 = −8

x4 = 0.

x4 = −5.

. Example 13
.Find the solu on to a linear system whose augmented matrix in
reduced row echelon form is
[
]
1 0 0 2 3
0 1 0 4 5
and give two par cular solu ons.
S

Conver ng the two rows into equa ons we have
x1 + 2x4 = 3
x2 + 4x4 = 5.

We see that x1 and x2 are our dependent variables, for they correspond to the
leading 1s. Therefore, x3 and x4 are independent variables. This situa on feels a li le
unusual,5 for x3 doesn’t appear in any of the equa ons above, but cannot overlook it;
it is s ll a free variable since there is not a leading 1 that corresponds to it. We write
our solu on as:
x1 = 3 − 2x4
x2 = 5 − 4x4
x3 is free
x4 is free.
To ﬁnd two par cular solu ons, we pick values for our free variables. Again, there
is no “right” way of doing this (in fact, there are . . . inﬁnite ways of doing this) so we
give only an example here.
5 What kind of situa on would lead to a column of all zeros? To have such a column, the original matrix needed to have a column of all zeros, meaning that while we acknowledged the existence of a certain
variable, we never actually used it in any equa on. In prac cal terms, we could respond by removing the
corresponding column from the matrix and just keep in mind that that variable is free. In very large systems,
it might be hard to determine whether or not a variable is actually used and one would not worry about it.
When we learn about eigenvectors and eigenvalues, we will see that under certain circumstances this
situa on arises. In those cases we leave the variable in the system just to remind ourselves that it is there.

31

Chapter 1 Systems of Linear Equa ons
Another par cular solu on is:

One par cular solu on is:

x1 = 3 − 2π
x2 = 5 − 4π

x1 = 3
x2 = 5
x3 = 1000

x3 = e2
x4 = π.

x4 = 0.

(In the second par cular solu on we picked “unusual” values for x3 and x4 just to highlight the fact that we can.) .
. Example 14

Find the solu on to the linear system
x1
x1

+ x2
− x2

+ x3
+ x3

= 5
= 3

and give two par cular solu ons.
The corresponding augmented matrix and its reduced row echelon
S
form are given below.
[
]
[
]
−→
1 1 1 5
1 0 1 4
rref
1 −1 1 3
0 1 0 1
Conver ng these two rows into equa ons, we have
x1 + x3 = 4
x2 = 1

giving us the solu on
x1 = 4 − x3
x2 = 1
x3 is free.

Once again, we get a bit of an “unusual” solu on; while x2 is a dependent variable,
it does not depend on any free variable; instead, it is always 1. (We can think of it
as depending on the value of 1.) By picking two values for x3 , we get two par cular
solu ons.
One par cular solu on is:

.
32

Another par cular solu on is:

x1 = 4
x2 = 1

x1 = 3
x2 = 1

x3 = 0.

x3 = 1.

1.4 Existence and Uniqueness of Solu ons
The constants and coeﬃcients of a matrix work together to determine whether a
given system of linear equa ons has one, inﬁnite, or no solu on. The concept will be
ﬂeshed out more in later chapters, but in short, the coeﬃcients determine whether a
matrix will have exactly one solu on or not. In the “or not” case, the constants determine whether or not inﬁnite solu ons or no solu on exists. (So if a given linear system
has exactly one solu on, it will always have exactly one solu on even if the constants
are changed.) Let’s look at an example to get an idea of how the values of constants
and coeﬃcients work together to determine the solu on type.
. Example 15
For what values of k will the given system have exactly one soluon, inﬁnite solu ons, or no solu on?
x1
3x1

+ 2x2
+ kx2

= 3
= 9

We answer this ques on by forming the augmented matrix and
S
star ng the process of pu ng it into reduced row echelon form. Below we see the
augmented matrix and one elementary row opera on that starts the Gaussian elimina on process.
[
]
[
]
−−−−−−−−−−−→
1 2 3
1
2
3
−3R1 + R2 → R2
3 k 9
0 k−9 0
This is as far as we need to go. In looking at the second row, we see that if k = 9,
then that row contains only zeros and x2 is a free variable; we have inﬁnite solu ons.
If k ̸= 9, then our next step would be to make that second row, second column entry a leading one. We don’t par cularly care about the solu on, only that we would
have exactly one as both x1 and x2 would correspond to a leading one and hence be
dependent variables.
Our ﬁnal analysis is then this. If k ̸= 9, there is exactly one solu on; if k = 9, there
are inﬁnite solu ons. In this example, it is not possible to have no solu ons. .
As an extension of the previous example, consider the similar augmented matrix
where the constant 9 is replaced with a 10. Performing the same elementary row
opera on gives
[
]
[
]
−−−−−−−−−−−→
1 2 3
1
2
3
−3R1 + R2 → R2
.
3 k 10
0 k−9 1
As in the previous example, if k ̸= 9, we can make the second row, second column
entry a leading one and hence we have one solu on. However, if k = 9, then our last
row is [0 0 1], meaning we have no solu on.
We have been studying the solu ons to linear systems mostly in an “academic”
se ng; we have been solving systems for the sake of solving systems. In the next secon, we’ll look at situa ons which create linear systems that need solving (i.e., “word
33

Chapter 1 Systems of Linear Equa ons
problems”).

Exercises 1.4
In Exercises 1 – 14, ﬁnd the solu on to the
given linear system. If the system has inﬁnite
solu ons, give 2 par cular solu ons.
1.

2x1
x1

+
+

4x2
2x2

2.

−x1
2x1

+
−

5x2
10x2

3.

x1
2x1

+
+

x2
x2

4.

−3x1
2x1

+
−

7x2
8x2

=
=

−7
8

5.

2x1
−2x1

+
−

3x2
3x2

=
=

1
1

6.

x1
−x1

7.

−2x1
x1

8.

−x1
2x1

9.
10.

+
−
+
−

2
1

=
=
=
=

2x2
2x2
4x2
3x2

+
+

+
+
+

x2
x2
2x2

+
+
+

2x3
3x3
5x3

=
=
=

0
1
3

12.

x1
2x1
4x1

+
−
+

3x2
x2
5x2

+
+
+

3x3
2x3
8x3

=
=
=

1
−1
2

13.

x1
2x1
3x1

+
+
+

2x2
x2
3x2

+
+
+

2x3
3x3
5x3

=
=
=

1
1
2

14.

2x1
1x1
−3x1

6x3
3x3
9x3

+
+
−

=
=

6
1
2
2

15.

x1
2x1

16.

x1
x1

+
+

2x2
kx2

=
=

1
1

+
+

2x2
4x2

=
=

2
1
−3

1
k

=
=

−x1 − x2 + x3 + x4
−2x1 − 2x2 + x3

=
=

0
−1

17.

x1
x1

+
+

2x2
kx2

=
=

1
2

x1 + x2 + 6x3 + 9x4
−x1 − x3 − 2x4

=
=

0
−3

18.

x1
x1

+
+

2x2
3x2

=
=

1
k

+
+

=
=
=

In Exercises 15 – 18, state for which values
of k the given system will have exactly 1 soluon, inﬁnite solu ons, or no solu on.

1
5
4x3
2x3

4x2
2x2
6x2

+
+
−

2x3
x3

+
+

2x2
5x2

=
=

2x1
x1
3x1

3
−6

3
4

=
=

11.

1.5 Applica ons of Linear Systems

.

.
AS YOU READ
...
1. How do most problems appear “in the real world?”
2. The unknowns in a problem are also called what?
3. How many points are needed to determine the coeﬃcients of a 5th degree polynomial?

We’ve started this chapter by addressing the issue of ﬁnding the solu on to a system of linear equa ons. In subsequent sec ons, we deﬁned matrices to store linear
34

1.5 Applica ons of Linear Systems
equa on informa on; we described how we can manipulate matrices without changing the solu ons; we described how to eﬃciently manipulate matrices so that a working solu on can be easily found.
We shouldn’t lose sight of the fact that our work in the previous sec ons was aimed
at ﬁnding solu ons to systems of linear equa ons. In this sec on, we’ll learn how to
apply what we’ve learned to actually solve some problems.
Many, many, many problems that are addressed by engineers, businesspeople,
scien sts and mathema cians can be solved by properly se ng up systems of linear
equa ons. In this sec on we highlight only a few of the wide variety of problems that
matrix algebra can help us solve.
We start with a simple example.
. Example 16
.A jar contains 100 blue, green, red and yellow marbles. There are
twice as many yellow marbles as blue; there are 10 more blue marbles than red; the
sum of the red and yellow marbles is the same as the sum of the blue and green. How
many marbles of each color are there?
Let’s call the number of blue balls b, and the number of the other
S
balls g, r and y, each represen ng the obvious. Since we know that we have 100 marbles, we have the equa on
b + g + r + y = 100.
The next sentence in our problem statement allows us to create three more equa ons.
We are told that there are twice as many yellow marbles as blue. One of the following two equa ons is correct, based on this statement; which one is it?
2y = b

or

2b = y

The ﬁrst equa on says that if we take the number of yellow marbles, then double
it, we’ll have the number of blue marbles. That is not what we were told. The second
equa on states that if we take the number of blue marbles, then double it, we’ll have
the number of yellow marbles. This is what we were told.
The next statement of “there are 10 more blue marbles as red” can be wri en as
either
b = r + 10

or

r = b + 10.

Which is it?
The ﬁrst equa on says that if we take the number of red marbles, then add 10,
we’ll have the number of blue marbles. This is what we were told. The next equa on
is wrong; it implies there are more red marbles than blue.
The ﬁnal statement tells us that the sum of the red and yellow marbles is the same
as the sum of the blue and green marbles, giving us the equa on
r + y = b + g.
35

Chapter 1 Systems of Linear Equa ons
We have four equa ons; altogether, they are
b + g + r + y = 100
2b = y
b = r + 10
r + y = b + g.

We want to write these equa ons in a standard way, with all the unknowns on the
le and the constants on the right. Let us also write them so that the variables appear
in the same order in each equa on (we’ll use alphabe cal order to make it simple).
We now have
b + g + r + y = 100
2b − y = 0
b − r = 10
−b − g + r + y = 0

To ﬁnd the solu on, let’s form the appropriate augmented matrix and put it into
reduced row echelon form. We do so here, without showing the steps.




1
1
1
1 100
1 0 0 0 20
 2
 0 1 0 0 30 
−→
0
0 −1
0 




rref
 1

 0 0 1 0 10 
0 −1 0
10
−1 −1 1
1
0
0 0 0 1 40
We interpret from the reduced row echelon form of the matrix that we have 20
blue, 30 green, 10 red and 40 yellow marbles. .
Even if you had a bit of diﬃculty with the previous example, in reality, this type
of problem is pre y simple. The unknowns were easy to iden fy, the equa ons were
pre y straigh orward to write (maybe a bit tricky for some), and only the necessary
informa on was given.
Most problems that we face in the world do not approach us in this way; most
problems do not approach us in the form of “Here is an equa on. Solve it.” Rather,
most problems come in the form of:
Here is a problem. I want the solu on. To help, here is lots of informa on.
It may be just enough; it may be too much; it may not be enough. You
ﬁgure out what you need; just give me the solu on.
Faced with this type of problem, how do we proceed? Like much of what we’ve
done in the past, there isn’t just one “right” way. However, there are a few steps
that can guide us. You don’t have to follow these steps, “step by step,” but if you ﬁnd
that you are having diﬃculty solving a problem, working through these steps may help.
36

1.5 Applica ons of Linear Systems
(Note: while the principles outlined here will help one solve any type of problem, these
steps are wri en speciﬁcally for solving problems that involve only linear equa ons.)
.
Key Idea 4

Mathema cal Problem Solving
1. Understand the problem.
asked?

What exactly is being

2. Iden fy the unknowns. What are you trying to ﬁnd?
What units are involved?

.

3. Give names to your unknowns (these are your variables).
4. Use the informa on given to write as many equa ons
as you can that involve these variables.
5. Use the equa ons to form an augmented matrix; use
Gaussian elimina on to put the matrix into reduced
row echelon form.
6. Interpret the reduced row echelon form of the matrix
to iden fy the solu on.
7. Ensure the solu on makes sense in the context of the
problem.

Having iden ﬁed some steps, let us put them into prac ce with some examples.
. Example 17
.A concert hall has sea ng arranged in three sec ons. As part of a
special promo on, guests will recieve two of three prizes. Guests seated in the ﬁrst
and second sec ons will receive Prize A, guests seated in the second and third sec ons
will receive Prize B, and guests seated in the ﬁrst and third sec ons will receive Prize
C. Concert promoters told the concert hall managers of their plans, and asked how
many seats were in each sec on. (The promoters want to store prizes for each sec on
separately for easier distribu on.) The managers, thinking they were being helpful,
told the promoters they would need 105 A prizes, 103 B prizes, and 88 C prizes, and
have since been unavailable for further help. How many seats are in each sec on?
Before we rush in and start making equa ons, we should be clear
S
about what is being asked. The ﬁnal sentence asks: “How many seats are in each
sec on?” This tells us what our unknowns should be: we should name our unknowns
for the number of seats in each sec on. Let x1 , x2 and x3 denote the number of seats
in the ﬁrst, second and third sec ons, respec vely. This covers the ﬁrst two steps of
our general problem solving technique.
37

Chapter 1 Systems of Linear Equa ons
(It is temp ng, perhaps, to name our variables for the number of prizes given away.
However, when we think more about this, we realize that we already know this – that
informa on is given to us. Rather, we should name our variables for the things we
don’t know.)
Having our unknowns iden ﬁed and variables named, we now proceed to forming
equa ons from the informa on given. Knowing that Prize A goes to guests in the ﬁrst
and second sec ons and that we’ll need 105 of these prizes tells us
x1 + x2 = 105.
Proceeding in a similar fashion, we get two more equa ons,
x2 + x3 = 103

and

x1 + x3 = 88.

Thus our linear system is
x1 + x2
x2 + x3
x1 + x3

= 105
= 103
= 88

and the corresponding augmented matrix is


1 1 0 105
 0 1 1 103  .
1 0 1 88
To solve our system, let’s put this matrix into reduced row echelon form.




1 0 0 45
1 1 0 105
−
→
 0 1 0 60 
 0 1 1 103 
rref
0 0 1 43
1 0 1 88
We can now read oﬀ our solu on. The ﬁrst sec on has 45 seats, the second has
60 seats, and the third has 43 seats. .
. Example 18
.A lady takes a 2-mile motorized boat trip down the Highwater River,
knowing the trip will take 30 minutes. She asks the boat pilot “How fast does this river
ﬂow?” He replies “I have no idea, lady. I just drive the boat.”
She thinks for a moment, then asks “How long does the return trip take?” He
replies “The same; half an hour.” She follows up with the statement, “Since both legs
take the same me, you must not drive the boat at the same speed.”
“Naw,” the pilot said. “While I really don’t know exactly how fast I go, I do know
that since we don’t carry any tourists, I drive the boat twice as fast.”
The lady walks away sa sﬁed; she knows how fast the river ﬂows.
(How fast does it ﬂow?)
This problem forces us to think about what informa on is given
S
and how to use it to ﬁnd what we want to know. In fact, to ﬁnd the solu on, we’ll ﬁnd
out extra informa on that we weren’t asked for!
38

1.5 Applica ons of Linear Systems
We are asked to ﬁnd how fast the river is moving (step 1). To ﬁnd this, we should
recognize that, in some sense, there are three speeds at work in the boat trips: the
speed of the river (which we want to ﬁnd), the speed of the boat, and the speed that
they actually travel at.
We know that each leg of the trip takes half an hour; if it takes half an hour to cover
2 miles, then they must be traveling at 4 mph, each way.
The other two speeds are unknowns, but they are related to the overall speeds.
Let’s call the speed of the river r and the speed of the boat b. (And we should be
careful. From the conversa on, we know that the boat travels at two diﬀerent speeds.
So we’ll say that b represents the speed of the boat when it travels downstream, so
2b represents the speed of the boat when it travels upstream.) Let’s let our speed be
measured in the units of miles/hour (mph) as we used above (steps 2 and 3).
What is the rate of the people on the boat? When they are travelling downstream,
their rate is the sum of the water speed and the boat speed. Since their overall speed
is 4 mph, we have the equa on r + b = 4.
When the boat returns going against the current, its overall speed is the rate of
the boat minus the rate of the river (since the river is working against the boat). The
overall trip is s ll taken at 4 mph, so we have the equa on 2b − r = 4. (Recall: the
boat is traveling twice as fast as before.)
The corresponding augmented matrix is
]
[
1 1 4
.
2 −1 4
Note that we decided to let the ﬁrst column hold the coeﬃcients of b.
Pu ng this matrix in reduced row echelon form gives us:
[
]
[
]
−→
1 1 4
1 0 8/3
rref
.
2 −1 4
0 1 4/3
We ﬁnish by interpre ng this solu on: the speed of the boat (going downstream)
is 8/3 mph, or 2.6 mph, and the speed of the river is 4/3 mph, or 1.3 mph. All we really
wanted to know was the speed of the river, at about 1.3 mph. .
. Example 19
.Find the equa on of the quadra c func on that goes through the
points (−1, 6), (1, 2) and (2, 3).
This may not seem like a “linear” problem since we are talking
S
about a quadra c func on, but closer examina on will show that it really is.
We normally write quadra c func ons as y = ax2 + bx + c where a, b and c are
the coeﬃcients; in this case, they are our unknowns. We have three points; consider
the point (−1, 6). This tells us directly that if x = −1, then y = 6. Therefore we know
that 6 = a(−1)2 + b(−1) + c. Wri ng this in a more standard form, we have the linear
equa on
a − b + c = 6.
The second point tells us that a(1)2 + b(1) + c = 2, which we can simplify as
a + b + c = 2, and the last point tells us a(2)2 + b(2) + c = 3, or 4a + 2b + c = 3.
39

Chapter 1 Systems of Linear Equa ons
Thus our linear system is

a−b+c = 6
a+b+c = 2
4a + 2b + c = 3.

Again, to solve our system, we ﬁnd the reduced row echelon form of the corresponding augmented matrix. We don’t show the steps here, just the ﬁnal result.




1 −1 1 6
1 0 0 1
−
−
→
1 1 1 2
 0 1 0 −2 
rref
4 2 1 3
0 0 1 3
This tells us that a = 1, b = −2 and c = 3, giving us the quadra c func on
y = x2 − 2x + 3. .
One thing interes ng about the previous example is that it conﬁrms for us something that we may have known for a while (but didn’t know why it was true). Why do
we need two points to ﬁnd the equa on of the line? Because in the equa on of the
a line, we have two unknowns, and hence we’ll need two equa ons to ﬁnd values for
these unknowns.
A quadra c has three unknowns (the coeﬃcients of the x2 term and the x term, and
the constant). Therefore we’ll need three equa ons, and therefore we’ll need three
points.
What happens if we try to ﬁnd the quadra c func on that goes through 3 points
that are all on the same line? The fast answer is that you’ll get the equa on of a line;
there isn’t a quadra c func on that goes through 3 colinear points. Try it and see!
(Pick easy points, like (0, 0), (1, 1) and (2, 2). You’ll ﬁnd that the coeﬃcient of the x2
term is 0.)
Of course, we can do the same type of thing to ﬁnd polynomials that go through 4,
5, etc., points. In general, if you are given n + 1 points, a polynomial that goes through
all n + 1 points will have degree at most n.
. Example 20
.A woman has 32 $1, $5 and $10 bills in her purse, giving her a total
of $100. How many bills of each denomina on does she have?
Let’s name our unknowns x, y and z for our ones, ﬁves and tens,
S
respec vely (it is temp ng to call them o, f and t, but o looks too much like 0). We
know that there are a total of 32 bills, so we have the equa on
x + y + z = 32.
We also know that we have $100, so we have the equa on
x + 5y + 10z = 100.

40

We have three unknowns but only two equa ons, so we know that we cannot expect
a unique solu on. Let’s try to solve this system anyway and see what we get.
Pu ng the system into a matrix and then ﬁnding the reduced row echelon form,
we have
]
[
]
[
−→
1 0 − 45 15
1 1 1
32
.
rref
9
1 5 10 100
17
0 1
4

1.5 Applica ons of Linear Systems
Reading from our reduced matrix, we have the inﬁnite solu on set
5
x = 15 + z
4
9
y = 17 − z
4
z is free.
While we do have inﬁnite solu ons, most of these solu ons really don’t make sense
in the context of this problem. (Se ng z = 12 doesn’t make sense, for having half a
ten dollar bill doesn’t give us $5. Likewise, having z = 8 doesn’t make sense, for then
we’d have “−1” $5 bills.) So we must make sure that our choice of z doesn’t give us
frac ons of bills or nega ve amounts of bills.
To avoid frac ons, z must be a mul ple of 4 (−4, 0, 4, 8, . . .). Of course, z ≥ 0 for
a nega ve number wouldn’t make sense. If z = 0, then we have 15 one dollar bills
and 17 ﬁve dollar bills, giving us $100. If z = 4, then we have x = 20 and y = 8.
We already men oned that z = 8 doesn’t make sense, nor does any value of z where
z ≥ 8.
So it seems that we have two answers; one with z = 0 and one with z = 4. Of
course, by the statement of the problem, we are led to believe that the lady has at
least one $10 bill, so probably the “best” answer is that we have 20 $1 bills, 8 $5 bills
and 4 $10 bills. The real point of this example, though, is to address how inﬁnite soluons may appear in a real world situa on, and how suprising things may result. .
. Example 21
.In a football game, teams can score points through touchdowns
worth 6 points, extra points (that follow touchdowns) worth 1 point, two point conversions (that also follow touchdowns) worth 2 points and ﬁeld goals, worth 3 points.
You are told that in a football game, the two compe ng teams scored on 7 occasions,
giving a total score of 24 points. Each touchdown was followed by either a successful
extra point or two point conversion. In what ways were these points scored?
The ques on asks how the points were scored; we can interpret
S
this as asking how many touchdowns, extra points, two point conversions and ﬁeld
goals were scored. We’ll need to assign variable names to our unknowns; let t represent the number of touchdowns scored; let x represent the number of extra points
scored, let w represent the number of two point conversions, and let f represent the
number of field goals scored.
Now we address the issue of wri ng equa ons with these variables using the given
informa on. Since we have a total of 7 scoring occasions, we know that
t + x + w + f = 7.
The total points scored is 24; considering the value of each type of scoring opportunity,
we can write the equa on
6t + x + 2w + 3f = 24.
41

Chapter 1 Systems of Linear Equa ons
Finally, we know that each touchdown was followed by a successful extra point or two
point conversion. This is subtle, but it tells us that the number of touchdowns is equal
to the sum of extra points and two point conversions. In other words,
t = x + w.
To solve our problem, we put these equa ons into a matrix and put the matrix into
reduced row echelon form. Doing so, we ﬁnd




1 1
1 1 7
1 0 0 0.5
3.5
−→
6 1
0 1 0
2 3 24 
1
4 .
rref
1 −1 −1 0 0
0 0 1 −0.5 −0.5
Therefore, we know that
t = 3.5 − 0.5f
x=4−f
w = −0.5 + 0.5f.
We recognize that this means there are “inﬁnite solu ons,” but of course most of these
will not make sense in the context of a real football game. We must apply some logic
to make sense of the situa on.
Progressing in no par cular order, consider the second equa on, x = 4 − f. In
order for us to have a posi ve number of extra points, we must have f ≤ 4. (And of
course, we need f ≥ 0, too.) Therefore, right away we know we have a total of only 5
possibili es, where f = 0, 1, 2, 3 or 4.
From the ﬁrst and third equa ons, we see that if f is an even number, then t and w
will both be frac ons (for instance, if f = 0, then t = 3.5) which does not make sense.
Therefore, we are down to two possible solu ons, f = 1 and f = 3.
If f = 1, we have 3 touchdowns, 3 extra points, no two point conversions, and
(of course), 1 ﬁeld goal. (Check to make sure that gives 24 points!) If f = 3, then
we 2 touchdowns, 1 extra point, 1 two point conversion, and (of course) 3 ﬁeld goals.
Again, check to make sure this gives us 24 points. Also, we should check each solu on
to make sure that we have a total of 7 scoring occasions and that each touchdown
could be followed by an extra point or a two point conversion. .
We have seen a variety of applica ons of systems of linear equa ons. We would
do well to remind ourselves of the ways in which solu ons to linear systems come:
there can be exactly one solu on, inﬁnite solu ons, or no solu ons. While we did see
a few examples where it seemed like we had only 2 solu ons, this was because we
were restric ng our solu ons to “make sense” within a certain context.
We should also remind ourselves that linear equa ons are immensely important.
The examples we considered here ask fundamentally simple ques ons like “How fast is
the water moving?” or “What is the quadra c func on that goes through these three
points?” or “How were points in a football game scored?” The real “important” situa ons ask much more diﬃcult ques ons that o en require thousands of equa ons!
42

1.5 Applica ons of Linear Systems
(Gauss began the systema c study of solving systems of linear equa ons while trying
to predict the next sigh ng of a comet; he needed to solve a system of linear equa ons
that had 17 unknowns. Today, this a rela vely easy situa on to handle with the help of
computers, but to do it by hand is a real pain.) Once we understand the fundamentals
of solving systems of equa ons, we can move on to looking at solving bigger systems
of equa ons; this text focuses on ge ng us to understand the fundamentals.

Exercises 1.5
In Exercises 1 – 5, ﬁnd the solu on of the
given problem by:
(a) crea ng an appropriate system of linear
equa ons
(b) forming the augmented matrix that corresponds to this system
(c) pu ng the augmented matrix into reduced row echelon form
(d) interpre ng the reduced row echelon
form of the matrix as a solu on
1. A farmer looks out his window at his
chickens and pigs. He tells his daughter that he sees 62 heads and 190 legs.
How many chickens and pigs does the
farmer have?
2. A lady buys 20 trinkets at a yard sale.
The cost of each trinket is either $0.30
or $0.65. If she spends $8.80, how
many of each type of trinket does she
buy?
3. A carpenter can make two sizes of table,
grande and ven . The grande table requires 4 table legs and 1 table top; the
ven requires 6 table legs and 2 table
tops. A er doing work, he counts up
spare parts in his warehouse and realizes that he has 86 table tops le over,
and 300 legs. How many tables of each
kind can he build and use up exactly all
of his materials?
4. A jar contains 100 marbles. We know
there are twice as many green marbles
as red; that the number of blue and yellow marbles together is the same as the
number of green; and that three mes
the number of yellow marbles together
with the red marbles gives the same

numbers as the blue marbles. How
many of each color of marble are in the
jar?
5. A rescue mission has 85 sandwiches, 65
bags of chips and 210 cookies. They
know from experience that men will
eat 2 sandwiches, 1 bag of chips and
4 cookies; women will eat 1 sandwich,
a bag of chips and 2 cookies; kids will
eat half a sandwhich, a bag of chips and
3 cookies. If they want to use all their
food up, how many men, women and
kids can they feed?
In Exercises 6 – 15, ﬁnd the polynomial with
the smallest degree that goes through the
given points.
6. (1, 3) and (3, 15)
7. (−2, 14) and (3, 4)
8. (1, 5), (−1, 3) and (3, −1)
9. (−4, −3), (0, 1) and (1, 4.5)
10. (−1, −8), (1, −2) and (3, 4)
11. (−3, 3), (1, 3) and (2, 3)
12. (−2, 15), (−1, 4), (1, 0) and (2, −5)
13. (−2, −7), (1, 2), (2, 9) and (3, 28)
14. (−3, 10), (−1, 2), (1, 2) and (2, 5)
15. (0, 1), (−3, −3.5), (−2, −2) and (4, 7)
16. The general exponen al func on has
the form f(x) = aebx , where a and b are
constants and e is Euler’s constant (≈
2.718). We want to ﬁnd the equa on
of the exponen al func on that goes
through the points (1, 2) and (2, 4).

43

Chapter 1 Systems of Linear Equa ons
(a) Show why we cannot simply subsitute in values for x and y in
y = aebx and solve using the
techniques we used for polynomials.
(b) Show how the equality y = aebx
leads us to the linear equa on
ln y = ln a + bx.
(c) Use the techniques we developed to solve for the unknowns
ln a and b.
(d) Knowing ln a, ﬁnd a; ﬁnd the exponen al func on f(x) = aebx
that goes through the points
(1, 2) and (2, 4).
17. In a football game, 24 points are scored
from 8 scoring occasions. The number
of successful extra point kicks is equal
to the number of successful two point
conversions. Find all ways in which the
points may have been scored in this
game.
18. In a football game, 29 points are scored
from 8 scoring occasions. There are 2
more successful extra point kicks than
successful two point conversions. Find
all ways in which the points may have
been scored in this game.

44

19. In a basketball game, where points are
scored either by a 3 point shot, a 2 point
shot or a 1 point free throw, 80 points
were scored from 30 successful shots.
Find all ways in which the points may
have been scored in this game.
20. In a basketball game, where points are
scored either by a 3 point shot, a 2 point
shot or a 1 point free throw, 110 points
were scored from 70 successful shots.
Find all ways in which the points may
have been scored in this game.
21. Describe the equa ons of the linear
func ons that go through the point
(1,3). Give 2 examples.
22. Describe the equa ons of the linear
func ons that go through the point
(2,5). Give 2 examples.
23. Describe the equa ons of the quadra c
func ons that go through the points
(2, −1) and (1,0). Give 2 examples.
24. Describe the equa ons of the quadra c
func ons that go through the points
(−1, 3) and (2,6). Give 2 examples.

2
M

B
A

B′
A′

.

A

A fundamental topic of mathema cs is arithme c; adding, subtrac ng, mul plying and
dividing numbers. A er learning how to do this, most of us went on to learn how to
add, subtract, mul ply and divide “x”. We are comfortable with expressions such as
x + 3x − x · x2 + x5 · x−1
and know that we can “simplify” this to
4x − x3 + x4 .
This chapter deals with the idea of doing similar opera ons, but instead of an unknown number x, we will be using a matrix A. So what exactly does the expression
A + 3A − A · A2 + A5 · A−1
mean? We are going to need to learn to deﬁne what matrix addi on, scalar mul plicaon, matrix mul plica on and matrix inversion are. We will learn just that, plus some
more good stuﬀ, in this chapter.

2.1

Matrix Addi on and Scalar Mul plica on

.

.
AS YOU READ
...
1. When are two matrices equal?
2. Write an explana on of how to add matrices as though wri ng to someone who
knows what a matrix is but not much more.
3. T/F: There is only 1 zero matrix.
4. T/F: To mul ply a matrix by 2 means to mul ply each entry in the matrix by 2.

Chapter 2 Matrix Arithme c
In the past, when we dealt with expressions that used “x,” we didn’t just add and
mul ply x’s together for the fun of it, but rather because we were usually given some
sort of equa on that had x in it and we had to “solve for x.”
This begs the ques on, “What does it mean to be equal?” Two numbers are equal,
when, . . ., uh, . . ., nevermind. What does it mean for two matrices to be equal? We
say that matrices A and B are equal when their corresponding entries are equal. This
seems like a very simple deﬁni on, but it is rather important, so we give it a box.
.
Deﬁni on 8

Matrix Equality
.equal if their corresponding
Two m × n matrices A and B are
entries are equal.

No ce that our more formal deﬁni on speciﬁes that if matrices are equal, they
have the same dimensions. This should make sense.
Now we move on to describing how to add two matrices together. To start oﬀ, take
a wild stab: what do you think the following sum is equal to?
[
] [
]
1 2
2 −1
+
=?
3 4
5 7
If you guessed

[

]
3 1
,
8 11

you guessed correctly. That wasn’t so hard, was it?
Let’s keep going, hoping that we are star ng to get on a roll. Make another wild
guess: what do you think the following expression is equal to?
[
]
1 2
3·
=?
3 4
If you guessed

[

]
3 6
,
9 12

you guessed correctly!
Even if you guessed wrong both mes, you probably have seen enough in these
two examples to have a fair idea now what matrix addi on and scalar mul plica on
are all about.
Before we formally deﬁne how to perform the above opera ons, let us ﬁrst recall
that if A is an m × n matrix, then we can write A as


a11 a12 · · · a1n
 a21 a22 · · · a2n 


A= .
..
..  .
..
 ..
.
.
. 
am1 am2 · · · amn
46

2.1 Matrix Addi on and Scalar Mul plica on
Secondly, we should deﬁne what we mean by the word scalar. A scalar is any number
that we mul ply a matrix by. (In some sense, we use that number to scale the matrix.)
We are now ready to deﬁne our ﬁrst arithme c opera ons.
.
Deﬁni on 9

Matrix Addi on
Let A and B be m × n matrices. The sum of A and B, denoted
A + B, is


a11 + b11 a12 + b12 · · · a1n + b1n
 a21 + b21 a22 + b22 · · · a2n + b2n 



.
..
..
..
..


.
.
.
.

.

am1 + bm1

.
Deﬁni on 10

am2 + bm2

···

amn + bmn

Scalar Mul plica on
Let A be an m × n matrix and let k be a scalar. The scalar
mul plica on of k and A, denoted kA, is


ka11 ka12 · · · ka1n
 ka21 ka22 · · · ka2n 


 ..
..
..  .
..
 .
.
.
. 
kam1 kam2 · · · kamn

.

We are now ready for an example.
. Example 22

.Let



1 2 3
A =  −1 2 1  ,
5 5 5




2 4 6
B =  1 2 2,
−1 0 4

[
C=

1 2
9 8

]
3
.
7

Simplify the following matrix expressions.
1. A + B

3. A − B

5. −3A + 2B

7. 5A + 5B

2. B + A

4. A + C

6. A − A

8. 5(A + B)

S

47

Chapter 2 Matrix Arithme c



3 6 9
1. A + B =  0 4 3 .
4 5 9


3 6 9
2. B + A =  0 4 3 .
4 5 9


−1 −2 −3
3. A − B =  −2 0 −1 .
6
5
1
4. A + C is not deﬁned. If we look at our deﬁni on of matrix addi on, we see
that the two matrices need to be the same size. Since A and C have diﬀerent
dimensions, we don’t even try to create something as an addi on; we simply
say that the sum is not deﬁned.


1
2
3
−2
1 .
5. −3A + 2B =  5
−17 −15 −7


0 0 0
6. A − A =  0 0 0 .
0 0 0

 
 

5 10 15
10 20 30
15 30 45
7. Strictly speaking, this is  −5 10 5 + 5 10 10  =  0 20 15 .
25 25 25
−5 0 20
20 25 45

.

8. Strictly speaking, this is

 



1 2 3
2 4 6
3 6 9
5  −1 2 1  +  1 2 2  = 5 ·  0 4 3 
5 5 5
−1 0 4
4 5 9


15 30 45
=  0 20 15  .
20 25 45

Our example raised a few interes ng points. No ce how A + B = B + A. We
probably aren’t suprised by this, since we know that when dealing with numbers, a +
b = b + a. Also, no ce that 5A + 5B = 5(A + B). In our example, we were careful to
compute each of these expressions following the proper order of opera ons; knowing
these are equal allows us to compute similar expressions in the most convenient way.
Another interes ng thing that came from our previous example is that


0 0 0
A − A = 0 0 0.
0 0 0
48

2.1 Matrix Addi on and Scalar Mul plica on
It seems like this should be a special matrix; a er all, every entry is 0 and 0 is a special
number.
In fact, this is a special matrix. We deﬁne 0, which we read as “the zero matrix,”
to be the matrix of all zeros.1 We should be careful; this previous “deﬁni on”
]
[ is a bit
0 0
ambiguous, for we have not stated what size the zero matrix should be. Is
0 0
[
]
the zero matrix? How about 0 0 ?
Let’s not get bogged down in seman cs. If we ever see 0 in an expression, we
will usually know right away what size 0 should be; it will be the size that allows the
expression to make sense. If A is a 3 × 5 matrix, and we write A + 0, we’ll simply
assume that 0 is also a 3 × 5 matrix. If we are ever in doubt, we can add a subscript;
for instance, 02×7 is the 2 × 7 matrix of all zeros.
Since the zero matrix is an important concept, we give it it’s own deﬁni on box.
.
Deﬁni on 11

The Zero Matrix
The m × n matrix of all zeros, denoted 0m×n , is the zero
matrix.

.

When the dimensions of the zero matrix are clear from the
context, the subscript is generally omi ed.

The following presents some of the proper es of matrix addi on and scalar mul plica on that we discovered above, plus a few more.
.
Theorem 2

Proper es of Matrix Addi on and Scalar Mul plica on
The following equali es hold for all m × n matrices A, B and
C and scalars k.
1. A + B = B + A (Commuta ve Property)

.

2. (A + B) + C = A + (B + C) (Associa ve Property)
3. k(A + B) = kA + kB (Scalar Mul plica on Distribu ve
Property)
4. kA = Ak
5. A + 0 = 0 + A = A (Addi ve Iden ty)
6. 0A = 0

Be sure that this last property makes sense; it says that if we mul ply any matrix
1 We use

the bold face to dis nguish the zero matrix, 0, from the number zero, 0.

49

Chapter 2 Matrix Arithme c
by the number 0, the result is the zero matrix, or 0.
We began this sec on with the concept of matrix equality. Let’s put our matrix
addi on proper es to use and solve a matrix equa on.
. Example 23

Let

[
A=

]
2 −1
.
3 6

Find the matrix X such that
2A + 3X = −4A.
We can use basic algebra techniques to manipulate this equa on
S
for X; ﬁrst, let’s subtract 2A from both sides. This gives us
3X = −6A.
Now divide both sides by 3 to get
X = −2A.
Now we just need to compute −2A; we ﬁnd that
[
]
−4
2
X=
.
−6 −12
.
Our matrix proper es iden ﬁed 0 as the Addi ve Iden ty; i.e., if you add 0 to any
matrix A, you simply get A. This is similar in no on to the fact that for all numbers a,
a + 0 = a. A Mul plica ve Iden ty would be a matrix I where I × A = A for all matrices A. (What would such a matrix look like? A matrix of all 1s, perhaps?) However, in
order for this to make sense, we’ll need to learn to mul ply matrices together, which
we’ll do in the next sec on.

Exercises 2.1
Matrices A and B are given below. In Exercises 1 – 6, simplify the given expression.
]
[
]
[
1 −1
−3 2
B=
A=
7
4
5
9

1. A + B

5. 3(A − B) + B
6. 2(A − B) − (A − 3B)
Matrices A and B are given below. In Exercises 7 – 10, simplify the given expression.
[ ]
[
]
3
−2
A=
B=
5
4

2. 2A − 3B

50

3. 3A − A

7. 4B − 2A

4. 4B − 2A

8. −2A + 3A

2.2 Matrix Mul plica on
9. −2A − 3A
10. −B + 3B − 2B
Matrices A and B are given below. In Exercises 11 – 14, ﬁnd X that sa sﬁes the equaon.
[
]
[
]
3 −1
1
7
A=
B=
2
5
3 −4

11. 2A + X = B

16.
17.
18.
19.

12. A − X = 3B
13. 3A + 2X = −1B

20.

14. A − 12 X = −B
In Exercises 15 – 21, ﬁnd values for the scalars
a and b that sa sfy the given equa on.

2.2

[ ]
[
] [ ]
1
−1
1
+b
=
2
5
9
[
]
[ ] [ ]
−3
8
7
+b
=
a
1
4
1
[
]
[
] [
]
4
−6
10
a
+b
=
−2
3
−5
[ ]
[
] [ ]
1
−1
5
a
+b
=
1
3
5
] [
]
[ ]
[
1
−3
4
a
+b
=
3
−9
−12
 
  

1
1
0
a  2  + b  1  =  −1 
3
2
−1
 
   
1
5
3
a0 + b1 = 4
1
2
7

15. a

21.

Matrix Mul plica on

.

.
AS YOU READ
...
1. T/F: Column vectors are used more in this text than row vectors, although some
other texts do the opposite.
2. T/F: To mul ply A × B, the number of rows of A and B need to be the same.
3. T/F: The entry in the 2nd row and 3rd column of the product AB comes from
mul pling the 2nd row of A with the 3rd column of B.
4. Name two proper es of matrix mul plica on that also hold for “regular mul plica on” of numbers.
5. Name a property of “regular mul plica on” of numbers that does not hold for
matrix mul plica on.
6. T/F: A3 = A · A · A

In the previous sec on we found that the deﬁni on of matrix addi on was very
intui ve, and we ended that sec on discussing the fact that eventually we’d like to
know what it means to mul ply matrices together.
In the spirit of the last sec on, take another wild stab: what do you think
[
] [
]
1 2
1 −1
×
3 4
2 2
51

Chapter 2 Matrix Arithme c
means?
You are likely to have guessed
[

1 −2
6 8

]

but this is, in fact, not right.2 The actual answer is
[
]
5 3
.
11 5
If you can look at this one example and suddenly understand exactly how matrix mulplica on works, then you are probably smarter than the author. While matrix mul plica on isn’t hard, it isn’t nearly as intui ve as matrix addi on is.
To further muddy the waters (before we clear them), consider
[
] [
]
1 2
1 −1 0
.
×
2 2 −1
3 4
Our experience from the last sec on would lend us to believe that this is not deﬁned,
but our conﬁdence is probably a bit shaken by now. In fact, this mul plica on is deﬁned, and it is
[
]
5 3 −2
.
11 5 −4
You may see some similarity in this answer to what we got before, but again, probably
not enough to really ﬁgure things out.
So let’s take a step back and progress slowly. The ﬁrst thing we’d like to do is deﬁne
a special type of matrix called a vector.
.
Deﬁni on 12

Column and Row Vectors

.

A m × 1 matrix is called a column vector.
A 1 × n matrix is called a row vector.

While it isn’t obvious right now, column vectors are going to become far more useful to us than row vectors. Therefore, we o en omit the word “column” when refering
to column vectors, and we just call them “vectors.”3
2I

guess you could deﬁne mul plica on this way. If you’d prefer this type of mul plica on, write your
own book.
3 In this text, row vectors are only used in this sec on when we discuss matrix mul plica on, whereas
we’ll make extensive use of column vectors. Other texts make great use of row vectors, but li le use of
column vectors. It is a ma er of preference and tradi on: “most” texts use column vectors more.

52

2.2 Matrix Mul plica on
We have been using upper case le ers to denote matrices; we use lower case letters with an arrow overtop to denote row and column vectors. An example of a row
vector is
[
]
⃗u = 1 2 −1 0
and an example of a column vector is
 
1
⃗v =  7  .
8
Before we learn how to mul ply matrices in general, we will learn what it means
to mul ply a row vector by a column vector.
.
Deﬁni on 13

Mul plying a row vector by a column vector
Let ⃗u be an 1 × n row vector with entries u1 , u2 , · · · , un and
let ⃗v be an n × 1 column vector with entries v1 , v2 , · · · , vn .
The product of ⃗u and ⃗v, denoted ⃗u · ⃗v or ⃗u⃗v, is

.

n
∑

ui vi = u1 v1 + u2 v2 + · · · + un vn .

i=1

Don’t worry if this deﬁni on doesn’t make immediate sense. It is really an easy
concept; an example will make things more clear.
. Example 24

.Let

[
⃗u = 1

2

]
[
3 , ⃗v = 2 0

 
1
−2
2
]

−1 , ⃗x =  4  , ⃗y = 
5.
3
0


1



Find the following products.
1. ⃗u⃗x

3. ⃗u⃗y

2. ⃗v⃗y

4. ⃗u⃗v

S

[

1. ⃗u⃗x = 1 2

5. ⃗x⃗u




−2
3  4  = 1(−2) + 2(4) + 3(3) = 15
3
]

53

Chapter 2 Matrix Arithme c

[
2. ⃗v⃗y = 2 0

1

 
1
]2

−1 
 5  = 2(1) + 0(2) + 1(5) − 1(0) = 7
0

3. ⃗u⃗y is not deﬁned; Deﬁni on 13 speciﬁes that in order to mul ply a row vector
and column vector, they must have the same number of entries.
4. ⃗u⃗v is not deﬁned; we only know how to mul py row vectors by column vectors. We haven’t deﬁned how to mul ply two row vectors (in general, it can’t
be done).
5. The product ⃗x⃗u is deﬁned, but we don’t know how to do it yet. Right now, we
only know how to mul ply a row vector mes a column vector; we don’t know
.
how to mul ply a column vector mes a row vector. (That’s right: ⃗u⃗x ̸= ⃗x⃗u!)
Now that we understand how to mul ply a row vector by a column vector, we are
ready to deﬁne matrix mul plica on.
.
Deﬁni on 14

Matrix Mul plica on

.

Let A be an m × r matrix, and let B be an r × n matrix. The
matrix product of A and B, denoted A · B, or simply AB, is the
m × n matrix M whose entry in the ith row and jth column is
the product of the ith row of A and the jth column of B.

It may help to illustrate it in this way. Let matrix A have rows a⃗1 , a⃗2 , · · · , a⃗m and let
B have columns b⃗1 , b⃗2 , · · · , b⃗n . Thus A looks like


− a⃗1 −
 − a⃗2 − 




..


.
−

a⃗m

−

where the “−” symbols just serve as reminders that the a⃗i represent rows, and B looks
like


|
|
|
 b⃗1 b⃗2 · · · b⃗n 
|
|
|
where again, the “|” symbols just remind us that the b⃗i represent column vectors. Then

 ⃗
a⃗1 b1 a⃗1 b⃗2 · · · a⃗1 b⃗n
 a⃗2 b⃗1 a⃗2 b⃗2 · · · a⃗2 b⃗n 


AB =  .
..
..  .
..
 ..
.
.
. 
⃗
⃗
a⃗m b1 a⃗m b2 · · · a⃗m b⃗n
54

2.2 Matrix Mul plica on
Two quick notes about this deﬁni on. First, no ce that in order to mul ply A and B,
the number of columns of A must be the same as the number of rows of B (we refer to
these as the “inner dimensions”). Secondly, the resul ng matrix has the same number
of rows as A and the same number of columns as B (we refer to these as the “outer
dimensions”).
ﬁnal dimensions are the outer
dimensions

z
}|
{
(m × r) × (r × n)
| {z }
these inner dimensions
must match

Of course, this will make much more sense when we see an example.
. Example 25
mul ply

.Revisit the matrix product we saw at the beginning of this sec on;
[

1
3

2
4

][

]
1 −1 0
.
2 2 −1

Let’s call our ﬁrst matrix A and the second B. We should ﬁrst check
S
to see that we can actually perform this mul plica on. Matrix A is 2 × 2 and B is
2 × 3. The “inner” dimensions match up, so we can compute the product; the “outer”
dimensions tell us that the product will be 2 × 3. Let
[
]
m11 m12 m13
AB =
.
m21 m22 m23
Let’s ﬁnd the value of each of the entries.
The entry m11 is in the ﬁrst row and ﬁrst column; therefore to ﬁnd its value, we
need to mul ply the ﬁrst row of A by the ﬁrst column of B. Thus
[ ]
[
] 1
m11 = 1 2
= 1(1) + 2(2) = 5.
2
So now we know that

[
AB =

5
m21

m12
m22

]
m13
.
m23

Finishing out the ﬁrst row, we have
[
]
[
] −1
m12 = 1 2
= 1(−1) + 2(2) = 3
2
using the ﬁrst row of A and the second column of B, and
[
]
[
] 0
m13 = 1 2
= 1(0) + 2(−1) = −2
−1
55

Chapter 2 Matrix Arithme c
using the ﬁrst row of A and the third column of B. Thus we have
[
]
5
3
−2
AB =
.
m21 m22 m23
To compute the second row of AB, we mul ply with the second row of A. We ﬁnd
[ ]
[
] 1
= 11,
m21 = 3 4
2
[
]
[
] −1
m22 = 3 4
= 5,
2
and

[

m23 = 3 4
[

Thus
AB =

1 2
3 4

][

]

[

0
−1

1 −1 0
2 2 −1

]
= −4.

]

[
=

5 3
11 5

]
−2
.
−4

.
. Example 26

.Mul ply



1 −1 [
1
 5
2 
2
−2 3

]
1 1 1
.
6 7 9

Let’s ﬁrst check to make sure this product is deﬁned. Again calling
S
the ﬁrst matrix A and the second B, we see that A is a 3 × 2 matrix and B is a 2 × 4
matrix; the inner dimensions match so the product is deﬁned, and the product will be
a 3 × 4 matrix,


m11 m12 m13 m14
AB =  m21 m22 m23 m24  .
m31 m32 m33 m34
We will demonstrate how to compute some of the entries, then give the ﬁnal answer. The reader can ﬁll in the details of how each entry was computed.
[ ]
[
] 1
m11 = 1 −1
= −1.
2
[ ]
[
] 1
m13 = 1 −1
= −6.
7
[ ]
] 1
[
m23 = 5 2
= 19.
7
[ ]
[
] 1
m24 = 5 2
= 23.
9
56

2.2 Matrix Mul plica on
[
m32 = −2
[
m34 = −2

[ ]
] 1
= 16.
6
[ ]
] 1
3
= 25.
9
3

So far, we’ve computed this much of AB:

−1 m12 −6

m
19
AB =
21 m22
m31 16 m33


m14
23  .
25

The ﬁnal product is


−1
AB =  9
4

−5
17
16


−6 −8
19 23  .
19 25

.
. Example 27

Mul ply, if possible,
[
][
2 3 4
3
9 8 7
5

]
6
.
−1

Again, we’ll call the ﬁrst matrix A and the second B. Checking the
S
dimensions of each matrix, we see that A is a 2 × 3 matrix, whereas B is a 2 × 2 matrix.
The inner dimensions do not match, therefore this mul plica on is not deﬁned. .
. Example 28
where

.In Example 24, we were told that the product ⃗x⃗u was deﬁned,


−2
[
]
⃗x =  4  and ⃗u = 1 2 3 ,
3

although we were not shown what that product was. Find ⃗x⃗u.
Again, we need to check to make sure the dimensions work corS
rectly (remember that even though we are referring to ⃗u and ⃗x as vectors, they are, in
fact, just matrices).
The column vector ⃗x has dimensions 3 × 1, whereas the row vector ⃗u has dimensions 1 × 3. Since the inner dimensions do match, the matrix product is deﬁned; the
outer dimensions tell us that the product will be a 3 × 3 matrix, as shown below:


m11 m12 m13
⃗x⃗u =  m21 m22 m23  .
m31 m32 m33
To compute the entry m11 , we mul ply the ﬁrst row of ⃗x by the ﬁrst column of ⃗u.
What is the ﬁrst row of ⃗x ? Simply the number −2. What is the ﬁrst column of ⃗u? Just
57

Chapter 2 Matrix Arithme c
the number 1. Thus m11 = −2. (This does seem odd, but through checking, you can
see that we are indeed following the rules.)
What about the entry m12 ? Again, we mul ply the ﬁrst row of ⃗x by the ﬁrst column
of ⃗u; that is, we mul ply −2(2). So m12 = −4.
What about m23 ? Mul ply the second row of ⃗x by the third column of ⃗u; mul ply
4(3), so m23 = 12.
One ﬁnal example: m31 comes from mul plying the third row of ⃗x , which is 3, by
the ﬁrst column of ⃗u, which is 1. Therefore m31 = 3.
So far we have computed


−2 −4 m13
⃗x⃗u =  m21 m22 12  .
3
m32 m33
A er performing all 9 mul plica ons, we ﬁnd


−2 −4 −6
⃗x⃗u =  4
8
12  .
3
6
9
.
In this last example, we saw a “nonstandard” mul plica on (at least, it felt nonstandard). Studying the entries of this matrix, it seems that there are several diﬀerent
pa erns that can be seen amongst the entries. (Remember that mathema cians like
to look for pa erns. Also remember that we o en guess wrong at ﬁrst; don’t be scared
and try to iden fy some pa erns.)
In Sec on 2.1, we iden ﬁed the zero matrix 0 that had a nice property in rela on to
matrix addi on (i.e., A+0 = A for any matrix A). In the following example we’ll iden fy
a matrix that works well with mul plica on as well as some mul plica ve proper es.
For instance, we’ve learned how 1 · A = A; is there a matrix that acts like the number
1? That is, can we ﬁnd a matrix X where X · A = A?4
. Example 29

.Let





1
2 3
1
A =  2 −7 5  , B =  1
−2 −8 3
1



1 0 2
1 0
C = 2 1 0, I = 0 1
0 2 1
0 0

1
1
1


1
1
1


0
0.
1

Find the following products.

4 We made

58

1. AB

3. A03×4

5. IA

7. BC

2. BA

4. AI

6. I2

8. B2

a guess in Sec on 2.1 that maybe a matrix of all 1s would work.

2.2 Matrix Mul plica on
S

We will ﬁnd each product, but we leave the details of each computaon to the reader.


 

1
2 3
1 1 1
6
6
6
0
0 
1. AB =  2 −7 5   1 1 1  =  0
−2 −8 3
1 1 1
−7 −7 −7


 

1 1 1
1
2 3
1 −13 11
2. BA =  1 1 1   2 −7 5  =  1 −13 11 
1 1 1
−2 −8 3
1 −13 11
3. A03×4 = 03×4 .


 
1
2 3
1 0 0
1
4. AI =  2 −7 5   0 1 0  =  2
−2 −8 3
0 0 1
−2


 
1 0 0
1
2 3
1
5. IA =  0 1 0   2 −7 5  =  2
0 0 1
−2 −8 3
−2


2 3
−7 5 
−8 3

2 3
−7 5 
−8 3

6. We haven’t formally deﬁned what I2 means, but we could probably make the
reasonable guess that I2 = I · I. Thus


 

1 0 0
1 0 0
1 0 0
I2 =  0 1 0   0 1 0  =  0 1 0 
0 0 1
0 0 1
0 0 1



 

1 1 1
1 0 2
3 3 3
7. BC =  1 1 1   2 1 0  =  3 3 3 
1 1 1
0 2 1
3 3 3


 

1 1 1
1 1 1
3 3 3
8. B2 = BB =  1 1 1   1 1 1  =  3 3 3 
.
1 1 1
1 1 1
3 3 3
This example is simply chock full of interes ng ideas; it is almost hard to think about
where to start.
Interes ng Idea #1: No ce that in our example, AB ̸= BA! When dealing with
numbers, we were used to the idea that ab = ba. With matrices, mul plica on is
not commuta ve. (Of course, we can ﬁnd special situa ons where it does work. In
general, though, it doesn’t.)
Interes ng Idea #2: Right before this example we wondered if there was a matrix
that “acted like the number 1,” and guessed it may be a matrix of all 1s. However,
we found out that such a matrix does not work in that way; in our example, AB ̸= A.
We did ﬁnd that AI = IA = A. There is a Mul plica ve Iden ty; it just isn’t what we
thought it would be. And just as 12 = 1, I2 = I.
Interes ng Idea #3: When dealing with numbers, we are very familiar with the
no on that “If ax = bx, then a = b.” (As long as x ̸= 0.) No ce that, in our example,
59

Chapter 2 Matrix Arithme c
BB = BC, yet B ̸= C. In general, just because AX = BX, we cannot conclude that A = B.
Matrix mul plica on is turning out to be a very strange opera on. We are very
used to mul plying numbers, and we know a bunch of proper es that hold when using
this type of mul plica on. When mul plying matrices, though, we probably ﬁnd ourselves asking two ques ons, “What does work?” and “What doesn’t work?” We’ll answer these ques ons; ﬁrst we’ll do an example that demonstrates some of the things
that do work.
. Example 30

.Let
[
A=

1
3

]
2
,
4

[
B=

1 1
1 −1

]

[
and

C=

]
1
.
2

2
1

Find the following:
1. A(B + C)

3. A(BC)

2. AB + AC

4. (AB)C

We’ll compute each of these without showing all the intermediate
steps. Keep in mind order of opera ons: things that appear inside of parentheses
are computed ﬁrst.
S

1.
[
A(B + C) =
[
=
[

1 2
3 4
1 2
3 4

] ([
][

=

7
4
17 10

[

][

1
1

3 2
2 1
]

] [
1
2
+
−1
1
]

1
2

])

2.
AB + AC =
[
=
[
=

60

] [
][
1 1
1 2
2
+
1 −1
3 4
1
] [
]
−1
4
5
+
−1
10 11
]
4
10

1 2
3 4
3
7
7
17

1
2

]

2.2 Matrix Mul plica on
3.
[
A(BC) =
[
=
[
=

1 2
3 4
1
3
5
13

] ([

1
1

1
−1
][
]
2
3 3
4
1 −1
]
1
5

][

2 1
1 2

])

4.
([
(AB) C =
[
=
[
=

1
3

3
7
5
13

2
4

][

1
1

1
−1
][
]
−1
2 1
−1
1 2
]
1
5

]) [

2 1
1 2

]

.
In looking at our example, we should no ce two things. First, it looks like the “distribu ve property” holds; that is, A(B + C) = AB + AC. This is nice as many algebraic
techniques we have learned about in the past (when doing “ordinary algebra”) will s ll
work. Secondly, it looks like the “associa ve property” holds; that is, A(BC) = (AB)C.
This is nice, for it tells us that when we are mul plying several matrices together, we
don’t have to be par cularly careful in what order we mul ply certain pairs of matrices
together.5
In leading to an important theorem, let’s deﬁne a matrix we saw in an earlier example.6
.
Deﬁni on 15

Iden ty Matrix

.

The n × n matrix with 1’s on the diagonal and zeros elsewhere is the n × n iden ty matrix, denoted In . When the
context makes the dimension of the iden ty clear, the subscript is generally omi ed.

Note that while the zero matrix can come in all diﬀerent shapes and sizes, the
5 Be careful:

in compu ng ABC together, we can ﬁrst mul ply AB or BC, but we cannot change the order
in which these matrices appear. We cannot mul ply BA or AC, for instance.
6 The following deﬁni on uses a term we won’t deﬁne un l Deﬁni on 20 on page 123: diagonal. In short,
a “diagonal matrix” is one in which the only nonzero entries are the “diagonal entries.” The examples given
here and in the exercises should suﬃce un l we meet the full deﬁni on later.

61

Chapter 2 Matrix Arithme c
iden ty matrix is always a square matrix. We show a few iden ty matrices below.




1 0 0 0
[
]
1 0 0
0 1 0 0
1 0

I2 =
, I3 =  0 1 0  , I4 = 
0 0 1 0
0 1
0 0 1
0 0 0 1
In our examples above, we have seen examples of things that do and do not work.
We should be careful about what examples prove, though. If someone were to claim
that AB = BA is always true, one would only need to show them one example where
they were false, and we would know the person was wrong. However, if someone
claims that A(B + C) = AB + AC is always true, we can’t prove this with just one
example. We need something more powerful; we need a true proof.
In this text, we forgo most proofs. The reader should know, though, that when
we state something in a theorem, there is a proof that backs up what we state. Our
jus ﬁca on comes from something stronger than just examples.
Now we give the good news of what does work when dealing with matrix mul plica on.
.
Theorem 3

Proper es of Matrix Mul plica on
Let A, B and C be matrices with dimensions so that the following opera ons make sense, and let k be a scalar. The
following equali es hold:

.

1. A(BC) = (AB)C (Associa ve Property)
2. A(B + C) = AB + AB and
(B + C)A = BA + CA (Distribu ve Property)
3. k(AB) = (kA)B = A(kB)
4. AI = IA = A

The above box contains some very good news, and probably some very surprising
news. Matrix mul plica on probably seems to us like a very odd opera on, so we
probably wouldn’t have been surprised if we were told that A(BC) ̸= (AB)C. It is a
very nice thing that the Associa ve Property does hold.
As we near the end of this sec on, we raise one more issue of nota on. We deﬁne
A0 = I. If n is a posi ve integer, we deﬁne
An = |A · A ·{z· · · · A} .
n mes

With numbers, we are used to a−n = a1n . Do nega ve exponents work with matrices, too? The answer is yes, sort of. We’ll have to be careful, and we’ll cover the topic
62

2.2 Matrix Mul plica on
in detail once we deﬁne the inverse of a matrix. For now, though, we recognize the
fact that A−1 ̸= A1 , for A1 makes no sense; we don’t know how to “divide” by a matrix.
We end this sec on with a reminder of some of the things that do not work with
matrix mul plica on. The good news is that there are really only two things on this
list.
1. Matrix mul plica on is not commuta ve; that is, AB ̸= BA.
2. In general, just because AX = BX, we cannot conclude that A = B.
The bad news is that these ideas pop up in many places where we don’t expect them.
For instance, we are used to
(a + b)2 = a2 + 2ab + b2 .
What about (A + B)2 ? All we’ll say here is that
(A + B)2 ̸= A2 + 2AB + B2 ;
we leave it to the reader to ﬁgure out why.
The next sec on is devoted to visualizing column vectors and “seeing” how some
of these arithme c proper es work together.

Exercises 2.2
In Exercises 1 – 12, row and column vectors ⃗u
and⃗v are deﬁned. Find the product ⃗u⃗v, where
possible.
]
[
[
]
−2
⃗v =
1. ⃗u = 1 −4
5
]
[
[
]
7
⃗v =
2. ⃗u = 2 3
−4
[ ]
[
]
3
⃗v =
3. ⃗u = 1 −1
3
[
]
[
]
0.6
⃗v =
4. ⃗u = 0.6 0.8
0.8


2
[
]
5. ⃗u = 1 2 −1 ⃗v =  1 
−1


−1
[
]
6. ⃗u = 3 2 −2 ⃗v =  0 
9
 
2
[
]
7. ⃗u = 8 −4 3 ⃗v =  4 
5

8.
9.

10.

11.

12.


1
⃗u = −3 6 1 ⃗v =  −1 
1
[
]
⃗u = 1 2 3 4


1
 −1 

⃗v = 
 1 
−1
[
]
⃗u = 6 2 −1 2
 
3
2

⃗v =  
9
5
[ ]
[
]
3
⃗u = 1 2 3
⃗v =
2
 
1
[
]
⃗u = 2 −5
⃗v =  1 
1
[

]



In Exercises 13 – 27, matrices A and B are deﬁned.

63

Chapter 2 Matrix Arithme c
(a) Give the dimensions of A and B. If the
dimensions properly match, give the
dimensions of AB and BA.
(b) Find the products AB and BA, if possible.
]
[
]
[
1
2
2
5
B=
13. A =
−1 4
3 −1
[
]
[
]
3 7
1 −1
14. A =
B=
2 5
3 −3
[
]
3 −1
15. A =
2
2
[
]
1 0 7
B=
4 2 9


0
1
−1 
16. A =  1
−2 −4
[
]
−2 0
B=
3
8
]
[
9
4
3
17. A =
9 −5 9
]
[
−2
5
B=
−2 −1


−2 −1
−5 
18. A =  9
3
−1
[
]
−5 6 −4
B=
0
6 −3


2
6
2 
19. A =  6
5 −1
]
[
−4 5
0
B=
−4 4 −4


−5
2
20. A =  −5 −2 
−5 −4
[
]
0
−5
6
B=
−5 −3 −1


8 −2
5 
21. A =  4
2 −5
[
]
−5 1 −5
B=
8
3 −2

64

[
22. A =
[
B=
[
23. A =
[

25.

26.

27.

1
−2
−1
6

4
6

]
−1
1
]
5
7

5
−2

−3
−5

−5
3

5
−5

−4 −4
−5 −1


−1 2
1
A =  −1 2 −1 
0
0 −2


0 0 −2
B =  1 2 −1 
1 0
0


−1
1
1
A =  −1 −1 −2 
1
1
−2


−2 −2 −2
−2
0 
B= 0
−2
0
2


−4
3
3
A =  −5 −1 −5 
−5
0
−1


0
5
0
B =  −5 −4 3 
5
−4 3


−4 −1 3
−3 5 
A= 2
1
5
3


−2 4
3
B =  −1 1 −1 
4
0
2
B=

24.

1
7

]

]

In Exercises 28 – 33, a diagonal matrix D and a
matrix A are given. Find the products DA and
AD, where possible.
[
]
3
0
28. D =
0 −1
[
]
2 4
A=
6 8
[
]
4
0
29. D =
0 −3
[
]
1 2
A=
1 2

2.2 Matrix Mul plica on


30.

31.

32.

33.


−1 0 0
2 0
D= 0
0
0 3


1 2 3
A=4 5 6
7 8 9


1
1
1
2
2 
D= 2
−3 −3 −3


2
0
0
A =  0 −3 0 
0
0
5
[
]
d1 0
D=
0 d2
]
[
a b
A=
c d


d1 0
0
D =  0 d2 0 
0
0 d3


a b c
A=d e f
g h i

In Exercises 34 – 39, a matrix A and a vector ⃗x
are given. Find the product A⃗x .
[
]
[ ]
2
3
4
34. A =
, ⃗x =
1 −1
9
[
]
[
]
−1 4
2
⃗
35. A =
, x=
7
3
−1

 

1
2
0
3
1
1 , ⃗x =  4 
36. A =  1
2
3 −1 2

 

4
−2 0
3
1 −2 , ⃗x =  3 
37. A =  1
1
4
2 −1
]
[ ]
[
x1
2 −1
, ⃗x =
38. A =
4
3
x2



1
39. A =  1
2
[
40. Let A =
[
41. Let A =

2
0
3


 
3
x1
2 , ⃗x =  x2 
1
x3

0
1

]
1
. Find A2 and A3 .
0

2
0

]
0
. Find A2 and A3 .
3




−1 0 0
3 0 . Find A2 and
42. Let A =  0
0
0 5
A3 .


0 1 0
43. Let A =  0 0 1 . Find A2 and A3 .
1 0 0


0
44. Let A =  0
0

0
0
1


1
0 . Find A2 and A3 .
0

45. In the text we state that (A + B)2 ̸=
A2 +2AB+B2 . We inves gate that claim
here.
[
]
5
3
(a) Let A =
and let B =
−3 −2
]
[
−5 −5
. Compute A + B.
−2
1
(b) Find (A + B)2 by using your answer from (a).
(c) Compute A2 + 2AB + B2 .
(d) Are the results from (a) and (b)
the same?
(e) Carefully expand the expression
(A + B)2 = (A + B)(A + B)
and show why this is not equal to
A2 + 2AB + B2 .

65

Chapter 2 Matrix Arithme c

2.3 Visualizing Matrix Arithme c in 2D

.

.
AS YOU READ
...
1. T/F: Two vectors with the same length and direc on are equal even if they start
from diﬀerent places.
2. One can visualize vector addi on using what law?
3. T/F: Mul plying a vector by 2 doubles its length.
4. What do mathema cians do?
5. T/F: Mul plying a vector by a matrix always changes its length and direc on.

When we ﬁrst learned about adding numbers together, it was useful to picture a
number line: 2 + 3 = 5 could be pictured by star ng at 0, going out 2 ck marks, then
another 3, and then realizing that we moved 5 ck marks from 0. Similar visualiza ons
helped us understand what 2 − 3 meant and what 2 × 3 meant.
We now inves gate a way to picture matrix arithme c – in par cular, opera ons
involving column vectors. This not only will help us be er understand the arithme c
opera ons, it will open the door to a great wealth of interes ng study. Visualizing
matrix arithme c has a wide variety of applica ons, the most common being computer
graphics. While we o en think of these graphics in terms of video games, there are
numerous other important applica ons. For example, chemists and biologists o en
use computer models to “visualize” complex molecules to “see” how they interact with
other molecules.
We will start with vectors in two dimensions (2D) – that is, vectors with only two
entries. We assume the reader is familiar with the Cartesian plane, that is, plo ng
points and graphing func ons on “the x–y plane.” We graph vectors in a manner very
similar to plo ng points. Given the vector
[ ]
1
⃗x =
,
2
we draw ⃗x by drawing an arrow whose p is 1 unit to the right and 2 units up from its
origin.7

1
.
1

. drawings of ⃗x
Figure 2.1: Various
66

7 To help

reduce clu er, in all ﬁgures each ck mark represents one unit.

2.3 Visualizing Matrix Arithme c in 2D
When drawing vectors, we do not specify where you start drawing; all we specify
is where the p lies based on where we started. Figure 2.1 shows vector ⃗x drawn 3
ways. In some ways, the “most common” way to draw a vector has the arrow start at
the origin,but this is by no means the only way of drawing the vector.
Let’s prac ce this concept by drawing various vectors from given star ng points.
. Example 31

Let
[
⃗x =

1
−1

]

[ ]
2
⃗y =
3

[
and

⃗z =

]
−3
.
2

Draw ⃗x star ng from the point (0, −1); draw ⃗y star ng from the point (−1, −1), and
draw⃗z star ng from the point (2, −1).
To draw ⃗x , start at the point (0, −1) as directed, then move to the
S
right one unit and down one unit and draw the p. Thus the arrow “points” from
(0, −1) to (1, −2).
To draw ⃗y, we are told to start and the point (−1, −1). We draw the p by moving
to the right 2 units and up 3 units; hence ⃗y points from (−1, −1) to (1,2).
To draw ⃗z, we start at (2, −1) and draw the p 3 units to the le and 2 units up; ⃗z
points from (2, −1) to (−1, 1).
Each vector is drawn as shown in Figure 2.2.
⃗y
⃗z

1
.
2
⃗x

.

Figure 2.2: Drawing vectors. ⃗x , ⃗y and ⃗z in Example 31

[ ]
0 8
? Following our basic procedure,
0
we start by going 0 units in the x direc on, followed by 0 units in the y direc on. In
other words, we don’t go anywhere. In general, we don’t actually draw ⃗0. At best,
one can draw a dark circle at the origin to convey the idea that ⃗0, when star ng at the
origin, points to the origin.
In sec on 2.1 we learned about matrix arithme c opera ons: matrix addi on and
scalar mul plica on. Let’s inves gate how we can “draw” these opera ons.
How does one draw the zero vector, ⃗0 =

8 Vectors are just special types of matrices. The zero vector, ⃗
0, is a special type of zero matrix, 0. It helps
to dis nguish the two by using diﬀerent nota on.

67

Chapter 2 Matrix Arithme c

Vector Addi on
Given two vectors ⃗x and ⃗y, how do we draw the vector ⃗x + ⃗y? Let’s look at this in
the context of an example, then study the result.
. Example 32

Let
[ ]
1
⃗x =
1

and

[ ]
3
⃗y =
.
1

Sketch ⃗x , ⃗y and ⃗x + ⃗y.
A star ng point for drawing each vector was not given; by default,
[ ]
3
we’ll start at the origin. (This is in many ways nice; this means that the vector
1
“points” to the point (3,1).) We ﬁrst compute ⃗x + ⃗y:
S

⃗x + ⃗y =

[ ] [ ] [ ]
1
3
4
+
=
1
1
2

Sketching each gives the picture in Figure 2.3.

⃗x + ⃗y
⃗x

1

⃗y
.
1

. ⃗x and ⃗y in Example 32
Figure 2.3: Adding vectors
.
This example is pre y basic; we were given two vectors, told to add them together,
then sketch all three vectors. Our job now is to go back and try to see a rela onship
between the drawings of ⃗x , ⃗y and ⃗x + ⃗y. Do you see any?
Here is one way of interpre ng the adding of ⃗x to ⃗y. Regardless of where we start,
we draw ⃗x . Now, from the p of ⃗x , draw ⃗y. The vector ⃗x + ⃗y is the vector found
by drawing an arrow from the origin of ⃗x to the p of ⃗y. Likewise, we could start by
drawing ⃗y. Then, star ng from the p of ⃗y, we can draw ⃗x . Finally, draw ⃗x + ⃗y by
drawing the vector that starts at the origin of ⃗y and ends at the p of ⃗x .
The picture in Figure 2.4 illustrates this. The gray vectors demonstrate drawing the
second vector from the p of the ﬁrst; we draw the vector ⃗x +⃗y dashed to set it apart
from the rest. We also lightly ﬁlled the parallelogram whose opposing sides are the
68

2.3 Visualizing Matrix Arithme c in 2D
vectors ⃗x and ⃗y. This highlights what is known as the Parallelogram Law.
⃗x + ⃗y
⃗y
⃗x

1

⃗x
⃗y

.
1

.
Figure 2.4: Adding vectors graphically
using the Parallelogram Law

.
Key Idea 5

Parallelogram Law

.

To draw the vector ⃗x + ⃗y, one can draw the parallelogram
with ⃗x and ⃗y as its sides. The vector that points from the
vertex where ⃗x and ⃗y originate to the vertex where ⃗x and ⃗y
meet is the vector ⃗x + ⃗y.

Knowing all of this allows us to draw the sum of two vectors without knowing
speciﬁcally what the vectors are, as we demonstrate in the following example.
. Example 33
vector ⃗x + ⃗y.

Consider the vectors ⃗x and ⃗y as drawn in Figure 2.5. Sketch the

S

⃗y

⃗x
.

Figure 2.5: Vectors ⃗x .and ⃗y in Example 33
We’ll apply the Parallelogram Law, as given in Key Idea 5. As before, we draw ⃗x +⃗y
dashed to set it apart. The result is given in Figure 2.6.
⃗x + ⃗y
⃗y

⃗x
.

. ⃗x + ⃗y in Example 33
Figure 2.6: Vectors ⃗x , ⃗y and
.

69

Chapter 2 Matrix Arithme c

Scalar Mul plica on
A er learning about matrix addi on, we learned about scalar mul plica on. We
apply that concept now to vectors and see how this is represented graphically.
. Example 34

Let
⃗x =

[ ]
1
1

[
and

⃗y =

]
−2
.
1

Sketch ⃗x , ⃗y, 3⃗x and −1⃗y.
S

We begin by compu ng 3⃗x and −⃗y:
[ ]
[
]
3
2
3⃗x =
and − ⃗y =
.
3
−1

All four vectors are sketched in Figure 2.7.

3⃗x
⃗y

⃗x

1
.

1

.

−⃗y

. x and −⃗y in Example 34
Figure 2.7: Vectors ⃗x , ⃗y, 3⃗

As we o en do, let us look at the previous example and see what we can learn from
it. We can see that ⃗x and 3⃗x point in the same direc on (they lie on the same line), but
3⃗x is just longer than ⃗x . (In fact, it looks like 3⃗x is 3 mes longer than ⃗x . Is it? How do
we measure length?)
We also see that ⃗y and −⃗y seem to have the same length and lie on the same line,
but point in the opposite direc on.
A vector inherently conveys two pieces of informa on: length and direc on. Mulplying a vector by a posi ve scalar c stretches the vectors by a factor of c; mul plying
by a nega ve scalar c both stretches the vector and makes it point in the opposite
direc on.
Knowing this, we can sketch scalar mul ples of vectors without knowing speciﬁcally what they are, as we do in the following example.
. Example 35
70

.Let vectors ⃗x and ⃗y be as in Figure 2.8. Draw 3⃗x , −2⃗x , and 21⃗y.

2.3 Visualizing Matrix Arithme c in 2D
⃗y
⃗x
.
Figure 2.8: Vectors ⃗x .and ⃗y in Example 35

To draw 3⃗x , we draw a vector in the same direc on as ⃗x , but 3 mes
as long. To draw −2⃗x , we draw a vector twice as long as ⃗x in the opposite direc on;
to draw 21⃗y, we draw a vector half the length of ⃗y in the same direc on as ⃗y. We again
use the default of drawing all the vectors star ng at the origin. All of this is shown in
Figure 2.9.
S

3⃗x

⃗y

1
⃗
2y

.

⃗x

−2⃗x

.

Figure 2.9: Vectors ⃗x , ⃗y, 3⃗x ,. −2x and 12 ⃗x in Example 35

Vector Subtrac on
The ﬁnal basic opera on to consider between two vectors is that of vector subtrac on: given vectors ⃗x and ⃗y, how do we draw ⃗x − ⃗y?
If we know explicitly what ⃗x and ⃗y are, we can simply compute what ⃗x − ⃗y is and
then draw it. We can also think in terms of vector addi on and scalar mul plica on:
we can add the vectors ⃗x + (−1)⃗y. That is, we can draw ⃗x and draw −⃗y, then add them
as we did in Example 33. This is especially useful we don’t know explicitly what ⃗x and
⃗y are.
. Example 36

.Let vectors ⃗x and ⃗y be as in Figure 2.10. Draw ⃗x − ⃗y.
⃗y
.

⃗x

Figure 2.10: Vectors ⃗x. and ⃗y in Example 36
71

Chapter 2 Matrix Arithme c
To draw ⃗x − ⃗y, we will ﬁrst draw −⃗y and then apply the Parallelogram
S
Law to add ⃗x to −⃗y. See Figure 2.11.

⃗y
.

⃗x
⃗x − ⃗y
−⃗y

. ⃗x − ⃗y in Example 36
Figure 2.11: Vectors ⃗x , ⃗y and

.

In Figure 2.12, we redraw Figure 2.11 from Example 36 but remove the gray vectors
that tend to add clu er, and we redraw the vector⃗x −⃗y do ed so that it starts from the
p of ⃗y.9 Note that the do ed version of ⃗x − ⃗y points from ⃗y to ⃗x . This is a “shortcut”
to drawing ⃗x − ⃗y; simply draw the vector that starts at the p of ⃗y and ends at the p
of ⃗x . This is important so we make it a Key Idea.
⃗y

⃗x − ⃗y
⃗x
.
⃗x − ⃗y
-⃗y

.
Figure 2.12: Redrawing
vector ⃗x − ⃗y

.
Key Idea 6

Vector Subtrac on

.

To draw the vector ⃗x −⃗y, draw ⃗x and ⃗y so that they have the
same origin. The vector ⃗x − ⃗y is the vector that starts from
the p of ⃗y and points to the p of ⃗x .

Let’s prac ce this once more with a quick example.
. Example 37

.Let ⃗x and ⃗y be as in Figure ?? (a). Draw ⃗x − ⃗y.

We simply apply Key Idea 6: we draw an arrow from ⃗y to ⃗x . We do
S
so in Figure 2.13; ⃗x − ⃗y is dashed.
72

9 Remember that we can draw vectors star

ng from anywhere.

2.3 Visualizing Matrix Arithme c in 2D
⃗y

⃗y

.
⃗x − ⃗y
⃗x

⃗x

(a)

(b)

. ⃗x − ⃗y in Example 37
Figure 2.13: Vectors ⃗x , ⃗y and

.

Vector Length
When we discussed scalar mul plica on, we made reference to a fundamental
ques on: How do we measure the length of a vector? Basic geometry gives us an
answer in the two dimensional case that we are dealing with right now, and later we
can extend these ideas to higher dimensions.
Consider Figure 2.14. A vector ⃗x is drawn in black, and dashed and do ed lines
have been drawn to make it the hypotenuse of a right triangle.
⃗x

1
.
1
Figure 2.14: Measuring. the length of a vector
It is easy to see that the dashed line has length 4 and the do ed line has length 3.
We’ll let c denote the length of⃗x ; according to the Pythagorean Theorem, 42 +32 = c2 .
Thus c2 = 25 and we quickly deduce that c = 5.
No ce that in our ﬁgure, ⃗x goes to the right 4 units and then up 3 units. In other
words, we can write
[ ]
4
⃗x =
.
3
√
We learned above that the length of ⃗x is 42 + 32 .10 This hints at a basic calcula on
that works for all vectors ⃗x , and we deﬁne the length of a vector according to this rule.
10 Remember

that

√

42 + 32 ̸= 4 + 3!

73

Chapter 2 Matrix Arithme c

.
Deﬁni on 16

Vector Length
[

Let
⃗x =

]
x1
.
x2

.

The length of ⃗x , denoted ||⃗x ||, is
√
||⃗x || = x21 + x22 .

. Example 38

Find the length of each of the vectors given below.
[ ]
[
]
[ ]
[ ]
1
2
.6
3
x⃗1 =
x⃗2 =
x⃗3 =
x⃗4 =
1
−3
.8
0
We apply Deﬁni on 16 to each vector.

S

√
√
12 + 12 = 2.
√
√
||⃗
x2 || = 22 + (−3)2 = 13.
√
√
||⃗
x3 || = .62 + .82 = .36 + .64 = 1.
√
||⃗
x4 || = 32 + 0 = 3.
||⃗
x1 || =

.

Now that we know how to compute the length of a vector, let’s revisit a statement
we made as we explored Examples 34 and 35: “Mul plying a vector by a posi ve scalar
c stretches the vectors by a factor of c . . .” At that me, we did not know how to measure the length of a vector, so our statement was unfounded. In the following example,
we will conﬁrm the truth of our previous statement.
[
]
2
. Example 39
.Let ⃗x =
. Compute ||⃗x ||, ||3⃗x ||, || − 2⃗x ||, and ||c⃗x ||, where c
−1
is a scalar.
We apply Deﬁni on 16 to each of the vectors.

S

||⃗x || =

√
√
4 + 1 = 5.

Before compu ng the length of ||3⃗x ||, we note that 3⃗x =
||3⃗x || =
74

√
√
√
36 + 9 = 45 = 3 5 = 3||⃗x ||.

[

]
6
.
−3

2.3 Visualizing Matrix Arithme c in 2D
[
Before compu ng the length of || − 2⃗x ||, we note that −2⃗x =

]
−4
.
2

√
√
√
16 + 4 = 20 = 2 5 = 2||⃗x ||.
]
[
2c
Finally, to compute ||c⃗x ||, we note that c⃗x =
. Thus:
−c
√
√
√
√
||c⃗x || = (2c)2 + (−c)2 = 4c2 + c2 = 5c2 = |c| 5.
|| − 2⃗x || =

This last line is true because the square
root of any number squared is the absolute
√
value of that number (for example, (−3)2 = 3). .
The last computa on of our example is the most important one. It shows that,
in general, mul plying a vector ⃗x by a scalar c stretches ⃗x by a factor of |c| (and the
direc on will change if c is nega ve). This is important so we’ll make it a Theorem.
.
Theorem 4

Vector Length and Scalar Mul plica on

.

Let ⃗x be a vector and let c be a scalar. Then the length of c⃗x
is
||c⃗x || = |c| · ||⃗x ||.

Matrix − Vector Mul plica on
The last arithme c opera on to consider visualizing is matrix mul plica on. Specifically, we want to visualize the result of mul plying a vector by a matrix. In order to
mul ply a 2D vector by a matrix and get a 2D vector back, our matrix must be a square,
2 × 2 matrix.11
We’ll start with an example. Given a matrix A and several vectors, we’ll graph the
vectors before and a er they’ve been mul plied by A and see what we learn.
. Example 40

.Let A be a matrix, and ⃗x , ⃗y, and⃗z be vectors as given below.
[
A=

1
2

]
4
,
3

⃗x =

[ ]
1
,
1

[
⃗y =

]
−1
,
1

[
⃗z =

3
−1

]

Graph ⃗x , ⃗y and⃗z, as well as A⃗x , A⃗y and A⃗z.
S
11 We can mul ply a 3 × 2 matrix by a 2D vector and get a 3D vector back, and this gives very interes ng
results. See sec on 5.2.

75

Chapter 2 Matrix Arithme c
y

A⃗x

A⃗z
⃗y

⃗x

A⃗y

.

x
⃗z

. by a matrix in Example 40.
Figure 2.15: Mul plying vectors
It is straigh orward to compute:
[ ]
[ ]
5
3
A⃗x =
, A⃗y =
,
5
1

[
and

A⃗z =

]
−1
.
3

The vectors are sketched in Figure 2.15 .
There are several things to no ce. When each vector is mul plied by A, the result
is a vector with a diﬀerent length (in this example, always longer), and in two of the
cases (for ⃗y and⃗z), the resul ng vector points in a diﬀerent direc on.
This isn’t surprising. In the previous sec on we learned about matrix mul plicaon, which is a strange and seemingly unpredictable opera on. Would you expect to
see some sort of immediately recognizable pa ern appear from mul plying a matrix
and a vector?12 In fact, the surprising thing from the example is that ⃗x and A⃗x point
in the same direc on! Why does the direc on of ⃗x not change a er mul plica on by
A? (We’ll answer this in Sec on 4.1 when we learn about something called “eigenvectors.”)
Diﬀerent matrices act on vectors in diﬀerent ways.13 Some always increase the
length of a vector through mul plica on, others always decrease the length, others
increase the length of some vectors and decrease the length of others, and others s ll
don’t change the length at all. A similar statement can be made about how matrices
aﬀect the direc on of vectors through mul plica on: some change every vector’s direc on, some change “most” vector’s direc on but leave some the same, and others
s ll don’t change the direc on of any vector.
How do we set about studying how matrix mul plica on aﬀects vectors? We could
just create lots of diﬀerent matrices and lots of diﬀerent vectors, mul ply, then graph,
but this would be a lot of work with very li le useful result. It would be too hard to
ﬁnd a pa ern of behavior in this.14
12 This is

a rhetorical ques on; the expected answer is “No.”
reason we call them “diﬀerent.”
14 Remember, that’s what mathema cians do. We look for pa erns.
13 That’s one

76

2.3 Visualizing Matrix Arithme c in 2D
Instead, we’ll begin by using a technique we’ve employed o en in the past. We
have a “new” opera on; let’s explore how it behaves with “old” opera ons. Speciﬁcally, we know how to sketch vector addi on. What happens when we throw matrix
mul plica on into the mix? Let’s try an example.
. Example 41

.Let A be a matrix and ⃗x and ⃗y be vectors as given below.
]
[ ]
[
]
[
1 1
2
−1
, ⃗x =
, ⃗y =
A=
1 2
1
1

Sketch ⃗x + ⃗y, A⃗x , A⃗y, and A(⃗x + ⃗y).
S

It is pre y straigh orward to compute:
[ ]
[ ]
[ ]
[ ]
1
3
0
3
⃗x + ⃗y =
; A⃗x =
; A⃗y =
, A(⃗x + ⃗y) =
.
2
4
1
5

In Figure 2.16, we have graphed the above vectors and have included dashed gray
vectors to highlight the addi ve nature of ⃗x +⃗y and A(⃗x +⃗y). Does anything strike you
as interes ng?
A(⃗x + ⃗y)

y

A⃗x

⃗x + ⃗y

⃗y

A⃗y

⃗x
.

x

.
Figure 2.16: Vector addi on and matrix
mul plica on in Example 41.
Let’s not focus on things which don’t ma er right now: let’s not focus on how long
certain vectors became, nor necessarily how their direc on changed. Rather, think
about how matrix mul plica on interacted with the vector addi on.
In some sense, we started with three vectors, ⃗x , ⃗y, and ⃗x + ⃗y. This last vector is
special; it is the sum of the previous two. Now, mul ply all three by A. What happens?
We get three new vectors, but the signiﬁcant thing is this: the last vector is s ll the
sum of the previous two! (We emphasize this by drawing do ed vectors to represent
part of the Parallelogram Law.)
77

Chapter 2 Matrix Arithme c
Of course, we knew this already: we already knew that A⃗x + A⃗y = A(⃗x + ⃗y), for
this is just the Distribu ve Property. However, now we get to see this graphically. .
In Sec on 5.1 we’ll study in greater depth how matrix mul plica on aﬀects vectors and the whole Cartesian plane. For now, we’ll se le for simple prac ce: given a
matrix and some vectors, we’ll mul ply and graph. Let’s do one more example.
. Example 42

Let A, ⃗x , ⃗y, and⃗z be as given below.
[
]
[ ]
[
]
1 −1
1
−1
A=
, ⃗x =
, ⃗y =
,
1 −1
1
1

[ ]
4
⃗z =
1

Graph ⃗x , ⃗y and⃗z, as well as A⃗x , A⃗y and A⃗z.
S

y
A⃗z
⃗y

⃗z

⃗x
.

x

A⃗x
A⃗y

. by a matrix in Example 42.
Figure 2.17: Mul plying vectors
It is straigh orward to compute:
[ ]
[
]
0
−2
A⃗x =
, A⃗y =
,
0
−2

and

A⃗z =

[ ]
3
.
3

The vectors are sketched in Figure 2.17.
These results are interes ng. While we won’t explore them in great detail here,
no ce how ⃗x got sent to the zero vector. No ce also that A⃗x , A⃗y and A⃗z are all in a
line (as well as ⃗x !). Why is that? Are ⃗x , ⃗y and⃗z just special vectors, or would any other
vector get sent to the same line when mul plied by A?15 .
This sec on has focused on vectors in two dimensions. Later on in this book, we’ll
extend these ideas into three dimensions (3D).
In the next sec on we’ll take a new idea (matrix mul plica on) and apply it to an
old idea (solving systems of linear equa ons). This will allow us to view an old idea in
a new way – and we’ll even get to “visualize” it.
15 Don’t just sit

78

there, try it out!

2.3 Visualizing Matrix Arithme c in 2D

Exercises 2.3
In Exercises 1 – 4, vectors ⃗x and ⃗y are given.
Sketch⃗x ,⃗y,⃗x +⃗y, and⃗x −⃗y on the same Cartesian axes.
[ ]
[
]
1
−2
1. ⃗x =
, ⃗y =
1
3
[ ]
[
]
3
1
2. ⃗x =
, ⃗y =
1
−2
[
]
[
]
−1
−2
, ⃗y =
3. ⃗x =
1
2
[ ]
[ ]
2
1
4. ⃗x =
, ⃗y =
0
3
In Exercises 5 – 8, vectors ⃗x and ⃗y are drawn.
Sketch 2⃗x , −⃗y, ⃗x + ⃗y, and ⃗x − ⃗y on the same
Cartesian axes.

⃗y
⃗x
1

5.

.
1
6.

1

⃗x
.
1

⃗y
⃗y

⃗x
1

7.

.

the lengths of ⃗x and a⃗x , then compare these
lengths.
[ ]
2
9. ⃗x =
, a = 3.
1
[ ]
4
10. ⃗x =
, a = −2.
7
[
]
−3
11. ⃗x =
, a = −1.
5
[
]
3
12. ⃗x =
, a = 13 .
−9
13. Four pairs of vectors ⃗x and ⃗y are given
below. For each pair, compute ||⃗x ||,
||⃗y||, and ||⃗x +⃗y||. Use this informa on
to answer: Is it always, some mes, or
never true that ||⃗x || + ||⃗y|| = ||⃗x +⃗y||?
If it always or never true, explain why.
If it is some mes true, explain when it
is true.
[ ]
[ ]
2
1
, ⃗y =
(a) ⃗x =
3
1
[
]
[
]
1
3
(b) ⃗x =
, ⃗y =
−2
−6
]
[ ]
[
2
−1
, ⃗y =
(c) ⃗x =
5
3
]
[
[ ]
−4
2
⃗
⃗
,y=
(d) x =
−2
1
In Exercises 14 – 17, a matrix A is given.
Sketch ⃗x , ⃗y, A⃗x and A⃗y on the same Cartesian
axes, where
]
[
[ ]
−1
1
⃗x =
.
and ⃗y =
2
1

1

[
14. A =
⃗x

1

8.

[
15. A =

.
⃗y

[

1

In Exercises 9 – 12, a vector ⃗x and a scalar
a are given. Using Deﬁni on 16, compute

16. A =
[
17. A =

1
2

−1
3

2
−1
1
1
1
−1

1
1

0
3
]

]
]

2
−2

]

79

Chapter 2 Matrix Arithme c

2.4 Vector Solu ons to Linear Systems

.

.
AS YOU READ
...
1. T/F: The equa on A⃗x = ⃗b is just another way of wri ng a system of linear equaons.
2. T/F: In solving A⃗x = ⃗0, if there are 3 free variables, then the solu on will be
“pulled apart” into 3 vectors.
3. T/F: A homogeneous system of linear equa ons is one in which all of the coeﬃcients are 0.
4. Whether or not the equa on A⃗x = ⃗b has a solu on depends on an intrinsic
property of
.

The ﬁrst chapter of this text was spent ﬁnding solu ons to systems of linear equaons. We have spent the ﬁrst two sec ons of this chapter learning opera ons that
can be performed with matrices. One may have wondered “Are the ideas of the ﬁrst
chapter related to what we have been doing recently?” The answer is yes, these ideas
are related. This sec on begins to show that rela onship.
We have o en hearkened back to previous algebra experience to help understand
matrix algebra concepts. We do that again here. Consider the equa on ax = b, where
a = 3 and b = 6. If we asked one to “solve for x,” what exactly would we be asking?
We would want to ﬁnd a number, which we call x, where a mes x gives b; in this case,
it is a number, when mul plied by 3, returns 6.
Now we consider matrix algebra expressions. We’ll eventually consider solving
equa ons like AX = B, where we know what the matrices A and B are and we want to
ﬁnd the matrix X. For now, we’ll only consider equa ons of the type A⃗x = ⃗b, where
we know the matrix A and the vector ⃗b. We will want to ﬁnd what vector ⃗x sa sﬁes
this equa on; we want to “solve for ⃗x .”
To help understand what this is asking, we’ll consider an example. Let




 
1 1 1
2
x1
A =  1 −1 2  , ⃗b =  −3  and ⃗x =  x2  .
2 0 1
1
x3
(We don’t know what ⃗x is, so we have to represent it’s entries with the variables x1 , x2
and x3 .) Let’s “solve for ⃗x ,” given the equa on A⃗x = ⃗b.
We can mul ply out the le hand side of this equa on. We ﬁnd that


x1 + x2 + x3
A⃗x =  x1 − x2 + 2x3  .
2x1 + x3
Be sure to note that the product is just a vector; it has just one column.
80

2.4 Vector Solu ons to Linear Systems
Since A⃗x is equal to ⃗b, we have

 

x1 + x2 + x3
2
 x1 − x2 + 2x3  =  −3  .
2x1 + x3
1
Knowing that two vectors are equal only when their corresponding entries are equal,
we know
x1 + x2 + x3 = 2
x1 − x2 + 2x3 = −3
2x1 + x3 = 1.
This should look familiar; it is a system of linear equa ons! Given the matrix-vector
equa on A⃗x = ⃗b, we can recognize A as the coeﬃcient matrix from a linear system
and ⃗b as the vector of the constants from the linear system. To solve a matrix–vector
equa on (and the corresponding linear system), we simply augment the matrix A with
the vector ⃗b, put this matrix into reduced row echelon form, and interpret the results.
We convert the above linear system into an augmented matrix and ﬁnd the reduced
row echelon form:




1 0 0 1
1 1 1 2
−
→
 1 −1 2 −3  rref  0 1 0 2  .
0 0 1 −1
2 0 1 1
This tells us that x1 = 1, x2 = 2 and x3 = −1, so


1
⃗x =  2  .
−1
We should check our work; mul ply out A⃗x and verify that we indeed get ⃗b:





2
1
1 1 1
 1 −1 2   2  does equal  −3  .
1
−1
2 0 1
We should prac ce.
. Example 43

.Solve the equa

1
A =  −1
1

on A⃗x = ⃗b for ⃗x where



2 3
5
2 1  and  −1  .
1 0
2

The solu on is rather straigh orward, even
S
[ though
] we did a lot of
work before to ﬁnd the answer. Form the augmented matrix A ⃗b and interpret its
reduced row echelon form.




1 2 3 5
1 0 0 2
−
→
 −1 2 1 −1  rref  0 1 0 0 
1 1 0 2
0 0 1 1

81

Chapter 2 Matrix Arithme c
In previous sec ons we were ﬁne sta ng that the result as
x1 = 2,

x2 = 0,

x3 = 1,

but we were asked to ﬁnd ⃗x ; therefore, we state the solu on as
 
2
⃗x =  0  .
1
.
This probably seems all well and good. While asking one to solve the equa on
A⃗x = ⃗b for ⃗x seems like a new problem, in reality it is just asking that we solve a
system of linear equa ons. Our variables x1 , etc., appear not individually but as the
entries of our vector ⃗x . We are simply wri ng an old problem in a new way.
In line with this new way of wri ng the problem, we have a new way of wri ng
the solu on. Instead of lis ng, individually, the values of the unknowns, we simply list
them as the elements of our vector ⃗x .
These are important ideas, so we state the basic principle once more: solving the
equa on A⃗x = ⃗b for ⃗x is the same thing as solving a linear system of equa ons. Equivalently, any system of linear equa ons can be wri en in the form A⃗x = ⃗b for some
matrix A and vector ⃗b.
Since these ideas are equivalent, we’ll refer to A⃗x = ⃗b both as a matrix–vector
equa on and as a system of linear equa ons: they are the same thing.
We’ve seen two examples illustra ng this idea so far, and in both cases the linear
system had exactly one solu on. We know from Theorem 1 that any linear system has
either one solu on, inﬁnite solu ons, or no solu on. So how does our new method of
wri ng a solu on work with inﬁnite solu ons and no solu ons?
Certainly, if A⃗x = ⃗b has no solu on, we simply say that the linear system has no
solu on. There isn’t anything special to write. So the only other op on to consider is
the case where we have inﬁnite solu ons. We’ll learn how to handle these situa ons
through examples.
. Example 44
.Solve the linear system A⃗x = ⃗0 for ⃗x and write the solu on in
vector form, where
[
]
[ ]
1 2
0
⃗
A=
and 0 =
.
2 4
0
S

(Note: we didn’t really need to specify that
[ ]
⃗0 = 0 ,
0

but we did just to eliminate any uncertainty.)
To solve this system, put the augmented matrix into reduced row echelon form,
which we do below.
[
]
[
]
−→
1 2 0
1 2 0
rref
2 4 0
0 0 0
82

2.4 Vector Solu ons to Linear Systems
We interpret the reduced row echelon form of this matrix to write the solu on as
x1 = −2x2
x2 is free.
We are not done; we need to write the solu on in vector form, for our solu on is the
vector ⃗x . Recall that
[ ]
x
⃗x = 1 .
x2
From above we know that x1 = −2x2 , so we replace the x1 in ⃗x with −2x2 . This gives
our solu on as
[
]
−2x2
⃗x =
.
x2
Now we pull the x2 out of the vector (it is just a scalar) and write ⃗x as
[
]
−2
⃗x = x2
.
1
For reasons that will become more clear later, set
[
]
−2
⃗v =
.
1
Thus our solu on can be wri en as
⃗x = x2⃗v.
.Recall that since our system was consistent and had a free variable, we have inﬁnite
solu ons. This form of the solu on highlights this fact; pick any value for x2 and we
get a diﬀerent solu on.
For instance, by se ng x2 = −1, 0, and 5, we get the solu ons
[
]
[ ]
[
]
2
0
−10
⃗x =
,
, and
,
−1
0
5
respec vely.
We should check our work; mul ply each of the above vectors by A to see if we
indeed get ⃗0.
We have oﬃcially solved this problem; we have found the solu on to A⃗x = ⃗0 and
wri en it properly. One ﬁnal thing we will do here is graph the solu on, using our skills
learned in the previous sec on.
Our solu on is
[
]
−2
⃗x = x2
.
1
[
]
−2
This means that any scalar mul ply of the vector ⃗v =
is a solu on; we know
1
how to sketch the scalar mul ples of ⃗v. This is done in Figure 2.18.
83

Chapter 2 Matrix Arithme c
y

⃗v
.

x

.
Figure 2.18: The solu on, as a line,
to A⃗x = ⃗0 in Example 44.
Here vector ⃗v is drawn as well as the line that goes through the origin in the direcon of⃗v. Any vector along this line is a solu on. So in some sense, we can say that the
solu on to A⃗x = ⃗0 is a line. .
Let’s prac ce this again.
. Example 45
form, where

.Solve the linear system A⃗x = ⃗0 and write the solu on in vector
[
]
2 −3
A=
.
−2 3

Again, to solve this problem, we form the proper augmented maS
trix and we put it into reduced row echelon form, which we do below.
[
]
[
]
−→
2 −3 0
1 −3/2 0
rref
−2 3 0
0
0
0
We interpret the reduced row echelon form of this matrix to ﬁnd that
x1 = 3/2x2
x2 is free.
As before,

[
⃗x =

]
x1
.
x2

Since x1 = 3/2x2 , we replace x1 in ⃗x with 3/2x2 :
[
]
3/2x2
⃗x =
.
x2
Now we pull out the x2 and write the solu on as
[
]
3/2
⃗x = x2
.
1
84

2.4 Vector Solu ons to Linear Systems
As before, let’s set

[
⃗v =

3/2
1

]

so we can write our solu on as
⃗x = x2⃗v.
Again, we have inﬁnite solu ons; any choice of x2 gives us one of these solu ons.
For instance, picking x2 = 2 gives the solu on
[ ]
3
⃗x =
.
2
(This is a par cularly nice solu on, since there are no frac ons. . .)
As in the previous example, our solu ons are mul ples of a vector, and hence we
can graph this, as done in Figure 2.19.
y

⃗v
.

x

.
Figure 2.19: The solu on, as a line,
to A⃗x = ⃗0 in Example 45.
.
Let’s prac ce some more; this me, we won’t solve a system of the form A⃗x = ⃗0,
but instead A⃗x = ⃗b, for some vector ⃗b.
. Example 46

.Solve the linear system A⃗x = ⃗b, where
[
]
[ ]
1 2
3
⃗
A=
and b =
.
2 4
6

(Note that this is the same matrix A that we used in Example 44.
S
This will be important later.)
Our methodology is the same as before; we form the augmented matrix and put it
into reduced row echelon form.
[
]
[
]
−→
1 2 3
1 2 3
rref
2 4 6
0 0 0
85

Chapter 2 Matrix Arithme c
Interpre ng this reduced row echelon form, we ﬁnd that
x1 = 3 − 2x2
x2 is free.
Again,

[
⃗x =

and we replace x1 with 3 − 2x2 , giving
⃗x =

[

]
x1
,
x2

]
3 − 2x2
.
x2

.This solu on is diﬀerent than what we’ve seen in the past two examples; we can’t
simply pull out a x2 since there is a 3 in the ﬁrst entry. Using the proper es of matrix
addi on, we can “pull apart” this vector and write it as the sum of two vectors: one
which contains only constants, and one that contains only “x2 stuﬀ.” We do this below.
[
]
3 − 2x2
⃗x =
x2
[ ] [
]
3
−2x2
=
+
0
x2
[ ]
[
]
3
−2
=
+ x2
.
0
1
Once again, let’s give names to the diﬀerent component vectors of this solu on
(we are ge ng near the explana on of why we are doing this). Let
[ ]
[
]
3
−2
x⃗p =
and ⃗v =
.
0
1
We can then write our solu on in the form
⃗x = x⃗p + x2⃗v.
We s ll have inﬁnite solu ons; by picking a value for x2 we get one of these soluons. For instance, by le ng x2 = −1, 0, or 2, we get the solu ons
[
]
[ ]
[
]
5
3
−1
,
and
.
−1
0
2
We have oﬃcially solved the problem; we have solved the equa on A⃗x = ⃗b for ⃗x
and have wri en the solu on in vector form. As an addi onal visual aid, we will graph
this solu on.
Each vector in the solu on can be wri en as the sum of two vectors: x⃗p and a
mul ple of ⃗v. In Figure 2.20, x⃗p is graphed and ⃗v is graphed with its origin star ng at
the p of x⃗p . Finally, a line is drawn in the direc on of ⃗v from the p of x⃗p ; any vector
poin ng to any point on this line is a solu on to A⃗x = ⃗b.
86

2.4 Vector Solu ons to Linear Systems
y

⃗v
.
x⃗p

.

x

.
Figure 2.20: The solu on, as a line,
to A⃗x = ⃗b in Example 46.

The previous examples illustrate some important concepts. One is that we can
“see” the solu on to a system of linear equa ons in a new way. Before, when we had
inﬁnite solu ons, we knew we could arbitrarily pick values for our free variables and
get diﬀerent solu ons. We knew this to be true, and we even prac ced it, but the
result was not very “tangible.” Now, we can view our solu on as a vector; by picking
diﬀerent values for our free variables, we see this as mul plying certain important
vectors by a scalar which gives a diﬀerent solu on.
Another important concept that these examples demonstrate comes from the fact
that Examples 44 and 46 were only “slightly diﬀerent” and hence had only “slightly
diﬀerent” answers. Both solu ons had
[
]
−2
x2
1
in them; in Example 46 the solu on also had another vector added to this. Was this
coincidence, or is there a deﬁnite pa ern here?
Of course there is a pa ern! Now . . . what exactly is it? First, we deﬁne a term.
.
Deﬁni on 17

Homogeneous Linear System of Equa ons
A system of linear equa ons is homogeneous if the constants in each equa on are zero.

.

Note: a homogeneous system of equa ons can be wri en
in vector form as A⃗x = ⃗0.

The term homogeneous comes from two Greek words; homo meaning “same” and
genus meaning “type.” A homogeneous system of equa ons is a system in which each
87

Chapter 2 Matrix Arithme c
equa on is of the same type – all constants are 0. No ce that the system of equa ons
in Examples 44 and 46 are homogeneous.
Note that A⃗0 = ⃗0; that is, if we set⃗x = ⃗0, we have a solu on to a homogeneous set
of equa ons. This fact is important; the zero vector is always a solu on to a homogeneous linear system. Therefore a homogeneous system is always consistent; we need
only to determine whether we have exactly one solu on (just ⃗0) or inﬁnite solu ons.
This idea is important so we give it it’s own box.
.
Key Idea 7

Homogeneous Systems and Consistency

.

All homogeneous linear systems are consistent.

How do we determine if we have exactly one or inﬁnite solu ons? Recall Key Idea
2: if the solu on has any free variables, then it will have inﬁnite solu [ons. How
] can
we tell if the system has free variables? Form the augmented matrix A ⃗0 , put it
into reduced row echelon form, and interpret the result.
It may seem that we’ve brought up a new ques on, “When does A⃗x = ⃗0 have exactly one or inﬁnite solu ons?” only to answer with “Look at the reduced row echelon
form of A and interpret the results, just as always.” Why bring up a new ques on if the
answer is an old one?
While the new ques on has an old solu on, it does lead to a great idea. Let’s
refresh our memory; earlier we solved two linear systems,
A⃗x = ⃗0 and
where

[
A=

1
2

2
4

A⃗x = ⃗b

]
and ⃗b =

[ ]
3
.
6

The solu on to the ﬁrst system of equa ons, A⃗x = ⃗0, is
[
]
−2
⃗x = x2
1
and the solu on to the second set of equa ons, A⃗x = ⃗b, is
[ ]
[
]
3
−2
⃗x =
+ x2
,
0
1
for all values of x2 .
Recalling our nota on used earlier, set
[ ]
[
]
3
−2
⃗
⃗
xp =
and let v =
.
0
1
88

2.4 Vector Solu ons to Linear Systems
Thus our solu on to the linear system A⃗x = ⃗b is
⃗x = x⃗p + x2⃗v.

Let us see how exactly this solu on works; let’s see why A⃗x equals ⃗b. Mul ply A⃗x :
A⃗x = A(⃗
xp + x2⃗v)
= A⃗
xp + A(x2⃗v)
= A⃗
xp + x2 (A⃗v)
= A⃗
xp + x2⃗0
= A⃗
xp + ⃗0
= A⃗
xp
= ⃗b

We know that the last line is true, that A⃗
xp = ⃗b, since we know that⃗x was a solu on
⃗
to A⃗x = b. The whole point is that x⃗p itself is a solu on to A⃗x = ⃗b, and we could ﬁnd
more solu ons by adding vectors “that go to zero” when mul plied by A. (The subscript
p of “⃗
xp ” is used to denote that this vector is a “par cular” solu on.)
Stated in a diﬀerent way, let’s say that we know two things: that A⃗
xp = ⃗b and
A⃗v = ⃗0. What is A(⃗
xp + ⃗v)? We can mul ply it out:
A(⃗
xp + ⃗v) = A⃗
xp + A⃗v
⃗
= b + ⃗0
= ⃗b
and see that A(⃗
xp + ⃗v) also equals ⃗b.
So we wonder: does this mean that A⃗x = ⃗b will have inﬁnite solu ons? A er all,
if x⃗p and x⃗p + ⃗v are both solu ons, don’t we have inﬁnite solu ons?
No. If A⃗x = ⃗0 has exactly one solu on, then ⃗v = ⃗0, and x⃗p = x⃗p + ⃗v; we only have
one solu on.
So here is the culmina on of all of our fun that started a few pages back. If ⃗v is
a solu on to A⃗x = ⃗0 and x⃗p is a solu on to A⃗x = ⃗b, then x⃗p + ⃗v is also a solu on to
A⃗x = ⃗b. If A⃗x = ⃗0 has inﬁnite solu ons, so does A⃗x = ⃗b; if A⃗x = ⃗0 has only one
solu on, so does A⃗x = ⃗b. This culmina ng idea is of course important enough to be
stated again.
89

Chapter 2 Matrix Arithme c

.
Key Idea 8

Solu ons of Consistent Systems
Let A⃗x = ⃗b be a consistent system of linear equa ons.

.

1. If A⃗x = ⃗0 has exactly one solu on (⃗x = ⃗0), then A⃗x =
⃗b has exactly one solu on.
2. If A⃗x = ⃗0 has inﬁnite solu ons, then A⃗x = ⃗b has inﬁnite solu ons.

A key word in the above statement is consistent. If A⃗x = ⃗b is inconsistent (the
linear system has no solu on), then it doesn’t ma er how many solu ons A⃗x = ⃗0 has;
A⃗x = ⃗b has no solu on.
Enough fun, enough theory. We need to prac ce.
. Example 47

.Let
[
A=

1 −1 1 3
4 2 4 6

]

[
and ⃗b =

]
1
.
10

Solve the linear systems A⃗x = ⃗0 and A⃗x = ⃗b for ⃗x , and write the solu ons in vector
form.
We’ll tackle A⃗x = ⃗0 ﬁrst. We form the associated augmented matrix, put it into reduced row echelon form, and interpret the result.
S

[

1 −1 1 3 0
4 2 4 6 0

]

−→
rref

[

1
0

0 1 2 0
1 0 −1 0

]

x1 = −x3 − 2x4
x2 = x4
x3 is free
x4 is free
To write our solu on in vector form, we rewrite x1 and x2 in ⃗x in terms of x3 and x4 .


 

x1
−x3 − 2x4
 x2  

x4
 

⃗x = 
 x3  = 

x3
x4
x4
Finally, we “pull apart” this vector into two vectors, one with the “x3 stuﬀ” and one
90

2.4 Vector Solu ons to Linear Systems
with the “x4 stuﬀ.”



−x3 − 2x4


x4

⃗x = 


x3
x4

 

−x3
−2x4
 0   x4 
 

=
 x3  +  0 
0
x4




−2
−1


 0 
 + x4  1 
= x3 
 0 
 1 
1
0
= x3⃗u + x4⃗v
We use ⃗u and ⃗v simply to give these vectors names (and save some space).
It is easy to conﬁrm that both ⃗u and ⃗v are solu ons to the linear system A⃗x = ⃗0.
(Just mul ply A⃗u and A⃗v and see that both are ⃗0.) Since both are solu ons to a homogeneous system of linear equa ons, any linear combina on of ⃗u and ⃗v will be a
solu on, too.
.
Now let’s tackle A⃗x = ⃗b. Once again we put the associated augmented matrix into
reduced row echelon form and interpret the results.
[

1 −1 1 3
4 2 4 6

1
10

]

−→
rref

[

1 0 1 2
0 1 0 −1

2
1

]

x1 = 2 − x3 − 2x4
x2 = 1 + x4
x3 is free
x4 is free
Wri ng this solu on in vector form gives


 

x1
2 − x3 − 2x4
 x2   1 + x4

 
.
⃗x = 
 x3  = 

x3
x4
x4
Again, we pull apart this vector, but this me we break it into three vectors: one with
91

Chapter 2 Matrix Arithme c
“x3 ” stuﬀ, one with “x4 ” stuﬀ, and one with just constants.


2 − x3 − 2x4
 1 + x4


⃗x = 


x3
x4
 

  
2
−x3
−2x4
 1   0   x4 
 
 

=
 0  +  x3  +  0 
0
0
x4




 
−1
−2
2
 0 
 1 
1





=
 0  + x3  1  + x4  0 
0
1
0
=
x⃗p
+
x3⃗u + x4⃗v
| {z }
|{z}
par cular
solu on

solu on to
homogeneous
equa ons A⃗x = ⃗0

Note that A⃗
xp = ⃗b; by itself, x⃗p is a solu on. To get inﬁnite solu ons, we add a bunch
of stuﬀ that “goes to zero” when we mul ply by A; we add the solu on to the homogeneous equa ons.
Why don’t we graph this solu on as we did in the past? Before we had only two
variables, meaning the solu on could be graphed in 2D. Here we have four variables,
meaning that our solu on “lives” in 4D. You can draw this on paper, but it is very confusing. .
. Example 48

.Rewrite the linear system
x1
3x1

+ 2x2
+ 4x2

− 3x3
+ 5x3

+ 2x4
+ 2x4

+ 7x5
+ 3x5

= 2
= −4

as a matrix–vector equa on, solve the system using vector nota on, and give the solu on to the related homogeneous equa ons.
S

Rewri ng the linear system in the form of A⃗x = ⃗b, we have that
 
x1
 x2 
[
]
[
]
 
1 2 −3 2 7
 and ⃗b = 2 .
x
A=
, ⃗x = 
3
 
3 4 5 2 3
−4
 x4 
x5

To solve the system, we put the associated augmented matrix into reduced row echelon form and interpret the results.
92

2.4 Vector Solu ons to Linear Systems

[

1 2
3 4

−3 2 7 2
5 2 3 −4

]

−→
rref

[

1 0
0 1

11 −2 −11 −8
−7 2
9
5

]

x1 = −8 − 11x3 + 2x4 + 11x5
x2 = 5 + 7x3 − 2x4 − 9x5
x3 is free
x4 is free
x5 is free
We use this informa on to write ⃗x , again pulling it apart. Since we have three free
variables and also constants, we’ll need to pull ⃗x apart into four separate vectors.



x1
 x2 
 

⃗x = 
 x3 
 x4 
x5


−8 − 11x3 + 2x4 + 11x5
 5 + 7x3 − 2x4 − 9x5 



x3
=




x4
x5

 
 
 

−8
−11x3
2x4
11x5
 5   7x3   −2x4   −9x5 

 
 
 

 
 
 

=
 0  +  x3  +  0  +  0 
 0   0   x4   0 
0
0
0
x5








−8
−11
2
11
 5 
 7 
 −2 
 −9 















=  0  + x3  1  + x4  0  + x5 
 0 
 0 
 0 
 1 
 0 
0
0
0
1
⃗
⃗
⃗
⃗
+
x3 u + x4 v + x5 w
=
xp
|
{z
}
|{z}
par cular
solu on

solu on to homogeneous
equa ons A⃗x = ⃗0

So x⃗p is a par cular solu on; A⃗
xp = ⃗b. (Mul ply it out to verify that this is true.)
⃗ , that are mul plied by our free variables x3 , x4 and x5 ,
The other vectors, ⃗u, ⃗v and w
are each solu ons to the homogeneous equa ons, A⃗x = ⃗0. Any linear combina on
of these three vectors, i.e., any vector found by choosing values for x3 , x4 and x5 in
⃗ is a solu on to A⃗x = ⃗0. .
x3⃗u + x4⃗v + x5 w

93

Chapter 2 Matrix Arithme c
. Example 49

Let

[
A=

1
4

2
5

]
and ⃗b =

[ ]
3
.
6

Find the solu ons to A⃗x = ⃗b and A⃗x = ⃗0.
We go through the familiar work of ﬁnding the reduced row echeS
lon form of the appropriate augmented matrix and interpre ng the solu on.
[
]
]
[
−→
1 2 3
1 0 −1
rref
0 1 2
4 5 6
x1 = −1
x2 = 2
[

Thus
⃗x =

x1
x2

]

[
=

]
−1
.
2

This may strike us as a bit odd; we are used to having lots of diﬀerent vectors in the
solu on. However, in this case, the linear system A⃗x = ⃗b has exactly one solu on, and
we’ve found it. What is the solu on to A⃗x = ⃗0? Since we’ve only found one solu on to
A⃗x = ⃗b, we can conclude from Key Idea 8 the related homogeneous equa ons A⃗x = ⃗0
have only one solu on, namely ⃗x = ⃗0. We can write our solu on vector ⃗x in a form
similar to our previous examples to highlight this:
[
]
−1
⃗x =
2
[
] [ ]
−1
0
=
+
2
0
⃗0
=
x⃗p
+
.
|{z}
|{z}
par cular
solu on

solu on to
A⃗x = ⃗0

.
. Example 50

.Let

[
A=

1
2

1
2

]
and ⃗b =

[ ]
1
.
1

Find the solu ons to A⃗x = ⃗b and A⃗x = ⃗0.
To solve A⃗x = ⃗b, we put the appropriate augmented matrix into
reduced row echelon form and interpret the results.
[
]
[
]
−→
1 1 1
1 1 0
rref
2 2 1
0 0 1
S

94

2.4 Vector Solu ons to Linear Systems
We immediately have a problem; we see that the second row tells us that 0x1 +
0x2 = 1, the sign that our system does not have a solu on. Thus A⃗x = ⃗b has no
solu on. Of course, this does not mean that A⃗x = ⃗0 has no solu on; it always has a
solu on.
To ﬁnd the solu on to A⃗x = ⃗0, we interpret the reduced row echelon form of the
appropriate augmented matrix.
[
]
[
]
−→
1 1 0
1 1 0
rref
2 2 0
0 0 0
x1 = −x2
x2 is free
Thus

[
⃗x =
[

x1
x2

]

]
−x2
x2
]
[
−1
= x2
1
=

= x2⃗u.
We have no solu on to A⃗x = ⃗b, but inﬁnite solu ons to A⃗x = ⃗0. .
The previous example may seem to violate the principle of Key Idea 8. A er all,
it seems that having inﬁnite solu ons to A⃗x = ⃗0 should imply inﬁnite solu ons to
A⃗x = ⃗b. However, we remind ourselves of the key word in the idea that we observed
before: consistent. If A⃗x = ⃗b is consistent and A⃗x = ⃗0 has inﬁnite solu ons, then so
will A⃗x = ⃗b. But if A⃗x = ⃗b is not consistent, it does not ma er how many solu ons
A⃗x = ⃗0 has; A⃗x = ⃗b is s ll inconsistent.
This whole sec on is highligh ng a very important concept that we won’t fully understand un l a er two sec ons, but we get a glimpse of it here. When solving any
system of linear equa ons (which we can write as A⃗x = ⃗b), whether we have exactly
one solu on, inﬁnite solu ons, or no solu on depends on an intrinsic property of A.
We’ll ﬁnd out what that property is soon; in the next sec on we solve a problem we
introduced at the beginning of this sec on, how to solve matrix equa ons AX = B.

Exercises 2.4
In Exercises 1 – 6, a matrix A and vectors ⃗b, ⃗u
and ⃗v are given. Verify that ⃗u and ⃗v are both
solu ons to the equa on A⃗x = ⃗b; that is,
show that A⃗u = A⃗v = ⃗b.

[

]
1
−2
,
−3
6
[ ]
[ ]
[
]
⃗b = 0 , ⃗u = 2 , ⃗v = −10
0
1
−5

1. A =

95

Chapter 2 Matrix Arithme c
[

]
−2
,
6
[
]
[
]
[ ]
⃗b = 2 , ⃗u = 0 , ⃗v = 2
−6
−1
0
[
]
1 0
A=
,
2 0
[ ]
[
]
[ ]
⃗b = 0 , ⃗u = 0 , ⃗v = 0
0
−1
59
[
]
1 0
A=
,
2 0
[
]
[
]
[
]
⃗b = −3 , ⃗u = −3 , ⃗v = −3
−6
−1
59
[
]
0
−3 −1 −3
A=
,
−4
2
−3
5


11
[ ]
 4 
0
⃗b =

, ⃗u = 
 −12 ,
0
0


9
 −12 


⃗v = 
0 
12
]
[
0
−3 −1 −3
,
A=
−4
2
−3
5


−17
[ ]


⃗b = 48 , ⃗u =  −16 ,
 0 
36
0


−8
 −28 

⃗v = 
 0 
12

2. A =

3.

4.

5.

6.

1
−3

In Exercises 7 – 9, a matrix A and vectors ⃗b, ⃗u
and ⃗v are given. Verify that A⃗u = ⃗0, A⃗v = ⃗b
and A(⃗u + ⃗v) = ⃗b.


2
−2 −1
1
−1 ,
7. A =  −1
−2
2
−1
 
 


1
1
1
⃗b =  1 , ⃗u =  1 , ⃗v =  1 
1
0
−1


1
−1
3
−3 −3 ,
8. A =  3
−1
1
1

96




 
 
−1
2
2
⃗b =  −3 , ⃗u =  2 , ⃗v =  3 
1
0
0


2 0
0
9. A =  0 1 −3 ,
3 1 −3


 


2
0
1
⃗b =  −4 , ⃗u =  6 , ⃗v =  −1 
−1
2
1
In Exercises 10 – 24, a matrix A and vector ⃗b
are given.
(a) Solve the equa on A⃗x = ⃗0.
(b) Solve the equa on A⃗x = ⃗b.
In each of the above, be sure to write your answer in vector format. Also, when possible,
give 2 par cular solu ons to each equa on.
[
]
[
]
0
2 ⃗
−2
10. A =
,b=
−1 3
−1
]
[ ]
[
1
−4 −1 ⃗
,b=
11. A =
−3 −2
4
]
]
[
[
0
1 −2 ⃗
,b=
12. A =
−5
0
1
]
]
[
[
−2
1
0
, ⃗b =
13. A =
−1
5 −4
]
]
[
[
1
2
−3 ⃗
,b=
14. A =
−1
−4
6
[
]
[
]
−4 3 2 ⃗
−4
15. A =
,b=
−4 5 0
−4
]
[ ]
[
0
1 5 −2 ⃗
,b=
16. A =
1
1 4
5
]
]
[
[
−4
−1 −2 −2 ⃗
,b=
17. A =
−4
3
4
−2
[
]
[
]
2 2
2
3
18. A =
, ⃗b =
5 5 −3
−3
[
]
1 5 −4 −1
19. A =
,
1 0 −2
1
[
]
⃗b = 0
−2
[
]
−4 2 −5 4
20. A =
,
0
1 −1 5
[
]
⃗b = −3
−2

2.5 Solving Matrix Equa ons AX = B
[

0
0
−2 −1
[ ]
⃗b = 3
4

3
0
3
22. A =  2
−5 0


−1
⃗b =  −5 
4

−1
3
−3
23. A =  3
−2
3


1
⃗b =  1 
−5

−4 −2
−4
24. A =  5
4
−5
21. A =

2.5

2
−4

−2
2
4

1
−1
−2

1
−1

−4
0
0

−3
1
−3

]
4
,
5

5
2 ,
5


4
−4 ,
1

 
3
⃗b =  2 
1
In Exercises 25 – 28, a matrix A and vector ⃗b
are given. Solve the equa on A⃗x = ⃗b, write
the solu on in vector format, and sketch the
solu on as the appropriate line on the Cartesian plane.
[
]
[ ]
2
4
0
25. A =
, ⃗b =
−1 −2
0
[
26. A =
[

−1
3
3

4
−1
1


0
1 ,
−4

27. A =
[
28. A =

2
−1

]
[
]
4
−6
, ⃗b =
−2
3

2
−4

]
[ ]
−5 ⃗
1
,b=
−10
2

2
−4

]
[ ]
−5 ⃗
0
,b=
−10
0

Solving Matrix Equa ons AX = B

.

.
AS YOU READ
...
[
1. T/F: To solve the matrix equa on AX = B, put the matrix A
row echelon form and interpret the result properly.

]
X into reduced

2. T/F: The ﬁrst column of a matrix product AB is A mes the ﬁrst column of B.
3. Give two reasons why one might solve for the columns of X in the equa on AX=B
separately.

We began last sec on talking about solving numerical equa ons like ax = b for x.
We men oned that solving matrix equa ons of the form AX = B is of interest, but we
ﬁrst learned how to solve the related, but simpler, equa ons A⃗x = ⃗b. In this sec on
we will learn how to solve the general matrix equa on AX = B for X.
We will start by considering the best case scenario when solving A⃗x = ⃗b; that is,
when A is square and we have exactly one solu on. For instance, suppose we want to
solve A⃗x = ⃗b where
[
]
[ ]
1 1
0
⃗
A=
and b =
.
2 1
1
97

Chapter 2 Matrix Arithme c
We know how to solve this; put the appropriate matrix into reduced row echelon form
and interpret the result.
[
]
[
]
−→
1 1 0
1 0 1
rref
2 1 1
0 1 −1
[

We read from this that
⃗x =

]
1
.
−1

Wri en in a more general form, we found our solu on by forming the augmented
matrix
[
]
A ⃗b
and interpre ng its reduced row echelon form:
[
] −→ [
]
I ⃗x
rref
A ⃗b
No ce that when the reduced row echelon form of A is the iden ty matrix I we have
exactly one solu on. This, again, is the best case scenario.
We apply the same general technique to solving the matrix equa on AX = B for X.
We’ll assume that A is a square matrix (B need not be) and we’ll form the augmented
matrix
[
]
A B .
Pu ng this matrix into reduced row echelon form will give us X, much like we found ⃗x
before.
[
] −→ [
]
A B
I X
rref
As long as the reduced row echelon form of A is the iden ty matrix, this technique
works great. A er a few examples, we’ll discuss why this technique works, and we’ll
also talk just a li le bit about what happens when the reduced row echelon form of A
is not the iden ty matrix.
First, some examples.
. Example 51

.Solve the matrix equa on AX = B where
[
]
[
]
1 −1
−8 −13 1
A=
and B =
.
5 3
32 −17 21

To solve AX = B for X, we form the proper augmented matrix, put
S
it into reduced row echelon form, and interpret the result.
[
]
[
]
−→
1 −1 −8 −13 1
1 0 1 −7 3
rref
5 3
32 −17 21
0 1 9 6 2
We read from the reduced row echelon form of the matrix that
[
]
1 −7 3
X=
.
9 6 2
98

2.5 Solving Matrix Equa ons AX = B
We can easily check to see if our answer is correct by mul plying AX. .
. Example 52

Solve the matrix equa on AX = B where

1 0
2
A =  0 −1 −2 
2 −1 0


−1 2
B =  2 −6  .
2 −4





and

To solve, let’s again form the augmented matrix

S

[

]
A B ,

put it into reduced row echelon form, and interpret the result.



1 0
2 −1 2
 0 −1 −2 2 −6 
2 −1 0
2 −4
We see from this that

.


−→
rref

1 0
0 1
0 0

0 1
0 0
1 −1


0
4
1


1 0
X =  0 4.
−1 1


Why does this work? To see the answer, let’s deﬁne ﬁve matrices.
[
A=

]
[ ]
[
]
[ ]
[
]
1 2
1
−1
5
1 −1 5
⃗ =
, ⃗u =
, ⃗v =
, w
and X =
3 4
1
1
6
1 1 6

⃗ are the ﬁrst, second and third columns of X, respec vely.
No ce that ⃗u, ⃗v and w
Now consider this list of matrix products: A⃗u, A⃗v, A⃗
w and AX.
[

][ ]
1
1

[

][ ]
5
6

1 2
A⃗u =
3 4
[ ]
3
=
7

1 2
A⃗
w=
3 4
[ ]
17
=
39

[

1 2
A⃗v =
3 4
[ ]
1
=
1
[
AX =
[
=

1 2
3 4

][

][

1
1

3 1 17
7 1 39

−1
1

]

]

−1
1

5
6

]

⃗ ; that is, we can write
So again note that the columns of X are ⃗u, ⃗v and w
[
]
⃗ .
X = ⃗u ⃗v w

99

Chapter 2 Matrix Arithme c
No ce also that the columns of AX are A⃗u, A⃗v and A⃗
w, respec vely. Thus we can write
[
]
⃗
AX = A ⃗u ⃗v w
[
]
w
= A⃗u A⃗v A⃗
[[ ] [ ] [ ]]
3
1
17
=
7
1
39
[
]
3 1 17
=
7 1 39
We summarize what we saw above in the following statement:
The columns of a matrix product AX are A mes the columns of X.
How does this help us solve the matrix equa on AX = B for X? Assume that A is a
square matrix (that forces X and B to be the same size). We’ll let x⃗1 , x⃗2 , · · · x⃗n denote
the columns of the (unknown) matrix X, and we’ll let b⃗1 , b⃗2 , · · · b⃗n denote the columns
of B. We want to solve AX = B for X. That is, we want X where
AX = B
] [
A x⃗1 x⃗2 · · · x⃗n = b⃗1
[
] [
A⃗
x1 A⃗
x2 · · · A⃗
xn = b⃗1
[

b⃗2
b⃗2

···
···

]
b⃗n
]
b⃗n

If the matrix on the le hand side is equal to the matrix on the right, then their
respec ve columns must be equal. This means we need to solve n equa ons:
A⃗
x1 = b⃗1
A⃗
x2 = b⃗2
..
.
. = ..
A⃗
xn = b⃗n
We already know how to do this; this is what we learned in the previous sec on.
Let’s do this in a concrete example. In our above work we deﬁned matrices A and X,
and looked at the product AX. Let’s call the product B; that is, set B= AX. Now, let’s
pretend that we don’t know what X is, and let’s try to ﬁnd the matrix X that sa sﬁes
the equa on AX = B. As a refresher, recall that
[
]
[
]
1 2
3 1 17
A=
and B =
.
3 4
7 1 39
Since A is a 2 × 2 matrix and B is a 2 × 3 matrix, what dimensions must X be in the
equa on AX = B? The number of rows of X must match the number of columns of A;
the number of columns of X must match the number of columns of B. Therefore we
know that X must be a 2 × 3 matrix.
100

2.5 Solving Matrix Equa ons AX = B
We’ll call the three columns of X x⃗1 , x⃗2 and x⃗3 . Our previous explana on tells us
that if AX = B, then:
AX = B
[
[
]
3
A x⃗1 x⃗2 x⃗3 =
7
[
[
]
3
A⃗
x1 A⃗
x2 A⃗
x3 =
7
Hence

1
1

17
39

]

]
1 17
.
1 39

[ ]
3
A⃗
x1 =
7
[ ]
1
A⃗
x2 =
1
[ ]
17
A⃗
x3 =
39

To ﬁnd x⃗1 , we form the proper augmented matrix and put it into reduced row echelon form and interpret the results.
[
]
[
]
−→
1 2 3
1 0 1
rref
3 4 7
0 1 1
This shows us that
x⃗1 =

[ ]
1
.
1

To ﬁnd x⃗2 , we again form an augmented matrix and interpret its reduced row echelon form.
[
]
[
]
−→
1 2 1
1 0 −1
rref
3 4 1
0 1 1
[

Thus
x⃗2 =

−1
1

]

which matches with what we already knew from above.
Before con nuing on in this manner to ﬁnd x⃗3 , we should stop and think. If the
matrix vector equa on A⃗x = ⃗b is consistent, then the steps involved in pu ng
[
]
A ⃗b
into reduced row echelon form depend only on A; it does not ma er what ⃗b is. So
when we put the two matrices
[
]
[
]
1 2 3
1 2 1
and
3 4 7
3 4 1
101

Chapter 2 Matrix Arithme c
from above into reduced row echelon form, we performed exactly the same steps! (In
fact, those steps are: −3R1 + R2 → R2 ; − 12 R2 → R2 ; −2R2 + R1 → R1 .)
Instead of solving for each column of X separately, performing the same steps to
put the necessary matrices into reduced row echelon form three diﬀerent mes, why
don’t we just do it all at once?16 Instead of individually pu ng
[
]
[
]
[
]
1 2 3
1 2 1
1 2 17
,
and
3 4 7
3 4 1
3 4 39
into reduced row echelon form, let’s just put
[
1 2 3 1
3 4 7 1
into reduced row echelon form.
[
]
1 2 3 1 17
3 4 7 1 39

−→
rref

17
39
[

]

1 0 1 −1 5
0 1 1 1 6

]

By looking at the last three columns, we see X:
[
]
1 −1 5
X=
.
1 1 6
Now that we’ve jus ﬁed the technique we’ve been using in this sec on to solve
AX = B for X, we reinfornce its importance by resta ng it as a Key Idea.
.
Key Idea 9

Solving AX = B
Let A be an n × n matrix, where the reduced row echelon
form of A is I. To solve the matrix equa on AX = B for X,
[
]
1. Form the augmented matrix A B .

.

2. Put this matrix into[reduced
] row echelon form. It
will be of the form I X , where X appears in the
columns where B once was.

These simple steps cause us to ask certain ques ons. First, we specify above that A
should be a square matrix. What happens if A isn’t square? Is a solu on s ll possible?
Secondly, we only considered cases where the reduced row echelon form of A was I
(and stated that as a requirement in our Key Idea). What if the reduced row echelon
form of A isn’t I? Would we s ll be able to ﬁnd a solu on? (Instead of having exactly
one solu on, could we have no solu on? Inﬁnite solu ons? How would we be able to
tell?)
16 One reason to do it three diﬀerent mes is that we enjoy doing unnecessary work. Another reason
could be that we are stupid.

102

2.6 The Matrix Inverse
These ques ons are good to ask, and we leave it to the reader to discover their
answers. Instead of tackling these ques ons, we instead tackle the problem of “Why
do we care about solving AX = B?” The simple answer is that, for now, we only care
about the special case when B = I. By solving AX = I for X, we ﬁnd a matrix X that,
when mul plied by A, gives the iden ty I. That will be very useful.

Exercises 2.5
In Exercises 1 – 12, matrices A and B are
given. Solve the matrix equa on AX = B.
[
]
4
−1
1. A =
,
−7
5
[
]
8
−31
B=
−27
38
[
]
1
−3
2. A =
,
−3
6
]
[
12
−10
B=
−27
27
[
]
3 3
3. A =
,
6 4
]
[
15 −39
B=
16 −66
[
]
−3 −6
4. A =
,
4
0
]
[
48 −30
B=
0
−8
]
[
−1 −2
,
5. A =
−2 −3
]
[
13 4 7
B=
22 5 12
]
[
−4
1
,
6. A =
−1 −2

2.6

[

]
−10 19
2
−2
[
]
1
0
A=
, B = I2
3 −1
[
]
2 2
A=
, B = I2
3 1


−2
0
4
5 ,
A =  −5 −4
−3
5
−3


−18 2 −14
B =  −38 18 −13 
10
2 −18


−5 −4 −1
−2 −3 ,
A= 8
6
1
−8


−21 −8 −19
−11 −10 
B =  65
75
−51
33


0 −2
1
2
2 , B = I3
A=0
1
2
−3


−3
3
−2
−3
2 , B = I3
A= 1
−1 −1
2
B=

7.
8.

9.

10.

11.

12.

−2
13

The Matrix Inverse

.

.
AS YOU READ
...
1. T/F: If A and B are square matrices where AB = I, then BA = I.
2. T/F: A matrix A has exactly one inverse, inﬁnite inverses, or no inverse.
3. T/F: Everyone is special.
103

Chapter 2 Matrix Arithme c
4. T/F: If A is inver ble, then A⃗x = ⃗0 has exactly 1 solu on.
5. What is a corollary?
6. Fill in the blanks:
is
.

a matrix is inver ble is useful; compu ng the inverse

Once again we visit the old algebra equa on, ax = b. How do we solve for x? We
know that, as long as a ̸= 0,
x=

b
, or, stated in another way, x = a−1 b.
a

What is a−1 ? It is the number that, when mul plied by a, returns 1. That is,
a−1 a = 1.
Let us now think in terms of matrices. We have learned of the iden ty matrix I that
“acts like the number 1.” That is, if A is a square matrix, then
IA = AI = A.
If we had a matrix, which we’ll call A−1 , where A−1 A = I, then by analogy to our algebra
example above it seems like we might be able to solve the linear system A⃗x = ⃗b for ⃗x
by mul plying both sides of the equa on by A−1 . That is, perhaps
⃗x = A−1⃗b.
Of course, there is a lot of specula on here. We don’t know that such a matrix like
A−1 exists. However, we do know how to solve the matrix equa on AX = B, so we
can use that technique to solve the equa on AX = I for X. This seems like it will get us
close to what we want. Let’s prac ce this once and then study our results.
. Example 53

.Let

[
A=

2
1

]
1
.
1

Find a matrix X such that AX = I.
We know how to solve this from the previous sec on: we form
S
the proper augmented matrix, put it into reduced row echelon form and interpret the
results.
[
]
[
]
−→
2 1 1 0
1 0 1 −1
rref
1 1 0 1
0 1 −1 2
We read from our matrix that

[
X=

104

]
1 −1
.
−1 2

2.6 The Matrix Inverse
Let’s check our work:

[
AX =
[
=

2
1

1
1

1
0

0
1

][
]

1 −1
−1 2

]

=I
Sure enough, it works. .
Looking at our previous example, we are tempted to jump in and call the matrix X
that we found “A−1 .” However, there are two obstacles in the way of us doing this.
First, we know that in general AB ̸= BA. So while we found that AX = I, we can’t
automa cally assume that XA = I.
Secondly, we have seen examples of matrices where AB = AC, but B ̸= C. So just
because AX = I, it is possible that another matrix Y exists where AY = I. If this is the
case, using the nota on A−1 would be misleading, since it could refer to more than
one matrix.
These obstacles that we face are not insurmountable. The ﬁrst obstacle was that
we know that AX = I but didn’t know that XA = I. That’s easy enough to check,
though. Let’s look at A and X from our previous example.
[
][
]
1 −1
2 1
XA =
−1 2
1 1
[
]
1 0
=
0 1
=I
Perhaps this ﬁrst obstacle isn’t much of an obstacle a er all. Of course, we only
have one example where it worked, so this doesn’t mean that it always works. We
have good news, though: it always does work. The only “bad” news to come with this
is that this is a bit harder to prove. We won’t worry about proving it always works, but
state formally that it does in the following theorem.
.
Theorem 5

Special Commu ng Matrix Products
Let A be an n × n matrix.

.

1. If there is a matrix X such that AX = In , then XA = In .
2. If there is a matrix X such that XA = In , then AX = In .

The second obstacle is easier to address. We want to know if another matrix Y
exists where AY = I = YA. Let’s suppose that it does. Consider the expression XAY.
105

Chapter 2 Matrix Arithme c
Since matrix mul plica on is associa ve, we can group this any way we choose. We
could group this as (XA)Y; this results in
(XA)Y = IY
= Y.
We could also group XAY as X(AY). This tells us
X(AY) = XI
=X
Combining the two ideas above, we see that X = XAY = Y; that is, X = Y. We
conclude that there is only one matrix X where XA = I = AX. (Even if we think we
have two, we can do the above exercise and see that we really just have one.)
We have just proved the following theorem.
.
Theorem 6

Uniqueness of Solu ons to AX = In

.

Let A be an n × n matrix and let X be a matrix where AX =
In . Then X is unique; it is the only matrix that sa sﬁes this
equa on.

So given a square matrix A, if we can ﬁnd a matrix X where AX = I, then we know
that XA = I and that X is the only matrix that does this. This makes X special, so we
give it a special name.
.
Deﬁni on 18

Inver ble Matrices and the Inverse of A

.

Let A and X be n × n matrices where AX = I = XA. Then:
1. A is inver ble.
2. X is the inverse of A, denoted by A−1 .

Let’s do an example.
[
. Example 54

.Find the inverse of A =

]
1 2
.
2 4

By solving the equa on AX = I for X will give us the inverse of A.
S
Forming the appropriate augmented matrix and ﬁnding its reduced row echelon form
106

2.6 The Matrix Inverse
gives us

[

1
2

2
4

1
0

0
1

]

−→
rref

[

1
0

2 0 1/2
0 1 −1/2

]

Yikes! We were expec ng to ﬁnd that the reduced row echelon form of this matrix
would look like
[
]
I A−1 .
However, we don’t have the iden ty on the le hand side. Our conclusion: A is not
inver ble. .
We have just seen that not all matrices are inver ble.17 With this thought in mind,
let’s complete the array of boxes we started before the example. We’ve discovered
that if a matrix has an inverse, it has only one. Therefore, we gave that special matrix
a name, “the inverse.” Finally, we describe the most general way to ﬁnd the inverse of
a matrix, and a way to tell if it does not have one.
.
Key Idea 10

Finding A−1
Let A be an n × n matrix. To ﬁnd A−1 , put the augmented
matrix
[
]
A In

.

into reduced row echelon form. If the result is of the form
[
]
In X ,
then A−1 = X. If not, (that is, if the ﬁrst n columns of the reduced row echelon form are not In ), then A is not inver ble.

Let’s try again.


1
.Find the inverse, if it exists, of A =  1
1

. Example 55

1
−1
2


−1
1 .
3

We’ll try to solve AX = I for X and see what happens.

S



1
1
1

1
−1
2

−1
1
3


1 0 0
0 1 0
0 0 1


−→
rref


1 0 0 0.5
0.5
0
 0 1 0 0.2 −0.4 0.2 
0 0 1 −0.3 0.1 0.2

17 Hence our previous deﬁni on; why bother calling A “inver ble” if every square matrix is? If everyone
is special, then no one is. Then again, everyone is special.

107

Chapter 2 Matrix Arithme c
We have a solu on, so



0.5
0.5
0
A =  0.2 −0.4 0.2  .
−0.3 0.1 0.2
Mul ply AA−1 to verify that it is indeed the inverse of A. .
In general,
given a matrix A, to ﬁnd A−1 we need to form the augmented matrix
]
A I and put it into reduced row echelon form and interpret the result. In the case
of a 2 × 2 matrix, though, there is a shortcut. We give the shortcut in terms of a
theorem.18
[

.
Theorem 7

The Inverse of a 2×2 Matrix
[

Let
A=

]
a b
.
c d

.

A is inver ble if and only if ad − bc ̸= 0.
If ad − bc ̸= 0, then
−1

A

1
=
ad − bc

[

]
d −b
.
−c a

We can’t divide by 0, so if ad − bc = 0, we don’t have an inverse. Recall Example
54, where
[
]
1 2
A=
.
2 4
Here, ad − bc = 1(4) − 2(2) = 0, which is why A didn’t have an inverse.
Although this idea is simple, we should prac ce it.
. Example 56

.Use Theorem 7 to ﬁnd the inverse of
[
A=

3 2
−1 9

]

if it exists.
18 We don’t

108

prove this theorem here, but it really isn’t hard to do. Put the matrix
[
]
a b 1 0
c d 0 1

into reduced row echelon form and you’ll discover the result of the theorem. Alterna vely, mul ply A by
what we propose is the inverse and see that we indeed get I.

2.6 The Matrix Inverse
S

Since ad − bc = 29 ̸= 0, A−1 exists. By the Theorem,
1
3(9) − 2(−1)
[
]
1 9 −2
=
29 1 3

A−1 =

[

9 −2
1 3

]

We can leave our answer in this form, or we could “simplify” it as
A−1 =

1
29

[

9 −2
1 3

]

[
=

]
9/29 −2/29
.
1/29 3/29

.
We started this sec on out by specula ng that just as we solved algebraic equaons of the form ax = b by compu ng x = a−1 b, we might be able to solve matrix
equa ons of the form A⃗x = ⃗b by compu ng ⃗x = A−1⃗b. If A−1 does exist, then we can
solve the equa on A⃗x = ⃗b this way. Consider:

A⃗x = ⃗b
A−1 A⃗x = A−1⃗b
I⃗x = A−1⃗b
⃗x = A−1⃗b

(original equa on)
(mul ply both sides on the le by A−1 )
(since A−1 A = I)
(since I⃗x = ⃗x )

Let’s step back and think about this for a moment. The only thing we know about
the equa on A⃗x = ⃗b is that A is inver ble. We also know that solu ons to A⃗x = ⃗b
come in three forms: exactly one solu on, inﬁnite solu ons, and no solu on. We just
showed that if A is inver ble, then A⃗x = ⃗b has at least one solu on. We showed that
by se ng ⃗x equal to A−1⃗b, we have a solu on. Is it possible that more solu ons exist?
No. Suppose we are told that a known vector⃗v is a solu on to the equa on A⃗x = ⃗b;
that is, we know that A⃗v = ⃗b. We can repeat the above steps:

A⃗v = ⃗b
A−1 A⃗v = A−1⃗b
I⃗v = A−1⃗b
⃗v = A−1⃗b.
This shows that all solu ons to A⃗x = ⃗b are exactly ⃗x = A−1⃗b when A is inver ble. We
have just proved the following theorem.
109

Chapter 2 Matrix Arithme c

.
Theorem 8

Inver ble Matrices and Solu ons to A⃗x = ⃗b

.

Let A be an inver ble n × n matrix, and let ⃗b be any n × 1
column vector. Then the equa on A⃗x = ⃗b has exactly one
solu on, namely
⃗x = A−1⃗b.

A corollary19 to this theorem is: If A is not inver ble, then A⃗x = ⃗b does not have
exactly one solu on. It may have inﬁnite solu ons and it may have no solu on, and
we
[ would
] need to examine the reduced row echelon form of the augmented matrix
A ⃗b to see which case applies.
We demonstrate our theorem with an example.
. Example 57

S

Solve A⃗x = ⃗b by compu ng ⃗x = A−1⃗b, where




1
0
−3
−15
A =  −3 −4 10  and ⃗b =  57  .
4 −5 −11
−46

Without showing our steps, we compute


94 15 −12
1
−1  .
A−1 =  7
31 5
−4

We then ﬁnd the solu on to A⃗x = ⃗b by compu ng A−1⃗b:
⃗x = A−1⃗b

94
= 7
31

−3
=  −2
4



15 −12
−15
1
−1   57 
5
−4
−46

.

We can easily check our answer:


 

1
0
−3
−3
−15
 −3 −4 10   −2  =  57  .
4 −5 −11
4
−46
.
19 a corollary

110

is an idea that follows directly from a theorem

2.6 The Matrix Inverse
Knowing a matrix is inver ble is incredibly useful.20 Among many other reasons, if
you know A is inver ble, then you know for sure that A⃗x = ⃗b has a solu on (as we just
stated in Theorem 8). In the next sec on we’ll demonstrate many diﬀerent proper es
of inver ble matrices, including sta ng several diﬀerent ways in which we know that
a matrix is inver ble.

Exercises 2.6
In Exercises 1 – 8, A matrix A is given. Find
A−1 using Theorem 7, if it exists.
[
]
1
5
1.
−5 −24
[
]
1 −4
2.
1 −3
[
]
3 0
3.
0 7
]
[
2 5
4.
3 4
]
[
1
−3
5.
−2
6
]
[
3 7
6.
2 4
[
]
1 0
7.
0 1
[
]
0 1
8.
1 0
In Exercises 9 – 28, a matrix A is given. Find
A−1 using Key Idea 10, if it exists.
]
[
−2 3
9.
1
5
[
]
−5 −2
10.
9
2
[
]
1 2
11.
3 4
]
[
5
7
12.
5/3 7/3


25
−10 −4
7
3 
13.  −18
−6
2
1


14.

15.

16.

17.

18.

19.

20.

21.

22.

23.

24.


2
3 4
 −3 6 9 
−1 9 13


1 0
0
 4 1 −7 
20 7 −48


−4 1 5
 −5 1 9 
−10 2 19


5
−1
0
 7
7
1 
−2 −8 −1


1
−5 0
 −2
15
4
4
−19 1


25
−8 0
 −78
25
0
48
−15 1


1
0
0
 7
5
8 
−2 −2 −3


0 0 1
1 0 0
0 1 0


0 1 0
1 0 0
0 0 1


1
0
0
0
 −19 −9 0
4 


 33
4
1 −7 
4
2
0 −1


1 0 0 0
 27 1 0 4 


 18 0 1 4 
4 0 0 1

20 As odd as it may sound, knowing a matrix is inver ble is useful; actually compu ng the inverse isn’t.
This is discussed at the end of the next sec on.

111

Chapter 2 Matrix Arithme c

25.

26.

27.

28.

−15
 55

 −215
−4

1
0
0
1

 0 −4
0 −3

0 0
0 0

1 0
0 1

1 0
0 2

0 0
0 0

45
−164
640
12

−3
15
−62
0


4
−15 

59 
1


2
8
0
0 

−29 −110 
−5
−19

1 0
0 1

0 0
0 0

0
0
0
0 

3
0 
0 −4

In Exercises 29 – 36, a matrix A and a vector
⃗b are given. Solve the equa on A⃗x = ⃗b using
Theorem 8.
[
]
[ ]
3 5
21
29. A =
, ⃗b =
2 3
13
]
[ ]
[
21
1 −4
, ⃗b =
30. A =
77
4 −15
]
]
[
[
−2
9
70
, ⃗b =
31. A =
1
−4 −31

[
32. A =


33.

34.

35.

36.

10
3

]
−57
,
−17

1
2
1
A= 0
−3 0


−17
⃗b =  −5 
20

1
0
A =  8 −2
12 −3


−34
⃗b =  −159 
−243

5
0
A =  −8 1
−2 0


33
⃗b =  −70 
−15

1 −6
1
A=0
2 −8


−69
⃗b =  10 
−102

[
⃗b =


12
6 ,
1

−14
−4

]


−3
−13  ,
−20


−2
5 ,
1


0
0,
1

2.7 Proper es of the Matrix Inverse

.

.
AS YOU READ
...
1. What does it mean to say that two statements are “equivalent?”
2. T/F: If A is not inver ble, then A⃗x = ⃗0 could have no solu ons.
3. T/F: If A is not inver ble, then A⃗x = ⃗b could have inﬁnite solu ons.
4. What is the inverse of the inverse of A?
5. T/F: Solving A⃗x = ⃗b using Gaussian elimina on is faster than using the inverse
of A.

We ended the previous sec on by sta ng that inver ble matrices are important.
Since they are, in this sec on we study inver ble matrices in two ways. First, we look
112

2.7 Proper es of the Matrix Inverse
at ways to tell whether or not a matrix is inver ble, and second, we study proper es
of inver ble matrices (that is, how they interact with other matrix opera ons).
We start with collec ng ways in which we know that a matrix is inver ble. We
actually already know the truth of this theorem from our work in the previous sec on,
but it is good to list the following statements in one place. As we move through other
sec ons, we’ll add on to this theorem.
.
Theorem 9

Inver ble Matrix Theorem
Let A be an n×n matrix. The following statements are equivalent.
(a) A is inver ble.

.

(b) There exists a matrix B such that BA = I.
(c) There exists a matrix C such that AC = I.
(d) The reduced row echelon form of A is I.
(e) The equa on A⃗x = ⃗b has exactly one solu on for every n × 1 vector ⃗b.
(f) The equa on A⃗x
(namely, ⃗x = ⃗0).

= ⃗0 has exactly one solu on

Let’s make note of a few things about the Inver ble Matrix Theorem.
1. First, note that the theorem uses the phrase “the following statements are equivalent.” When two or more statements are equivalent, it means that the truth of
any one of them implies that the rest are also true; if any one of the statements
is false, then they are all false. So, for example, if we determined that the equaon A⃗x = ⃗0 had exactly one solu on (and A was an n × n matrix) then we would
know that A was inver ble, that A⃗x = ⃗b had only one solu on, that the reduced
row echelon form of A was I, etc.
2. Let’s go through each of the statements and see why we already knew they all
said essen ally the same thing.
(a) This simply states that A is inver ble – that is, that there exists a matrix
A−1 such that A−1 A = AA−1 = I. We’ll go on to show why all the other
statements basically tell us “A is inver ble.”
(b) If we know that A is inver ble, then we already know that there is a matrix
B where BA = I. That is part of the deﬁni on of inver ble. However, we
can also “go the other way.” Recall from Theorem 5 that even if all we know
113

Chapter 2 Matrix Arithme c
is that there is a matrix B where BA = I, then we also know that AB = I.
That is, we know that B is the inverse of A (and hence A is inver ble).
(c) We use the same logic as in the previous statement to show why this is the
same as “A is inver ble.”
(d) If A is inver ble, we can ﬁnd the inverse by using Key Idea 10 (which in turn
depends on Theorem 5). The crux of Key Idea 10 is that the reduced row
echelon form of A is I; if it is something else, we can’t ﬁnd A−1 (it doesn’t
exist). Knowing that A is inver ble means that the reduced row echelon
form of A is I. We can go the other way; if we know that the reduced row
echelon form of A is I, then we can employ Key Idea 10 to ﬁnd A−1 , so A is
inver ble.
(e) We know from Theorem 8 that if A is inver ble, then given any vector ⃗b,
A⃗x = ⃗b has always has exactly one solu on, namely ⃗x = A−1⃗b. However,
we can go the other way; let’s say we know that A⃗x = ⃗b always has exactly
solu on. How can we conclude that A is inver ble?
Think about how we, up to this point,
the solu on to A⃗x =
[ determined
]
⃗b. We set up the augmented matrix A ⃗b and put it into reduced row
echelon form. We know that ge ng the iden ty matrix on the le means
that we had a unique solu on (and not ge ng the iden ty means we either
have no solu on or inﬁnite solu ons). So ge ng I on the le means having
a unique solu on; having I on the le means that the reduced row echelon
form of A is I, which we know from above is the same as A being inver ble.
(f) This is the same as the above; simply replace the vector ⃗b with the vector
⃗0.

So we came up with a list of statements that are all equivalent to the statement
“A is inver ble.” Again, if we know that if any one of them is true (or false), then
they are all true (or all false).

Theorem 9 states formally that if A is inver ble, then A⃗x = ⃗b has exactly one soluon, namely A−1⃗b. What if A is not inver ble? What are the possibili es for solu ons
to A⃗x = ⃗b?
We know that A⃗x = ⃗b cannot have exactly one solu on; if it did, then by our theorem it would be inver ble. Recalling that linear equa ons have either one solu on,
inﬁnite solu ons, or no solu on, we are le with the la er op ons when A is not inver ble. This idea is important and so we’ll state it again as a Key Idea.
114

2.7 Proper es of the Matrix Inverse

.
Key Idea 11

Solu ons to A⃗x = ⃗b and the Inver bility of A
Consider the system of linear equa ons A⃗x = ⃗b.

.

1. If A is inver ble, then A⃗x = ⃗b has exactly one solu on,
namely A−1⃗b.
2. If A is not inver ble, then A⃗x = ⃗b has either inﬁnite
solu ons or no solu on.

In Theorem 9 we’ve come up with a list of ways in which we can tell whether or
not a matrix is inver ble. At the same me, we have come up with a list of proper es
of inver ble matrices – things we know that are true about them. (For instance, if we
know that A is inver ble, then we know that A⃗x = ⃗b has only one solu on.)
We now go on to discover other proper es of inver ble matrices. Speciﬁcally, we
want to ﬁnd out how inver bility interacts with other matrix opera ons. For instance,
if we know that A and B are inver ble, what is the inverse of A+B? What is the inverse
of AB? What is “the inverse of the inverse?” We’ll explore these ques ons through an
example.
. Example 58

.Let

[
A=

3 2
0 1

]

[
and B =

]
−2 0
.
1 1

Find:
1. A−1

3. (AB)−1

5. (A + B)−1

2. B−1

4. (A−1 )−1

6. (5A)−1

In addi on, try to ﬁnd connec ons between each of the above.
S

1. Compu ng A−1 is straigh orward; we’ll use Theorem 7.
[
] [
]
1 1 −2
1/3 −2/3
−1
A =
=
0
1
3 0 3
2. We compute B−1 in the same way as above.
[
] [
]
1
1
0
−1/2 0
−1
B =
=
1/2 1
−2 −1 −2
3. To compute (AB)−1 , we ﬁrst compute AB:
[
][
] [
]
3 2
−2 0
−4 2
AB =
=
0 1
1 1
1 1
115

Chapter 2 Matrix Arithme c
We now apply Theorem 7 to ﬁnd (AB)−1 .
[
] [
]
1
1 −2
−1/6 1/3
(AB)−1 =
=
1/6 2/3
−6 −1 −4
4. To compute (A−1 )−1 , we simply apply Theorem 7 to A−1 :
[
] [
]
1
1 2/3
3 2
−1 −1
(A ) =
=
.
0 1
1/3 0 1/3
5. To compute (A + B)−1 , we ﬁrst compute A + B then apply Theorem 7:
[
] [
] [
]
3 2
−2 0
1 2
A+B=
+
=
.
0 1
1 1
1 2
Hence
(A + B)−1 =

1
0

[

2 −2
−1 1

]
=!

Our last expression is really nonsense; we know that if ad − bc = 0, then the
given matrix is not inver ble. That is the case with A + B, so we conclude that
A + B is not inver ble.
6. To compute (5A)−1 , we compute 5A and then apply Theorem 7.
(5A)−1 =

([

15 10
0
5

])−1
=

1
75

[

5
0

−10
15

]

[
=

1/15 −2/15
0
1/5

]

We now look for connec ons between A−1 , B−1 , (AB)−1 , (A−1 )−1 and (A + B)−1 .
.
3. Is there some sort of rela onship between (AB)−1 and A−1 and B−1 ? A ﬁrst
guess that seems plausible is (AB)−1 = A−1 B−1 . Is this true? Using our work
from above, we have
[
][
] [
]
1/3 −2/3
−1/2 0
−1/2 −2/3
A−1 B−1 =
=
.
0
1
1/2 1
1/2
1
Obviously, this is not equal to (AB)−1 . Before we do some further guessing, let’s
think about what the inverse of AB is supposed to do. The inverse – let’s call it
C – is supposed to be a matrix such that
(AB)C = C(AB) = I.
In examining the expression (AB)C, we see that we want B to somehow “cancel”
with C. What “cancels” B? An obvious answer is B−1 . This gives us a thought:
116

2.7 Proper es of the Matrix Inverse
perhaps we got the order of A−1 and B−1 wrong before. A er all, we were hoping to ﬁnd that
?
ABA−1 B−1 = I,
but algebraically speaking, it is hard to cancel out these terms.21 However,
switching the order of A−1 and B−1 gives us some hope. Is (AB)−1 = B−1 A−1 ?
Let’s see.
(AB)(B−1 A−1 ) = A(BB−1 )A−1
= AIA−1
= AA−1

(regrouping by the associa ve property)

=I

(BB−1 = I)
(AI = A)
(AA−1 = I)

Thus it seems that (AB)−1 = B−1 A−1 . Let’s conﬁrm this with our example matrices.
[
][
] [
]
−1/2 0
1/3 −2/3
−1/6 1/3
−1 −1
B A =
=
= (AB)−1 .
1/2 1
0
1
1/6 2/3
It worked!
4. Is there some sort of connec on between (A−1 )−1 and A? The answer is pre y
obvious: they are equal. The “inverse of the inverse” returns one to the original
matrix.
5. Is there some sort of rela onship between (A + B)−1 , A−1 and B−1 ? Certainly, if
we were forced to make a guess without working any examples, we would guess
that
?
(A + B)−1 = A−1 + B−1 .
However, we saw that in our example, the matrix (A + B) isn’t even inver ble.
This pre y much kills any hope of a connec on.
6. Is there a connec on between (5A)−1 and A−1 ? Consider:
[
]
1/15 −2/15
−1
(5A) =
0
1/5
[
]
1 1/3 −2/3
=
0
1/5
5
1
= A−1
5
Yes, there is a connec on!
.
Let’s summarize the results of this example. If A and B are both inver ble matrices,
then so is their product, AB. We demonstrated this with our example, and there is
21 Recall that matrix mul

plica on is not commuta ve.

117

Chapter 2 Matrix Arithme c
more to be said. Let’s suppose that A and B are n × n matrices, but we don’t yet know
if they are inver ble. If AB is inver ble, then each of A and B are; if AB is not inver ble,
then A or B is also not inver ble.
In short, inver bility “works well” with matrix mul plica on. However, we saw
that it doesn’t work well with matrix addi on. Knowing that A and B are inver ble
does not help us ﬁnd the inverse of (A + B); in fact, the la er matrix may not even be
inver ble.22
Let’s do one more example, then we’ll summarize the results of this sec on in a
theorem.


. Example 59

2 0
Find the inverse of A =  0 3
0 0


0
0 .
−7

We’ll ﬁnd A−1 using Key Idea 10.

S



2 0
0 3
0 0




0 1 0 0
0 0 1 0
−7 0 0 1

−→
rref


1 0 0 1/2
0
0
0 1 0
0
1/3
0 
0 0 1
0
0
−1/7

Therefore


A−1


1/2
0
0
1/3
0 .
= 0
0
0
−1/7

.
The matrix A in the previous example is a diagonal matrix: the only nonzero entries
of A lie on the diagonal.23 The rela onship between A and A−1 in the above example
seems pre y strong, and it holds true in general. We’ll state this and summarize the
results of this sec on with the following theorem.

22 The fact that inver bility works well with matrix mul plica on should not come as a surprise. A er
all, saying that A is inver ble makes a statement about the muli plica ve proper es of A. It says that I can
mul ply A with a special matrix to get I. Inver bility, in and of itself, says nothing about matrix addi on,
therefore we should not be too surprised that it doesn’t work well with it.
23 We s ll haven’t formally deﬁned diagonal, but the deﬁni on is rather visual so we risk it. See Deﬁni on
20 on page 123 for more details.

118

2.7 Proper es of the Matrix Inverse

.
Theorem 10

Proper es of Inver ble Matrices
Let A and B be n × n inver ble matrices. Then:
1. AB is inver ble; (AB)−1 = B−1 A−1 .
2. A−1 is inver ble; (A−1 )−1 = A.
3. nA is inver ble for any nonzero scalar n; (nA)−1 =
1 −1
nA .

.

4. If A is a diagonal matrix, with diagonal entries
d1 , d2 , · · · , dn , where none of the diagonal entries are 0, then A−1 exists and is a diagonal matrix. Furthermore, the diagonal entries of A−1 are
1/d1 , 1/d2 , · · · , 1/dn .
Furthermore,
1. If a product AB is not inver ble, then A or B is not inver ble.
2. If A or B are not inver ble, then AB is not inver ble.

We end this sec on with a comment about solving systems of equa ons “in real
life.”24 Solving a system A⃗x = ⃗b by compu ng A−1⃗b seems pre y slick, so it would
make sense that this is the way it is normally done. However, in prac ce, this is rarely
done. There are two main reasons why this is the case.
First, compu ng A−1 and A−1⃗b is “expensive” in the sense that it takes up a lot of
compu ng me. Certainly, our calculators have no trouble dealing with the 3×3 cases
we o en consider in this textbook, but in real life the matrices being considered are
very large (as in, hundreds of thousand rows and columns). Compu ng A−1 alone is
rather imprac cal, and we waste a lot of me if we come to ﬁnd out that A−1 does
not exist. Even if we already know what A−1 is, compu ng A−1⃗b is computa onally
expensive – Gaussian elimina on is faster.
Secondly, compu ng A−1 using the method we’ve described o en gives rise to
numerical roundoﬀ errors. Even though computers o en do computa ons with an
accuracy to more than 8 decimal places, a er thousands of computa ons, roundoﬀs
24 Yes, real people do solve linear equa ons in real life. Not just mathema cians, but economists, engineers, and scien sts of all ﬂavors regularly need to solve linear equa ons, and the matrices they use are
o en huge.
Most people see matrices at work without thinking about it. Digital pictures are simply “rectangular
arrays” of numbers represen ng colors – they are matrices of colors. Many of the standard image processing
opera ons involve matrix opera ons. The author’s wife has a “7 megapixel” camera which creates pictures
that are 3072 × 2304 in size, giving over 7 million pixels, and that isn’t even considered a “large” picture
these days.

119

Chapter 2 Matrix Arithme c
can cause big errors. (A “small” 1, 000 × 1, 000 matrix has 1, 000, 000 entries! That’s
a lot of places to have roundoﬀ errors accumulate!) It is not unheard of to have a
computer compute A−1 for a large matrix, and then immediately have it compute AA−1
and not get the iden ty matrix.25
Therefore, in real life, solu ons to A⃗x = ⃗b are usually found using the methods we
learned in Sec on 2.4. It turns out that even with all of our advances in mathema cs,
it is hard to beat the basic method that Gauss introduced a long me ago.

Exercises 2.7
In Exercises 1 – 4, matrices A and B are given.
Compute (AB)−1 and B−1 A−1 .
[
]
[
]
1 2
3 5
1. A =
, B=
1 1
2 5
]
]
[
[
7 1
1 2
2. A =
, B=
3 4
2 1
[
]
[
]
2 5
1 −1
3. A =
, B=
3 8
1
4
[
]
[
]
2 4
2 2
4. A =
, B=
2 5
6 5
In Exercises 5 – 8, a 2 × 2 matrix A is given.
Compute A−1 and (A−1 )−1 using Theorem 7.
[
]
−3
5
5. A =
1
−2
[
]
3 5
6. A =
2 4
]
[
2 7
7. A =
1 3

[
8. A =

9
7

0
9

]

9. Find 2×2 matrices A and B that are each
inver ble, but A + B is not.
10. Create a random 6 × 6 matrix A, then
have a calculator or computer compute
AA−1 . Was the iden ty matrix returned
exactly? Comment on your results.
11. Use a calculator or computer to compute AA−1 , where


1
1
A=
1
1

2
4
8
16

3
9
27
81


4
16 
.
64 
256

Was the iden ty matrix returned exactly? Comment on your results.

25 The result is usually very close, with the numbers on the diagonal close to 1 and the other entries near
0. But it isn’t exactly the iden ty matrix.

120

3
O

.

M

In the previous chapter we learned about matrix arithme c: adding, subtrac ng, and
mul plying matrices, ﬁnding inverses, and mul plying by scalars. In this chapter we
learn about some opera ons that we perform on matrices. We can think of them as
func ons: you input a matrix, and you get something back. One of these opera ons,
the transpose, will return another matrix. With the other opera ons, the trace and
the determinant, we input matrices and get numbers in return, an idea that is diﬀerent
than what we have seen before.

3.1

The Matrix Transpose

.

.
AS YOU READ
...
1. T/F: If A is a 3 × 5 matrix, then AT will be a 5 × 3 matrix.
2. Where are there zeros in an upper triangular matrix?
3. T/F: A matrix is symmetric if it doesn’t change when you take its transpose.
4. What is the transpose of the transpose of A?
5. Give 2 other terms to describe symmetric matrices besides “interes ng.”

We jump right in with a deﬁni on.

Chapter 3 Opera ons on Matrices

.
Deﬁni on 19

Transpose

.

Let A be an m × n matrix. The tranpsose of A, denoted AT ,
is the n × m matrix whose columns are the respec ve rows
of A.

Examples will make this deﬁni on clear.
[
. Example 60

Find the transpose of A =

1
4

2
5

]
3
.
6

Note that A is a 2 × 3 matrix, so AT will be a 3 × 2 matrix. By the
S
deﬁni on, the ﬁrst column of AT is the ﬁrst row of A; the second column of AT is the
second row of A. Therefore,


1 4
AT =  2 5  .
.
3 6
. Example 61
Find the transpose of the following matrices.




7
2 9 1
1 10 −2
[
A =  2 −1 3 0  B =  3 −5 7  C = 1 −1
−5 3 0 11
4 2 −3

7

8

3

]

We ﬁnd each transpose using the deﬁni on without explana on.
S
Make note of the dimensions of the original matrix and the dimensions of its transpose.




1


7 2 −5
 −1 
1
3
4
 2 −1 3 


T
T
T


 7 


10
−5
2
A =
C
=
B
=


9 3
0 
 8 
−2 7 −3
1 0
11
3
.
No ce that with matrix B, when we took the transpose, the diagonal did not change.
We can see what the diagonal is below where we rewrite B and BT with the diagonal in
bold. We’ll follow this by a deﬁni on of what we mean by “the diagonal of a matrix,”
along with a few other related deﬁni ons.




1 10 −2
1
3 4
B =  3 –5 7  BT =  10 –5 2 
4 2 –3
−2 7 –3
It is probably pre y clear why we call those entries “the diagonal.” Here is the
formal deﬁni on.
122

3.1 The Matrix Transpose

.
Deﬁni on 20

The Diagonal, a Diagonal Matrix, Triangular Matrices
Let A be an m × n matrix. The diagonal of A consists of the
entries a11 , a22 , . . . of A.

.

A diagonal matrix is an n × n matrix in which the only
nonzero entries lie on the diagonal.
An upper (lower) triangular matrix is a matrix in which any
nonzero entries lie on or above (below) the diagonal.

. Example 62
where

Consider the matrices A, B, C and I4 , as well as their transposes,


1
A = 0
0

2
4
0


3
5
6



3
B = 0
0

0
7
0


0
0 
−1



1
0
C=
0
0

2
4
0
0


3
5
.
6
0

Iden fy the diagonal of each matrix, and state whether each matrix is diagonal, upper
triangular, lower triangular, or none of the above.
We ﬁrst compute the transpose of each matrix.

S



1
AT =  2
3

0
4
5


0
0
6



3 0
BT =  0 7
0 0


0
0 
−1



1 0
CT =  2 4
3 5

0
0
6


0
0
0

Note that IT4 = I4 .
The diagonals of A and AT are the same, consis ng of the entries 1, 4 and 6. The
diagonals of B and BT are also the same, consis ng of the entries 3, 7 and −1. Finally,
the diagonals of C and CT are the same, consis ng of the entries 1, 4 and 6.
The matrix A is upper triangular; the only nonzero entries lie on or above the diagonal. Likewise, AT is lower triangular.
The matrix B is diagonal. By their deﬁni ons, we can also see that B is both upper
and lower triangular. Likewise, I4 is diagonal, as well as upper and lower triangular.
Finally, C is upper triangular, with CT being lower triangular. .
Make note of the deﬁni ons of diagonal and triangular matrices. We specify that
a diagonal matrix must be square, but triangular matrices don’t have to be. (“Most”
of the me, however, the ones we study are.) Also, as we men oned before in the
example, by deﬁni on a diagonal matrix is also both upper and lower triangular. Finally, no ce that by deﬁni on, the transpose of an upper triangular matrix is a lower
triangular matrix, and vice-versa.
123

Chapter 3 Opera ons on Matrices
There are many ques ons to probe concerning the transpose opera ons.1 The
ﬁrst set of ques ons we’ll inves gate involve the matrix arithme c we learned from
last chapter. We do this inves ga on by way of examples, and then summarize what
we have learned at the end.
. Example 63

Let
[
A=

1
4

2
5

3
6

]

[
and B =

1
3

]
2 1
.
−1 0

Find AT + BT and (A + B)T .
S

We note that




1 4
1 3
AT =  2 5  and BT =  2 −1  .
3 6
1 0

Therefore



1
A T + BT =  2
3

2
= 4
4

 

4
1 3
5  +  2 −1 
6
1 0

7
4.
6

Also,
([
T

(A + B) =
([
=


1
4

2
5

2
7

4
4


2
= 4
4

] [
])T
3
1 2 1
+
6
3 −1 0
])T
4
6

7
4.
6

.
It looks like “the sum of the transposes is the transpose of the sum.”2 This should
lead us to wonder how the transpose works with mul plica on.
. Example 64

.Let
[
A=

1 2
3 4

1 Remember, this is what mathema

124

]

[
and B =

1 2
1 0

]
−1
.
1

cians do. We learn something new, and then we ask lots of ques ons
about it. O en the ﬁrst ques ons we ask are along the lines of “How does this new thing relate to the old
things I already know about?”
2 This is kind of fun to say, especially when said fast. Regardless of how fast we say it, we should think
about this statement. The “is” represents “equals.” The stuﬀ before “is” equals the stuﬀ a erwards.

3.1 The Matrix Transpose
Find (AB)T , AT BT and BT AT .
S

We ﬁrst note that
[
AT =

1
2

]

3
4


1 1
and BT =  2 0  .
−1 1


Find (AB)T :
([
(AB)T =
([
=


2
4

3
7

2 1
6 1


3
= 2
1
Now ﬁnd AT BT :
[
AT BT =

][

1
3

1 2
1 0
])T

−1
1

])T

7
6
1


1 1
1 3 
2 0
2 4
−1 1
]



= Not deﬁned!
So we can’t compute AT BT . Let’s ﬁnish by compu ng BT AT :


]
1 1 [
1 3
T T


2 0
BA =
2 4
−1 1


3 7
= 2 6
1 1
.
We may have suspected that (AB)T = AT BT . We saw that this wasn’t the case,
though – and not only was it not equal, the second product wasn’t even deﬁned! Oddly
enough, though, we saw that (AB)T = BT AT . 3 To help understand why this is true,
look back at the work above and conﬁrm the steps of each mul plica on.
We have one more arithme c opera on to look at: the inverse.
. Example 65

.Let

[
A=

]
2 7
.
1 4

3 Then again, maybe this isn’t all that “odd.” It is reminiscent of the fact that, when inver ble, (AB)−1 =
B−1 A−1 .

125

Chapter 3 Opera ons on Matrices
Find (A−1 )T and (AT )−1 .
S

We ﬁrst ﬁnd A−1 and AT :
]
]
[
[
4 −7
2 1
−1
T
A =
and A =
.
−1 2
7 4

Finding (A−1 )T :
(A−1 )T =

[
[

=
Finding (AT )−1 :
T −1

(A )

[
=
[
=

]T

4 −7
−1 2

]

4 −1
−7 2

2 1
7 4

]−1

4 −1
−7 2

]

.
It seems that “the inverse of the transpose is the transpose of the inverse.”4
We have just looked at some examples of how the transpose opera on interacts
with matrix arithme c opera ons.5 We now give a theorem that tells us that what we
saw wasn’t a coincidence, but rather is always true.
.
Theorem 11

Proper es of the Matrix Transpose
Let A and B be matrices where the following opera ons are
deﬁned. Then:

.

1. (A + B)T = AT + BT and (A − B)T = AT − BT
2. (kA)T = kAT
3. (AB)T = BT AT
4. (A−1 )T = (AT )−1
5. (AT )T = A
We included in the theorem two ideas we didn’t discuss already. First, that (kA)T =
4 Again, we should think about this statement. The part before “is” states that we take the transpose of
a matrix, then ﬁnd the inverse. The part a er “is” states that we ﬁnd the inverse of the matrix, then take
the transpose. Since these two statements are linked by an “is,” they are equal.
5 These examples don’t prove anything, other than it worked in speciﬁc examples.

126

3.1 The Matrix Transpose
kAT . This is probably obvious. It doesn’t ma er when you mul ply a matrix by a scalar
when dealing with transposes.
The second “new” item is that (AT )T = A. That is, if we take the transpose of a
matrix, then take its transpose again, what do we have? The original matrix.
Now that we know some proper es of the transpose opera on, we are tempted
to play around with it and see what happens. For instance, if A is an m × n matrix, we
know that AT is an n × m matrix. So no ma er what matrix A we start with, we can
always perform the mul plica on AAT (and also AT A) and the result is a square matrix!
Another thing to ask ourselves as we “play around” with the transpose: suppose A
is a square matrix. Is there anything special about A + AT ? The following example has
us try out these ideas.
. Example 66

Let



2
A = 2
1


1 3
−1 1  .
0 1

Find AAT , A + AT and A − AT .
S

Finding AAT :


2
AAT =  2
1

14
= 6
5

Finding A + AT :



2
A + AT =  2
1

2
= 3
4
Finding A − AT :


3
2
11
1
3

6 5
4 3
3 2

1
−1
0


2 1
−1 0 
1 1

 

3
2 2 1
1  +  1 −1 0 
1
3 1 1

3 4
−2 1 
1 2

1
−1
0



 

2 1 3
2 2 1
A − AT =  2 −1 1  −  1 −1 0 
1 0 1
3 1 1


0 −1 2
0 1
= 1
−2 −1 0

.

127

Chapter 3 Opera ons on Matrices
Let’s look at the matrices we’ve formed in this example. First, consider AAT . Something seems to be nice about this matrix – look at the loca on of the 6’s, the 5’s and
the 3’s. More precisely, let’s look at the transpose of AAT . We should no ce that if we
take the transpose of this matrix, we have the very same matrix. That is,


T 

14 6 5
14 6 5
 6 4 3  =  6 4 3  !
5 3 2
5 3 2
We’ll formally deﬁne this in a moment, but a matrix that is equal to its transpose
is called symmetric.
Look at the next part of the example; what do we no ce about A + AT ? We should
see that it, too, is symmetric. Finally, consider the last part of the example: do we
no ce anything about A − AT ?
We should immediately no ce that it is not symmetric, although it does seem
“close.” Instead of it being equal to its transpose, we no ce that this matrix is the
opposite of its transpose. We call this type of matrix skew symmetric.6 We formally
deﬁne these matrices here.
.
Deﬁni on 21

Symmetric and Skew Symmetric Matrices

.

A matrix A is symmetric if AT = A.
A matrix A is skew symmetric if AT = −A.

Note that in order for a matrix to be either symmetric or skew symmetric, it must
be square.
So why was AAT symmetric in our previous example? Did we just luck out?7 Let’s
take the transpose of AAT and see what happens.
(AAT )T = (AT )T (A)T

transpose mul plica on rule

T

= AA

T T

(A ) = A

We have just proved that no ma er what matrix A we start with, the matrix AAT will
be symmetric. Nothing in our string of equali es even demanded that A be a square
matrix; it is always true.
We can do a similar proof to show that as long as A is square, A + AT is a symmetric
matrix.8 We’ll instead show here that if A is a square matrix, then A − AT is skew
6 Some mathema

cians use the term an symmetric

7 Of course not.
8 Why do

128

we say that A has to be square?

3.1 The Matrix Transpose
symmetric.
(A − AT )T = AT − (AT )T

transpose subtrac on rule

=A −A
T

= −(A − AT )
So we took the transpose of A − AT and we got −(A − AT ); this is the deﬁni on of
being skew symmetric.
We’ll take what we learned from Example 66 and put it in a box. (We’ve already
proved most of this is true; the rest we leave to solve in the Exercises.)
.
Theorem 12

Symmetric and Skew Symmetric Matrices
1. Given any matrix A, the matrices AAT and AT A are symmetric.

.

2. Let A be a square matrix. The matrix A + AT is symmetric.
3. Let A be a square matrix. The matrix A − AT is skew
symmetric.

Why do we care about the transpose of a matrix? Why do we care about symmetric
matrices?
There are two answers that each answer both of these ques ons. First, we are
interested in the tranpose of a matrix and symmetric matrices because they are interes ng.9 One par cularly interes ng thing about symmetric and skew symmetric
matrices is this: consider the sum of (A + AT ) and (A − AT ):
(A + AT ) + (A − AT ) = 2A.
This gives us an idea: if we were to mul ply both sides of this equa on by 12 , then the
right hand side would just be A. This means that
A=

1
1
(A + AT ) + (A − AT ) .
|2 {z }
|2 {z }
symmetric

skew symmetric

That is, any matrix A can be wri en as the sum of a symmetric and skew symmetric
matrix. That’s interes ng.
The second reason we care about them is that they are very useful and important in
various areas of mathema cs. The transpose of a matrix turns out to be an important
9 Or:

“neat,” “cool,” “bad,” “wicked,” “phat,” “fo-shizzle.”

129

Chapter 3 Opera ons on Matrices
opera on; symmetric matrices have many nice proper es that make solving certain
types of problems possible.
Most of this text focuses on the preliminaries of matrix algebra, and the actual uses
are beyond our current scope. One easy to describe example is curve ﬁ ng. Suppose
we are given a large set of data points that, when plo ed, look roughly quadra c. How
do we ﬁnd the quadra c that “best ﬁts” this data? The solu on can be found using
matrix algebra, and speciﬁcally a matrix called the pseudoinverse. If A is a matrix, the
pseudoinverse of A is the matrix A† = (AT A)−1 AT (assuming that the inverse exists).
We aren’t going to worry about what all the above means; just no ce that it has a cool
sounding name and the transpose appears twice.
In the next sec on we’ll learn about the trace, another opera on that can be performed on a matrix that is rela vely simple to compute but can lead to some deep
results.

Exercises 3.1
In Exercises 1 – 24, a matrix A is given. Find
AT ; make note if A is upper/lower triangular,
diagonal, symmetric and/or skew symmetric.
[
]
−7
4
1.
4
−6
]
[
3
1
2.
−7 8
]
[
1 0
3.
0 9
[
]
13 −3
4.
−3
1


−5 −9
1 
5.  3
−10 −8


−2 10
−7 
6.  1
9
−2
]
[
4
−7 −4 −9
7.
−9
6
3
−9
]
[
3
−10
0
6
8.
−10 −2 −3 1
[
]
9. −7 −8 2 −3
[
]
10. −9 8 2 −7


−9
4
10
−3 −7 
11.  6
−8
1
−1

130


12.

13.

14.

15.

16.

17.

18.

19.

20.

4
1
9

4
 0
−2

0
 3
−2

2
5
7

0
 6
−1

4
 5
−6

4
 −2
4

−3
 0
0

6
0
0

−5
5
2
0
2
3
3
−4
1
−5
5
−4
−6
0
−4
2
−4
6
0
−7
−2
−4
−3
0
−7
−8
0


2
9
3

−2
3 
6

−2
1 
0

−3
−6 
−10

1
4
0

−9
−10 
9

0
0
5

−5
5 
−3
2
−1
1


6
0 
−7

3.2 The Matrix Trace



1 0
0
0 
21.  0 2
0 0 −1


6
−4 −5
0
2 
22.  −4
−5
2
−2

3.2




0
1
−2
0
4 
23.  −1
2
−4
0


0 0 0
24.  0 0 0 
0 0 0

The Matrix Trace

.

.
AS YOU READ
...
1. T/F: We only compute the trace of square matrices.
2. T/F: One can tell if a matrix is inver ble by compu ng the trace.

In the previous sec on, we learned about an opera on we can peform on matrices,
namely the transpose. Given a matrix A, we can “ﬁnd the transpose of A,” which is
another matrix. In this sec on we learn about a new opera on called the trace. It is
a diﬀerent type of opera on than the transpose. Given a matrix A, we can “ﬁnd the
trace of A,” which is not a matrix but rather a number. We formally deﬁne it here.
.
Deﬁni on 22

The Trace

.

Let A be an n × n matrix. The trace of A, denoted tr(A), is
the sum of the diagonal elements of A. That is,
tr(A) = a11 + a22 + · · · + ann .

This seems like a simple deﬁni on, and it really is. Just to make sure it is clear, let’s
prac ce.
. Example 67

.Find the trace of A, B, C and I4 , where


[
]
[
1 2 0
1 2
1 2
A=
, B =  3 8 1  and C =
3 4
4 5
−2 7 −5

]
3
.
6

To ﬁnd the trace of A, note that the diagonal elements of A are 1
S
and 4. Therefore, tr(A) = 1 + 4 = 5.
We see that the diagonal elements of B are 1, 8 and -5, so tr(B)= 1 + 8 − 5 = 4.
131

Chapter 3 Opera ons on Matrices
The matrix C is not a square matrix, and our deﬁni on states that we must start
with a square matrix. Therefore tr(C) is not deﬁned.
Finally, the diagonal of I4 consists of four 1s. Therefore tr(I4 ) = 4. .
Now that we have deﬁned the trace of a matrix, we should think like mathema cians and ask some ques ons. The ﬁrst ques ons that should pop into our minds
should be along the lines of “How does the trace work with other matrix opera ons?”10
We should think about how the trace works with matrix addi on, scalar mul plica on,
matrix mul plica on, matrix inverses, and the transpose.
We’ll give a theorem that will formally tell us what is true in a moment, but ﬁrst
let’s play with two sample matrices and see if we can see what will happen. Let




2 1
3
2 0
A =  2 0 −1  and B =  −1 2
3 −1 3
0 2


1
0 .
−1

It should be clear that tr(A) = 5 and tr(B) = 3. What is tr(A + B)?


2
tr(A + B) = tr  2
3

4
= tr  1
3

 
2 0
1
3
0 −1  +  −1 2
0 2
−1 3

1 4
2 −1 
1 2


1
0 
−1

=8
So we no ce that tr(A + B) = tr(A) + tr(B). This probably isn’t a coincidence.
How does the trace work with scalar mul plica on? If we mul ply A by 4, then the
diagonal elements will be 8, 0 and 12, so tr(4A) = 20. Is it a coincidence that this is 4
mes the trace of A?
Let’s move on to matrix mul plica on. How will the trace of AB relate to the traces
of A and B? Let’s see:




2 1
3
2 0 1
tr(AB) = tr  2 0 −1   −1 2 0 
3 −1 3
0 2 −1


3 8 −1
= tr  4 −2 3 
7 4
0
=1
10 Recall that we asked a

132

similar ques on once we learned about the transpose.

3.2 The Matrix Trace
It isn’t exactly clear what the rela onship is among tr(A), tr(B) and tr(AB). Before
moving on, let’s ﬁnd tr(BA):



2 0 1
2 1
3
tr(BA) = tr  −1 2 0   2 0 −1 
0 2 −1
3 −1 3


7 1
9
= tr  2 −1 −5 
1 1 −5
=1
We no ce that tr(AB) = tr(BA). Is this coincidental?
How are the traces of A and A−1 related? We compute A−1 and ﬁnd that


1/17 6/17
1/17
A−1 =  9/17 3/17 −8/17  .
2/17 −5/17 2/17
Therefore tr(A−1 ) = 6/17. Again, the rela onship isn’t clear.11
Finally, let’s see how the trace is related to the transpose. We actually don’t have
to formally compute anything. Recall from the previous sec on that the diagonals
of A and AT are iden cal; therefore, tr(A) = tr(AT ). That, we know for sure, isn’t a
coincidence.
We now formally state what equali es are true when considering the interac on
of the trace with other matrix opera ons.
.
Theorem 13

Proper es of the Matrix Trace
Let A and B be n × n matrices. Then:
1. tr(A + B) = tr(A) + tr(B)

.

2. tr(A − B) = tr(A) − tr(B)
3. tr(kA) = k·tr(A)
4. tr(AB) = tr(BA)
5. tr(AT ) = tr(A)

One of the key things to note here is what this theorem does not say. It says nothing
about how the trace relates to inverses. The reason for the silence in these areas is
that there simply is not a rela onship.
11 Something to think about: we know that not all square matrices are inver ble. Would we be able to
tell just by the trace? That seems unlikely.

133

Chapter 3 Opera ons on Matrices
We end this sec on by again wondering why anyone would care about the trace of
matrix. One reason mathema cians are interested in it is that it can give a measurement of the “size”12 of a matrix.
Consider the following 2 × 2 matrices:
[
]
]
[
1 −2
6
7
A=
and B =
.
1 1
11 −4
These matrices have the same trace, yet B clearly has bigger elements in it. So how
can we use the trace to determine a “size” of these matrices? We can consider tr(AT A)
and tr(BT B).
([
][
])
1 1
1 −2
T
tr(A A) = tr
−2 1
1 1
([
])
2 −1
= tr
−1 5
=7
([
T

tr(B B) = tr
([
= tr

6 11
7 −4

][

157 −2
−2 65

6
11
])

7
−4

])

= 222
Our concern is not how to interpret what this “size” measurement means, but
rather to demonstrate that the trace (along with the transpose) can be used to give
(perhaps useful) informa on about a matrix.13
12 There are many diﬀerent measurements of a matrix size. In this text, we just refer to its dimensions.
Some measurements of size refer the magnitude of the elements in the matrix. The next sec on describes
yet another measurement of matrix size.
13 This example brings to light many interes ng ideas that we’ll ﬂesh out just a li le bit here.
1. No ce that the elements of A are 1, −2, 1 and 1. Add the squares of these numbers: 12 + (−2)2 +
12 + 12 = 7 = tr(AT A).
No ce that the elements of B are 6, 7, 11 and -4. Add the squares of these numbers: 62 + 72 +
112 + (−4)2 = 222 = tr(BT B).
Can you see why this is true? When looking at mul plying AT A, focus only on where the elements
on the diagonal come from since they are the only ones that ma er when taking the trace.

2. You can conﬁrm on your own that regardless of the dimensions of A, tr(AT A) = tr(AAT ). To see why
this is true, consider the previous point. (Recall also that AT A and AAT are always square, regardless
of the dimensions of A.)
√
3. Mathema cians are actually more interested in tr(AT A) than just tr(AT A). The reason for this is a
bit complicated; the short answer is that “it works be er.” The reason “it works be er” is related to
the Pythagorean Theorem, all of all√
things. If we know that the legs of a right triangle have length
a and b, we are more interested in a2 + b2 than just a2 + b2 . Of course, this explana on raises
more ques ons than it answers; our goal here is just to whet your appe te and get you to do some
more reading. A Numerical Linear Algebra book would be a good place to start.

134

3.3 The Determinant

Exercises 3.2
In Exercises 1 – 15, ﬁnd the trace of the given
matrix.
]
[
1 −5
1.
9
5
]
[
−3 −10
2.
−6
4
[
]
7
5
3.
−5 −4
[
]
−6 0
4.
−10 9


−4
1
1
0
0 
5.  −2
−1 −2 −5


0
−3 1
−5 5 
6.  5
−4
1
0


−2 −3 5
2
0
7.  5
−1 −3 1


4
2
−1
1
4 
8.  −4
0
−5
5
[
]
2
6
4
9.
−1 8 −10


6 5
10.  2 10 
3 3


−10
6
−7 −9
 −2
1
6
−9 

11. 
 0
4
−4
0 
−3 −9
3
−10

3.3



5
 −7
12. 
 9
−4

2
4
−9
8

2
−7
−7
−8


2
−3 

2 
−2

13. I4
14. In
15. A matrix A that is skew symmetric.
In Exercises 16 – 19, verify Theorem 13 by:
1. Showing that tr(A)+tr(B) = tr(A + B)
and
2. Showing that tr(AB) = tr(BA).
[
]
[
1 −1
−1
16. A =
, B=
9 −6
−6
[
17. A =

0
1



]
−8
,
8

−8
18. A =  10
−10

−10
B =  −4
3

−10
5
1
−4
−5
7

[
B=

−4
−4

0
3
5
2

]
]


10
−6 
3

−3
4 
3


−10
7
5
7
−5 
19. A =  7
8
−9
2


−3 −4
9
−1 −9 
B= 4
−7 −8 10


The Determinant

.

.
AS YOU READ
...
1. T/F: The determinant of a matrix is always posi ve.
2. T/F: To compute the determinant of a 3 × 3 matrix, one needs to compute the
determinants of 3 2 × 2 matrices.
3. Give an example of a 2 × 2 matrix with a determinant of 3.
135

Chapter 3 Opera ons on Matrices
In this chapter so far we’ve learned about the transpose (an opera on on a matrix that returns another matrix) and the trace (an opera on on a square matrix that
returns a number). In this sec on we’ll learn another opera on on square matrices
that returns a number, called the determinant. We give a pseudo-deﬁni on of the
determinant here.
The determinant of an n × n matrix A is a number, denoted
det(A), that is determined by A.
That deﬁni on isn’t meant to explain everything; it just gets us started by making
us realize that the determinant is a number. The determinant is kind of a tricky thing
to deﬁne. Once you know and understand it, it isn’t that hard, but ge ng started is a
bit complicated.14 We start simply; we deﬁne the determinant for 2 × 2 matrices.
.
Deﬁni on 23

Determinant of 2 × 2 Matrices
[

Let
A=

]
a b
.
c d

.

The determinant of A, denoted by
det (A) or

a b
c d

,

is ad − bc.
We’ve seen the expression ad − bc before. In Sec on 2.6, we saw that a 2 × 2 matrix
A has inverse
[
]
1
d −b
ad − bc −c a
as long as ad − bc ̸= 0; otherwise, the inverse does not exist. We can rephrase the
above statement now: If det(A)̸= 0, then
[
]
1
d −b
A−1 =
.
det (A) −c a
A brief word about the nota on: no ce that we can refer to the determinant by using what looks like absolute value bars around the entries of a matrix. We discussed at
the end of the last sec on the idea of measuring the “size” of a matrix, and men oned
that there are many diﬀerent ways to measure size. The determinant is one such way.
Just as the absolute value of a number measures its size (and ignores its sign), the determinant of a matrix is a measurement of the size of the matrix. (Be careful, though:
det(A) can be nega ve!)
Let’s prac ce.
14 It’s similar

136

to learning to ride a bike. The riding itself isn’t hard, it is ge ng started that’s diﬃcult.

3.3 The Determinant
. Example 68

Find the determinant of A, B and C where
[
[
[
]
]
]
1 2
3 −1
1 −3
A=
, B=
and C =
.
3 4
2 7
−2 6
Finding the determinant of A:

S

det (A) =

1 2
3 4

= 1(4) − 2(3)
= −2.
Similar computa ons show that det (B) = 3(7) − (−1)(2) = 23 and det (C) =
1(6) − (−3)(−2) = 0. .
Finding the determinant of a 2 × 2 matrix is pre y straigh orward. It is natural to
ask next “How do we compute the determinant of matrices that are not 2 × 2?” We
ﬁrst need to deﬁne some terms.15
.
Deﬁni on 24

Matrix Minor, Cofactor
Let A be an n × n matrix. The i, j minor of A, denoted Ai,j , is
the determinant of the (n − 1) × (n − 1) matrix formed by
dele ng the ith row and jth column of A.

.

The i, j-cofactor of A is the number
Cij = (−1)i+j Ai,j .
No ce that this deﬁni on makes reference to taking the determinant of a matrix, while
we haven’t yet deﬁned what the determinant is beyond 2 × 2 matrices. We recognize
this problem, and we’ll see how far we can go before it becomes an issue.
Examples will help.
. Example 69

.Let


1 2
A = 4 5
7 8





1
3
 −3
6  and B = 
 −1
9
1


2 0 8
5 7 2
.
9 −4 6 
1 1 1

Find A1,3 , A3,2 , B2,1 , B4,3 and their respec ve cofactors.
15 This is

the standard deﬁni on of these two terms, although slight varia ons exist.

137

Chapter 3 Opera ons on Matrices
To compute the minor A1,3 , we remove the ﬁrst row and third colS
umn of A then take the determinant.




[
]
1 2 3
1 2 3
4 5
A = 4 5 6 ⇒ 4 5 6 ⇒
7 8
7 8 9
7 8 9
A1,3 =

4
7

5
8

= 32 − 35 = −3.

The corresponding cofactor, C1,3 , is
C1,3 = (−1)1+3 A1,3 = (−1)4 (−3) = −3.
The minor A3,2 is found by removing the third row and second column of A then
taking the determinant.




[
]
1 2 3
1 2 3
1 3




A= 4 5 6 ⇒ 4 5 6 ⇒
4 6
7 8 9
7 8 9
A3,2 =

1 3
4 6

= 6 − 12 = −6.

The corresponding cofactor, C3,2 , is
C3,2 = (−1)3+2 A3,2 = (−1)5 (−6) = 6.
.
The minor B2,1 is found by removing the second
taking the determinant.



1 2 0 8
1 2 0
 −3 5 7 2 
 -3 5 7


B=
 −1 9 −4 6  ⇒  -1 9 −4
1 1 1 1
1 1 1
B2,1 =

2
9
1

0 8
−4 6
1 1

row and ﬁrst column of B then


8
2
2
 ⇒ 9
6
1
1


0 8
−4 6 
1 1

!

=?

We’re a bit stuck. We don’t know how to ﬁnd the determinate of this 3 × 3 matrix.
We’ll come back to this later. The corresponding cofactor is
C2,1 = (−1)2+1 B2,1 = −B2,1 ,
whatever this number happens to be.
The minor B4,3 is found by removing the fourth row and third column of B then
taking the determinant.






1 2 0 8
1 2 0 8
1 2 8
 −3 5 7 2 
 −3 5 7 2 





B=
 −1 9 −4 6  ⇒  −1 9 -4 6  ⇒ −3 5 2
−1 9 6
1 1 1 1
1 1 1 1
138

3.3 The Determinant

B4,3 =

1 2 8
−3 5 2
−1 9 6

!

=?

Again, we’re stuck. We won’t be able to fully compute C4,3 ; all we know so far is that
C4,3 = (−1)4+3 B4,3 = (−1)B4,3 .
Once we learn how to compute determinates for matrices larger than 2 × 2 we can
come back and ﬁnish this exercise. .
In our previous example we ran into a bit of trouble. By our deﬁni on, in order
to compute a minor of an n × n matrix we needed to compute the determinant of a
(n − 1) × (n − 1) matrix. This was ﬁne when we started with a 3 × 3 matrix, but when
we got up to a 4 × 4 matrix (and larger) we run into trouble.
We are almost ready to deﬁne the determinant for any square matrix; we need
one last deﬁni on.
.
Deﬁni on 25

Cofactor Expansion
Let A be an n × n matrix.

.

The cofactor expansion of A along the ith row is the sum
ai,1 Ci,1 + ai,2 Ci,2 + · · · + ai,n Ci,n .
The cofactor expansion of A down the jth column is the sum
a1,j C1,j + a2,j C2,j + · · · + an,j Cn,j .

The nota on of this deﬁni on might be a li le in mida ng, so let’s look at an example.
. Example 70

.Let



1 2
A = 4 5
7 8


3
6.
9

Find the cofactor expansions along the second row and down the ﬁrst column.
S

By the deﬁni on, the cofactor expansion along the second row is

the sum
a2,1 C2,1 + a2,2 C2,2 + a2,3 C2,3 .
(Be sure to compare the above line to the deﬁni on of cofactor expansion, and see
how the “i” in the deﬁni on is replaced by “2” here.)
139

Chapter 3 Opera ons on Matrices
We’ll ﬁnd each cofactor and then compute the sum.
(
)
we removed the second row and
2 3
2+1
ﬁrst column of A to compute the
C2,1 = (−1)
= (−1)(−6) = 6
8 9
minor

C2,2 = (−1)2+2

C2,3 = (−1)2+3

(

1 3
7 9

= (1)(−12) = −12

1 2
7 8

(
= (−1)(−6) = 6

we removed the second row and
second column of A to compute
the minor

we removed the second row and
third column of A to compute the
minor

)

)

Thus the cofactor expansion along the second row is
a2,1 C2,1 + a2,2 C2,2 + a2,3 C2,3 = 4(6) + 5(−12) + 6(6)
= 24 − 60 + 36
=0
At the moment, we don’t know what to do with this cofactor expansion; we’ve just
successfully found it.
We move on to ﬁnd the cofactor expansion down the ﬁrst column. By the deﬁnion, this sum is
a1,1 C1,1 + a2,1 C2,1 + a3,1 C3,1 .
(Again, compare this to the above deﬁni on and see how we replaced the “j” with “1.”)
We ﬁnd each cofactor:
(
)
5 6
we removed the ﬁrst row and ﬁrst
C1,1 = (−1)1+1
= (1)(−3) = −3
column
of
A
to
compute
the
minor
8 9

C2,1 = (−1)2+1

2
8

3
9

= (−1)(−6) = 6

C3,1 = (−1)3+1

2 3
5 6

= (1)(−3) = −3

( we computed this cofactor above )
(

we removed the third row and ﬁrst
column of A to compute the minor

)

The cofactor expansion down the ﬁrst column is

.

a1,1 C1,1 + a2,1 C2,1 + a3,1 C3,1 = 1(−3) + 4(6) + 7(−3)
= −3 + 24 − 21
=0

Is it a coincidence that both cofactor expansions were 0? We’ll answer that in a
while.
140

3.3 The Determinant
This sec on is en tled “The Determinant,” yet we don’t know how to compute it
yet except for 2 × 2 matrices. We ﬁnally deﬁne it now.
.
Deﬁni on 26

The Determinant
The determinant of an n×n matrix A, denoted det(A) or |A|,
is a number given by the following:
• if A is a 1 × 1 matrix A = [a], then det (A) = a.
• if A is a 2 × 2 matrix
A=

.
[

]
a b
,
c d

then det (A) = ad − bc.
• if A is an n × n matrix, where n ≥ 2, then det(A) is the
number found by taking the cofactor expansion along
the ﬁrst row of A. That is,
det (A) = a1,1 C1,1 + a1,2 C1,2 + · · · + a1,n C1,n .

No ce that in order to compute the determinant of an n × n matrix, we need to compute the determinants of n (n − 1) × (n − 1) matrices. This can be a lot of work. We’ll
later learn how to shorten some of this. First, let’s prac ce.
. Example 71

.Find the determinant of


1 2
A = 4 5
7 8


3
6.
9

No ce that this is the matrix from Example 70. The cofactor exS
pansion along the ﬁrst row is
det (A) = a1,1 C1,1 + a1,2 C1,2 + a1,3 C1,3 .
We’ll compute each cofactor ﬁrst then take the appropriate sum.

141

Chapter 3 Opera ons on Matrices

C1,1 = (−1)1+1 A1,1

C1,2 = (−1)1+2 A1,2

5 6
8 9

=1·

= (−1) ·

= 45 − 48
= −3

4 6
7 9

= (−1)(36 − 42)
=6

C1,3 = (−1)1+3 A1,3
4 5
7 8

=1·

= 32 − 35
= −3

Therefore the determinant of A is
det (A) = 1(−3) + 2(6) + 3(−3) = 0.
.
. Example 72

Find the determinant of


3 6
7
A =  0 2 −1  .
3 −1 1

We’ll compute each cofactor ﬁrst then ﬁnd the determinant.

S

C1,1 = (−1)1+1 A1,1
=1·

2 −1
−1 1

C1,2 = (−1)1+2 A1,2
= (−1) ·

0 −1
3 1

C1,3 = (−1)1+3 A1,3
=1·

0
3

=2−1

= (−1)(0 + 3)

=0−6

=1

= −3

= −6

2
−1

Thus the determinant is
det (A) = 3(1) + 6(−3) + 7(−6) = −57.

.
. Example 73

.Find the determinant of


1 2 1 2
 −1 2 3 4 

A=
 8 5 −3 1  .
5 9 −6 3

This, quite frankly, will take quite a bit of work. In order to compute
S
this determinant, we need to compute 4 minors, each of which requires ﬁnding the
determinant of a 3 × 3 matrix! Complaining won’t get us any closer to the solu on,16
16 But it

142

might make us feel a li le be er. Glance ahead: do you see how much work we have to do?!?

3.3 The Determinant
so let’s get started. We ﬁrst compute the cofactors:

C1,1 = (−1)1+1 A1,1
2 3 4
5 −3 1
9 −6 3

=1·

(we must compute the determinant)
of this 3 × 3 matrix

−3 1
−6 3

= 2 · (−1)1+1

+ 3 · (−1)1+2

5 1
9 3

+ 4 · (−1)1+3

5 −3
9 −6

= 2(−3) + 3(−6) + 4(−3)
= −36

C1,2 = (−1)1+2 A1,2
= (−1) ·
[

−1
8
5

3 4
−3 1
−6 3

= (−1) (−1) · (−1)
|

1+1

(we must compute the determinant)
of this 3 × 3 matrix

−3
−6

1
3

+ 3 · (−1)
{z

1+2

8 1
5 3

+ 4 · (−1)

1+3

the determinate of the 3 × 3 matrix

8 −3
5 −6

]
}

= (−1) [(−1)(−3) + 3(−19) + 4(−33)]
= 186

.

C1,3 = (−1)1+3 A1,3
=1·

−1 2 4
8 5 1
5 9 3

= (−1) · (−1)1+1

(we must compute the determinant)
of this 3 × 3 matrix

5
9

1
3

+ 2 · (−1)1+2

8 1
5 3

+ 4 · (−1)1+3

8 5
5 9

= (−1)(6) + 2(−19) + 4(47)
= 144
143

Chapter 3 Opera ons on Matrices

C1,4 = (−1)1+4 A1,4
= (−1) ·
[

−1 2
8 5
5 9

3
−3
−6

= (−1) (−1) · (−1)
|

(we must compute the determinant)
of this 3 × 3 matrix

5 −3
9 −6

1+1

+ 2 · (−1)
{z

1+2

8 −3
5 −6

+ 3 · (−1)

1+3

8
5

the determinate of the 3 × 3 matrix

= (−1) [(−1)(−3) + 2(33) + 3(47)]
= −210
We’ve computed our four cofactors. All that is le is to compute the cofactor expansion.
det (A) = 1(−36) + 2(186) + 1(144) + 2(−210) = 60.
As a way of “visualizing” this, let’s write out the cofactor expansion again but including the matrices in their place.
det (A) = a1,1 C1,1 + a1,2 C1,2 + a1,3 C1,3 + a1,4 C1,4
= 1(−1)2
|

2
5
9

3 4
−1 3 4
−3 1 + 2(−1)3 8 −3 1
−6 3
5 −6 3
{z
}
{z
}
|

= −36

= −186

+
1(−1)4

−1 2 4
−1 2 3
8 5 1 + 2(−1)5 8 5 −3
5 9 3
5 9 −6
{z
}
{z
}
|
|
= 144

.

= 210

= 60

That certainly took a while; it required more than 50 mul plica ons (we didn’t
count the addi ons). To compute the determinant of a 5 × 5 matrix, we’ll need to
compute the determinants of ﬁve 4 × 4 matrices, meaning that we’ll need over 250
mul plica ons! Not only is this a lot of work, but there are just too many ways to make
silly mistakes.17 There are some tricks to make this job easier, but regardless we see
the need to employ technology. Even then, technology quickly bogs down. A 25 × 25
matrix is considered “small” by today’s standards,18 but it is essen ally impossible for
a computer to compute its determinant by only using cofactor expansion; it too needs
to employ “tricks.”
17 The author

made three when the above example was originally typed.
is common for mathema cians, scien sts and engineers to consider linear systems with thousands
of equa ons and variables.
18 It

144

5
9

]
}

3.3 The Determinant
In the next sec on we will learn some of these tricks as we learn some of the
proper es of the determinant. Right now, let’s review the essen als of what we have
learned.
1. The determinant of a square matrix is a number that is determined by the matrix.
2. We ﬁnd the determinant by compu ng the cofactor expansion along the ﬁrst
row.
3. To compute the determinant of an n × n matrix, we need to compute n determinants of (n − 1) × (n − 1) matrices.

Exercises 3.3
In Exercises 1 – 8, ﬁnd the determinant of the
2 × 2 matrix.
[
]
10 7
1.
8 9
[
]
6
−1
2.
−7
8
[
]
−1 −7
3.
−5
9
]
[
−10 −1
4.
−4
7
]
[
8 10
5.
2 −3
]
[
10
−10
6.
−10
0
[
]
1 −3
7.
7
7
]
[
−4 −5
8.
−1 −4
In Exercises 9 – 12, a matrix A is given.
(a) Construct the submatrices used to
compute the minors A1,1 , A1,2 and A1,3 .
(b) Find the cofactors C1,1 , C1,2 , and C1,3 .


−7 −3 10
7
6 
9.  3
1
6
10


−2 −9
6
8 
10.  −10 −6
0
−3 −2




−5 −3 3
3
10 
11.  −3
−9
3
9


−6 −4
6
0
0 
12.  −8
−10
8
−1
In Exercises 13 – 24, ﬁnd the determinant
of the given matrix using cofactor expansion
along the ﬁrst row.


3
2
3
1
−10 
13.  −6
−8 −9 −9


8
−9 −2
9
−7 
14.  −9
5
−1
9


−4
3
−4
3 
15.  −4 −5
3
−4
5


1 −2 1
5
4
16.  5
4
0
0


1 −4 1
3
0
17.  0
1
2
2


3
−1
0
0
−4 
18.  −3
0
−1 −4


−5 0 −4
4 −1 
19.  2
−5 0 −4


1
0 0
1 0
20.  0
−1 1 1

145

Chapter 3 Opera ons on Matrices


0
 1

21. 
1
−1

−1
 −1

22. 
1
1

−5
 −3

23. 
−2
5

0
1
1
0

−1
0
−1
1

0
0
1
0

0
0
1
−1

1
−5
4
4

0
2
−3
−3


−1
1 

0 
0

−1
1 

0 
−1

0
5

4
3



2
 3

24. 
0
−2

−1
−3
4
−5


4
2 

1 
−5

4
3
−5
−2

25. Let A be a 2 × 2 matrix;
[
A=

a
c

]
b
.
d

Show why det(A) = ad−bc by computing the cofactor expansion of A along
the ﬁrst row.

3.4 Proper es of the Determinant

.

.
AS YOU READ
...
1. Having the choice to compute the determinant of a matrix using cofactor expansion along any row or column is most useful when there are lots of what in a row
or column?
2. Which elementary row opera on does not change the determinant of a matrix?
3. Why do mathema cians rarely smile?
4. T/F: When computers are used to compute the determinant of a matrix, cofactor
expansion is rarely used.

In the previous sec on we learned how to compute the determinant. In this secon we learn some of the proper es of the determinant, and this will allow us to
compute determinants more easily. In the next sec on we will see one applica on
of determinants.
We start with a theorem that gives us more freedom when compu ng determinants.
.
Theorem 14

Cofactor Expansion Along Any Row or Column

.

Let A be an n × n matrix. The determinant of A can be computed using cofactor expansion along any row or column of
A.

146

3.4 Proper es of the Determinant
We alluded to this fact way back a er Example 70. We had just learned what cofactor expansion was and we prac ced along the second row and down the third column.
Later, we found the determinant of this matrix by compu ng the cofactor expansion
along the ﬁrst row. In all three cases, we got the number 0. This wasn’t a coincidence.
The above theorem states that all three expansions were actually compu ng the determinant.
How does this help us? By giving us freedom to choose any row or column to use
for the expansion, we can choose a row or column that looks “most appealing.” This
usually means “it has lots of zeros.” We demonstrate this principle below.
. Example 74

Find the determinant of


1
2 0 9
 2 −3 0 5 
.
A=
 7
2 3 8
−4 1 0 2

Our ﬁrst reac on may well be “Oh no! Not another 4 × 4 deterS
minant!” However, we can use cofactor expansion along any row or column that we
choose. The third column looks great; it has lots of zeros in it. The cofactor expansion
along this column is
det (A) = a1,3 C1,3 + a2,3 C2,3 + a3,3 C3,3 + a4,3 C4,3
= 0 · C1,3 + 0 · C2,3 + 3 · C3,3 + 0 · C4,3
The wonderful thing here is that three of our cofactors are mul plied by 0. We
won’t bother compu ng them since they will not contribute to the determinant. Thus
det (A) = 3 · C3,3
= 3 · (−1)3+3 ·
= 3 · (−147)
.

1
2 9
2 −3 5
−4 1 2
( we computed the determinant of the 3 × 3 matrix )
without showing our work; it is −147

= −447

Wow. That was a lot simpler than compu ng all that we did in Example 73. Of
course, in that example, we didn’t really have any shortcuts that we could have employed.
. Example 75

.Find the determinant of

1 2 3
0 6 7

A=
 0 0 10
0 0 0
0 0 0


4
5
8
9 

11 12 
.
13 14 
0 15
147

Chapter 3 Opera ons on Matrices

At ﬁrst glance, we think “I don’t want to ﬁnd the determinant of
S
a 5 × 5 matrix!” However, using our newfound knowledge, we see that things are not
that bad. In fact, this problem is very easy.
What row or column should we choose to ﬁnd the determinant along? There are
two obvious choices: the ﬁrst column or the last row. Both have 4 zeros in them. We
choose the ﬁrst column.19 We omit most of the cofactor expansion, since most of it is
just 0:
6 7
8
9
0 10 11 12
1+1
.
det (A) = 1 · (−1)
·
0 0 13 14
0 0
0 15
Similarly, this determinant is not bad to compute; we again choose to use cofactor expansion along the ﬁrst column. Note: technically, this cofactor expansion is
6 · (−1)1+1 A1,1 ; we are going to drop the (−1)1+1 terms from here on out in this
example (it will show up a lot...).
det (A) = 1 · 6 ·

10 11 12
0 13 14 .
0
0 15

You can probably can see a trend. We’ll ﬁnish out the steps without explaining
each one.
det (A) = 1 · 6 · 10 ·

13 14
0 15

= 1 · 6 · 10 · 13 · 15
.

= 11700

We see that the ﬁnal determinant is the product of the diagonal entries. This works
for any triangular matrix (and since diagonal matrices are triangular, it works for diagonal matrices as well). This is an important enough idea that we’ll put it into a box.
.
Key Idea 12

The Determinant of Triangular Matrices

.

The determinant of a triangular matrix is the product of its
diagonal elements.

It is now again me to start thinking like a mathema cian. Remember, mathema cians see something new and o en ask “How does this relate to things I already
19 We do not choose this because it is the be er choice; both op ons are good. We simply had to make
a choice.

148

3.4 Proper es of the Determinant
know?” So now we ask, “If we change a matrix in some way, how is it’s determinant
changed?”
The standard way that we change matrices is through elementary row opera ons.
If we perform an elementary row opera on on a matrix, how will the determinant of
the new matrix compare to the determinant of the original matrix?
Let’s experiment ﬁrst and then we’ll oﬃcially state what happens.
. Example 76

Let

[
A=

1
3

]
2
.
4

Let B be formed from A by doing one of the following elementary row opera ons:
1. 2R1 + R2 → R2
2. 5R1 → R1
3. R1 ↔ R2
Find det(A) as well as det(B) for each of the row opera ons above.
It is straigh orward to compute det (A) = −2.
S
Let B be formed by performing the row opera on in 1) on A; thus
[
B=

1
5

]
2
.
8

It is clear that det (B) = −2, the same as det(A).
Now let B be formed by performing the elementary row opera on in 2) on A; that
is,
[
]
5 10
B=
.
3 4
We can see that det (B) = −10, which is 5 · det (A).
Finally, let B be formed by the third row opera on given; swap the two rows of A.
We see that
[
]
3 4
B=
1 2
and that det (B) = 2, which is (−1) · det (A). .
We’ve seen in the above example that there seems to be a rela onship between
the determinants of matrices “before and a er” being changed by elementary row
opera ons. Certainly, one example isn’t enough to base a theory on, and we have not
proved anything yet. Regardless, the following theorem is true.
149

Chapter 3 Opera ons on Matrices

.
Theorem 15

The Determinant and Elementary Row Opera ons
Let A be an n × n matrix and let B be formed by performing
one elementary row opera on on A.

.

1. If B is formed from A by adding a scalar mul ple of
one row to another, then det (B) = det (A).
2. If B is formed from A by mul plying one row of A by a
scalar k, then det (B) = k · det (A).
3. If B is formed from A by interchanging two rows of A,
then det (B) = −det (A).

Let’s put this theorem to use in an example.
. Example 77

Let



1 2
A = 0 1
1 1


1
1.
1

Compute det(A), then ﬁnd the determinants of the following matrices by inspec on
using Theorem 15.






1 1 1
1 2 1
1 −1 −2
1 
B = 1 2 1 C = 0 1 1 D = 0 1
0 1 1
7 7 7
1 1
1

Compu ng det(A) by cofactor expansion down the ﬁrst column or
S
along the second row seems like the best choice, u lizing the one zero in the matrix.
We can quickly conﬁrm that det (A) = 1.
To compute det(B), no ce that the rows of A were rearranged to form B. There are
diﬀerent ways to describe what happened; saying R1 ↔ R2 was followed by R1 ↔ R3
produces B from A. Since there were two row swaps, det (B) = (−1)(−1)det (A) =
det (A) = 1.
No ce that C is formed from A by mul plying the third row by 7. Thus det (C) =
7 · det (A) = 7.
It takes a li le thought, but we can form D from A by the opera on −3R2 +R1 → R1 .
This type of elementary row opera on does not change determinants, so det (D) =
det (A). .

150

3.4 Proper es of the Determinant
Let’s con nue to think like mathema cians; mathema cians tend to remember
“problems” they’ve encountered in the past,20 and when they learn something new,
in the backs of their minds they try to apply their new knowledge to solve their old
problem.
What “problem” did we recently uncover? We stated in the last chapter that even
computers could not compute the determinant of large matrices with cofactor expansion. How then can we compute the determinant of large matrices?
We just learned two interes ng and useful facts about matrix determinants. First,
the determinant of a triangular matrix is easy to compute: just mul ply the diagonal
elements. Secondly, we know how elementary row opera ons aﬀect the determinant.
Put these two ideas together: given any square matrix, we can use elementary row opera ons to put the matrix in triangular form,21 ﬁnd the determinant of the new matrix
(which is easy), and then adjust that number by recalling what elementary opera ons
we performed. Let’s prac ce this.
. Example 78
where

.Find the determinant of A by ﬁrst pu ng A into a triangular form,


2
4 −2
A =  −1 −2 5  .
3
2
1

In pu ng A into a triangular form, we need not worry about getS
ng leading 1s, but it does tend to make our life easier as we work out a problem by
hand. So let’s scale the ﬁrst row by 1/2:


1
2 −1
1
 −1 −2 5  .
R → R1
2 1
3
2
1
Now let’s get 0s below this leading 1:

R1 + R2 → R2
−3R1 + R3 → R3

1
0
0

2
0
−4


−1
4 .
4

We can ﬁnish in one step; by interchanging rows 2 and 3 we’ll have our matrix in
triangular form.


1 2 −1
 0 −4 4  .
R2 ↔ R3
0 0
4
Let’s name this last matrix B. The determinant of B is easy to compute as it is
triangular; det (B) = −16. We can use this to ﬁnd det(A).
Recall the steps we used to transform A into B. They are:
20 which is
21 or

why mathema cians rarely smile: they are remembering their problems
echelon form

151

Chapter 3 Opera ons on Matrices
→ R1
R1 + R2 → R2
−3R1 + R3 → R3
R2 ↔ R3
1
2 R1

The ﬁrst opera on mul plied a row of A by 21 . This means that the resul ng matrix
had a determinant that was 12 the determinant of A.
The next two opera ons did not aﬀect the determinant at all. The last opera on,
the row swap, changed the sign. Combining these eﬀects, we know that
1
−16 = det (B) = (−1) det (A) .
2
Solving for det (A) we have that det (A) = 32. .
In prac ce, we don’t need to keep track of opera ons where we add mul ples
of one row to another; they simply do not aﬀect the determinant. Also, in prac ce,
these steps are carried out by a computer, and computers don’t care about leading 1s.
Therefore, row scaling opera ons are rarely used. The only things to keep track of are
row swaps, and even then all we care about are the number of row swaps. An odd
number of row swaps means that the original determinant has the opposite sign of
the triangular form matrix; an even number of row swaps means they have the same
determinant.
Let’s prac ce this again.
. Example 79
The matrix B was formed from A using the following elementary
row opera ons, though not necessarily in this order. Find det(A).


1 2
B = 0 4
0 0


3
5
6

2R1 → R1
1
3 R3 → R3
R1 ↔ R2
6R1 + R2 → R2

It is easy to compute det (B) = 24. In looking at our list of elemenS
tary row opera ons, we see that only the ﬁrst three have an eﬀect on the determinant.
Therefore
1
24 = det (B) = 2 · · (−1) · det (A)
3
and hence
det (A) = −36.
.
In the previous example, we may have been tempted to “rebuild” A using the elementary row opera ons and then compu ng the determinant. This can be done, but
in general it is a bad idea; it takes too much work and it is too easy to make a mistake.
Let’s think some more like a mathema cian. How does the determinant work with
other matrix opera ons that we know? Speciﬁcally, how does the determinant interact with matrix addi on, scalar mul plica on, matrix mul plica on, the transpose
152

3.4 Proper es of the Determinant
and the trace? We’ll again do an example to get an idea of what is going on, then give
a theorem to state what is true.
. Example 80

.Let
[
A=

1 2
3 4

]

[
and B =

]
2 1
.
3 5

Find the determinants of the matrices A, B, A + B, 3A, AB, AT , A−1 , and compare the
determinant of these matrices to their trace.
S

We can quickly compute that det (A) = −2 and that det (B) = 7.
([
det (A − B) = det
=

] [
])
1 2
2 1
−
3 4
3 5

−1 1
0 −1

=1
It’s tough to ﬁnd a connec on between det(A − B), det(A) and det(B).

det (3A) =

3
9

6
12

= −18
We can ﬁgure this one out; mul plying one row of A by 3 increases the determinant
by a factor of 3; doing it again (and hence mul plying both rows by 3) increases the
determinant again by a factor of 3. Therefore det (3A) = 3 · 3 · det (A), or 32 · A.
([
det (AB) = det
=

1 2
3 4

][

2 1
3 5

])

8 11
18 23

= −14
This one seems clear; det (AB) = det (A) det (B).
( )
det AT =

1
2

3
4

= −2
153

Chapter 3 Opera ons on Matrices
( )
Obviously det AT = det (A); is this always going to be the case? If we think about
it, we can see that the cofactor expansion along the ﬁrst row of A will give us the same
result as the cofactor expansion along the ﬁrst column of AT .22
(
)
det A−1 =

−2
1
3/2 −1/2

= 1 − 3/2
= −1/2
It seems as though

(
)
det A−1 =

1
.
det (A)

We end by remarking that there seems to be no connec on whatsoever between
the trace of a matrix and its determinant. We leave it to the reader to compute the
trace for some of the above matrices and conﬁrm this statement.
.
We now state a theorem which will conﬁrm our conjectures from the previous
example.
.
Theorem 16

Determinant Proper es
Let A and B be n × n matrices and let k be a scalar. The
following are true:
1. det (kA) = kn · det (A)
( )
2. det AT = det (A)

.

3. det (AB) = det (A) det (B)
4. If A is inver ble, then

(
)
det A−1 =

1
.
det (A)

5. A matrix A is inver ble if and only if det (A) ̸= 0.

This last statement
( of )the above theorem is signiﬁcant: what happens if det (A) =
0? It seems that det A−1 =“1/0”, which is undeﬁned. There actually isn’t a problem
here; it turns out that if det (A) = 0, then A is not inver ble (hence part 5 of Theorem
16). This allows us to add on to our Inver ble Matrix Theorem.
22 This can be a bit tricky to think out in your head. Try it with a 3×3 matrix A and see how it works. All
the 2 × 2 submatrices that are created in AT are the transpose of those found in A; this doesn’t ma er since
it is easy to see that the determinant isn’t aﬀected by the transpose in a 2 × 2 matrix.

154

3.4 Proper es of the Determinant

.
Theorem 17

Inver ble Matrix Theorem
Let A be an n×n matrix. The following statements are equivalent.

.

(a) A is inver ble.
(g) det (A) ̸= 0.

This new addi on to the Inver ble Matrix Theorem is very useful; we’ll refer back
to it in Chapter 4 when we discuss eigenvalues.
We end this sec on with a shortcut for compu ng the determinants of 3 × 3 matrices. Consider the matrix A:


1 2 3
4 5 6.
7 8 9
We can compute its determinant using cofactor expansion as we did in Example 71.
Once one becomes proﬁcient at this method, compu ng the determinant of a 3 × 3
isn’t all that hard. A method many ﬁnd easier, though, starts with rewri ng the matrix
without the brackets, and repea ng the ﬁrst and second columns at the end as shown
below.
1 2 3 1 2
4 5 6 4 5
7 8 9 7 8
In this 3 × 5 array of numbers, there are 3 full “upper le to lower right” diagonals,
and 3 full “upper right to lower le ” diagonals, as shown below with the arrows.
1
4
7
105 48 72

2
5
8

3
6.
9

1
4
7

2
5
8
45 84 96

The numbers that appear at the ends of each of the arrows are computed by mulplying the numbers found along the arrows. For instance, the 105 comes from mulplying 3 · 5 · 7 = 105. The determinant is found by adding the numbers on the right,
and subtrac ng the sum of the numbers on the le . That is,
det (A) = (45 + 84 + 96) − (105 + 48 + 72) = 0.
To help remind ourselves of this shortcut, we’ll make it into a Key Idea.
155

Chapter 3 Opera ons on Matrices

.
Key Idea 13

3 × 3 Determinant Shortcut
Let A be a 3 × 3 matrix. Create a 3 × 5 array by repea ng
the ﬁrst 2 columns and consider the products of the 3 “right
hand” diagonals and 3 “le hand” diagonals as shown previously. Then

.

det (A) = “(the sum of the right hand numbers)
− (the sum of the le hand numbers)”.

We’ll prac ce once more in the context of an example.
. Example 81
where

Find the determinant of A using the previously described shortcut,


1 3 9
A =  −2 3 4  .
−5 7 2

Rewri ng the ﬁrst 2 columns, drawing the proper diagonals, and
S
mul plying, we get:
1 3 9 1 3
−2 3 4. −2 3
−5 7 2 −5 7
−135 28 −12

6 −60 −126

Summing the numbers on the right and subtrac ng the sum of the numbers on the
le , we get
det (A) = (6 − 60 − 126) − (−135 + 28 − 12) = −61.
.
In the next sec on we’ll see how the determinant can be used to solve systems of
linear equa ons.

Exercises 3.4
In Exercises 1 – 14, ﬁnd the determinant
of the given matrix using cofactor expansion
along any row or column you choose.


1
1.  −5
4

156

2
0
0


3
3
6



−4
2.  0
−2


−4
3.  0
−1

4
0
−2


−4
−3 
−1

1
0
−2


1
0 
−5

3.4 Proper es of the Determinant

4.

5.

6.

7.

8.

9.

10.

11.

12.

13.

14.

0
 0
−4

−2
 5
−1

−2
 2
−5

−3
 −2
−1

0
 3
−3

5
 2

 5
−1

−1
 0

 4
0

−5
 0

 1
−4

−1
 3

 −5
−1

4
1

2

1
4

2
4

0

1
5

−3
0
1
−3
2
0


1
5
0

5
0
0

4
1
−4


0
−3 
0

−5
3 
1

−4
−3 
0

−5
4
0
−2

0
−1
0
0

3
0
−5
0

3
0
−2
2

−5
0
3
−2

0
5
3
−1

0
−5
−2
0

−2
1
−1
0

−2
−5
1
0
−3
0

0
0
2
0
4

5
3
0
0
2

1
1
2
0
5

1
1
0
3
0

1
2
1
2
5

1
0
0
0
0


1
−1 

4 
5

4
0

0
0

−2
0 

1 
−5

5
−2 

−3 
0


0
5

2

0
3

1
2

0

3
4

In Exercises 15 – 18, a matrix M and det(M)
are given. Matrices A, B and C are formed by

performing opera ons on M. Determine the
determinants of A, B and C using Theorems
15 and 16, and indicate the opera ons used
to form A, B and C.


0
3
5
1
0 ,
15. M =  3
−2 −4 −1
det(M) = −41.


0
3
5
(a) A =  −2 −4 −1 
3
1
0


0 3 5
(b) B =  3 1 0 
8 16 4


3
4
5
1
0 
(c) C =  3
−2 −4 −1


9 7 8
16. M =  1 3 7 ,
6 3 3
det(M) = 45.


18 14 16
3
7 
(a) A =  1
6
3
3


9
7
8
3
7 
(b) B =  1
96 73 83


9 1 6
(c) C =  7 3 3 
8 7 3


5 1 5
17. M =  4 0 2 ,
0 0 4
det(M) = −16.


0 0 4
(a) A =  5 1 5 
4 0 2


−5 −1 −5
0
−2 
(b) B =  −4
0
0
4


15 3 15
(c) C =  12 0 6 
0 0 12

157

Chapter 3 Opera ons on Matrices

5 4 0
18. M =  7 9 3 ,
1 3 9
det(M) = 120.


1 3 9
(a) A =  7 9 3 
5 4 0


5
4
0
(b) B =  14 18 6 
3
9 27


−5 −4
0
(c) C =  −7 −9 −3 
−1 −3 −9

[



In Exercises 19 – 22, matrices A and B are
given. Verify part 3 of Theorem 16 by compu ng det(A), det(B) and det(AB).
[
]
2 0
19. A =
,
1 2
[
]
0 −4
B=
1
3
[
]
3 −1
20. A =
,
4
1
]
[
−4 −1
B=
−5
3
]
[
−4
4
,
21. A =
5
−2
]
[
−3 −4
B=
5
−3
]
[
−3 −1
,
22. A =
2
−3

158

B=

0
4

0
−4

]

In Exercises 23 – 30, ﬁnd the determinant of
the given matrix using Key Idea 13.


3
2
3
1
−10 
23.  −6
−8 −9 −9


8
−9 −2
9
−7 
24.  −9
5
−1
9


−4
3
−4
3 
25.  −4 −5
3
−4
5


1 −2 1
5
4
26.  5
4
0
0


1 −4 1
3
0
27.  0
1
2
2


3
−1
0
0
−4 
28.  −3
0
−1 −4


−5 0 −4
4 −1 
29.  2
−5 0 −4


1
0 0
1 0
30.  0
−1 1 1

3.5 Cramer’s Rule

3.5

Cramer’s Rule

.

.
AS YOU READ
...
1. T/F: Cramer’s Rule is another method to compute the determinant of a matrix.
2. T/F: Cramer’s Rule is o en used because it is more eﬃcient than Gaussian elimina on.
3. Mathema cians use what word to describe the connec ons between seemingly
unrelated ideas?

In the previous sec ons we have learned about the determinant, but we haven’t
given a really good reason why we would want to compute it.23 This sec on shows one
applica on of the determinant: solving systems of linear equa ons. We introduce this
idea in terms of a theorem, then we will prac ce.
.
Theorem 18

Cramer’s Rule
Let A be an n × n matrix with det (A) ̸= 0 and let ⃗b be an
n × 1 column vector. Then the linear system

.

A⃗x = ⃗b
(

has solu on
xi =

)
det Ai (⃗b)
det (A)

,

where Ai (⃗b) is the matrix formed by replacing the ith column
of A with ⃗b.

Let’s do an example.
. Example 82

.Use Cramer’s Rule to solve the linear system A⃗x = ⃗b where


1
A = 1
2

5
4
−1




−3
−36
2  and ⃗b =  −11  .
0
7

23 The closest we came to mo va on is that if det (A) = 0, then we know that A is not inver ble. But it
seems that there may be easier ways to check.

159

Chapter 3 Opera ons on Matrices
S

We ﬁrst compute the determinant of A to see if we can apply Cramer’s

Rule.

1 5 −3
1 4
2
2 −1 0

det (A) =

= 49.

Since(det (A)) ̸= 0,(we can
) apply Cramer’s
(
)Rule. Following Theorem 18, we com⃗
⃗
⃗
pute det A1 (b) , det A2 (b) and det A3 (b) .
(
)
det A1 (⃗b) =

− 36 5 −3
−11
4
2
7
−1 0

= 49.

(We used a bold font to show where ⃗b replaced the ﬁrst column of A.)
(
)
det A2 (⃗b) =

(
)
det A3 (⃗b) =

1
1
2

−36
−11
7

−3
2
0

= −245.

1 5 −36
1 4 −11
2 −1
7

= 196.

Therefore we can compute ⃗x :

x1 =
x2 =
x3 =
Therefore

(
)
det A1 (⃗b)
det (A)
(
)
det A2 (⃗b)
det (A)
(
)
det A3 (⃗b)
det (A)

=

49
=1
49

=

−245
= −5
49

=

196
=4
49



 

x1
1
⃗x =  x2  =  −5  .
x3
4

.
Let’s do another example.
. Example 83

160

.Use Cramer’s Rule to solve the linear system A⃗x = ⃗b where
[
]
[
]
1 2
−1
⃗
A=
and b =
.
3 4
1

3.5 Cramer’s Rule
S

The determinant of A is −2, so we can apply Cramer’s Rule.
(
)
det A1 (⃗b) =

−1 2
1 4

(
)
det A2 (⃗b) =
Therefore
x1 =
x2 =

−1
1

1
3

(
)
det A1 (⃗b)
det (A)
(
)
det A2 (⃗b)
det (A)

[

and
⃗x =

x1
x2

]

= 4.

=

−6
=3
−2

=

4
= −2
−2

[
=

= −6.

]
3
.
−2

.
We learned in Sec on 3.4 that when considering a linear system A⃗x = ⃗b where A
is square, if det (A) ̸= 0 then A is inver ble and A⃗x = ⃗b has exactly one solu on. We
also stated in Key Idea 11 that if det (A) = 0, then A is not inver ble and so therefore
either A⃗x = ⃗b has no solu on or inﬁnite solu ons. Our method
of
[
] ﬁguring out which
of these cases applied was to form the augmented matrix A ⃗b , put it into reduced
row echelon form, and then interpret the results.
Cramer’s Rule speciﬁes that det (A) ̸= 0 (so we are guaranteed a solu on). When
det (A) = 0 we are not able to discern whether inﬁnite solu ons or no solu on exists
for a given vector ⃗b. Cramer’s Rule is only applicable to the case when exactly one
solu on exists.
We end this sec on with a prac cal considera on. We have men oned before
that ﬁnding determinants is a computa onally intensive opera on. To solve a linear
system with 3 equa ons and 3 unknowns, we need to compute 4 determinants. Just
think: with 10 equa ons and 10 unknowns, we’d need to compute 11 really hard determinants of 10 × 10 matrices! That is a lot of work!
The upshot of this is that Cramer’s Rule makes for a poor choice in solving numerical linear systems. It simply is not done in prac ce; it is hard to beat Gaussian
elimina on.24
So why include it? Because its truth is amazing. The determinant is a very strange
opera on; it produces a number in a very odd way. It should seem incredible to the
24 A version of Cramer’s Rule is o en taught in introductory diﬀeren al equa ons courses as it can be
used to ﬁnd solu ons to certain linear diﬀeren al equa ons. In this situa on, the entries of the matrices
are func ons, not numbers, and hence compu ng determinants is easier than using Gaussian elimina on.
Again, though, as the matrices get large, other solu on methods are resorted to.

161

Chapter 3 Opera ons on Matrices
reader that by manipula ng determinants in a par cular way, we can solve linear systems.
In the next chapter we’ll see another use for the determinant. Meanwhile, try to
develop a deeper apprecia on of math: odd, complicated things that seem completely
unrelated o en are intricately ed together. Mathema cians see these connec ons
and describe them as “beau ful.”

Exercises 3.5



In Exercises 1 – 12, matrices A and ⃗b are given.
(a) Give det(A) and det(Ai ) for all i.
(b) Use Cramer’s Rule to solve A⃗x = ⃗b. If
Cramer’s Rule cannot be used to ﬁnd
the solu on, then state whether or not
a solu on exists.
[
]
[
]
7
−7
28
1. A =
, ⃗b =
−7
9
−26
[
]
[
]
9
5
−45
2. A =
, ⃗b =
−4 −7
20
[
]
[
]
−8
16
−48
3. A =
, ⃗b =
10 −20
60
[
]
[
]
0 −6
6
4. A =
, ⃗b =
9 −10
−17
]
[ ]
[
42
2
10
, ⃗b =
5. A =
19
−1 3
]
]
[
[
−1
7
14
, ⃗b =
6. A =
4
−2 −4

 

24
3 0 −3
4 , ⃗b =  0 
7. A =  5 4
31
5 5 −4


4
9
3
8. A =  −5 −2 −13 ,
−1 10 −13

162

9.

10.

11.

12.


−28
⃗b =  35 
7


 
4 −4
0
16
1
−1 , ⃗b =  22 
A=5
3 −1
2
8


1
0
−10
−3 −10 ,
A= 4
−9
6
−2


−40
⃗b =  −94 
132


7
−4 25
1
−7 ,
A =  −2
9
−7 34


−1
⃗b =  −3 
5


−6 −7 −7
4
1 ,
A= 5
5
4
8


58
⃗b =  −35 
−49

4
E

.

E

We have o en explored new ideas in matrix algebra by making connec ons to our
previous algebraic experience. Adding two numbers, x + y, led us to adding vectors
⃗x + ⃗y and adding matrices A + B. We explored mul plica on, which then led us to
solving the matrix equa on A⃗x = ⃗b, which was reminiscent of solving the algebra
equa on ax = b.
This chapter is mo vated by another analogy. Consider: when we mul ply an unknown number x by another number such as 5, what do we know about the result?
Unless, x = 0, we know that in some sense 5x will be “5 mes bigger than x.” Applying
this to vectors, we would readily agree that 5⃗x gives a vector that is “5 mes bigger
than ⃗x .” Each entry in ⃗x is mul plied by 5.
Within the matrix algebra context, though, we have two types of mul plica on:
scalar and matrix mul plica on. What happens to ⃗x when we mul ply it by a matrix
A? Our ﬁrst response is likely along the lines of “You just get another vector. There is
no deﬁnable rela onship.” We might wonder if there is ever the case where a matrix
– vector mul plica on is very similar to a scalar – vector mul plica on. That is, do we
ever have the case where A⃗x = a⃗x , where a is some scalar? That is the mo va ng
ques on of this chapter.

4.1

Eigenvalues and Eigenvectors

.

.
AS YOU READ
...
1. T/F: Given any matrix A, we can always ﬁnd a vector ⃗x where A⃗x = ⃗x .
2. When is the zero vector an eigenvector for a matrix?
3. If ⃗v is an eigenvector of a matrix A with eigenvalue of 2, then what is A⃗v?
4. T/F: If A is a 5 × 5 matrix, to ﬁnd the eigenvalues of A, we would need to ﬁnd the
roots of a 5th degree polynomial.

Chapter 4 Eigenvalues and Eigenvectors
We start by considering the matrix A and vector ⃗x as given below.1
[
[ ]
]
1 4
1
⃗x =
A=
2 3
1
Mul plying A⃗x gives:
[

][ ]
1 4
1
2 3
1
[ ]
5
=
5
[ ]
1
=5
!
1

A⃗x =

Wow! It looks like mul plying A⃗x is the same as 5⃗x ! This makes us wonder lots
of things: Is this the only case in the world where something like this happens?2 Is A
somehow a special matrix, and A⃗x = 5⃗x for any vector ⃗x we pick?3 Or maybe ⃗x was a
special vector, and no ma er what 2×2 matrix A we picked, we would have A⃗x = 5⃗x .4
A more likely explana on is this: given the matrix A, the number 5 and the vector
⃗x formed a special pair that happened to work together in a nice way. It is then natural
to wonder if other “special” pairs exist. For instance, could we ﬁnd a vector ⃗x where
A⃗x = 3⃗x ?
This equa on is hard to solve at ﬁrst; we are not used to matrix equa ons where
⃗x appears on both sides of “=.” Therefore we put oﬀ solving this for just a moment to
state a deﬁni on and make a few comments.
.
Deﬁni on 27

Eigenvalues and Eigenvectors
Let A be an n × n matrix, ⃗x a nonzero n × 1 column vector
and λ a scalar. If
A⃗x = λ⃗x ,

.

then ⃗x is an eigenvector of A and λ is an eigenvalue of A.

The word “eigen” is German for “proper” or “characteris c.” Therefore, an eigenvector of A is a “characteris c vector of A.” This vector tells us something about A.
Why do we use the Greek le er λ (lambda)? It is pure tradi on. Above, we used a
to represent the unknown scalar, since we are used to that nota on. We now switch
to λ because that is how everyone else does it.5 Don’t get hung up on this; λ is just a
number.
1 Recall this

matrix and vector were used in Example 40 on page 75.

2 Probably not.
3 Probably not.
4 See footnote

2.
mathema cal peer pressure.

5 An example of

164

4.1 Eigenvalues and Eigenvectors
Note that our deﬁni on requires that A be a square matrix. If A isn’t square then
A⃗x and λ⃗x will have diﬀerent sizes, and so they cannot be equal. Also note that ⃗x must
be nonzero. Why? What if ⃗x = ⃗0? Then no ma er what λ is, A⃗x = λ⃗x . This would
then imply that every number is an eigenvalue; if every number is an eigenvalue, then
we wouldn’t need a deﬁni on for it.6 Therefore we specify that ⃗x ̸= ⃗0.
Our last comment before trying to ﬁnd eigenvalues and eigenvectors for given matrices deals with “why we care.” Did we stumble upon a mathema cal curiosity, or
does this somehow help us build be er bridges, heal the sick, send astronauts into
orbit, design op cal equipment, and understand quantum mechanics? The answer, of
course, is “Yes.”7 This is a wonderful topic in and of itself: we need no external applicaon to appreciate its worth. At the same me, it has many, many applica ons to “the
real world.” A simple Internet seach on “applica ons of eigenvalues” with conﬁrm this.
Back to our math. Given a square matrix A, we want to ﬁnd a nonzero vector ⃗x
and a scalar λ such that A⃗x = λ⃗x . We will solve this using the skills we developed in
Chapter 2.
A⃗x
A⃗x − λ⃗x
(A − λI)⃗x

=
=
=

λ⃗x
⃗0
⃗0

original equa on
subtract λ⃗x from both sides
factor out ⃗x

Think about this last factoriza on. We are likely tempted to say
A⃗x − λ⃗x = (A − λ)⃗x ,
but this really doesn’t make sense. A er all, what does “a matrix minus a number”
mean? We need the iden ty matrix in order for this to be logical.
Let us now think about the equa on (A − λI)⃗x = ⃗0. While it looks complicated, it
really is just matrix equa on of the type we solved in Sec on 2.4. We are just trying
to solve B⃗x = ⃗0, where B = (A − λI).
We know from our previous work that this type of equa on8 always has a solu on,
namely, ⃗x = ⃗0. However, we want ⃗x to be an eigenvector and, by the deﬁni on,
eigenvectors cannot be ⃗0.
This means that we want solu ons to (A − λI)⃗x = ⃗0 other than ⃗x = ⃗0. Recall
that Theorem 8 says that if the matrix (A − λI) is inver ble, then the only solu on to
(A − λI)⃗x = ⃗0 is ⃗x = ⃗0. Therefore, in order to have other solu ons, we need (A − λI)
to not be inver ble.
Finally, recall from Theorem 16 that noninver ble matrices all have a determinant of 0. Therefore, if we want to ﬁnd eigenvalues λ and eigenvectors ⃗x , we need
det (A − λI) = 0.
Let’s start our prac ce of this theory by ﬁnding λ such that det (A − λI) = 0; that
is, let’s ﬁnd the eigenvalues of a matrix.
6 Recall footnote

17 on page 107.
for the “understand quantum mechanics” part. Nobody truly understands that stuﬀ; they just
probably understand it.
8 Recall this is a homogeneous system of equa ons.
7 Except

165

Chapter 4 Eigenvalues and Eigenvectors
. Example 84
where

Find the eigenvalues of A, that is, ﬁnd λ such that det (A − λI) = 0,
[
A=

]
1 4
.
2 3

(Note that this is the matrix we used at the beginning of this secS
on.) First, we write out what A − λI is:
[
]
[
]
1 0
1 4
A − λI =
−λ
2 3
0 1
[
] [
]
1 4
λ 0
=
−
2 3
0 λ
[
]
1−λ
4
=
2
3−λ
Therefore,
det (A − λI) =

1−λ
2

4
3−λ

= (1 − λ)(3 − λ) − 8
= λ2 − 4λ − 5
Since we want det (A − λI) = 0, we want λ2 − 4λ − 5 = 0. This is a simple
quadra c equa on that is easy to factor:
λ2 − 4λ − 5 = 0
(λ − 5)(λ + 1) = 0
λ = −1, 5
According to our above work, det (A − λI) = 0 when λ = −1, 5. Thus, the eigenvalues of A are −1 and 5. .
Earlier, when looking at the same matrix as used in our example, we wondered if
we could ﬁnd a vector ⃗x such that A⃗x = 3⃗x . According to this example, the answer is
“No.” With this matrix A, the only values of λ that work are −1 and 5.
Let’s restate the above in a diﬀerent way: It is pointless to try to ﬁnd ⃗x where
A⃗x = 3⃗x , for there is no such ⃗x . There are only 2 equa ons of this form that have a
solu on, namely
A⃗x = −⃗x
and
A⃗x = 5⃗x .
As we introduced this sec on, we gave a vector ⃗x such that A⃗x = 5⃗x . Is this the
only one? Let’s ﬁnd out while calling our work an example; this will amount to ﬁnding
the eigenvectors of A that correspond to the eigenvector of 5.

166

4.1 Eigenvalues and Eigenvectors
. Example 85

S

Find ⃗x such that A⃗x = 5⃗x , where
[
]
1 4
A=
.
2 3
Recall that our algebra from before showed that if
A⃗x = λ⃗x

then

(A − λI)⃗x = ⃗0.

Therefore, we need to solve the equa on (A − λI)⃗x = ⃗0 for ⃗x when λ = 5.
[

]
[
]
4
1 0
A − 5I =
−5
3
0 1
[
]
−4 4
=
2 −2
1
2

To solve (A − 5I)⃗x = ⃗0, we form the augmented matrix and put it into reduced row
echelon form:
[
]
[
]
−→
−4 4 0
1 −1 0
rref
.
2 −2 0
0 0 0
Thus
x1 = x2
x2 is free
and

[
⃗x =

x1
x2

]
= x2

[ ]
1
.
1

We have
solu ons to the equa on A⃗x = 5⃗x ; any nonzero scalar mul ple of the
[ inﬁnite
]
1
vector
is a solu on. We can do a few examples to conﬁrm this:
1
[
[

1
2

4
3

][ ] [ ]
[ ]
2
10
2
=
=5
;
2
10
2

][ ] [ ]
[ ]
4
7
35
7
=
=5
;
3
7
35
7
[
][
] [
]
[
]
1 4
−3
−15
−3
=
=5
.
2 3
−3
−15
−3
.
Our method of ﬁnding the eigenvalues of a matrix A boils down to determining
which values of λ give the matrix (A−λI) a determinant of 0. In compu ng det (A − λI),
we get a polynomial in λ whose roots are the eigenvalues of A. This polynomial is important and so it gets its own name.
1
2

167

Chapter 4 Eigenvalues and Eigenvectors

.
Deﬁni on 28

Characteris c Polynomial

.

Let A be an n × n matrix. The characteris c polynomial of A
is the nth degree polynomial p(λ) = det (A − λI).

Our deﬁni on just states what the characteris c polynomial is. We know from our
work so far why we care: the roots of the characteris c polynomial of an n × n matrix
A are the eigenvalues of A.
In Examples 84 and 85, we found eigenvalues and eigenvectors, respec vely, of
a given matrix. That is, given a matrix A, we found values λ and vectors ⃗x such that
A⃗x = λ⃗x . The steps that follow outline the general procedure for ﬁnding eigenvalues
and eigenvectors; we’ll follow this up with some examples.
.
Key Idea 14

Finding Eigenvalues and Eigenvectors
Let A be an n × n matrix.

.

1. To ﬁnd the eigenvalues of A, compute p(λ), the characteris c polynomial of A, set it equal to 0, then solve
for λ.
2. To ﬁnd the eigenvectors of A, for each eigenvalue
solve the homogeneous system (A − λI)⃗x = ⃗0.

. Example 86
vector where

S

.Find the eigenvalues of A, and for each eigenvalue, ﬁnd an eigen[
]
−3 15
A=
.
3
9
To ﬁnd the eigenvalues, we must compute det (A − λI) and set it

equal to 0.
det (A − λI) =

−3 − λ
3

15
9−λ

= (−3 − λ)(9 − λ) − 45
= λ2 − 6λ − 27 − 45
= λ2 − 6λ − 72
= (λ − 12)(λ + 6)
Therefore, det (A − λI) = 0 when λ = −6 and 12; these are our eigenvalues. (We
168

4.1 Eigenvalues and Eigenvectors
should note that p(λ) = λ2 − 6λ − 72 is our characteris c polynomial.) It some mes
helps to give them “names,” so we’ll say λ1 = −6 and λ2 = 12. Now we ﬁnd eigenvectors.
For λ1 = −6:
We need to solve the equa on (A − (−6)I)⃗x = ⃗0. To do this, we form the appropriate augmented matrix and put it into reduced row echelon form.
[
]
[
]
−→
3 15 0
1 5 0
rref
.
3 15 0
0 0 0
Our solu on is
x1 = −5x2
x2 is free;
in vector form, we have

[
⃗x = x2

]
−5
.
1

We may pick any nonzero value for x2 to get an eigenvector; a simple op on is x2 = 1.
Thus we have the eigenvector
[
]
−5
x⃗1 =
.
1
(We used the nota on x⃗1 to associate this eigenvector with the eigenvalue λ1 .)
We now repeat this process to ﬁnd an eigenvector for λ2 = 12: .
In solving (A − 12I)⃗x = ⃗0, we ﬁnd
[
]
[
]
−→
−15 15 0
1 −1 0
rref
.
3
−3 0
0 0 0
In vector form, we have

[ ]
1
⃗x = x2
.
1

Again, we may pick any nonzero value for x2 , and so we choose x2 = 1. Thus an
eigenvector for λ2 is
[ ]
1
x⃗2 =
.
1

To summarize, we have:
eigenvalue λ1 = −6 with eigenvector x⃗1 =
and
eigenvalue λ2 = 12 with eigenvector x⃗2 =

[

−5
1

]

[ ]
1
.
1
169

Chapter 4 Eigenvalues and Eigenvectors
We should take a moment and check our work: is it true that A⃗
x1 = λ1 x⃗1 ?
[
][
]
−3 15
−5
A⃗
x1 =
3
9
1
]
[
30
=
−6
[
]
−5
= (−6)
1
= λ1 x⃗1 .
Yes; it appears we have truly found an eigenvalue/eigenvector pair for the matrix A. .
Let’s do another example.
[
]
−3 0
. Example 87
.Let A =
. Find the eigenvalues of A and an eigenvector
5 1
for each eigenvalue.
S

We ﬁrst compute the characteris c polynomial, set it equal to 0,

then solve for λ.
−3 − λ
5

det (A − λI) =

0
1−λ

= (−3 − λ)(1 − λ)
From this, we see that det (A − λI) = 0 when λ = −3, 1. We’ll set λ1 = −3 and
λ2 = 1.
Finding an eigenvector for λ1 :
We solve (A − (−3)I)⃗x = ⃗0 for ⃗x by row reducing the appropriate matrix:
[
]
[
]
−→
0 0 0
1 5/4 0
rref
.
5 4 0
0
0
0
Our solu on, in vector form, is
[
⃗x = x2

]
−5/4
.
1

Again, we can pick any nonzero value for x2 ; a nice choice would eliminate the frac on.
Therefore we pick x2 = 4, and ﬁnd
[
]
−5
x⃗1 =
.
4

Finding an eigenvector for λ2 :
170

4.1 Eigenvalues and Eigenvectors
We solve (A − (1)I)⃗x = ⃗0 for ⃗x by row reducing the appropriate matrix:
[

−4 0 0
5 0 0

]

−→
rref

[

]
1 0 0
.
0 0 0

We’ve seen a matrix like this before,9 but we may need a bit of a refreshing. Our
ﬁrst row tells us that x1 = 0, and we see that no rows/equa ons involve x2 . We conclude that x2 is free. Therefore, our solu on, in vector form, is
⃗x = x2
We pick x2 = 1, and ﬁnd
x⃗2 =

[ ]
0
.
1

[ ]
0
.
1

To summarize, we have:
[
eigenvalue λ1 = −3 with eigenvector x⃗1 =
and

−5
4

]

[ ]
0
eigenvalue λ2 = 1 with eigenvector x⃗2 =
.
1

.
So far, our examples have involved 2 × 2 matrices. Let’s do an example with a 3 × 3
matrix.
. Example 88
vector, where

.Find the eigenvalues of A, and for each eigenvalue, give one eigen

−7
A =  −3
−6


−2 10
2
3 .
−2 9

We ﬁrst compute the characteris c polynomial, set it equal to 0,
S
then solve for λ. A warning: this process is rather long. We’ll use cofactor expansion
along the ﬁrst row; don’t get bogged down with the arithme c that comes from each
step; just try to get the basic idea of what was done from step to step.
9 See page 31.

Our future need of knowing how to handle this situa on is foretold in footnote 5.

171

Chapter 4 Eigenvalues and Eigenvectors

−7 − λ
−3
−6

−2
2−λ
−2

10
3
9−λ

= (−7 − λ)

2−λ
−2

3
9−λ

det (A − λI) =

− (−2)

−3
−6

3
9−λ

+ 10

−3 2 − λ
−6
−2

= (−7 − λ)(λ2 − 11λ + 24) + 2(3λ − 9) + 10(−6λ + 18)
= −λ3 + 4λ2 − λ − 6
= −(λ + 1)(λ − 2)(λ − 3)
.In the last step we factored the characteris c polynomial −λ3 +4λ2 −λ−6. Factoring
polynomials of degree > 2 is not trivial; we’ll assume the reader has access to methods
for doing this accurately.10
Our eigenvalues are λ1 = −1, λ2 = 2 and λ3 = 3. We now ﬁnd corresponding
eigenvectors.
For λ1 = −1:
We need to solve the equa on (A − (−1)I)⃗x = ⃗0. To do this, we form the appropriate augmented matrix and put it into reduced row echelon form.


−6 −2
 −3 3
−6 −2


10 0
3 0
10 0


−→
rref

1 0
0 1
0 0

−1.5
−.5
0


0
0
0

Our solu on, in vector form, is



3/2
⃗x = x3  1/2  .
1
We can pick any nonzero value for
; a nice choice would get rid of the frac ons.
 x3
3
So we’ll set x3 = 2 and choose x⃗1 =  1  as our eigenvector.
2
For λ2 = 2:
We need to solve the equa on (A − 2I)⃗x = ⃗0. To do this, we form the appropriate
augmented matrix and put it into reduced row echelon form.
10 You

probably learned how to do this in an algebra course. As a reminder, possible roots can be found
by factoring the constant term (in this case, −6) of the polynomial. That is, the roots of this equa on could
be ±1, ±2, ±3 and ±6. That’s 12 things to check.
One could also graph this polynomial to ﬁnd the roots. Graphing will show us that λ = 3 looks like a root,
and a simple calcula on will conﬁrm that it is.

172

4.1 Eigenvalues and Eigenvectors


−9 −2 10
 −3 0
3
−6 −2 7
Our solu on, in vector form, is


0
0
0


−→
rref


1 0 −1 0
 0 1 −.5 0 
0 0
0
0




1
⃗x = x3  1/2  .
1

We can pick any nonzero value for x3 ; again,
 a nice choice would get rid of the frac2
ons. So we’ll set x3 = 2 and choose x⃗2 =  1  as our eigenvector.
2
For λ3 = 3:
We need to solve the equa on (A − 3I)⃗x = ⃗0. To do this, we form the appropriate
augmented matrix and put it into reduced row echelon form.




1 0 −1 0
−10 −2 10 0
−
→
 −3 −1 3 0  rref  0 1 0 0 
−6 −2 6 0
0 0 0 0
Our solu on, in vector form, is (note that x2 = 0):
 
1
⃗x = x3  0  .
1
 
1
We can pick any nonzero value for x3 ; an easy choice is x3 = 1, so x⃗3 =  0  as
1
our eigenvector.

.

To summarize, we have the following eigenvalue/eigenvector pairs:
 
3
eigenvalue λ1 = −1 with eigenvector x⃗1 =  1 
2
 
2
eigenvalue λ2 = 2 with eigenvector x⃗2 =  1 
2
 
1
eigenvalue λ3 = 3 with eigenvector x⃗3 =  0 
1
Let’s prac ce once more.
173

Chapter 4 Eigenvalues and Eigenvectors
. Example 89
vector, where

.Find the eigenvalues of A, and for each eigenvalue, give one eigen

2 −1 1
A = 0 1 6.
0 3 4

We ﬁrst compute the characteris c polynomial, set it equal to 0,
S
then solve for λ. We’ll use cofactor expansion down the ﬁrst column (since it has lots
of zeros).
2−λ
0
0

−1
1−λ
3

1
6
4−λ

= (2 − λ)

1−λ
3

6
4−λ

det (A − λI) =

= (2 − λ)(λ2 − 5λ − 14)
= (2 − λ)(λ − 7)(λ + 2)

No ce that while the characteris c polynomial is cubic, we never actually saw a
cubic; we never distributed the (2 − λ) across the quadra c. Instead, we realized that
this was a factor of the cubic, and just factored the remaining quadra c. (This makes
this example quite a bit simpler than the previous example.)
Our eigenvalues are λ1 = −2, λ2 = 2 and λ3 = 7. We now ﬁnd corresponding
eigenvectors.
For λ1 = −2:
We need to solve the equa on (A − (−2)I)⃗x = ⃗0. To do this, we form the appropriate augmented matrix and put it into reduced row echelon form.




4 −1 1 0
1 0 3/4 0
−
→
 0 3 6 0  rref  0 1
2
0
0 3 6 0
0 0
0
0
Our solu on, in vector form, is



−3/4
⃗x = x3  −2  .
1
We can pick any nonzero value for
 x3 ; anice choice would get rid of the frac ons.
−3
So we’ll set x3 = 4 and choose x⃗1 =  −8  as our eigenvector.
4
174

4.1 Eigenvalues and Eigenvectors
For λ2 = 2:
We need to solve the equa on (A − 2I)⃗x = ⃗0. To do this, we form the appropriate
augmented matrix and put it into reduced row echelon form.




0 −1 1 0
0 1 0 0
→
 0 −1 6 0  −
rref  0 0 1 0 
0 3 2 0
0 0 0 0
This looks funny, so we’ll look remind ourselves how to solve this. The ﬁrst two
rows tell us that x2 = 0 and x3 = 0, respec vely. No ce that no row/equa on uses x1 ;
we conclude that it is free. Therefore, our solu on in vector form is
 
1
⃗x = x1  0  .
0
.
 We
 can pick any nonzero value for x1 ; an easy choice is x1 = 1 and choose x⃗2 =
1
 0  as our eigenvector.
0
For λ3 = 7:
We need to solve the equa on (A − 7I)⃗x = ⃗0. To do this, we form the appropriate
augmented matrix and put it into reduced row echelon form.




−5 −1 1 0
1 0 0 0
−
→
 0 −6 6 0  rref  0 1 −1 0 
0
3 −3 0
0 0 0 0
Our solu on, in vector form, is (note that x1 = 0):
 
0
⃗x = x3  1  .
1

 
0
We can pick any nonzero value for x3 ; an easy choice is x3 = 1, so x⃗3 =  1  as
1
our eigenvector.
To summarize, we have the following eigenvalue/eigenvector pairs:


−3
eigenvalue λ1 = −2 with eigenvector x⃗1 =  −8 
4
 
1
eigenvalue λ2 = 2 with eigenvector x⃗2 =  0 
0
175

Chapter 4 Eigenvalues and Eigenvectors
 
0
eigenvalue λ3 = 7 with eigenvector x⃗3 =  1 
1
.
In this sec on we have learned about a new concept: given a matrix A we can ﬁnd
certain values λ and vectors ⃗x where A⃗x = λ⃗x . In the next sec on we will con nue to
the pa ern we have established in this text: a er learning a new concept, we see how
it interacts with other concepts we know about. That is, we’ll look for connec ons
between eigenvalues and eigenvectors and things like the inverse, determinants, the
trace, the transpose, etc.

Exercises 4.1
In Exercises 1 – 6, a matrix A and one of its
eigenvectors are given. Find the eigenvalue
of A for the given eigenvector.
[
]
9
8
1. A =
−6 −5
[
]
−4
⃗x =
3
[
]
19 −6
2. A =
48 −15
[ ]
1
⃗x =
3
[
]
1
−2
3. A =
−2
4
[ ]
2
⃗x =
1


−11 −19 14
−8
6 
4. A =  −6
−12 −22 15
 
3
⃗x =  2 
4


−7
1
3
2
−3 
5. A =  10
−20 −14
1


1
⃗x =  −2 
4


−12 −10
0
13
0 
6. A =  15
15
18
−5

176




−1
⃗x =  1 
1
In Exercises 7 – 11, a matrix A and one of its
eigenvalues are given. Find an eigenvector of
A for the given eigenvalue.
[
]
16
6
7. A =
−18 −5
λ=4
[
]
−2 6
8. A =
−9 13
λ=7


−16 −28 −19
69
46 
9. A =  42
−42 −72 −49
λ=5


7 −5 −10
2
−6 
10. A =  6
2 −5 −5
λ = −3


4
5
−3
3 
11. A =  −7 −8
1
−5
8
λ=2
In Exercises 12 – 28, ﬁnd the eigenvalues of
the given matrix. For each eigenvalue, give an
eigenvector.
[
]
−1 −4
12.
−3 −2
[
]
−4 72
13.
−1 13
[
]
2 −12
14.
2 −8

4.2 Proper es of Eigenvalues and Eigenvectors
[
15.
[
16.
[
17.
[
18.
[
19.


3
1

]

5
−1

9
−5

3
−1
0
25

−1
3
]
1
0

−3
0

1
−1

1
20.  0
0

5
21.  0
0

1
22.  2
1

4.2

12
−1

−2
3
−1
−2
4
−1
0
−5
0



1
23.  −4
1

]



−1
24.  1
5

]

5
25.  1
−1


−3
0 
−1

3
0
3

12
0 
2



2
26.  0
0

0
1
5
−1
3
0



3
27.  −2
−2


1
28.  1
1


0
0 
−1

18
2
−3



]


−18
−1 
−8

0
3
0

5
3
5
2
2
1


0
0 
−2

1
6
7

−5
2 
0

1
3
1

Proper es of Eigenvalues and Eigenvectors

.

.
AS YOU READ
...
1. T/F: A and AT have the same eigenvectors.
2. T/F: A and A−1 have the same eigenvalues.
3. T/F: Marie Ennemond Camille Jordan was a guy.
4. T/F: Matrices with a trace of 0 are important, although we haven’t seen why.
5. T/F: A matrix A is inver ble only if 1 is an eigenvalue of A.

In this sec on we’ll explore how the eigenvalues and eigenvectors of a matrix relate
to other proper es of that matrix. This sec on is essen ally a hodgepodge of interes ng facts about eigenvalues; the goal here is not to memorize various facts about
matrix algebra, but to again be amazed at the many connec ons between mathematical concepts.
We’ll begin our inves ga ons with an example that will give a founda on for other
discoveries.

177

Chapter 4 Eigenvalues and Eigenvectors


. Example 90

S

1
Let A =  0
0

2
4
0


3
5 . Find the eigenvalues of A.
6

To ﬁnd the eigenvalues, we compute det (A − λI):
det (A − λI) =

1−λ
0
0

2
4−λ
0

3
5
6−λ

= (1 − λ)(4 − λ)(6 − λ)
Since our matrix is triangular, the determinant is easy to compute; it is just the
product of the diagonal elements. Therefore, we found (and factored) our characteris c polynomial very easily, and we see that we have eigenvalues of λ = 1, 4, and 6. .
This examples demonstrates a wonderful fact for us: the eigenvalues of a triangular
matrix are simply the entries on the diagonal. Finding the corresponding eigenvectors
s ll takes some work, but ﬁnding the eigenvalues is easy.
With that fact in the backs of our minds, let us proceed to the next example where
we will come across some more interes ng facts about eigenvalues and eigenvectors.


[
]
−7 −2 10
−3 15
. Example 91
3  (as used in
.Let A =
and let B =  −3 2
3
9
−6 −2 9
Examples 86 and 88, respec vely). Find the following:
1. eigenvalues and eigenvectors of A and B
2. eigenvalues and eigenvectors of A−1 and B−1
3. eigenvalues and eigenvectors of AT and BT
4. The trace of A and B
5. The determinant of A and B
S

We’ll answer each in turn.
1. We already know the answer to these for we did this work in previous examples.
Therefore we just list the answers.
For A, we have eigenvalues λ = −6 and 12, with eigenvectors
[
]
[ ]
−5
1
⃗x = x2
and x2
, respec vely.
1
1
For B, we have eigenvalues λ = −1, 2, and 3 with eigenvectors
 
 
 
3
2
1
⃗x = x3  1  , x3  1  and x3  0  , respec vely.
2
2
1

178

4.2 Proper es of Eigenvalues and Eigenvectors
2. We ﬁrst compute the inverses of A and B. They are:

[
]
−4
1/3
−1/8 5/24
A−1 =
and B−1 =  −3/2 1/2
1/24 1/24
−3
1/3


13/3
3/2  .
10/3

Finding the eigenvalues and eigenvectors of these matrices is not terribly hard,
but it is not “easy,” either. Therefore, we omit showing the intermediate steps
and go right to the conclusions.
For A−1 , we have eigenvalues λ = −1/6 and 1/12, with eigenvectors
[
]
[ ]
−5
1
⃗x = x2
and x2
, respec vely.
1
1
For B−1 , we have eigenvalues λ = −1, 1/2 and 1/3 with eigenvectors
 
 
 
3
2
1
⃗x = x3  1  , x3  1  and x3  0  , respec vely.
2
2
1
3. Of course, compu ng the transpose of A and B is easy; compu ng their eigenvalues and eigenvectors takes more work. Again, we omit the intermediate steps.
For AT , we have eigenvalues λ = −6 and 12 with eigenvectors
[
]
[ ]
−1
5
⃗x = x2
and x2
, respec vely.
1
1
For BT , we have eigenvalues λ = −1, 2 and 3 with eigenvectors






−1
−1
0
⃗x = x3  0  , x3  1  and x3  −2  , respec vely.
1
1
1
4. The trace of A is 6; the trace of B is 4.
5. The determinant of A is −72; the determinant of B is −6.
.
Now that we have completed the “grunt work,” let’s analyze the results of the previous example. We are looking for any pa erns or rela onships that we can ﬁnd.
The eigenvalues and eigenvectors of A and A−1 .
In our example, we found that the eigenvalues of A are −6 and 12; the eigenvalues
of A−1 are −1/6 and 1/12. Also, the eigenvalues of B are −1, 2 and 3, whereas the
179

Chapter 4 Eigenvalues and Eigenvectors
eigenvalues of B−1 are −1, 1/2 and 1/3. There is an obvious rela onship here; it
seems that if λ is an eigenvalue of A, then 1/λ will be an eigenvalue of A−1 . We can
also note that the corresponding eigenvectors matched, too.
Why is this the case? Consider an inver ble matrix A with eigenvalue λ and eigenvector ⃗x . Then, by deﬁni on, we know that A⃗x = λ⃗x . Now mul ply both sides by
A−1 :
A⃗x = λ⃗x
A

−1

A⃗x = A−1 λ⃗x
⃗x = λA−1⃗x

1
⃗x = A−1⃗x
λ
We have just shown that A−1⃗x = 1/λ⃗x ; this, by deﬁni on, shows that⃗x is an eigenvector of A−1 with eigenvalue 1/λ. This explains the result we saw above.
The eigenvalues and eigenvectors of A and AT .
Our example showed that A and AT had the same eigenvalues but diﬀerent (but
somehow similar) eigenvectors; it also showed that B and BT had the same eigenvalues
but unrelated eigenvectors. Why is this?
We can answer the eigenvalue ques on rela vely easily; it follows from the proper es of the determinant and the transpose. Recall the following two facts:
1. (A + B)T = AT + BT (Theorem 11) and
( )
2. det (A) = det AT (Theorem 16).
We ﬁnd the eigenvalues of a matrix by compu ng the characteris c polynomial;
that is, we ﬁnd det (A − λI). What is the characteris c polynomial of AT ? Consider:
(
)
(
)
det AT − λI = det AT − λIT
(
)
= det (A − λI)T
= det (A − λI)

since I = IT
Theorem 11
Theorem 16

So we see that the characteris c polynomial of AT is the same as that for A. Therefore they have the same eigenvalues.
What about their respec ve eigenvectors? Is there any rela onship? The simple
answer is “No.”11
11 We have deﬁned an eigenvector to be a column vector. Some mathema cians prefer to use row vectors
instead; in that case, the typical eigenvalue/eigenvector equa on looks like⃗x A = λ⃗x . It turns out that doing
things this way will give you the same eigenvalues as our method. What is more, take the transpose of the
above equa on: you get (⃗x A)T = (λ⃗x )T which is also AT⃗x T = λ⃗x T . The transpose of a row vector is a
column vector, so this equa on is actually the kind we are used to, and we can say that ⃗x T is an eigenvector
of AT .
In short, what we ﬁnd is that the eigenvectors of AT are the “row” eigenvectors of A, and vice–versa.

180

4.2 Proper es of Eigenvalues and Eigenvectors
The eigenvalues and eigenvectors of A and The Trace.

Note that the eigenvalues of A are −6 and 12, and the trace is 6; the eigenvalues
of B are −1, 2 and 3, and the trace of B is 4. Do we no ce any rela onship?
It seems that the sum of the eigenvalues is the trace! Why is this the case?
The answer to this is a bit out of the scope of this text; we can jus fy part of this
fact, and another part we’ll just state as being true without jus ﬁca on.
First, recall from Theorem 13 that tr(AB) =tr(BA). Secondly, we state without jusﬁca on that given a square matrix A, we can ﬁnd a square matrix P such that P−1 AP is
an upper triangular matrix with the eigenvalues of A on the diagonal.12 Thus tr(P−1 AP)
is the sum of the eigenvalues; also, using our Theorem 13, we know that tr(P−1 AP) =
tr(P−1 PA) = tr(A). Thus the trace of A is the sum of the eigenvalues.

The eigenvalues and eigenvectors of A and The Determinant.

Again, the eigenvalues of A are −6 and 12, and the determinant of A is −72. The
eigenvalues of B are −1, 2 and 3; the determinant of B is −6. It seems as though the
product of the eigenvalues is the determinant.
This is indeed true; we defend this with our argument from above. We know that
the determinant of a triangular matrix is the product of the diagonal elements. Therefore, given a matrix A, we can ﬁnd P such( that P)−1 AP is upper triangular with the
eigenvalues of A on the diagonal. Thus(det P−1
product
) AP is the
( −1
) of the eigenvalues.
−1
Using Theorem 16, we know that det P AP = det P PA = det (A). Thus the
determinant of A is the product of the eigenvalues.
We summarize the results of our example with the following theorem.

12 Who in the world thinks up this stuﬀ? It seems that the answer is Marie Ennemond Camille Jordan,
who, despite having at least two girl names, was a guy.

181

Chapter 4 Eigenvalues and Eigenvectors

.
Theorem 19

Proper es of Eigenvalues and Eigenvectors
Let A be an n × n inver ble matrix. The following are true:
1. If A is triangular, then the diagonal elements of A are
the eigenvalues of A.

.

2. If λ is an eigenvalue of A with eigenvector ⃗x , then
an eigenvalue of A−1 with eigenvector ⃗x .

1
λ

is

3. If λ is an eigenvalue of A then λ is an eigenvalue of
AT .
4. The sum of the eigenvalues of A is equal to tr(A), the
trace of A.
5. The product of the eigenvalues of A is the equal to
det (A), the determinant of A.

There is one more concept concerning eigenvalues and eigenvectors that we will
explore. We do so in the context of an example.
[
]
1 2
. Example 92
.Find the eigenvalues and eigenvectors of the matrix A =
.
1 2
S

To ﬁnd the eigenvalues, we compute det (A − λI):
det (A − λI) =

1−λ
1

2
2−λ

= (1 − λ)(2 − λ) − 2
= λ2 − 3λ
= λ(λ − 3)
Our eigenvalues are therefore λ = 0, 3.
For λ = 0, we ﬁnd the eigenvectors:
[
]
−→
1 2 0
rref
1 2 0

[

1
0

2 0
0 0

]

This shows that x1 = −2x2 , and so our eigenvectors ⃗x are
[
]
−2
⃗x = x2
.
1
For λ = 3, we ﬁnd the eigenvectors:
182

4.2 Proper es of Eigenvalues and Eigenvectors
[

−2 2 0
1 −1 0

]

−→
rref

[

1 −1 0
0 0 0

]

This shows that x1 = x2 , and so our eigenvectors ⃗x are
[ ]
1
⃗x = x2
.
1
.
One interes ng thing about the above example is that we see that 0 is an eigenvalue of A; we have not oﬃcially encountered this before. Does this mean anything
signiﬁcant?13
Think about what an eigenvalue of 0 means: there exists an nonzero vector⃗x where
A⃗x = 0⃗x = ⃗0. That is, we have a nontrivial solu on to A⃗x = ⃗0. We know this only
happens when A is not inver ble.
So if A is inver ble, there is no nontrivial solu on to A⃗x = ⃗0, and hence 0 is not
an eigenvalue of A. If A is not inver ble, then there is a nontrivial solu on to A⃗x = ⃗0,
and hence 0 is an eigenvalue of A. This leads us to our ﬁnal addi on to the Inver ble
Matrix Theorem.
.
Theorem 20

Inver ble Matrix Theorem
Let A be an n×n matrix. The following statements are equivalent.

.

(a) A is inver ble.
(h) A does not have an eigenvalue of 0.

This sec on is about the proper es of eigenvalues and eigenvectors. Of course, we
have not inves gated all of the numerous proper es of eigenvalues and eigenvectors;
we have just surveyed some of the most common (and most important) concepts.
Here are four quick examples of the many things that s ll exist to be explored.
First, recall the matrix
[
]
1 4
A=
2 3
that we used in Example 84. It’s characteris c polynomial is p(λ) = λ2 − 4λ − 5.
Compute p(A); that is, compute A2 − 4A − 5I. You should get something “interes ng,”
and you should wonder “does this always work?”14
13 Since

0 is a “special” number, we might think so – a erall, we found that having a determinant of 0 is
important. Then again, a matrix with a trace of 0 isn’t all that important. (Well, as far as we have seen; it
actually is). So, having an eigenvalue of 0 may or may not be signiﬁcant, but we would be doing well if we
recognized the possibility of signiﬁcance and decided to inves gate further.
14 Yes.

183

Chapter 4 Eigenvalues and Eigenvectors
Second, in all of our examples, we have considered matrices where eigenvalues
“appeared only once.” Since we know that the eigenvalues of a triangular matrix appear on the diagonal, we know that the eigenvalues of
]
[
1 1
A=
0 1
are “1 and 1;” that is, the eigenvalue λ = 1 appears twice. What does that mean when
we consider the eigenvectors of λ = 1? Compare the result of this to the matrix
[
]
1 0
A=
,
0 1
which also has the eigenvalue λ = 1 appearing twice.15
Third, consider the matrix
[
]
0 −1
A=
.
1 0
What are the eigenvalues?16 We quickly compute
√the characteris c polynomial to be
p(λ) = λ2 + 1. Therefore the eigenvalues are ± −1 = ±i. What does this mean?
Finally, we have found the eigenvalues of matrices by ﬁnding the roots of the characteris c polynomial. We have limited our examples to quadra c and cubic polynomials; one would expect for larger sized matrices that a computer would be used to factor
the characteris c polynomials. However, in general, this is not how the eigenvalues
are found. Factoring high order polynomials is too unreliable, even with a computer
– round oﬀ errors can cause unpredictable results. Also, to even compute the characteris c polynomial, one needs to compute the determinant, which is also expensive
(as discussed in the previous chapter).
So how are eigenvalues found? There are itera ve processes that can progressively
transform a matrix A into another matrix that is almost an upper triangular matrix (the
entries below the diagonal are almost zero) where the entries on the diagonal are the
eigenvalues. The more itera ons one performs, the be er the approxima on is.
These methods are so fast and reliable that some computer programs convert polynomial root ﬁnding problems into eigenvalue problems!
Most textbooks on Linear Algebra will provide direc on on exploring the above
topics and give further insight to what is going on. We have men oned all the eigenvalue and eigenvector proper es in this sec on for the same reasons we gave in the
previous sec on. First, knowing these proper es helps us solve numerous real world
problems, and second, it is fascina ng to see how rich and deep the theory of matrices
is.

15 To direct further study, it helps to know that mathema cians refer to this as the duplicity of an eigenvalue. In each of these two examples, A has the eigenvalue λ = 1 with duplicity of 2.
16 Be careful; this matrix is not triangular.

184

4.2 Proper es of Eigenvalues and Eigenvectors

Exercises 4.2

[

In Exercises 1 – 6, a matrix A is given. For
each,

2.

(a) Find the eigenvalues of A, and for each
eigenvalue, ﬁnd an eigenvector.

3.

(b) Do the same for AT .
(c) Do the same for A−1 .
(d) Find tr(A).
(e) Find det (A).
Use Theorem 19 to verify your results.
[
]
0
4
1.
−1 5

[
[
4.


−2
−1
5
−1
−4
−1

5
5.  1
2

0
6.  1
1

]
−14
3
]
30
−6
]
72
13

−9 0
−5 0 
4
3

25
0
0
0 
1 −3

185

5
G

.

E

V

We already looked at the basics of graphing vectors. In this chapter, we’ll explore these
ideas more fully. One o en gains a be er understanding of a concept by “seeing” it.
For instance, one can study the func on f(x) = x2 and describe many proper es of
how the output relates to the input without producing a graph, but the graph can
quickly bring meaning and insight to equa ons and formulae. Not only that, but the
study of graphs of func ons is in itself a wonderful mathema cal world, worthy of
explora on.
We’ve studied the graphing of vectors; in this chapter we’ll take this a step further
and study some fantas c graphical proper es of vectors and matrix arithme c. We
men oned earlier that these concepts form the basis of computer graphics; in this
chapter, we’ll see even be er how that is true.

5.1

Transforma ons of the Cartesian Plane

.

.
AS YOU READ
...
1. To understand how the Cartesian plane is aﬀected by mul plica on by a matrix,
it helps to study how what is aﬀected?
2. Transforming the Cartesian plane through matrix mul plica on transforms straight
lines into what kind of lines?
3. T/F: If one draws a picture of a sheep on the Cartesian plane, then transformed
the plane using the matrix
[
]
−1 0
,
0 1
one could say that the sheep was “sheared.”

Chapter 5 Graphical Explora ons of Vectors
We studied in Sec on 2.3 how to visualize vectors and how certain matrix arithme c opera ons can be graphically represented. We limited our visual understanding
of matrix mul plica on to graphing a vector, mul plying it by a matrix, then graphing
the resul ng vector. In this sec on we’ll explore these mul plica on ideas in greater
depth. Instead of mul plying individual vectors by a matrix A, we’ll study what happens when we mul ply every vector in the Cartesian plans by A.1
Because of the Distribu ve Property as we saw demonstrated way back in Example
41, we can say that the Cartesian plane will be transformed in a very nice, predictable
way. Straight lines will be transformed into other straight lines (and they won’t become
curvy, or jagged, or broken). Curved lines will be transformed into other curved lines
(perhaps the curve will become “straight,” but it won’t become jagged or broken).
One way of studying how the whole Cartesian plane is aﬀected by mul plica on by
a matrix A is to study how the unit square is aﬀected. The unit square is the square with
corners at the points (0, 0), (1, 0), (1, 1), and (0, 1). Each corner can be represented
by the vector that points to it; mul ply each of these vectors by A and we can get an
idea of how A aﬀects the whole Cartesian plane.
Let’s try an example.
. Example 93
.Plot the vectors of the unit square before and a er they have been
mul plied by A, where
[
]
1 4
A=
.
2 3

S

tors

The four corners of the unit square can be represented by the vec[ ]
[ ]
[ ]
[ ]
0
1
1
0
,
,
,
.
0
0
1
1

Mul plying each by A gives the vectors
[ ]
[ ]
0
1
,
,
0
2

[ ]
5
,
5

[ ]
4
,
3

respec vely.
(Hint: one way of using your calculator to do this for you quickly is to make a 2 × 4
matrix whose columns are each of these vectors. In this case, create a matrix
[
]
0 1 1 0
B=
.
0 0 1 1
Then mul ply B by A and read oﬀ the transformed vectors from the respec ve columns:
[
]
0 1 5 4
AB =
.
0 2 5 3
1 No, we won’t do

188

them one by one.

5.1 Transforma ons of the Cartesian Plane
This saves me, especially if you do a similar procedure for mul ple matrices A. Of
course, we can save more me by skipping the ﬁrst column; since it is the column of
zeros, it will stay the column of zeros a er mul plica on by A.)
The unit square and its transforma on are graphed in Figure 5.1, where the shaped
ver ces correspond to each other across the two graphs. Note how the square got
turned into some sort of quadrilateral (it’s actually a parallelogram). A really interesting thing is how the triangular and square ver ces seem to have changed places – it is
as though the square, in addi on to being stretched out of shape, was ﬂipped.
y

y

1

1
.
1

x

1

x

Figure 5.1: Transforming the unit square. by matrix mul plica on in Example 93.
y

y

1

1
.
1

x

1

x

Figure 5.2: Emphasizing straight lines .going to straight lines in Example 93.
To stress how “straight lines get transformed to straight lines,” consider Figure 5.2.
Here, the unit square has some addi onal points drawn on it which correspond to the
shaded dots on the transformed parallelogram. Note how rela ve distances are also
preserved; the dot halfway between the black and square dots is transformed to a posi on along the line, halfway between the black and square dots. .
Much more can be said about this example. Before we delve into this, though, let’s
try one more example.

189

Chapter 5 Graphical Explora ons of Vectors
. Example 94
A, where

Plot the transformed unit square a er it has been transformed by
[
A=

]
0 −1
.
1 0

We’ll put the vectors that correspond to each corner in a matrix B
S
as before and then mul ply it on the le by A. Doing so gives:
[
][
]
0 −1
0 1 1 0
AB =
1 0
0 0 1 1
[
]
0 0 −1 −1
=
0 1 1
0
In Figure 5.3 the unit square is again drawn along with its transforma on by A.
y

y

1
.

x
1

x
1

Figure 5.3: Transforming the unit square. by matrix mul plica on in Example 94.
Make note of how the square moved. It did not simply “slide” to the le ;2 nor
did it “ﬂip” across the y axis. Rather, it was rotated counterclockwise about the origin
90◦ . In a rota on, the shape of an object does not change; in our example, the square
remained a square of the same size. .
We have broached the topic of how the Cartesian plane can be transformed via
mul plica on by a 2 ×2 matrix A. We have seen two examples so far, and our intui on
as to how the plane is changed has been informed only by seeing how the unit square
changes. Let’s explore this further by inves ga ng two ques ons:
1. Suppose we want to transform the Cartesian plane in a known way (for instance,
we may want to rotate the plane counterclockwise 180◦ ). How do we ﬁnd the
matrix (if one even exists) which performs this transforma on?
2. How does knowing how the unit square is transformed really help in understanding how the en re plane is transformed?
These ques ons are closely related, and as we answer one, we will help answer
the other.
2 mathema

190

cally, that is called a transla on

5.1 Transforma ons of the Cartesian Plane
To get started with the ﬁrst ques on, look back at Examples 93 and 94 and consider again how the unit square was transformed. In par cular, is there any correla on
between where the ver ces ended up and the matrix A?
If you are just reading on, and haven’t actually gone back and looked at the examples, go back now and try to make some sort of connec on. Otherwise – you may have
noted some of the following things:
1. The zero vector (⃗0, the “black” corner) never moved. That makes sense, though;
A⃗0 = ⃗0.
[ ]
1
2. The “square” corner, i.e., the corner corresponding to the vector
, is always
0
transformed to the vector in the ﬁrst column of A!
3. Likewise,
the “triangular” corner, i.e., the corner corresponding to the vector
[ ]
0
, is always transformed to the vector in the second column of A!3
1
4. The “white dot” corner is always transformed to the sum of the two column
vectors of A.4
Let’s now take the me to understand these four points. The ﬁrst point should be
clear; ⃗0 will always be transformed to ⃗0 via matrix mul plica on. (Hence the hint in the
middle of Example 93, where we are told that we can ignore entering in the column of
zeros in the matrix B.)
We can understand the second and third points simultaneously. Let
[
]
[ ]
[ ]
a b
1
0
A=
, e⃗1 =
and e⃗2 =
.
c d
0
1
What are Ae⃗1 and Ae⃗2 ?
[

][ ]
1
0

[

][ ]
0
1

a b
c d
[ ]
a
=
c

Ae⃗1 =

a b
c d
[ ]
b
=
d

Ae⃗2 =

3 Although this

is less of a surprise, given the result of the previous point.
observa on is a bit more obscure than the ﬁrst three. It follows from the fact that this corner of
the unit square is the “sum” of the other two nonzero corners.
4 This

191

Chapter 5 Graphical Explora ons of Vectors
So by mere mechanics of matrix mul plica on, the square corner e⃗1 is transformed
to the ﬁrst column of A, and the triangular corner e⃗2 is transformed to the second column of A. A similar argument demonstrates why the white dot corner is transformed
to the sum of the columns of A.5
Revisit now the ques on “How do we ﬁnd the matrix that performs a given transforma on on the Cartesian plane?” The answer follows from what we just did. Think
about the given transforma on and how it would transform the corners of the unit
square. Make the ﬁrst column of A the vector where e⃗1 goes, and make the second
column of A the vector where e⃗2 goes.
Let’s prac ce this in the context of an example.
. Example 95
.Find the matrix A that ﬂips the Cartesian plane about the x axis and
then stretches the plane horizontally by a factor of two.
[ ]
1
We ﬁrst consider e⃗1 =
. Where does this corner go to under
S
0
the given transforma on? Flipping the
[ ]plane across the x axis does not change
[ ]e⃗1 at
2
2
all; stretching the plane sends e⃗1 to
. Therefore, the ﬁrst column of A is
.
0
0
[ ]
0
Now consider e⃗2 =
. Flipping the plane about the x axis sends e⃗2 to the vec1
[
]
0
tor
; subsequently stretching the plane horizontally does not aﬀect this vector.
−1
[
]
0
Therefore the second column of A is
.
−1
Pu ng this together gives
[
A=

]
2 0
.
0 −1

To help visualize this, consider Figure 5.4 where a shape is transformed under this
matrix. No ce how it is turned upside down and is stretched horizontally by a factor
of two. (The gridlines are given as a visual aid.)

5 Another way of looking at all of this is to consider what A · I is: of course, it is just A. What are the
columns of I? Just e⃗1 and e⃗2 .

192

5.1 Transforma ons of the Cartesian Plane

.

.
Figure 5.4: Transforming the Cartesian
plane in Example 95

.

A while ago we asked two ques ons. The ﬁrst was “How do we ﬁnd the matrix that
performs a given transforma on?” We have just answered that ques on (although we
will do more to explore it in the future). The second ques on was “How does knowing
how the unit square is transformed really help us understand how the en re plane is
transformed?”
Consider Figure 5.5 where the unit square (with ver ces marked with shapes as
before) is shown transformed under an unknown matrix. How does this help us understand how the whole Cartesian plane is transformed? For instance, how can we
use this picture to ﬁgure out how the point (2, 3) will be transformed?
y

.

x

. an unknown transforma on.
Figure 5.5: The unit square under
There are two ways to consider the solu on to this ques on. First, we know now
how to compute the transforma on matrix; the new posi on of e⃗1 is the ﬁrst column
of A, and the new posi on of e⃗2 is the second column of A. Therefore, by looking at
the ﬁgure, we can deduce that
[
]
1 −1 6
A=
.
1 2
6 At least,

A is close to that. The square corner could actually be at the point (1.01, .99).

193

Chapter 5 Graphical Explora ons of Vectors
To ﬁnd where the point (2, 3) is sent, simply mul ply
[
][ ] [
]
1 −1
2
−1
=
.
1 2
3
8
There is another way of doing this which isn’t as computa onal – it doesn’t involve
compu ng the transforma on matrix. Consider the following equali es:
[ ] [ ] [ ]
2
2
0
=
+
3
0
3
[ ]
[ ]
1
0
=2
+3
0
1
= 2e⃗1 + 3e⃗2
[ ]This last equality states something that is somewhat obvious: to arrive at the vector
2
, one needs to go 2 units in the e⃗1 direc on and 3 units in the e⃗2 direc on. To ﬁnd
3
where the point (2, 3) is transformed, one needs to go 2 units in the new e⃗1 direc on
and 3 units in the new e⃗2 direc on. This is demonstrated in Figure 5.6.
new loca on of
the point (2, 3)

y

3 × “new” e⃗2

.

2 × “new” e⃗1
x

Figure 5.6: Finding the new .loca on of the point (2, 3).
We are coming to grips with how matrix transforma ons work. We asked two basic ques ons: “How do we ﬁnd the matrix for a given transforma on?” and “How
do we understand the transforma on without the matrix?”, and we’ve answered each
accompanied by one example. Let’s do another example that demonstrates both techniques at once.
. Example 96
.First, ﬁnd the matrix A that transforms the Cartesian plane by stretching it ver cally by a factor of 1.5, then stretches it horizontally by a factor of 0.5, then
rotates it clockwise about the origin 90◦ . Secondly, using the new loca ons of e⃗1 and
e⃗2 , ﬁnd the transformed loca on of the point (−1, 2).
194

5.1 Transforma ons of the Cartesian Plane
To ﬁnd A, ﬁrst consider the new loca on of e⃗1 . Stretching the plane
S
ver cally
does
not
aﬀect
e⃗1 ; stretching the plane horizontally by a factor
of]0.5 changes
[
]
[
1/2
0
◦
e⃗1 to
, and then rota ng it 90 about the origin moves it to
. This is the
0
−1/2
ﬁrst column of A.
[ Now
] consider the new loca on of e⃗2 . Stretching the plane ver cally changes
[ it to
]
3/2
0
◦
; stretching horizontally does not aﬀect it, and rota ng 90 moves it to
.
3/2
0
This is then the second column of A. This gives
[
]
0
3/2
A=
.
−1/2
0
[
]
−1
Where does the point (−1, 2) get sent to? The corresponding vector
is
2
found by going −1 units in the e⃗1 direc on and 2 units in the e⃗2 direc on. Therefore,
the transforma on will send the vector to −1 units in the new e⃗1 direc on and 2 units
in the new e⃗2 direc on. This is sketched in Figure 5.7, along with the transformed unit
square. We can also check this mul plica vely:
[
][
] [
]
0
3/2
−1
3
=
.
−1/2
0
2
1/2
Figure 5.8 shows the eﬀects of the transforma on on another shape.
y

.

x

Figure 5.7: Understanding the .transforma on in Example 96.

.

.

.
Figure 5.8: Transforming the Cartesian
plane in Example 96

195

Chapter 5 Graphical Explora ons of Vectors
Right now we are focusing on transforming the Cartesian plane – we are making 2D
transforma ons. Knowing how to do this provides a founda on for transforming 3D
space,7 which, among other things, is very important when producing 3D computer
graphics. Basic shapes can be drawn and then rotated, stretched, and/or moved to
other regions of space. This also allows for things like “moving the camera view.”
What kinds of transforma ons are possible? We have already seen some of the
things that are possible: rota ons, stretches, and ﬂips. We have also men oned some
things that are not possible. For instance, we stated that straight lines always get transformed to straight lines. Therefore, we cannot transform the unit square into a circle
using a matrix.
Let’s look at some common transforma ons of the Cartesian plane and the matrices that perform these opera ons. In the following ﬁgures, a transforma on matrix
will be given alongside a picture of the transformed unit square. (The original unit
square is drawn lightly as well to serve as a reference.)

2D Matrix Transforma ons
y
Horizontal stretch by a
factor of k.
[
]
k 0
0 1

(k, 1)
.

x

y
Ver cal stretch by a
factor of k.
[
]
1 0
0 k

7 Actually, it provides a founda

196

(1, k)

.

x

on for doing it in 4D, 5D, . . ., 17D, etc. Those are just harder to visualize.

5.1 Transforma ons of the Cartesian Plane
y
Horizontal shear by a
factor of k.
[
]
1 k
0 1

(k, 1)
.

x

y
Ver cal shear by a factor
of k.
[
]
1 0
k 1

(k, 1)

.

x

y
Horizontal reﬂec on
across the y axis.
[
]
−1 0
0 1

.

x

y
Ver cal reﬂec on across
the x axis.
[
]
1 0
0 −1

.

x

197

Chapter 5 Graphical Explora ons of Vectors
y
Diagonal reﬂec on
across the line y = x.
[
]
0 1
1 0

.

x

y
Rota on around the
origin by an angle of θ.
[
]
cos θ − sin θ
sin θ cos θ
.

Projec on onto the x
axis.
(Note how the square is
“squashed” down onto
the x-axis.)
[
]
1 0
0 0

y

Projec on onto the y
axis.
(Note how the square is
“squashed” over onto
the y-axis.)
[
]
0 0
0 1

y

.

.

θ

x

x

x

Now that we have seen a healthy list of transforma ons that we can perform on
the Cartesian plane, let’s prac ce a few more mes crea ng the matrix that gives the
desired transforma on. In the following example, we develop our understanding one
198

5.1 Transforma ons of the Cartesian Plane
more cri cal step.
. Example 97
.Find the matrix A that transforms the Cartesian plane by performing the following opera ons in order:
1. Ver cal shear by a factor of
0.5

3. Horizontal stretch by a factor of 2

2. Counterclockwise rota on
about the origin by an angle
of θ = 30◦

4. Diagonal reﬂec on across
the line y = x

Wow! We already know how to do this – sort of. We know we
S
can ﬁnd the columns of A by tracing where e⃗1 and e⃗2 end up, but this also seems
diﬃcult. There is so much that is going on. Fortunately, we can accomplish what we
need without much diﬃculty by being systema c.
First, let’s perform the ver cal shear. The matrix that performs this is
[
]
1 0
A1 =
.
0.5 1
A er that, we want to rotate everything clockwise by 30◦ . To do this, we use
[
A2 =

cos 30◦
sin 30◦

− sin 30◦
cos 30◦

]
=

]
[√
3/2 √
−1/2
.
1/2
3/2

In order to do both of these opera ons, in order, we mul ply A2 A1 .8
To perform the ﬁnal two opera ons, we note that
[
]
[
]
2 0
0 1
A3 =
and A4 =
0 1
1 0
perform the horizontal stretch and diagonal reﬂec on, respec vely. Thus to perform
all of the opera ons “at once,” we need to mul ply by
A = A4 A3 A2 A1
[
][
][√
][
]
0 1
2 0
3/2 √
−1/2
1 0
=
1 0
0 1
0.5 1
1/2
3/2
√
]
[ √
( √3 + 2)/4
3/2
=
(2 3 − 1)/2
−1
[
]
0.933 0.866
≈
.
1.232
−1
8 The reader might ask, “Is it important to do mul ply these in that order? Could we have mul plied
A1 A2 instead?” Our answer starts with “Is matrix mul plica on commuta ve?” The answer to our ques on
is “No,” so the answers to the reader’s ques ons are “Yes” and “No,” respec vely.

199

Chapter 5 Graphical Explora ons of Vectors
Let’s consider this closely. Suppose I want to know where a vector ⃗x ends up. We
claim we can ﬁnd the answer by mul plying A⃗x . Why does this work? Consider:
A⃗x = A4 A3 A2 A1⃗x
= A4 A3 A2 (A1⃗x )
= A4 A3 (A2⃗x 1 )
= A4 (A3⃗x 2 )
= A4⃗x 3

(performs the ver cal shear)
(performs the rota on)
(performs the horizontal stretch)
(performs the diagonal reﬂec on)

= ⃗x 4

(the result of transforming ⃗x )

Most readers are not able to visualize exactly what the given list of opera ons does
to the Cartesian plane. In Figure 5.9 we sketch the transformed unit square; in Figure
5.10 we sketch a shape and its transforma on.
y

.

x

Figure 5.9: The transformed. unit square in Example 97.

.

.

. shape in Example 97.
Figure 5.10: A transformed

Once we know what matrices perform the basic transforma ons,9 performing complex transforma ons on the Cartesian plane really isn’t that . . . complex. It boils down
200

9 or know where to ﬁnd

them

5.1 Transforma ons of the Cartesian Plane
to mul plying by a series of matrices.
We’ve shown many examples of transforma ons that we can do, and we’ve menoned just a few that we can’t – for instance, we can’t turn a square into a circle. Why
not? Why is it that straight lines get sent to straight lines? We spent a lot of me within
this text looking at inver ble matrices; what connec ons, if any,10 are there between
inver ble matrices and their transforma ons on the Cartesian plane?
All these ques ons require us to think like mathema cians – we are being asked to
study the proper es of an object we just learned about and their connec ons to things
we’ve already learned. We’ll do all this (and more!) in the following sec on.

Exercises 5.1
In Exercises 1 – 4, a sketch of transformed unit
square is given. Find the matrix A that performs this transforma on.
y

In Exercises 5 – 10, a list of transforma ons is
given. Find the matrix A that performs those
transforma ons, in order, on the Cartesian
plane.
5.

(a) ver cal shear by a factor of 2
(b) horizontal shear by a factor of 2

6.

(a) horizontal shear by a factor of 2
(b) ver cal shear by a factor of 2

7.

(a) horizontal stretch by a factor of 3
(b) reﬂec on across the line y = x

1.

1
.

x

1

8.

(b) ver cal stretch by a factor of 1/2

y
9.

2.

(a) counterclockwise rota on by an
angle of 45◦
(a) clockwise rota on by an angle of
90◦
(b) horizontal reﬂec on across the y
axis

1
.
1

x

(c) ver cal shear by a factor of 1
10.

y

(a) ver cal reﬂec on across the x
axis
(b) horizontal reﬂec on across the y
axis

3.

1
.
1
y

4.

1
.
1

10 By now,

(c) diagonal reﬂec on across the line
y=x

x

x

In Exercises 11 – 14, two sets of transformaons are given. Sketch the transformed unit
square under each set of transforma ons.
Are the transforma ons the same? Explain
why/why not.

the reader should expect connec ons to exist.

201

Chapter 5 Graphical Explora ons of Vectors
11.

(a) a horizontal reﬂec on across the
y axis, followed by a ver cal reﬂec on across the x axis, compared to
(b) a counterclockise rota on of
180◦

12.

13.

(a) a horizontal stretch by a factor of
1/2 followed by a ver cal stretch
by a factor of 3, compared to
(b) the same opera ons but in opposite order

14.

(a) a horizontal stretch by a factor of
2 followed by a reﬂec on across
the line y = x, compared to

(a) a reﬂec on across the line y =
x followed by a reﬂec on across
the x axis, compared to
(b) a reﬂec on across the the y axis,
followed by a reﬂec on across
the line y = x.

(b) a ver cal stretch by a factor of 2

5.2 Proper es of Linear Transforma ons

.

.
AS YOU READ
...
1. T/F: Transla ng the Cartesian plane 2 units up is a linear transforma on.
2. T/F: If T is a linear transforma on, then T(⃗0) = ⃗0.

In the previous sec on we discussed standard transforma ons of the Cartesian
plane – rota ons, reﬂec ons, etc. As a mo va onal example for this sec on’s study,
let’s consider another transforma on – let’s ﬁnd the matrix that moves the unit square
one unit to the right (see Figure 5.11). This is called a transla on.
y

.

y

x

x

. square one unit to the right.
Figure 5.11: Transla ng the unit
Our work from the previous sec on allows us to ﬁnd
[ the
] matrix quickly. By looking
[ ]
2
1
at the picture, it is easy to see that e⃗1 is moved to
and e⃗2 is moved to
.
0
1
Therefore, the transforma on matrix should be
[
]
2 1
A=
.
0 1
However, look at Figure 5.12 where the unit square is drawn a er being transformed by A. It is clear that we did not get the desired result; the unit square was not
translated, but rather stretched/sheared in some way.
202

5.2 Proper es of Linear Transforma ons
y

.

y

x

x

. of the unit square by matrix A.
Figure 5.12: Actual transforma on
What did we do wrong? We will answer this ques on, but ﬁrst we need to develop
a few thoughts and vocabulary terms.
We’ve been using the term “transforma on” to describe how we’ve changed vectors. In fact, “transforma on” is synonymous to “func on.” We are used to func ons
like f(x) = x2 , where the input is a number and the output is another number. In the
previous sec on, we learned about transforma ons (func ons) where the input was
a vector and the output was another vector. If A is a “transforma on matrix,” then we
could create a func on of the form T(⃗x ) = A⃗x . That is, a vector ⃗x is the input, and the
output is ⃗x mul plied by A.11
When we deﬁned f(x) = x2 above, we let the reader assume that the input was
indeed a number. If we wanted to be complete, we should have stated
f:R→R

where

f(x) = x2 .

The ﬁrst part of that line told us that the input was a real number (that was the ﬁrst
R) and the output was also a real number (the second R).
To deﬁne a transforma on where a 2D vector is transformed into another 2D vector
via mul plica on by a 2 × 2 matrix A, we should write
T : R2 → R2

where

T(⃗x ) = A⃗x .

Here, the ﬁrst R2 means that we are using 2D vectors as our input, and the second R2
means that a 2D vector is the output.
Consider a quick example:
 2 
([ ])
x1
x
1
T : R2 → R3 where T
=  2x1  .
x2
x1 x2
No ce that this takes 2D vectors as input and returns 3D vectors as output. For instance,


([
])
9
3
T
=  6 .
−2
−6
We now deﬁne a special type of transforma on (func on).
11 We

used T instead of f to deﬁne this func on to help diﬀeren ate it from “regular” func ons. “Normally” func ons are deﬁned using lower case le ers when the input is a number; when the input is a vector,
we use upper case le ers.

203

Chapter 5 Graphical Explora ons of Vectors

.
Deﬁni on 29

Linear Transforma on
A transforma on T : Rn → Rm is a linear transforma on if
it sa sﬁes the following two proper es:

.

1. T(⃗x + ⃗y) = T(⃗x ) + T(⃗y) for all vectors ⃗x and ⃗y, and
2. T(k⃗x ) = kT(⃗x ) for all vectors ⃗x and all scalars k.
If T is a linear transforma on, it is o en said that “T is linear.”

Let’s learn about this deﬁni on through some examples.
. Example 98
Determine whether or not the transforma on T : R2 → R3 is a
linear transforma on, where
 2 
([ ])
x1
x1
T
=  2x1  .
x2
x1 x2

S

We’ll arbitrarily pick two vectors ⃗x and ⃗y:
[
]
[ ]
3
1
⃗x =
and ⃗y =
.
−2
5

Let’s check to see if T is linear by using the deﬁni on.
1. Is T(⃗x + ⃗y) = T(⃗x ) + T(⃗y)? First, compute ⃗x + ⃗y:
[
] [ ] [ ]
3
1
4
⃗x + ⃗y =
+
=
.
−2
5
3
Now compute T(⃗x ), T(⃗y), and T(⃗x + ⃗y):
([ ])
1
T(⃗y) = T
5
 
1
= 2
5

([
])
3
T(⃗x ) = T
−2


9
= 6 
−6
Is T(⃗x + ⃗y) = T(⃗x ) + T(⃗y)?


    
9
1 ! 16
 6  +  2  ̸=  8  .
−6
5
12

204
Therefore, T is not a linear transforma on. .

([ ])
4
T(⃗x + ⃗y) = T
3
 
16
= 8 
12

5.2 Proper es of Linear Transforma ons
So we have an example of something that doesn’t work. Let’s try an example where
things do work.12
. Example 99
.Determine whether or not the transforma on T : R2 → R2 is a
linear transforma on, where T(⃗x ) = A⃗x and
[
A=

]
1 2
.
3 4

Let’s start by again considering arbitrary ⃗x and ⃗y. Let’s choose the
S
same ⃗x and ⃗y from Example 98.
[

3
−2

⃗x =

]
and

[ ]
1
⃗y =
.
5

If the lineararity proper es hold for these vectors, then maybe it is actually linear (and
we’ll do more work).
1. Is T(⃗x + ⃗y) = T(⃗x ) + T(⃗y)? Recall:
⃗x + ⃗y =

[ ]
4
.
3

Now compute T(⃗x ), T(⃗y), and T(⃗x ) + T(⃗y):
([
])
3
T(⃗x ) = T
−2
[
]
−1
=
1

([ ])
1
T(⃗y) = T
5
[ ]
11
=
23

([ ])
4
T(⃗x + ⃗y) = T
3
[ ]
10
=
24

Is T(⃗x + ⃗y) = T(⃗x ) + T(⃗y)?
[

] [ ] [ ]
−1
11 ! 10
+
=
.
24
1
23

So far, so good: T(⃗x + ⃗y) is equal to T(⃗x ) + T(⃗y).
12 Recall a principle of logic: to show that something doesn’t work, we just need to show one case where
it fails, which we did in Example 98. To show that something always works, we need to show it works
for all cases – simply showing it works for a few cases isn’t enough. However, doing so can be helpful in
understanding the situa on be er.

205

Chapter 5 Graphical Explora ons of Vectors
2. Is T(k⃗x ) = kT(⃗x )? Let’s arbitrarily pick k = 7, and use ⃗x as before.
([
])
21
T(7⃗x ) = T
−14
[
]
−7
=
7
[
]
−1
=7
1
= 7 · T(⃗x )

!

So far it seems that T is indeed linear, for it worked in one example with arbitrarily
chosen vectors and scalar. Now we need to try to show it is always true.
Consider T(⃗x + ⃗y). By the deﬁni on of T, we have
T(⃗x + ⃗y) = A(⃗x + ⃗y).
By Theorem 3, part 2 (on page 62) we state that the Distribu ve Property holds for
matrix mul plica on.13 So A(⃗x + ⃗y) = A⃗x + A⃗y. Recognize now that this last part is
just T(⃗x ) + T(⃗y)! We repeat the above steps, all together:
T(⃗x + ⃗y) = A(⃗x + ⃗y)
= A⃗x + A⃗y
= T(⃗x ) + T(⃗y)

(by the deﬁni on of T in this example)
(by the Distribu ve Property)
(again, by the deﬁni on of T)

Therefore, no ma er what ⃗x and ⃗y are chosen, T(⃗x + ⃗y) = T(⃗x ) + T(⃗y). Thus the ﬁrst
part of the lineararity deﬁni on is sa sﬁed.
The second part is sa sﬁed in a similar fashion. Let k be a scalar, and consider:
T(k⃗x ) = A(k⃗x )
= kA⃗x
= kT(⃗x )

(by the deﬁni on of T is this example)
(by Theorem 3 part 3)
(again, by the deﬁni on of T)

Since T sa sﬁes both parts of the deﬁni on, we conclude that T is a linear transforma on. .
We have seen two examples of transforma ons so far, one which was not linear and
one that was. One might wonder “Why is linearity important?”, which we’ll address
shortly.
First, consider how we proved the transforma on in Example 99 was linear. We
deﬁned T by matrix mul plica on, that is, T(⃗x ) = A⃗x . We proved T was linear using
proper es of matrix mul plica on – we never considered the speciﬁc values of A! That
is, we didn’t just choose a good matrix for T; any matrix A would have worked. This
13 Recall that a vector is just a special type of matrix, so this theorem applies to matrix–vector mul plicaon as well.

206

5.2 Proper es of Linear Transforma ons
leads us to an important theorem. The ﬁrst part we have essen ally just proved; the
second part we won’t prove, although its truth is very powerful.
.
Theorem 21

Matrices and Linear Transforma ons
1. Deﬁne T : Rn → Rm by T(⃗x ) = A⃗x , where A is an
m × n matrix. Then T is a linear transforma on.

.

2. Let T : R → R be any linear transforma on. Then
there exists an unique m×n matrix A such that T(⃗x ) =
A⃗x .
n

m

The second part of the theorem says that all linear transforma ons can be described using matrix mul plica on. Given any linear transforma on, there is a matrix
that completely deﬁnes that transforma on. This important matrix gets its own name.
.
Deﬁni on 30

Standard Matrix of a Linear Transforma on
Let T : Rn → Rm be a linear transforma on. By Theorem
21, there is a matrix A such that T(⃗x ) = A⃗x . This matrix A
is called the standard matrix of the linear transforma on T,
and is denoted [ T ].a

.

a The matrix–like brackets around T suggest that the standard matrix A
is a matrix “with T inside.”

While exploring all of the ramiﬁca ons of Theorem 21 is outside the scope of this
text, let it suﬃce to say that since 1) linear transforma ons are very, very important
in economics, science, engineering and mathema cs, and 2) the theory of matrices is
well developed and easy to implement by hand and on computers, then 3) it is great
news that these two concepts go hand in hand.
We have already used the second part of this theorem in a small way. In the previous sec on we looked at transforma ons graphically and found the matrices that
produced them. At the me, we didn’t realize that these transforma ons were linear,
but indeed they were.
This brings us back to the mo va ng example with which we started this sec on.
We tried to ﬁnd the matrix that translated the unit square one unit to the right. Our
a empt failed, and we have yet to determine why. Given our link between matrices
and linear transforma ons, the answer is likely “the transla on transforma on is not
a linear transforma on.” While that is a true statement, it doesn’t really explain things
all that well. Is there some way we could have recognized that this transforma on
207

Chapter 5 Graphical Explora ons of Vectors
wasn’t linear?14
Yes, there is. Consider the second part of the linear transforma on deﬁni on. It
states that T(k⃗x ) = kT(⃗x ) for all scalars k. If we let k = 0, we have T(0⃗x ) = 0 · T(⃗x ), or
more simply, T(⃗0) = ⃗0. That is, if T is to be a linear transforma on, it must send the
zero vector to the zero vector.
This is a quick way to see that the transla on transforma on fails to be linear. By
shi ing the unit square to the right one unit, the corner at the point (0, 0) was sent to
the point (1, 0), i.e.,
[ ]
[ ]
0
1
the vector
was sent to the vector
.
0
0
This property rela ng to ⃗0 is important, so we highlight it here.
.
Key Idea 15

Linear Transforma ons and ⃗0
Let T : Rn → Rm be a linear transforma on. Then:

.

T(⃗0n ) = ⃗0m .
That is, the zero vector in Rn gets sent to the zero vector in
Rm .

The interested reader may wish to read the footnote below.15

The Standard Matrix of a Linear Transforma on
It is o en the case that while one can describe a linear transforma on, one doesn’t
know what matrix performs that transforma on (i.e., one doesn’t know the standard
matrix of that linear transforma on). How do we systema cally ﬁnd it? We’ll need a
new deﬁni on.
.
Deﬁni on 31

Standard Unit Vectors

.

In Rn , the standard unit vectors e⃗i are the vectors with a 1
in the ith entry and 0s everywhere else.

14 That is,

apart from applying the deﬁni on directly?
ons “send zero to zero” has an interes ng rela on to terminology. The
reader is likely familiar with func ons like f(x) = 2x + 3 and would likely refer to this as a “linear func on.”
However, f(0) ̸= 0, so f is not “linear” by our new deﬁni on of linear. We erroneously call f “linear” since
its graph produces a line, though we should be careful to instead state that “the graph of f is a line.”
15 The idea that linear transforma

208

5.2 Proper es of Linear Transforma ons
We’ve already seen these vectors in the previous sec on. In R2 , we iden ﬁed
e⃗1 =

[ ]
1
0

and

e⃗2 =

[ ]
0
.
1

In R4 , there are 4 standard unit vectors:
 
1
0

e⃗1 = 
0,
0

 
0
1

e⃗2 = 
0,
0

 
0
0

e⃗3 = 
1,
0

and

 
0
0

e⃗4 = 
0.
1

How do these vectors help us ﬁnd the standard matrix of a linear transforma on?
Recall again our work in the previous sec on. There, we prac ced looking at the transformed unit square and deducing the standard transforma on matrix A. We did this
by making the ﬁrst column of A the vector where e⃗1 ended up and making the second
column of A the vector where e⃗2 ended up. One could represent this with:
[
A = T(e⃗1 )

]
T(e⃗2 ) = [ T ].

That is, T(e⃗1 ) is the vector where e⃗1 ends up, and T(e⃗2 ) is the vector where e⃗2 ends up.
The same holds true in general. Given a linear transforma on T : Rn → Rm , the
standard matrix of T is the matrix whose ith column is the vector where e⃗i ends up.
While we won’t prove this is true, it is, and it is very useful. Therefore we’ll state it
again as a theorem.
.
Theorem 22

The Standard Matrix of a Linear Transforma on

.

Let T : Rn → Rm be a linear transforma on. Then [ T ] is the
m × n matrix:
[
]
[ T ] = T(e⃗1 ) T(e⃗2 ) · · · T(e⃗n ) .

Let’s prac ce this theorem in an example.
. Example 100

.Deﬁne T : R3 → R4 to be the linear transforma on where


x1

T  x2  = 

x3



x1 + x2

3x1 − x3
.

2x2 + 5x3
4x1 + 3x2 + 2x3

Find [ T ].
209

Chapter 5 Graphical Explora ons of Vectors
S

Note that

T takes vectors from R3 into R4 , so [ T ] is going to be a 4×3 matrix.
 
 
 
1
0
0
e⃗1 =  0  , e⃗2 =  1  and e⃗3 =  0  .
0
0
1

We ﬁnd the columns of [ T ] by ﬁnding where e⃗1 , e⃗2 and e⃗3 are sent, that is, we ﬁnd
T(e⃗1 ), T(e⃗2 ) and T(e⃗3 ).
 
1
T(e⃗1 ) = T  0 
0
 
1
3

=
0
4

 
0
T(e⃗2 ) = T  1 
0
 
1
0

=
2
3

Thus



1
3
[T] = A = 
0
4

 
0
T(e⃗3 ) = T  0 
1


0
 −1 

=
 5 
2


1 0
0 −1 
.
2 5 
3 2

Let’s check this. Consider the vector
 
1
⃗x =  2  .
3
Strictly from the original deﬁni on, we can compute that

  
  
1+2
3
1
 3−3   0 
  
T(⃗x ) = T  2  = 
 4 + 15  =  19  .
3
4+6+6
16
Now compute T(⃗x ) by compu

1
3
A⃗x = 
0
4

ng [ T ]⃗x = A⃗x .

 
1 0  
3
1
 0 
0 −1 
2 =  .
 19 
2 5 
3
3 2
16

They match!16 .
Let’s do another example, one that is more applica on oriented.
16 Of course they do.

210

That was the whole point.

5.2 Proper es of Linear Transforma ons
. Example 101
.A baseball team manager has collected basic data concerning his
hi ers. He has the number of singles, doubles, triples, and home runs they have hit
over the past year. For each player, he wants two more pieces of informa on: the total
number of hits and the total number of bases.
Using the techniques developed in this sec on, devise a method for the manager
to accomplish his goal.
If the manager only wants to compute this for a few players, then
S
he could do it by hand fairly easily. A er all:
total # hits = # of singles + # of doubles + # of triples + # of home runs,
and
total # bases = # of singles + 2×# of doubles + 3×# of triples + 4×# of home runs.
However, if he has a lot of players to do this for, he would likely want a way to
automate the work. One way of approaching the problem starts with recognizing that
he wants to input four numbers into a func on (i.e., the number of singles, doubles,
etc.) and he wants two numbers as output (i.e., number of hits and bases). Thus he
wants a transforma on T : R4 → R2 where each vector in R4 can be interpreted as



# of singles
 # of doubles 


 # of triples  ,
# of home runs

and each vector in R2 can be interpreted as
[

]
# of hits
.
# of bases

To ﬁnd [ T ], he computes T(e⃗1 ), T(e⃗2 ), T(e⃗3 ) and T(e⃗4 ).

211

Chapter 5 Graphical Explora ons of Vectors
 
0
 1 



T(e⃗2 ) = T  
0 
0
[ ]
1
=
2

 
1
 0 



T(e⃗1 ) = T  
0 
0
[ ]
1
=
1

 
0
 0 
 
T(e⃗4 ) = T 
 0 
1
[ ]
1
=
4

 
0
 0 
 
T(e⃗3 ) = T 
 1 
0
[ ]
1
=
3

(What do these calcula ons mean? For example, ﬁnding T(e⃗3 ) =

[ ]
1
means that
3

one triple counts as 1 hit and 3 bases.)
Thus our transforma on matrix [ T ]is
[
[T] = A =

]
1 1 1 1
.
1 2 3 4

As an example, consider a player who had 102 singles, 30 doubles, 8 triples and 14
home runs. By using A, we ﬁnd that
[

1
1

1
2

1
3


102
[
]

1 
 30  = 154 ,
4  8 
242
14
]



meaning the player had 154 hits and 242 total bases. .
A ques on that we should ask concerning the previous example is “How do we
know that the func on the manager used was actually a linear transforma on? A er
all, we were wrong before – the transla on example at the beginning of this sec on
had us fooled at ﬁrst.”
This is a good point; the answer is fairly easy. Recall from Example 98 the transforma on
 2 
([ ])
x1
x1
T98
=  2x1 
x2
x1 x2
212

5.2 Proper es of Linear Transforma ons
and from Example 100


x1

T100  x2  = 

x3



x1 + x2

3x1 − x3
,

2x2 + 5x3
4x1 + 3x2 + 2x3

where we use the subscripts for T to remind us which example they came from.
We found that T98 was not a linear transforma on, but stated that T100 was (although we didn’t prove this). What made the diﬀerence?
Look at the entries of T98 (⃗x ) and T100 (⃗x ). T98 contains entries where a variable is
squared and where 2 variables are mul plied together – these prevent T98 from being
linear. On the other hand, the entries of T100 are all of the form a1 x1 + · · · + an xn ; that
is, they are just sums of the variables mul plied by coeﬃcients. T is linear if and only if
the entries of T(⃗x ) are of this form. (Hence linear transforma ons are related to linear
equa ons, as deﬁned in Sec on 1.1.) This idea is important.
.
Key Idea 16

Condi ons on Linear Transforma ons
Let T : Rn → Rm be a transforma on and consider the
entries of
 
x1
 x2 
 
T(⃗x ) = T  .  .
 .. 
xn

.

T is linear if and only if each entry of T(⃗x ) is of the form a1 x1 +
a2 x2 + · · · an xn .

Going back to our baseball example, the manager could have deﬁned his transforma on as
 
x1
[
]
 x2 
x1 + x2 + x3 + x4




T   =
.
x3
x1 + 2x2 + 3x3 + 4x4
x4
Since that ﬁts the model shown in Key Idea 16, the transforma on T is indeed linear
and hence we can ﬁnd a matrix [ T ] that represents it.
Let’s prac ce this concept further in an example.
. Example 102
.Using Key Idea 16, determine whether or not each of the following
transforma ons is linear.
([ ]) [
]
]
([ ]) [
x1
x1 + 1
x1
x1 /x2
√
T1
=
T2
=
x2
x2
x2
x2
213

Chapter 5 Graphical Explora ons of Vectors
([
T3

x1
x2

])
=

[√
]
7x1 − x2
πx2

T1 is not linear! This may come as a surprise, but we are not alS
lowed to add constants to the variables. By thinking about this, we can see that this
transforma on is trying to accomplish the transla on that got us started in this sec on
– it adds 1 to all the x values and leaves the y values alone, shi ing everything to the
right one unit. However, this is not linear; again, no ce how ⃗0 does not get mapped
to ⃗0.
T2 is also not linear. We cannot divide variables, nor can we put variabless inside the square root func on (among other other things; again, see Sec on 1.1). This
means that the baseball manager would not be able to use matrices to compute a
ba ng average, which is (number
of hits)/(number of at bats).
√
T3 is linear. Recall that 7 and π are just numbers, just coeﬃcients. .
We’ve men oned before that we can draw vectors other than 2D vectors, although
the more dimensions one adds, the harder it gets to understand. In the next sec on
we’ll learn about graphing vectors in 3D – that is, how to draw on paper or a computer
screen a 3D vector.

Exercises 5.2
In Exercises 1 – 5, a transforma on T is given.
Determine whether or not T is linear; if not,
state why not.
]
([ ]) [
x1 + x2
x1
=
1. T
3x1 − x2
x2
]
([ ]) [
x1 + x22
x1
=
2. T
x1 − x2
x2
([ ]) [
]
x1
x1 + 1
3. T
=
x2
x2 + 1
([ ]) [ ]
x1
1
4. T
=
x2
1
([ ]) [ ]
x1
0
5. T
=
x2
0
In Exercises 6 – 11, a linear transforma on T
is given. Find [ T ].
]
([ ]) [
x1
x1 + x2
6. T
=
x1 − x2
x2

214

([
7. T

x1
x2

])




x1 + 2x2
=  3x1 − 5x2 
2x2




x1 + 2x2 − 3x3
x1


0

8. T  x2  = 


x1 + 4x3
x3
5x2 + x3



 
x1 + 3x3
x1
9. T  x2  =  x1 − x3 
x1 + x3
x3


([
10. T

x1
x2



])
=

[ ]
0
0


x1
 x2  [
]
 
11. T 
 x3  = x1 + 2x2 + 3x3 + 4x4
x4

5.3 Visualizing Vectors: Vectors in Three Dimensions

5.3

Visualizing Vectors: Vectors in Three Dimensions

.

.
AS YOU READ
...
1. T/F: The viewpoint of the reader makes a diﬀerence in how vectors in 3D look.
2. T/F: If two vectors are not near each other, then they will not appear to be near
each other when graphed.
3. T/F: The parallelogram law only applies to adding vectors in 2D.

We ended the last sec on by sta ng we could extend the ideas of drawing 2D vectors to drawing 3D vectors. Once we understand how to properly draw these vectors,
addi on and subtrac on is rela vely easy. We’ll also discuss how to ﬁnd the length of
a vector in 3D.
We start with the basics of drawing a vector in 3D. Instead of having just the tradional x and y axes, we now add a third axis, the z axis. Without any addi onal vectors,
a generic 3D coordinate system can be seen in Figure 5.13.
z

.

y

x
Figure 5.13: The 3D. coordinate system
In 2D, the point (2, 1) refers to going 2 units in the x direc on followed by 1 unit in
the y direc on. In 3D, each point is referenced by 3 coordinates. The point (4, 2, 3) is
found by going 4 units in the x direc on, 2 units in the y direc on, and 3 units in the z
direc on.
How does one sketch avector
 on this coordinate system? As one might expect, we
1
can sketch the vector⃗v =  2  by drawing an arrow from the origin (the point (0,0,0))
3
to the point (1, 2, 3).17 The only “tricky” part comes from the fact that we are trying
to represent three dimensional space on a two dimensional sheet of paper. However,
17 Of course, we don’t have to start at the origin; all that really ma ers is that the p of the arrow is 1 unit
in the x direc on, 2 units in the y direc on, and 3 units in the z direc on from the origin of the arrow.

215

Chapter 5 Graphical Explora ons of Vectors
it isn’t really hard. We’ll discover a good way of approaching this in the context of an
example.
. Example 103

.Sketch the following vectors with their origin at the origin.
 


2
1
⃗v =  1  and ⃗u =  3 
3
−1

We’ll start with ⃗v ﬁrst. Star ng at the origin, move 2 units in the x
S
direc on. This puts us at the point (2, 0, 0) on the x axis. Then, move 1 unit in the y
direc on. (In our method of drawing, this means moving 1 unit directly to the right.
Of course, we don’t have a grid to follow, so we have to make a good approxima on of
this distance.) Finally, we move 3 units in the z direc on. (Again, in our drawing, this
means going straight “up” 3 units, and we must use our best judgment in a sketch to
measure this.)
This allows us to locate the point (2, 1, 3); now we draw an arrow from the origin
to this point. In Figure 5.14 we have all 4 stages of this sketch. The dashed lines show
us moving down the x axis in (a); in (b) we move over in the y direc on; in (c) we move
up in the z direc on, and ﬁnally in (d) the arrow is drawn.
z

.

x

z

y

y

x

(a)
z

(b)
z

(2,1,3)
y

x

(c)

y

x

(d)

Figure 5.14: Stages of sketching. the vector ⃗v for Example 103.

216

5.3 Visualizing Vectors: Vectors in Three Dimensions
Drawing the dashed lines help us ﬁnd our way in our representa on of three dimensional space. Without them, it is hard to see how far in each direc on the vector
is supposed to have gone.
To draw ⃗u, we follow the same procedure we used to draw ⃗v. We ﬁrst locate the
point (1, 3, −1), then draw the appropriate arrow. In Figure 5.15 we have ⃗u drawn
along with ⃗v. We have used diﬀerent dashed and do ed lines for each vector to help
dis nguish them.
No ce that this me we had to go in the nega ve z direc on; this just means we
moved down one unit instead of up a unit.
z

.

y

x
.

Figure 5.15: Vectors ⃗v .and ⃗u in Example 103.
As in 2D, we don’t usually draw the zero vector,
 
0
⃗0 =  0  .
0

It doesn’t point anywhere. It is a conceptually important vector that does not have a
terribly interes ng visualiza on.
Our method of drawing 3D objects on a ﬂat surface – a 2D surface – is pre y clever.
It isn’t perfect, though; visually, drawing vectors with nega ve components (especially
nega ve x coordinates) can look a bit odd. Also, two very diﬀerent vectors can point
to the same place. We’ll highlight this with our next two examples.


−3
. Example 104
.Sketch the vector ⃗v =  −1 .
2
We use the same procedure we used in Example 103. Star ng at
S
the origin, we move in the nega ve x direc on 3 units, then 1 unit in the nega ve y
direc on, and then ﬁnally up 2 units in the z direc on to ﬁnd the point (−3, −1, 2).
We follow by drawing an arrow. Our sketch is found in Figure 5.16; ⃗v is drawn in two
coordinate systems, once with the helpful dashed lines, and once without. The second
drawing makes it pre y clear that the dashed lines truly are helpful.
217

Chapter 5 Graphical Explora ons of Vectors

z

.

z

y

x

y

x
Figure 5.16: Vector. ⃗v in Example 104.

.
. Example 105
nate system.

 


2
−2
Draw the vectors ⃗v =  4  and ⃗u =  1  on the same coordi2
−1

We follow the steps we’ve taken before to sketch these vectors,
S
shown in Figure 5.17. The dashed lines are aides for ⃗v and the do ed lines are aids
for ⃗u. We again include the vectors without the dashed and do ed lines; but without
these, it is very diﬃcult to tell which vector is which!
z

.

x
.

z

y

y

x
Figure 5.17: Vectors ⃗v .and ⃗u in Example 105.

Our three examples have demonstrated that we have a pre y clever, albeit imperfect, method for drawing 3D vectors. The vectors in Example 105 look similar because
of our viewpoint. In Figure 5.18 (a), we have rotated the coordinate axes, giving the
vectors a diﬀerent appearance. (Vector ⃗v now looks like it lies on the y axis.)
Another important factor in how things look is the scale we use for the x, y, and
z axes. In 2D, it is easy to make the scale uniform for both axes; in 3D, it can be a bit
tricky to make the scale the same on the axes that are “slanted.” Figure 5.18 (b) again
shows the same 2 vectors found in Example 105, but this me the scale of the x axis
218

5.3 Visualizing Vectors: Vectors in Three Dimensions
is a bit diﬀerent. The end result is that again the vectors appear a bit diﬀerent than
they did before. These facts do not necessarily pose a big problem; we must merely
be aware of these facts and not make judgments about 3D objects based on one 2D
image.18
z

z

.

.

y

y

x

x
(a)

(b)

Figure 5.18: Vectors ⃗v and
. ⃗u in Example 105 with
a diﬀerent viewpoint (a) and x axis scale (b).
We now inves gate proper es of vector arithme c: what happens (i.e., how do
we draw) when we add 3D vectors and mul ply by a scalar? How do we compute the
length of a 3D vector?

Vector Addi on and Subtrac on
In 2D, we saw that we could add vectors together graphically using the Parallelogram Law. Does the same apply for adding vectors in 3D? We inves gate in an example.
 


2
1
. Example 106
.Let ⃗v =  1  and ⃗u =  3 . Sketch ⃗v + ⃗u.
3
−1
We sketchedeach
 of these vectors previously in Example 103. We
3
sketch them, along with⃗v +⃗u =  4 , in Figure 5.19 (a). (We use loosely dashed lines
2
for ⃗v + ⃗u.)
S

18 The human brain uses both eyes to convey 3D, or depth, informa on. With one eye closed (or missing),
we can have a very hard me with “depth percep on.” Two objects that are far apart can seem very close
together. A simple example of this problem is this: close one eye, and place your index ﬁnger about a foot
above this text, directly above this WORD. See if you were correct by dropping your ﬁnger straight down.
Did you actually hit the proper spot? Try it again with both eyes, and you should see a no cable diﬀerence
in your accuracy.
Looking at 3D objects on paper is a bit like viewing the world with one eye closed.

219

Chapter 5 Graphical Explora ons of Vectors
Does the Parallelogram Law s ll hold? In Figure 5.19 (b), we draw addi onal representa ons of ⃗v and ⃗u to form a parallelogram (without all the do ed lines), which
seems to aﬃrm the fact that the Parallelogram Law does indeed hold.

z

.

x

(b)

z

y

x

y

(b)

Figure 5.19: Vectors ⃗v, ⃗u, .and ⃗v + ⃗u Example 106.
.
We also learned that in 2D, we could subtract vectors by drawing a vector from the
p of one vector to the other.19 Does this also work in 3D? We’ll inves gate again with
an example, using the familiar vectors ⃗v and ⃗u from before.

. Example 107

 


2
1
.Let ⃗v =  1  and ⃗u =  3 . Sketch ⃗v − ⃗u.
3
−1



1
It is simple to compute that ⃗v − ⃗u =  −2 . All three of these
S
4
vectors are sketched in Figure 5.20 (a), where again⃗v is guided by the dashed, ⃗u by the
do ed, and ⃗v − ⃗u by the loosely dashed lines.
Does the 2D subtrac on rule s ll hold? That is, can we draw ⃗v − ⃗u by drawing an
arrow from the p of ⃗u to the p of ⃗v? In Figure 5.20 (b), we translate the drawing of
⃗v −⃗u to the p of ⃗u, and sure enough, it looks like it works. (And in fact, it really does.)

19 Recall that it

220

is important which vector we used for the origin and which was used for the p.

5.3 Visualizing Vectors: Vectors in Three Dimensions
z

z

.

y

x

y

x
(a)

(b)

. ⃗v − ⃗u from Example 107.
Figure 5.20: Vectors ⃗v, ⃗u, and
.
The previous two examples highlight the fact that even in 3D, we can sketch vectors without explicitly knowing what they are. We prac ce this one more me in the
following example.
. Example 108
.Vectors⃗v and⃗u are drawn in Figure 5.21. Using this drawing, sketch
the vectors ⃗v + ⃗u and ⃗v − ⃗u.
z

⃗v

⃗u
.

y

x
. ⃗u for Example 108.
Figure 5.21: Vectors ⃗v and

Using the Parallelogram Law, we draw ⃗v + ⃗u by ﬁrst drawing a gray
S
version of ⃗u coming from the p of ⃗v; ⃗v + ⃗u is drawn dashed in Figure 5.22.
To draw ⃗v − ⃗u, we draw a do ed arrow from the p of ⃗uto the p of ⃗v.
221

Chapter 5 Graphical Explora ons of Vectors
z

.

y

x
Figure 5.22: Vectors ⃗v, ⃗u, ⃗v + .⃗u and ⃗v − ⃗u for Example 108.

.

Scalar Mul plica on
Given a vector ⃗v in 3D, what does the vector 2⃗v look like? How about −⃗v? A er
learning about vector addi on and subtrac on in 3D, we are probably gaining conﬁdence in working in 3D and are tempted to say that 2⃗v is a vector twice as long as ⃗v,
poin ng in the same direc on, and −⃗v is a vector of the same length as ⃗v, poin ng
in the opposite direc on. We would be right. We demonstrate this in the following
example.
. Example 109

.Sketch ⃗v, 2⃗v, and −⃗v, where
 
1
⃗v =  2  .
3

S

z

.

y

x
.
Figure 5.23: Sketching scalar mul
ples of ⃗v in Example 109.
222

5.3 Visualizing Vectors: Vectors in Three Dimensions
It is easy to compute
 
2
2⃗v =  4 
6




−1
− ⃗v =  −2  .
−3

and

These are drawn in Figure 5.23. This ﬁgure is, in many ways, a mess, with all the
dashed and do ed lines. They are useful though. Use them to see how each vector was formed, and note that 2⃗v at least looks twice as long as ⃗v, and it looks like −⃗v
points in the opposite direc on.20 .

Vector Length
How do we measure the length of a vector in 3D? In 2D, we were able to answer this
ques on by using the Pythagorean Theorem. Does the Pythagorean Theorem apply in
3D? In a sense, it does.
 
1
Consider the vector ⃗v =  2 , as drawn in Figure 5.24 (a), with guiding dashed
3
lines. Now look at part (b) of the same ﬁgure. Note how two lengths of the dashed
lines have now been drawn gray, and another do ed line has been added.
z

.

x

z

y

y

x
(a)

(b)

Figure 5.24: Compu. ng the length of ⃗v
These gray dashed and do ed lines form a right triangle with the do ed line forming the hypotenuse. We can ﬁnd the length of the do ed line using the Pythagorean
Theorem.
√
sum of the squares of the dashed line lengths
√
√
That is, the length of the do ed line = 12 + 22 = 5.
length of the do ed line =

20 Our previous work showed that looks can be

deceiving, but it is indeed true in this case.

223

Chapter 5 Graphical Explora ons of Vectors
Now consider this: the vector ⃗v is the hypotenuse of another right triangle: the
one formed by the do ed line and the ver cal dashed line. Again, we employ the
Pythagorean Theorem to ﬁnd its length.

length of ⃗v =

√
(length of dashed gray line)2 + (length of black dashed line)2

Thus, the length of ⃗v is (recall, we denote the length of ⃗v with ||⃗v||):
√
(length of gray line)2 + (length of black line)2
√
√ 2
=
5 + 32
√
= 5 + 32

||⃗v|| =

Let’s stop for a moment and think: where did this 5 come from in the previous
equa on? It came from ﬁnding the length of the gray dashed line – it came from 12 +22 .
Let’s subs tute that into the previous equa on:
√
5 + 32
√
= 12 + 22 + 32
√
= 14

||⃗v|| =

√
The key comes from the middle equa on: ||⃗v|| = 12 + 22 + 32 . Do those numbers 1, 2, and 3 look familiar? They are the component values of ⃗v! This is very similar
to the deﬁni on of the length of a 2D vector. A er formally deﬁning this, we’ll prac ce
with an example.
.
Deﬁni on 32

3D Vector Length
Let




x1
⃗v =  x2  .
x3

.

The length of ⃗v, denoted ||⃗v||, is
√
||⃗v|| = x21 + x22 + x23 .

224

5.3 Visualizing Vectors: Vectors in Three Dimensions
. Example 110

S

Find the lengths of vectors ⃗v and ⃗u, where




2
−4
⃗v =  −3  and ⃗u =  7  .
5
0
We apply Deﬁni on 32 to each vector:
||⃗v|| =
=
=
||⃗u|| =
=
=

√
√
√

4 + 9 + 25
38

√
√
√

22 + (−3)2 + 52

(−4)2 + 72 + 02

16 + 49
65

.
Here we end our inves ga on into the world of graphing vectors. Extensions into
graphing 4D vectors and beyond can be done, but they truly are confusing and not
really done except for abstract purposes.
There are further things to explore, though. Just as in 2D, we can transform 3D
space by matrix mul plica on. Doing this properly – rota ng, stretching, shearing,
etc. – allows one to manipulate 3D space and create incredible computer graphics.

Exercises 5.3
In Exercises 1 – 4, vectors ⃗x and ⃗y are given.
Sketch⃗x ,⃗y,⃗x +⃗y, and⃗x −⃗y on the same Cartesian axes.

 

2
1
1. ⃗x =  −1 , ⃗y =  3 
2
2




2
−1
2. ⃗x =  4 , ⃗y =  −3 
−1
−1
 
 
1
3
3. ⃗x =  1 , ⃗y =  3 
2
6
 


0
0
4. ⃗x =  1 , ⃗y =  −1 
1
1

In Exercises 5 – 8, vectors ⃗x and ⃗y are drawn.
Sketch 2⃗x , −⃗y, ⃗x + ⃗y, and ⃗x − ⃗y on the same
Cartesian axes.

⃗x
z

5.
.

⃗y

y

x

225

Chapter 5 Graphical Explora ons of Vectors
z

⃗y

z

⃗y
8.

6.
.

⃗x

z
⃗y
.

x

226

⃗x

y

x

x

7.

.

y

y

⃗x

In Exercises 9 – 12, a vector ⃗x and a scalar
a are given. Using Deﬁni on 32, compute
the lengths of ⃗x and a⃗x , then compare these
lengths.


1
9. ⃗x =  −2 , a = 2
5


−3
10. ⃗x =  4 , a = −1
3
 
7
11. ⃗x =  2 , a = 5
1


1
12. ⃗x =  2 , a = 3
−2

A

.

S

T S

P



Chapter 1

2
13.  0
5

2
15.  0
0

Sec on 1.1
1. y
3. y


7
−2 
−1

−1
4
8


7
−2 
−29/2

−1
4
5/2

17. R1 + R2 → R2

5. n

19. R1 ↔ R2

7. y

21. x = 2, y = 1

9. y

23. x = −1, y = 0

11. x = 1, y = −2

25. x1 = −2, x2 = 1, x3 = 2

13. x = −1, y = 0, z = 2

Sec on 1.3

15. 29 chickens and 33 pigs
1.

Sec on 1.2


3
1.  −1
2

1
3.  −1
2

4
1
−2
3
0
3


7
1
5

5
−3
3
−4
4
4

5
8
5


17
1 
6

[
5.
[

5.

x1 + 2x2 =
−x1 + 3x2 =

7.

x1 + x2 − x3 − x4 =
2x1 + x2 + 3x3 + 5x4 =

3
9

x1 + x3 + 7x5 =
x2 + 3x3 + 2x4 =


2 −1
7

5
0
3 
11.
0
4
−2
9.

3.

7.
[

2
7

9.
[

2
5

11.


(a) yes

(c) no

(b) no

(d) yes

(a) no

(c) yes

(b) yes

(d) yes

0
1

1
0

3
0

1
0

0
1

1
0

−1
0

1
13.  0
0

227

]

1
0

0
1
0

]

3
7

]
]
2
0


0
0
1

Chapter A Solu ons To Selected Problems

15.

17.

19.

21.

1
0
0

1
0
0

1
0
0
[
1
0

0
1
0


0
0
1

0
1
0

0
1
0

0
1
0

1
−2
0

1
0

0
1


1
1
0

3
4
0
0
3

0
1

0
4

]

Sec on 1.4
1. x1 = 1 − 2x2 ; x2 is free. Possible solu ons:
x1 = 1, x2 = 0 and x1 = −1, x2 = 1.
3. x1 = 1; x2 = 2
5. No solu on; the system is inconsistent.
7. x1 = −11 + 10x3 ; x2 = −4 + 4x3 ; x3 is
free. Possible solu ons: x1 = −11,
x2 = −4, x3 = 0 and x1 = −1, x2 = 0 and
x3 = 1.
9. x1 = 1 − x2 − x4 ; x2 is free; x3 = 1 − 2x4 ;
x4 is free. Possible solu ons: x1 = 1,
x2 = 0, x3 = 1, x4 = 0 and x1 = −2,
x2 = 1, x3 = −3, x4 = 2

17. The
 this system is
 augmented matrix from
1 1
1
1
8
6 1
2
3 24 . From this we
0 1 −1 0
0
ﬁnd the solu on
8
1
t= − f
3
3
1
8
x= − f
3
3
8
1
w = − f.
3
3
The only me each of these variables are
nonnega ve integers is when f = 2 or
f = 8. If f = 2, then we have 2
touchdowns, 2 extra points and 2 two point
conversions (and 2 ﬁeld goals); this doesn’t
make sense since the extra points and two
point conversions follow touchdowns. If
f = 8, then we have no touchdowns, extra
points or two point conversions (just 8 ﬁeld
goals). This is the only solu on; all points
were scored from ﬁeld goals.
19. Let x1 , x2 and x3 represent the number of
free throws, 2 point and 3 point shots
taken. The[ augmented matrix
] from this
1 1 1 30
system is
. From this we
1 2 3 80
ﬁnd the solu on
x1 = −20 + x3
x2 = 50 − 2x3 .

11. No solu on; the system is inconsistent.

In order for x1 and x2 to be nonnega ve, we
need 20 ≤ x3 ≤ 25. Thus there are 6
diﬀerent scenerios: the “ﬁrst” is where 20
three point shots are taken, no free throws,
and 10 two point shots; the “last” is where
25 three point shots are taken, 5 free
throws, and no two point shots.

13. x1 = 31 − 43 x3 ; x2 = 13 − 13 x3 ; x3 is free.
Possible solu ons: x1 = 13 , x2 = 13 , x3 = 0
and x1 = −1, x2 = 0, x3 = 1
15. Never exactly 1 solu on; inﬁnite solu ons if
k = 2; no solu on if k ̸= 2.
17. Exactly 1 solu on if k ̸= 2; no solu on if
k = 2; never inﬁnite solu ons.

Sec on 1.5
1. 29 chickens and 33 pigs
3. 42 grande tables, 22 ven tables
5. 30 men, 15 women, 20 kids
7. f(x) = −2x + 10
9. f(x) = 21 x2 + 3x + 1
11. f(x) = 3

21. Let y = ax + b; all linear func ons through
(1,3) come in the form y = (3 − b)x + b.
Examples: b = 0 yields y = 3x; b = 2
yields y = x + 2.
23. Let y = ax2 + bx + c; we ﬁnd that
a = − 21 + 21 c and b = 12 − 32 c. Examples:
c = 1 yields y = −x + 1; c = 3 yields
y = x2 − 4x + 3.

Chapter 2
Sec on 2.1
[
1.

13. f(x) = x3 + 1
15. f(x) =

228

3
x
2

+1

[
3.

−2
12
2
14

]
−1
13
]
−2
8

[
5.
[
7.
[

9
11
−14
6

−7
−6
]



]

]
−15
−25
[
]
−5
9
11. X =
−1 −14
[
]
−5
−2
13. X =
−9/2 −19/2

25.

9.

27.

15. a = 2, b = 1
17. a = 5/2 + 3/2b

29.

19. No solu on.
21. No solu on.

Sec on 2.2

31.

1. −22
3. 0
5. 5
7. 15

33.

9. −2
11. Not possible.
[
]
8
3
13. AB =
10 −9
[
]
−3 24
BA =
4
2
[
]
−1 −2 12
15. AB =
10
4
32
BA is not possible.
17. AB is not possible.
[
]
27
−33
39
BA =
−27
−3
−15


−32 34 −24

−8 
19. AB = −32 38
−16 21
4
[
]
22 −14
BA =
−4 −12


−56
2
−36

20
19
−30 
21. AB =
−50 −13
0
[
]
−46 40
BA =
72
9
[
]
−15 −22 −21
−1
23. AB =
16
−53 −59 −31
BA is not possible.

35.

37.

39.

41.

43.

45.


0
0
4

4
−2 
AB = 6
2 −4 −6


2 −2
6

2
4 
BA = 2
4
0
−6


21 −17 −5
5
19 
AB =  19
5
9
4


19
5
23

5
−7 −1 
BA =
−14
6
18
[
]
4 −6
DA =
4 −6
[
]
4
8
AD =
−3 −6


2
2
2

−6
−6
−6 
DA =
−15 −15 −15


2
−3
5
−6
10 
AD =  4
−6
9
−15


d1 a d1 b d 1 c
DA =  d2 d d2 e d2 f 
d g d 3 h d3 i
 3

d1 a d2 b d 3 c

AD = d1 d d2 e d3 f 
d1 g d 2 h d3 i
[
]
−6
A⃗x =
11


−5
A⃗x =  5 
21


x1 + 2x2 + 3x3


x1 + 2x3
A⃗x =
2x1 + 3x2 + x3
[
]
[
]
4 0
8
0
A2 =
; A3 =
0 9
0 27



0 0 1
1 0
2
3
A =  1 0 0 ; A =  0 1
0 1 0
0 0
[
]
0
−2
(a)
−5 −1
[
]
10
2
(b)
5
11
[
]
−11 −15
(c)
37
32


0
0
1

(d) No.
(e) (A+B)(A+B) = AA+AB+BA+BB =
A2 + AB + BA + B2 .

229

Chapter A Solu ons To Selected Problems
⃗x + ⃗y

Sec on 2.3
[
1. ⃗x + ⃗y =

]

−1
, ⃗x − ⃗y =
4

[

3
−2

2⃗x

]

Sketches will vary depending on choice of
origin of each vector.

1

⃗x − ⃗y

.
1

⃗x + ⃗y
−⃗y
√
√
9. ||⃗x || = 5; ||a⃗x || = 45 = 3 5. The
vector a⃗x is 3 mes as long as ⃗x .
√
√
11. ||⃗x || = 34; ||a⃗x || = 34. The vectors a⃗x
and ⃗x are the same length (they just point
in opposite direc ons).
√
√
13.
(a) ||⃗x || = 2; ||⃗y|| = 13;
||⃗x + ⃗y|| = 5.
√
√
(b) ||⃗x || = 5; ||⃗
y|| = 3 5;
√
||⃗x + ⃗y|| = 4 5.
√
√
(c) ||⃗x || = 10;√||⃗y|| = 29;
||⃗x + ⃗y|| = 65.
√
√
(d) ||⃗x || = 5; √
||⃗y|| = 2 5;
||⃗x + ⃗y|| = 5.
The equality holds some mes; only when ⃗x
and⃗y point along the same line, in the same
direc on.
√

⃗y
⃗x − ⃗y

1
.

⃗x
2

[
3. ⃗x + ⃗y =

]
[
]
−3
1
, ⃗x − ⃗y =
3
−1

Sketches will vary depending on choice of
origin of each vector.
⃗x

1

⃗x − ⃗y

.
2

⃗x + ⃗y
⃗y

y

A⃗y

5. Sketches will vary depending on choice of
origin of each vector.
⃗x + ⃗y

⃗y

A⃗x

2⃗x

⃗x
.

15.
1

⃗y

x

y

.
1

⃗x
⃗x − ⃗y

.

x

−⃗y

7. Sketches will vary depending on choice of
origin of each vector.

230

17.

Sec on 2.4

A⃗x
A⃗y



1. Mul ply A⃗u and A⃗v to verify.
3. Mul ply A⃗u and A⃗v to verify.
5. Mul ply A⃗u and A⃗v to verify.
7. Mul ply A⃗u, A⃗v and A(⃗u + ⃗v) to verify.
9. Mul ply A⃗u, A⃗v and A(⃗u + ⃗v) to verify.
[ ]
0
11.
(a) ⃗x =
0
[
]
2/5
(b) ⃗x =
−13/5
[ ]
0
13.
(a) ⃗x =
0
[
]
−2
(b) ⃗x =
−9/4


5/4
15.
(a) ⃗x = x3  1 
1
 


1
5/4
(b) ⃗x =  0  + x3  1 
0
1


14
17.
(a) ⃗x = x3  −10 
0


[
]
14
−4
(b) ⃗x =
+ x3  −10 
2
0




−1
2
 2/5 
 2/5 



19.
(a) ⃗x = x3 
 1  + x4  0 
1
0

21.

(b) ⃗x = 




−1
2
−2
 2/5 
 2/5 
 2/5 






 0  + x3  1  + x4  0 
1
0
0




1/2
−1/2
 0 
 1 




(a) ⃗x = x2  0  + x4  −1/2  +
 1 
 0 
0
0


13/2
 0 


x5  −2 
 0 
1




−5
−1/2
 0 
 1 




(b) ⃗x =  3/2  + x2  0  +
 0 
 0 
0
0

23.




1/2
13/2
 0 
 0 




x4  −1/2  + x5  −2 
 1 
 0 
0
1




1
0
 13/9 
 −1 




(a) ⃗x = x4  −1/3  + x5  −1 
 1 
 0 
0
1

(b) ⃗x =






1
1
0
 1/9 
 13/9 
 −1 






 5/3  +x4  −1/3  +x5  −1 
 0 
 1 
 0 
0
0
1
[
]
−2
25. ⃗x = x2
= x2⃗v
1
y
⃗v
.

[
27. ⃗x =

0.5
0

x

[

]
+ x2

2.5
1

]
= x⃗p + x2⃗v

y
⃗v
.

x

x⃗p

Sec on 2.5
[
1. X =
[
3. X =
[
5. X =
[
7. X =


1
−4

−9
−5

−2
7

−7
−6

−5
−4

2
−3
]
0
−1

1
3

]
]

−3
−2

]


3
−3
3
−2 −3 
9. X =  2
−3 −1 −2


5/3
2/3 1

11. X = −1/3 1/6 0 
1/3
1/3 0

Sec on 2.6

231

Chapter A Solu ons To Selected Problems
[
1.
[
3.

−24
5

−5
1

1/3
0

0
1/7

]
]

5. A−1 does not exist.
[
]
1 0
7.
0 1
[
]
−5/13 3/13
9.
1/13
2/13
[
]
−2
1
11.
3/2 −1/2


1
2
−2
1
−3 
13.  0
6 10 −5


1
0
0
15.  52 −48 7 
8
−7
1
17. A−1 does not exist.


25
8
0

78
25 0 
19.
−30 −9 1


0 1 0


0
0
1
21.
1 0 0


1
0
0
0
 −3
−1
0
−4 

23. 
 −35 −10 1 −47 
−2
−2
0
−9


28 18
3
−19
 5
1
0
−5 

25. 
 4
5
1
0 
52 60 12 −15


0 0 1 0
0 0 0 1

27. 
1 0 0 0
0 1 0 0
[ ]
2
29. ⃗x =
3
[
]
−8
31. ⃗x =
1


−7
33. ⃗x =  1 
−1


3

35. ⃗x = −1 
−9

Sec on 2.7

232

1. (AB)−1 =

[
[

−2
1

3
−1.4

]

29/5
−18/5
−11/5
7/5
[
]
−2 −5
5. A−1 =
,
−1 −3
[
]
−3
5
(A−1 )−1 =
1
−2
[
]
−3
7
7. A−1 =
,
1
−2
[
]
2 7
(A−1 )−1 =
1 3
3. (AB)−1 =

]

9. Solu ons will vary.
11. Likely some entries that should be 0 will not
be exactly 0, but rather very small values.

Chapter 3
Sec on 3.1
[
1. A is symmetric.

−7
4

[

3. A is diagonal, as is AT .
[
5.


−5
−9

3
1

−10
−8


]

4
−9
 −7
6 

7. 
 −4
3 
−9 −9


−7
 −8 

9. 
 2 
−3


−9
6
−8

4
−3
1 
11.
10 −7 −1

4
13. A is symmetric.  0
−2


2
5
7

−5
5
−4 
15.
−3 −6 −10


4
5
−6
−4
6 
17.  2
−9 −10
9

4
−6
1
0

0
2
3

]

0
9

]


−2
3 
6

T
19. A
 is upper triangular;
 A is lower triangular.
−3
0
0
 −4 −3
0 
−5
5
−3


21. A is diagonal, as is

AT .


1
0
0

0
2
0


0
0 
−1
−1
0
4

0
23. A is skew symmetric.  1
−2

25. Hint: C1,1 = d.


2
−4 
0

Sec on 3.4
1. 84
3. 0
5. 10

Sec on 3.2

7. 24
1. 6

9. 175

3. 3

11. −200

5. −9

13. 34

7. 1

(a) det(A) = 41; R2 ↔ R3

15.

9. Not deﬁned; the matrix must be square.

(b) det(B) = 164; −4R3 → R3

11. −23

(c) det(C) = −41; R2 + R1 → R1

13. 4

(a) det(A) = −16; R1 ↔ R2 then
R1 ↔ R3

17.

15. 0
(a) tr(A)=8; tr(B)=−2; tr(A + B)=6

(b) det(B) = −16; −R1 → R1 and
−R2 → R2

(b) tr(AB) = 53 = tr(BA)

(c) det(C) = −432; C = 3 ∗ M

17.
19.

(a) tr(A)=−1; tr(B)=6; tr(A + B)=5

19. det(A) = 4, det(B) = 4, det(AB) = 16

(b) tr(AB) = 201 = tr(BA)

21. det(A) = −12, det(B) = 29,
det(AB) = −348

Sec on 3.3

23. −59
1. 34

25. 15

3. −44

27. 3

5. −44

29. 0

7. 28
9.

11.

[

]
7
6
(a) The submatrices are
,
6 10
[
]
[
]
3
6
3 7
, and
,
1 10
1 6
respec vely.

(b) C1,2 = 34, C1,2 = −24, C1,3 = 11
[
]
3 10
(a) The submatrices are
,
3
9
[
]
[
]
−3 10
−3 3
, and
,
−9
9
−9 3
respec vely.

Sec on 3.5
1.

(a) det (A) = 14, det (A1 ) = 70,
det (A2 ) = 14
[ ]
5
(b) ⃗x =
1

3.

(a) det (A) = 0, det (A1 ) = 0,
det (A2 ) = 0
(b) Inﬁnite solu ons exist.

5.

(a) det (A) = 16, det (A1 ) = −64,
det (A2 ) = 80
[
]
−4
(b) ⃗x =
5

7.

(a) det (A) = −123, det (A1 ) = −492,
det (A2 ) = 123, det (A3 ) = 492


4
(b) ⃗x =  −1 
−4

9.

(a) det (A) = 56, det (A1 ) = 224,
det (A2 ) = 0, det (A3 ) = −112

(b) C1,2 = −3, C1,2 = −63, C1,3 = 18
13. −59
15. 15
17. 3
19. 0
21. 0
23. −113

233

Chapter A Solu ons To Selected Problems



4

0 
(b) ⃗x =
−2
11.



23. λ1

(a) det (A) = 0, det (A1 ) = 147,
det (A2 ) = −49, det (A3 ) = −49

λ2

(b) No solu on exists.
λ3

Chapter 4
Sec on 4.1

25. λ1

1. λ = 3
λ2

3. λ = 0
5. λ = 3
[
]
−1
7. ⃗x =
2


3

9. ⃗x = −7 
7


−1

1 
11. ⃗x =
1

λ3

27. λ1

λ2
[

]
9
;
1
[ ]
8
= 5 with x⃗2 =
1
[
]
−2
= −3 with x⃗1 =
;
1
[ ]
6
= 5 with x⃗2 =
1
[ ]
1
= 2 with x⃗1 =
;
1
[
]
−1
= 4 with x⃗2 =
1
[ ]
1
= −1 with x⃗1 =
;
2
[ ]
1
= −3 with x⃗2 =
0


−3

0 ;
= 3 with x⃗1 =
2


−5

= 4 with x⃗2 = −1 
1
 
1
= 5 with x⃗3 =  0 
0

λ3

13. λ1 = 4 with x⃗1 =
λ2
15. λ1
λ2
17. λ1
λ2
19. λ1
λ2

21. λ1

λ2

λ3

234


24

= −5 with x⃗1 = 13 ;
8
 
6
= −2 with x⃗2 =  5 
1
 
0
= 3 with x⃗3 =  1 
0
 
0
= −2 with x⃗1 =  0 ;
1
 
0
= 1 with x⃗2 =  3 
5


28

7 
= 5 with x⃗3 =
1
 
1
= −2 with ⃗x =  0 ;
1
 
1
= 3 with ⃗x =  1 ;
1
 
0
= 5 with ⃗x =  1 
1

Sec on 4.2
[

1.

λ2
(b) λ1
λ2
(c) λ1
λ2
(d) 5
(e) 4
3.

]
4
;
1
[ ]
1
= 4 with x⃗2 =
1
[
]
−1
= 1 with x⃗1 =
;
1
[
]
−1
= 4 with x⃗2 =
4
[ ]
1
= 1/4 with x⃗1 =
;
1
[ ]
4
= 4 with x⃗2 =
1

(a) λ1 = 1 with x⃗1 =

[
]
−5
(a) λ1 = −1 with x⃗1 =
;
1
[
]
−6
λ2 = 0 with x⃗2 =
1
[ ]
1
(b) λ1 = −1 with x⃗1 =
;
6
[ ]
1
λ2 = 0 with x⃗2 =
5
(c) Ais not inver ble.

(d) -1
(e) 0
5.

(a) λ1

λ2

λ3

(b) λ1

λ2

λ3

(c) λ1

λ2

λ3




−7

= −4 with x⃗1 = −7 ;
6
 
0
= 3 with x⃗2 =  0 
1


9

1 
= 4 with x⃗3 =
22


−1
= −4 with x⃗1 =  9 ;
0


−20
= 3 with x⃗2 =  26 
7


−1
= 4 with x⃗3 =  1 
0


−7

= −1/4 with x⃗1 = −7 ;
6
 
0
= 1/3 with x⃗2 =  0 
1


9

1 
= 1/4 with x⃗3 =
22

(d) 3

13. Yes, these are the same.
the
[ Each produces
]
1/2 0
transforma on matrix
.
0
3

Sec on 5.2
1. Yes
3. No; cannot add a constant.
5. Yes.


1
7. [ T ] =  3
0



2
−5 
2

1
9. [ T ] =  1
1

0
0
0


3
−1 
1

[
11. [ T ] = 1

2

3

4

]

Sec on 5.3





3
−1
1. ⃗x + ⃗y =  2 , ⃗x − ⃗y =  −4 
4
0
Sketches will vary slightly depending on
orienta on.

(e) −48

Chapter 5

z

Sec on 5.1
[
1. A =
[
3. A =
[
5. A =
[
7. A =
[
9. A =

1
3

2
4

1
1

2
2

5
2

2
1

0
3

1
0

0
−1

⃗x

]
⃗x − ⃗y

]

⃗y

⃗x + ⃗y

.

y

]
]

−1
−1

x
]

11. Yes, these are the[same; the transforma
on
]
−1
0
matrix in each is
.
0
−1






4
−2



3. ⃗x + ⃗y = 4 , ⃗x − ⃗y = −2 
8
−4
Sketches will vary slightly depending on
orienta on.

235

Chapter A Solu ons To Selected Problems
⃗x + ⃗y
z

2⃗x

⃗y
⃗x − ⃗y
⃗x + ⃗y
z
⃗x

-⃗y
.

.

y

y
x

⃗x − ⃗y

7. Sketches may vary slightly.
z

x

.
⃗x + ⃗y

-⃗y

x

y

⃗x − ⃗y
2⃗x

√
√
30, ||a⃗x || = 120 = 2 30
√
√
11. ||⃗x || = √
54 = 3 6,√
||a⃗x || = 270 = 15 6
9. ||⃗x || =

5. Sketches may vary slightly.

236

√

Index
an symmetric, 128
augmented matrix, 8
basic variable, 27
characteris c polynomial, 168
cofactor, 137
expansion, 139, 146
consistent, 24, 88, 90
Cramer’s Rule, 159

inconsistent, 24
inverse
compu ng, 107
deﬁni on, 106
Inver ble Matrix Theorem, 113
proper es, 113, 119
uniqueness, 106
Inver ble Matrix Theorem, 113, 155, 183

leading one, 14, 27, 29
linear equa on, 3
determinant
linear transforma on
3 × 3 shortcut, 155
and ⃗0, 208
and elementary row opera ons, 150
condi ons on, 213
deﬁni on, 141
deﬁni on, 204
of 2 × 2 matrices, 136
standard matrix of, 207, 209
of triangular matrices, 148
matrix
proper es, 154
addi on, 47
diagonal, 118
arithme c proper es, 49
deﬁni on, 123
augmented, 8
cofactor, 137
eigenvalue
deﬁni on, 7
deﬁni on, 164
determinant, 136, 141
ﬁnding, 168
diagonal, 123
proper es, 182
equality, 46
eigenvector, see eigenvalue
iden ty matrix, 61
elementary row opera ons, 13
inverse, 106, 107
and determinants, 150
minor, 137
mul plica on, 54
free variable, 25, 27
proper es, 62
Gaussian elimina on, 15, 18
scalar mul plica on, 47
backward steps, 17
the zero matrix, 49
forward steps, 17
transpose, 122
triangular, 123
homogeneous, 87, 88, 90
minor, 137
iden ty matrix, 61

Parallelogram Law, 69
237

Index
par cular solu on, 30
problem solving, 37
pseudoinverse, 130
reduced echelon form, 14
reduced row echelon form, 14
row echelon form, 14
skew symmetric, 128
deﬁni on, 128
theorem, 129
solu on, 3
inﬁnite, 24, 27, 90
none, 24
par cular, 30
types, 24
unique, 24, 90, 110
standard unit vector, 208
symmetric, 128
deﬁni on, 128
theorem, 129
system of linear equa ons
consistent, 24, 27, 88, 90
deﬁni on, 3
homogeneous, 87
inconsistent, 24, 29
solu on, 3
trace
deﬁni on, 131
proper es, 133
transpose, 121
deﬁni on, 122
proper es, 126
skew-symmetric, 128
symmetric, 128
triangular matrix
deﬁni on, 123
determinant, 148
variable
basic, 27
dependent, 27
free, 25, 27
independent, 27
vector
238

column, 52
length, 74, 224
row, 52
zero matrix, 49

