Linear Algebra, Theory And Applications
Kenneth Kuttler
January 29, 2012

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2

Linear Algebra, Theory and Applications was written by Dr. Kenneth Kuttler of Brigham Young University for
teaching Linear Algebra II. After The Saylor Foundation accepted his submission to Wave I of the Open
Textbook Challenge, this textbook was relicensed as CC-BY 3.0.
Information on The Saylor Foundationâ€™s Open Textbook Challenge can be found at www.saylor.org/otc/.
Linear Algebra, Theory, and Applications Â© January 29, 2012 by Kenneth Kuttler, is licensed under a Creative
Commons Attribution (CC BY) license made possible by funding from The Saylor Foundation's Open
Textbook Challenge in order to be incorporated into Saylor.org's collection of open courses available at: http://
www.saylor.org" Full license terms may be viewed at: http://creativecommons.org/licenses/by/3.0/legalcode

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Contents
1 Preliminaries
1.1 Sets And Set Notation . . . . . . . . . . . . . . . . . .
1.2 Functions . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 The Number Line And Algebra Of The Real Numbers
1.4 Ordered ï¬elds . . . . . . . . . . . . . . . . . . . . . . .
1.5 The Complex Numbers . . . . . . . . . . . . . . . . . .
1.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . .
1.7 Completeness of R . . . . . . . . . . . . . . . . . . . .
1.8 Well Ordering And Archimedean Property . . . . . . .
1.9 Division And Numbers . . . . . . . . . . . . . . . . . .
1.10 Systems Of Equations . . . . . . . . . . . . . . . . . .
1.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . .
1.12 Fn . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.13 Algebra in Fn . . . . . . . . . . . . . . . . . . . . . . .
1.14 Exercises . . . . . . . . . . . . . . . . . . . . . . . . .
1.15 The Inner Product In Fn . . . . . . . . . . . . . . . .
1.16 What Is Linear Algebra? . . . . . . . . . . . . . . . . .
1.17 Exercises . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

11
11
12
12
14
15
19
20
21
23
26
31
32
32
33
33
36
36

2 Matrices And Linear Transformations
2.1 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.1 The ij th Entry Of A Product . . . . . . . . . . . .
2.1.2 Digraphs . . . . . . . . . . . . . . . . . . . . . . .
2.1.3 Properties Of Matrix Multiplication . . . . . . . .
2.1.4 Finding The Inverse Of A Matrix . . . . . . . . . .
2.2 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Linear Transformations . . . . . . . . . . . . . . . . . . .
2.4 Subspaces And Spans . . . . . . . . . . . . . . . . . . . .
2.5 An Application To Matrices . . . . . . . . . . . . . . . . .
2.6 Matrices And Calculus . . . . . . . . . . . . . . . . . . . .
2.6.1 The Coriolis Acceleration . . . . . . . . . . . . . .
2.6.2 The Coriolis Acceleration On The Rotating Earth
2.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

37
37
41
43
45
48
51
53
56
61
62
63
66
71

3 Determinants
3.1 Basic Techniques And Properties . . . . . .
3.2 Exercises . . . . . . . . . . . . . . . . . . .
3.3 The Mathematical Theory Of Determinants
3.3.1 The Function sgn . . . . . . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

77
77
81
83
84

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.

3

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

4

CONTENTS

3.4
3.5
3.6

3.3.2 The Deï¬nition Of The Determinant .
3.3.3 A Symmetric Deï¬nition . . . . . . . .
3.3.4 Basic Properties Of The Determinant
3.3.5 Expansion Using Cofactors . . . . . .
3.3.6 A Formula For The Inverse . . . . . .
3.3.7 Rank Of A Matrix . . . . . . . . . . .
3.3.8 Summary Of Determinants . . . . . .
The Cayley Hamilton Theorem . . . . . . . .
Block Multiplication Of Matrices . . . . . . .
Exercises . . . . . . . . . . . . . . . . . . . .

4 Row Operations
4.1 Elementary Matrices . . . . . . .
4.2 The Rank Of A Matrix . . . . .
4.3 The Row Reduced Echelon Form
4.4 Rank And Existence Of Solutions
4.5 Fredholm Alternative . . . . . . .
4.6 Exercises . . . . . . . . . . . . .

. .
. .
. .
To
. .
. .

. . . .
. . . .
. . . .
Linear
. . . .
. . . .

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

86
87
88
90
92
94
96
97
98
102

. . . . .
. . . . .
. . . . .
Systems
. . . . .
. . . . .

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

105
105
110
112
116
117
118

5 Some Factorizations
5.1 LU Factorization . . . . . . . . . . . . . . . . . . .
5.2 Finding An LU Factorization . . . . . . . . . . . .
5.3 Solving Linear Systems Using An LU Factorization
5.4 The P LU Factorization . . . . . . . . . . . . . . .
5.5 Justiï¬cation For The Multiplier Method . . . . . .
5.6 Existence For The P LU Factorization . . . . . . .
5.7 The QR Factorization . . . . . . . . . . . . . . . .
5.8 Exercises . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

123
123
123
125
126
127
128
130
133

6 Linear Programming
6.1 Simple Geometric Considerations .
6.2 The Simplex Tableau . . . . . . . .
6.3 The Simplex Algorithm . . . . . .
6.3.1 Maximums . . . . . . . . .
6.3.2 Minimums . . . . . . . . . .
6.4 Finding A Basic Feasible Solution .
6.5 Duality . . . . . . . . . . . . . . .
6.6 Exercises . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

135
135
136
140
140
143
150
152
156

7 Spectral Theory
7.1 Eigenvalues And Eigenvectors Of A Matrix . . . . .
7.2 Some Applications Of Eigenvalues And Eigenvectors
7.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . .
7.4 Schurâ€™s Theorem . . . . . . . . . . . . . . . . . . . .
7.5 Trace And Determinant . . . . . . . . . . . . . . . .
7.6 Quadratic Forms . . . . . . . . . . . . . . . . . . . .
7.7 Second Derivative Test . . . . . . . . . . . . . . . . .
7.8 The Estimation Of Eigenvalues . . . . . . . . . . . .
7.9 Advanced Theorems . . . . . . . . . . . . . . . . . .
7.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

157
157
164
167
173
180
181
182
186
187
190

Saylor URL: http://www.saylor.org/courses/ma212/

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

The Saylor Foundation

CONTENTS

5

8 Vector Spaces And Fields
8.1 Vector Space Axioms . . . . . . . . . . . . . .
8.2 Subspaces And Bases . . . . . . . . . . . . . .
8.2.1 Basic Deï¬nitions . . . . . . . . . . . .
8.2.2 A Fundamental Theorem . . . . . . .
8.2.3 The Basis Of A Subspace . . . . . . .
8.3 Lots Of Fields . . . . . . . . . . . . . . . . . .
8.3.1 Irreducible Polynomials . . . . . . . .
8.3.2 Polynomials And Fields . . . . . . . .
8.3.3 The Algebraic Numbers . . . . . . . .
8.3.4 The Lindemannn Weierstrass Theorem
8.4 Exercises . . . . . . . . . . . . . . . . . . . .

. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
And
. . .

. . . .
. . . .
. . . .
. . . .
. . . .
. . . .
. . . .
. . . .
. . . .
Vector
. . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
Spaces .
. . . . .

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

199
199
200
200
201
205
205
205
210
215
219
219

9 Linear Transformations
9.1 Matrix Multiplication As A Linear Transformation . . . . .
9.2 L (V, W ) As A Vector Space . . . . . . . . . . . . . . . . . .
9.3 The Matrix Of A Linear Transformation . . . . . . . . . . .
9.3.1 Some Geometrically Deï¬ned Linear Transformations
9.3.2 Rotations About A Given Vector . . . . . . . . . . .
9.3.3 The Euler Angles . . . . . . . . . . . . . . . . . . . .
9.4 Eigenvalues And Eigenvectors Of Linear Transformations .
9.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

225
225
225
227
234
237
238
240
242

10 Linear Transformations Canonical Forms
10.1 A Theorem Of Sylvester, Direct Sums . .
10.2 Direct Sums, Block Diagonal Matrices . .
10.3 Cyclic Sets . . . . . . . . . . . . . . . . .
10.4 Nilpotent Transformations . . . . . . . . .
10.5 The Jordan Canonical Form . . . . . . . .
10.6 Exercises . . . . . . . . . . . . . . . . . .
10.7 The Rational Canonical Form . . . . . . .
10.8 Uniqueness . . . . . . . . . . . . . . . . .
10.9 Exercises . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

245
245
248
251
255
257
262
266
269
273

11 Markov Chains And Migration Processes
11.1 Regular Markov Matrices . . . . . . . . .
11.2 Migration Matrices . . . . . . . . . . . . .
11.3 Markov Chains . . . . . . . . . . . . . . .
11.4 Exercises . . . . . . . . . . . . . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

275
275
279
279
284

12 Inner Product Spaces
12.1 General Theory . . . . . . . . . . . .
12.2 The Gram Schmidt Process . . . . .
12.3 Riesz Representation Theorem . . .
12.4 The Tensor Product Of Two Vectors
12.5 Least Squares . . . . . . . . . . . . .
12.6 Fredholm Alternative Again . . . . .
12.7 Exercises . . . . . . . . . . . . . . .
12.8 The Determinant And Volume . . .
12.9 Exercises . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

287
287
289
292
295
296
298
298
303
306

Saylor URL: http://www.saylor.org/courses/ma212/

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

The Saylor Foundation

6

CONTENTS

13 Self Adjoint Operators
13.1 Simultaneous Diagonalization . . . . . . . . . . .
13.2 Schurâ€™s Theorem . . . . . . . . . . . . . . . . . .
13.3 Spectral Theory Of Self Adjoint Operators . . . .
13.4 Positive And Negative Linear Transformations .
13.5 Fractional Powers . . . . . . . . . . . . . . . . . .
13.6 Polar Decompositions . . . . . . . . . . . . . . .
13.7 An Application To Statistics . . . . . . . . . . .
13.8 The Singular Value Decomposition . . . . . . . .
13.9 Approximation In The Frobenius Norm . . . . .
13.10Least Squares And Singular Value Decomposition
13.11The Moore Penrose Inverse . . . . . . . . . . . .
13.12Exercises . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

307
307
310
312
317
319
322
325
327
329
331
331
334

14 Norms For Finite Dimensional Vector Spaces
14.1 The p Norms . . . . . . . . . . . . . . . . . . .
14.2 The Condition Number . . . . . . . . . . . . .
14.3 The Spectral Radius . . . . . . . . . . . . . . .
14.4 Series And Sequences Of Linear Operators . . .
14.5 Iterative Methods For Linear Systems . . . . .
14.6 Theory Of Convergence . . . . . . . . . . . . .
14.7 Exercises . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

337
343
345
348
350
354
360
363

15 Numerical Methods For Finding Eigenvalues
15.1 The Power Method For Eigenvalues . . . . . . . . . . . . .
15.1.1 The Shifted Inverse Power Method . . . . . . . . .
15.1.2 The Explicit Description Of The Method . . . . .
15.1.3 Complex Eigenvalues . . . . . . . . . . . . . . . . .
15.1.4 Rayleigh Quotients And Estimates for Eigenvalues
15.2 The QR Algorithm . . . . . . . . . . . . . . . . . . . . . .
15.2.1 Basic Properties And Deï¬nition . . . . . . . . . .
15.2.2 The Case Of Real Eigenvalues . . . . . . . . . . .
15.2.3 The QR Algorithm In The General Case . . . . . .
15.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

371
371
375
376
381
383
386
386
390
394
401

.
.
.
.
.
.
.

A Positive Matrices

403

B Functions Of Matrices

411

C Applications To Diï¬€erential Equations
C.1 Theory Of Ordinary Diï¬€erential Equations
C.2 Linear Systems . . . . . . . . . . . . . . . .
C.3 Local Solutions . . . . . . . . . . . . . . . .
C.4 First Order Linear Systems . . . . . . . . .
C.5 Geometric Theory Of Autonomous Systems
C.6 General Geometric Theory . . . . . . . . . .
C.7 The Stable Manifold . . . . . . . . . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

417
417
418
419
421
428
432
434

D Compactness And Completeness
439
D.0.1 The Nested Interval Lemma . . . . . . . . . . . . . . . . . . . . . . . . 439
D.0.2 Convergent Sequences, Sequential Compactness . . . . . . . . . . . . . 440

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

CONTENTS

7

E The Fundamental Theorem Of Algebra

443

F Fields And Field Extensions
F.1 The Symmetric Polynomial Theorem . . . .
F.2 The Fundamental Theorem Of Algebra . . .
F.3 Transcendental Numbers . . . . . . . . . . .
F.4 More On Algebraic Field Extensions . . . .
F.5 The Galois Group . . . . . . . . . . . . . .
F.6 Normal Subgroups . . . . . . . . . . . . . .
F.7 Normal Extensions And Normal Subgroups
F.8 Conditions For Separability . . . . . . . . .
F.9 Permutations . . . . . . . . . . . . . . . . .
F.10 Solvable Groups . . . . . . . . . . . . . . .
F.11 Solvability By Radicals . . . . . . . . . . . .
G Answers To Selected Exercises
G.1 Exercises . . . . . . . . . . .
G.2 Exercises . . . . . . . . . . .
G.3 Exercises . . . . . . . . . . .
G.4 Exercises . . . . . . . . . . .
G.5 Exercises . . . . . . . . . . .
G.6 Exercises . . . . . . . . . . .
G.7 Exercises . . . . . . . . . . .
G.8 Exercises . . . . . . . . . . .
G.9 Exercises . . . . . . . . . . .
G.10 Exercises . . . . . . . . . . .
G.11 Exercises . . . . . . . . . . .
G.12 Exercises . . . . . . . . . . .
G.13 Exercises . . . . . . . . . . .
G.14 Exercises . . . . . . . . . . .
G.15 Exercises . . . . . . . . . . .
G.16 Exercises . . . . . . . . . . .
G.17 Exercises . . . . . . . . . . .
G.18 Exercises . . . . . . . . . . .
G.19 Exercises . . . . . . . . . . .
G.20 Exercises . . . . . . . . . . .
G.21 Exercises . . . . . . . . . . .
G.22 Exercises . . . . . . . . . . .
G.23 Exercises . . . . . . . . . . .
c 2012,
Copyright âƒ

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

Saylor URL: http://www.saylor.org/courses/ma212/

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

445
445
447
451
459
464
469
470
471
475
479
482

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

487
487
487
487
487
487
488
489
489
490
491
492
492
493
494
494
494
495
495
495
496
496
496
496

The Saylor Foundation

8

Saylor URL: http://www.saylor.org/courses/ma212/

CONTENTS

The Saylor Foundation

Preface
This is a book on linear algebra and matrix theory. While it is self contained, it will work
best for those who have already had some exposure to linear algebra. It is also assumed that
the reader has had calculus. Some optional topics require more analysis than this, however.
I think that the subject of linear algebra is likely the most signiï¬cant topic discussed in
undergraduate mathematics courses. Part of the reason for this is its usefulness in unifying
so many diï¬€erent topics. Linear algebra is essential in analysis, applied math, and even in
theoretical mathematics. This is the point of view of this book, more than a presentation
of linear algebra for its own sake. This is why there are numerous applications, some fairly
unusual.
This book features an ugly, elementary, and complete treatment of determinants early
in the book. Thus it might be considered as Linear algebra done wrong. I have done this
because of the usefulness of determinants. However, all major topics are also presented in
an alternative manner which is independent of determinants.
The book has an introduction to various numerical methods used in linear algebra.
This is done because of the interesting nature of these methods. The presentation here
emphasizes the reasons why they work. It does not discuss many important numerical
considerations necessary to use the methods eï¬€ectively. These considerations are found in
numerical analysis texts.
In the exercises, you may occasionally see â†‘ at the beginning. This means you ought to
have a look at the exercise above it. Some exercises develop a topic sequentially. There are
also a few exercises which appear more than once in the book. I have done this deliberately
because I think that these illustrate exceptionally important topics and because some people
donâ€™t read the whole book from start to ï¬nish but instead jump in to the middle somewhere.
There is one on a theorem of Sylvester which appears no fewer than 3 times. Then it is also
proved in the text. There are multiple proofs of the Cayley Hamilton theorem, some in the
exercises. Some exercises also are included for the sake of emphasizing something which has
been done in the preceding chapter.

9

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

10

Saylor URL: http://www.saylor.org/courses/ma212/

CONTENTS

The Saylor Foundation

Preliminaries
1.1

Sets And Set Notation

A set is just a collection of things called elements. For example {1, 2, 3, 8} would be a set
consisting of the elements 1,2,3, and 8. To indicate that 3 is an element of {1, 2, 3, 8} , it is
customary to write 3 âˆˆ {1, 2, 3, 8} . 9 âˆˆ
/ {1, 2, 3, 8} means 9 is not an element of {1, 2, 3, 8} .
Sometimes a rule speciï¬es a set. For example you could specify a set as all integers larger
than 2. This would be written as S = {x âˆˆ Z : x > 2} . This notation says: the set of all
integers, x, such that x > 2.
If A and B are sets with the property that every element of A is an element of B, then A is
a subset of B. For example, {1, 2, 3, 8} is a subset of {1, 2, 3, 4, 5, 8} , in symbols, {1, 2, 3, 8} âŠ†
{1, 2, 3, 4, 5, 8} . It is sometimes said that â€œA is contained in Bâ€ or even â€œB contains Aâ€.
The same statement about the two sets may also be written as {1, 2, 3, 4, 5, 8} âŠ‡ {1, 2, 3, 8}.
The union of two sets is the set consisting of everything which is an element of at least
one of the sets, A or B. As an example of the union of two sets {1, 2, 3, 8} âˆª {3, 4, 7, 8} =
{1, 2, 3, 4, 7, 8} because these numbers are those which are in at least one of the two sets. In
general
A âˆª B â‰¡ {x : x âˆˆ A or x âˆˆ B} .
Be sure you understand that something which is in both A and B is in the union. It is not
an exclusive or.
The intersection of two sets, A and B consists of everything which is in both of the sets.
Thus {1, 2, 3, 8} âˆ© {3, 4, 7, 8} = {3, 8} because 3 and 8 are those elements the two sets have
in common. In general,
A âˆ© B â‰¡ {x : x âˆˆ A and x âˆˆ B} .
The symbol [a, b] where a and b are real numbers, denotes the set of real numbers x,
such that a â‰¤ x â‰¤ b and [a, b) denotes the set of real numbers such that a â‰¤ x < b. (a, b)
consists of the set of real numbers x such that a < x < b and (a, b] indicates the set of
numbers x such that a < x â‰¤ b. [a, âˆ) means the set of all numbers x such that x â‰¥ a and
(âˆ’âˆ, a] means the set of all real numbers which are less than or equal to a. These sorts of
sets of real numbers are called intervals. The two points a and b are called endpoints of the
interval. Other intervals such as (âˆ’âˆ, b) are deï¬ned by analogy to what was just explained.
In general, the curved parenthesis indicates the end point it sits next to is not included
while the square parenthesis indicates this end point is included. The reason that there
will always be a curved parenthesis next to âˆ or âˆ’âˆ is that these are not real numbers.
Therefore, they cannot be included in any set of real numbers.
A special set which needs to be given a name is the empty set also called the null set,
denoted by âˆ…. Thus âˆ… is deï¬ned as the set which has no elements in it. Mathematicians like
to say the empty set is a subset of every set. The reason they say this is that if it were not
11

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

12

PRELIMINARIES

so, there would have to exist a set A, such that âˆ… has something in it which is not in A.
However, âˆ… has nothing in it and so the least intellectual discomfort is achieved by saying
âˆ… âŠ† A.
If A and B are two sets, A \ B denotes the set of things which are in A but not in B.
Thus
A \ B â‰¡ {x âˆˆ A : x âˆˆ
/ B} .
Set notation is used whenever convenient.

1.2

Functions

The concept of a function is that of something which gives a unique output for a given input.
Deï¬nition 1.2.1 Consider two sets, D and R along with a rule which assigns a unique
element of R to every element of D. This rule is called a function and it is denoted by a
letter such as f. Given x âˆˆ D, f (x) is the name of the thing in R which results from doing
f to x. Then D is called the domain of f. In order to specify that D pertains to f , the
notation D (f ) may be used. The set R is sometimes called the range of f. These days it
is referred to as the codomain. The set of all elements of R which are of the form f (x)
for some x âˆˆ D is therefore, a subset of R. This is sometimes referred to as the image of
f . When this set equals R, the function f is said to be onto, also surjective. If whenever
x Ì¸= y it follows f (x) Ì¸= f (y), the function is called one to one. , also injective It is
common notation to write f : D 7â†’ R to denote the situation just described in this deï¬nition
where f is a function deï¬ned on a domain D which has values in a codomain R. Sometimes
f
you may also see something like D 7â†’ R to denote the same thing.

1.3

The Number Line And Algebra Of The Real Numbers

Next, consider the real numbers, denoted by R, as a line extending inï¬nitely far in both
directions. In this book, the notation, â‰¡ indicates something is being deï¬ned. Thus the
integers are deï¬ned as
Z â‰¡ {Â· Â· Â· âˆ’ 1, 0, 1, Â· Â· Â· } ,
the natural numbers,
N â‰¡ {1, 2, Â· Â· Â· }
and the rational numbers, deï¬ned as the numbers which are the quotient of two integers.
{m
}
Qâ‰¡
such that m, n âˆˆ Z, n Ì¸= 0
n
are each subsets of R as indicated in the following picture.



âˆ’4 âˆ’3 âˆ’2 âˆ’1

0

1

2

3

4
-

1/2
As shown in the picture, 21 is half way between the number 0 and the number, 1. By
analogy, you can see where to place all the other rational numbers. It is assumed that R has

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

1.3. THE NUMBER LINE AND ALGEBRA OF THE REAL NUMBERS

13

the following algebra properties, listed here as a collection of assertions called axioms. These
properties will not be proved which is why they are called axioms rather than theorems. In
general, axioms are statements which are regarded as true. Often these are things which
are â€œself evidentâ€ either from experience or from some sort of intuition but this does not
have to be the case.
Axiom 1.3.1 x + y = y + x, (commutative law for addition)
Axiom 1.3.2 x + 0 = x, (additive identity).
Axiom 1.3.3 For each x âˆˆ R, there exists âˆ’x âˆˆ R such that x + (âˆ’x) = 0, (existence of
additive inverse).
Axiom 1.3.4 (x + y) + z = x + (y + z) , (associative law for addition).
Axiom 1.3.5 xy = yx, (commutative law for multiplication).
Axiom 1.3.6 (xy) z = x (yz) , (associative law for multiplication).
Axiom 1.3.7 1x = x, (multiplicative identity).
Axiom 1.3.8 For each x Ì¸= 0, there exists xâˆ’1 such that xxâˆ’1 = 1.(existence of multiplicative inverse).
Axiom 1.3.9 x (y + z) = xy + xz.(distributive law).
These axioms are known as the ï¬eld axioms and any set (there are many others besides
R) which has two such operations satisfying the above axioms is called a ï¬eld.
and
( Division
)
subtraction are deï¬ned in the usual way by x âˆ’ y â‰¡ x + (âˆ’y) and x/y â‰¡ x y âˆ’1 .
Here is a little proposition which derives some familiar facts.
Proposition 1.3.10 0 and 1 are unique. Also âˆ’x is unique and xâˆ’1 is unique. Furthermore, 0x = x0 = 0 and âˆ’x = (âˆ’1) x.
Proof: Suppose 0â€² is another additive identity. Then
0â€² = 0â€² + 0 = 0.
Thus 0 is unique. Say 1â€² is another multiplicative identity. Then
1 = 1â€² 1 = 1â€² .
Now suppose y acts like the additive inverse of x. Then
âˆ’x = (âˆ’x) + 0 = (âˆ’x) + (x + y) = (âˆ’x + x) + y = y
Finally,
0x = (0 + 0) x = 0x + 0x
and so
0 = âˆ’ (0x) + 0x = âˆ’ (0x) + (0x + 0x) = (âˆ’ (0x) + 0x) + 0x = 0x
Finally
x + (âˆ’1) x = (1 + (âˆ’1)) x = 0x = 0
and so by uniqueness of the additive inverse, (âˆ’1) x = âˆ’x. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

14

PRELIMINARIES

1.4

Ordered ï¬elds

The real numbers R are an example of an ordered ï¬eld. More generally, here is a deï¬nition.
Deï¬nition 1.4.1 Let F be a ï¬eld. It is an ordered ï¬eld if there exists an order, < which
satisï¬es
1. For any x Ì¸= y, either x < y or y < x.
2. If x < y and either z < w or z = w, then, x + z < y + w.
3. If 0 < x, 0 < y, then xy > 0.
With this deï¬nition, the familiar properties of order can be proved. The following
proposition lists many of these familiar properties. The relation â€˜a > bâ€™ has the same
meaning as â€˜b < aâ€™.
Proposition 1.4.2 The following are obtained.
1. If x < y and y < z, then x < z.
2. If x > 0 and y > 0, then x + y > 0.
3. If x > 0, then âˆ’x < 0.
4. If x Ì¸= 0, either x or âˆ’x is > 0.
5. If x < y, then âˆ’x > âˆ’y.
6. If x Ì¸= 0, then x2 > 0.
7. If 0 < x < y then xâˆ’1 > y âˆ’1 .
Proof: First consider 1, called the transitive law. Suppose that x < y and y < z. Then
from the axioms, x + y < y + z and so, adding âˆ’y to both sides, it follows
x<z
Next consider 2. Suppose x > 0 and y > 0. Then from 2,
0 = 0 + 0 < x + y.
Next consider 3. It is assumed x > 0 so
0 = âˆ’x + x > 0 + (âˆ’x) = âˆ’x
Now consider 4. If x < 0, then
0 = x + (âˆ’x) < 0 + (âˆ’x) = âˆ’x.
Consider the 5. Since x < y, it follows from 2
0 = x + (âˆ’x) < y + (âˆ’x)
and so by 4 and Proposition 1.3.10,
(âˆ’1) (y + (âˆ’x)) < 0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

1.5. THE COMPLEX NUMBERS

15

Also from Proposition 1.3.10 (âˆ’1) (âˆ’x) = âˆ’ (âˆ’x) = x and so
âˆ’y + x < 0.
Hence
âˆ’y < âˆ’x.
Consider 6. If x > 0, there is nothing to show. It follows from the deï¬nition. If x < 0,
then by 4, âˆ’x > 0 and so by Proposition 1.3.10 and the deï¬nition of the order,
2

(âˆ’x) = (âˆ’1) (âˆ’1) x2 > 0
By this proposition again, (âˆ’1) (âˆ’1) = âˆ’ (âˆ’1) = 1 and so x2 > 0 as claimed. Note that
1 > 0 because it equals 12 .
Finally, consider 7. First, if x > 0 then if xâˆ’1 < 0, it would follow (âˆ’1) xâˆ’1 > 0 and so
x (âˆ’1) xâˆ’1 = (âˆ’1) 1 = âˆ’1 > 0. However, this would require
0 > 1 = 12 > 0
from what was just shown. Therefore, xâˆ’1 > 0. Now the assumption implies y + (âˆ’1) x > 0
and so multiplying by xâˆ’1 ,
yxâˆ’1 + (âˆ’1) xxâˆ’1 = yxâˆ’1 + (âˆ’1) > 0
Now multiply by y âˆ’1 , which by the above satisï¬es y âˆ’1 > 0, to obtain
xâˆ’1 + (âˆ’1) y âˆ’1 > 0
and so

xâˆ’1 > y âˆ’1 . 

In an ordered ï¬eld the symbols â‰¤ and â‰¥ have the usual meanings. Thus a â‰¤ b means
a < b or else a = b, etc.

1.5

The Complex Numbers

Just as a real number should be considered as a point on the line, a complex number is
considered a point in the plane which can be identiï¬ed in the usual way using the Cartesian
coordinates of the point. Thus (a, b) identiï¬es a point whose x coordinate is a and whose
y coordinate is b. In dealing with complex numbers, such a point is written as a + ib and
multiplication and addition are deï¬ned in the most obvious way subject to the convention
that i2 = âˆ’1. Thus,
(a + ib) + (c + id) = (a + c) + i (b + d)
and
(a + ib) (c + id)

= ac + iad + ibc + i2 bd
= (ac âˆ’ bd) + i (bc + ad) .

Every non zero complex number, a+ib, with a2 +b2 Ì¸= 0, has a unique multiplicative inverse.
1
a âˆ’ ib
a
b
= 2
= 2
âˆ’i 2
.
a + ib
a + b2
a + b2
a + b2
You should prove the following theorem.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

16

PRELIMINARIES

Theorem 1.5.1 The complex numbers with multiplication and addition deï¬ned as above
form a ï¬eld satisfying all the ï¬eld axioms listed on Page 13.
Note that if x + iy is a complex number, it can be written as
(
)
âˆš
x
y
2
2
âˆš
x + iy = x + y
+ iâˆš
x2 + y 2
x2 + y 2
(
)
y
x
Now âˆš 2 2 , âˆš 2 2 is a point on the unit circle and so there exists a unique Î¸ âˆˆ [0, 2Ï€)
x +y
x +y
âˆš
such that this ordered pair equals (cos Î¸, sin Î¸) . Letting r = x2 + y 2 , it follows that the
complex number can be written in the form
x + iy = r (cos Î¸ + i sin Î¸)
This is called the polar form of the complex number.
The ï¬eld of complex numbers is denoted as C. An important construction regarding
complex numbers is the complex conjugate denoted by a horizontal line above the number.
It is deï¬ned as follows.
a + ib â‰¡ a âˆ’ ib.
What it does is reï¬‚ect a given complex number across the x axis. Algebraically, the following
formula is easy to obtain.
(
)
a + ib (a + ib) = a2 + b2 .
Deï¬nition 1.5.2 Deï¬ne the absolute value of a complex number as follows.
âˆš
|a + ib| â‰¡ a2 + b2 .
Thus, denoting by z the complex number, z = a + ib,
|z| = (zz)

1/2

.

With this deï¬nition, it is important to note the following. Be sure to verify this. It is
not too hard but you need to do it.
âˆš
2
2
Remark 1.5.3 : Let z = a + ib and w = c + id. Then |z âˆ’ w| = (a âˆ’ c) + (b âˆ’ d) . Thus
the distance between the point in the plane determined by the ordered pair, (a, b) and the
ordered pair (c, d) equals |z âˆ’ w| where z and w are as just described.
For example, consider
the distance between (2, 5) and (1, 8) . From the distance formula
âˆš
âˆš
2
2
this distance equals (2 âˆ’ 1) + (5 âˆ’ 8) = 10. On the other hand, letting z = 2 + i5 and
âˆš
w = 1 + i8, z âˆ’ w = 1 âˆ’ i3 and so (z âˆ’ w) (z âˆ’ w) = (1 âˆ’ i3) (1 + i3) = 10 so |z âˆ’ w| = 10,
the same thing obtained with the distance formula.
Complex numbers, are often written in the so called polar form which is described next.
Suppose x + iy is a complex number. Then
(
)
âˆš
x
y
2
2
âˆš
+ iâˆš
.
x + iy = x + y
x2 + y 2
x2 + y 2
Now note that

(
âˆš

x
x2 + y 2

)2

(
+

Saylor URL: http://www.saylor.org/courses/ma212/

y

âˆš
x2 + y 2

)2
=1

The Saylor Foundation

1.5. THE COMPLEX NUMBERS

17

(

and so

y

x

)

âˆš
,âˆš
x2 + y 2
x2 + y 2
is a point on the unit circle. Therefore, there exists a unique angle, Î¸ âˆˆ [0, 2Ï€) such that
x
y
cos Î¸ = âˆš
, sin Î¸ = âˆš
.
2
2
2
x +y
x + y2
The polar form of the complex number is then
r (cos Î¸ + i sin Î¸)
âˆš
where Î¸ is this angle just described and r = x2 + y 2 .
A fundamental identity is the formula of De Moivre which follows.
Theorem 1.5.4 Let r > 0 be given. Then if n is a positive integer,
n

[r (cos t + i sin t)] = rn (cos nt + i sin nt) .
Proof: It is clear the formula holds if n = 1. Suppose it is true for n.
[r (cos t + i sin t)]

n+1

n

= [r (cos t + i sin t)] [r (cos t + i sin t)]

which by induction equals
= rn+1 (cos nt + i sin nt) (cos t + i sin t)
= rn+1 ((cos nt cos t âˆ’ sin nt sin t) + i (sin nt cos t + cos nt sin t))
= rn+1 (cos (n + 1) t + i sin (n + 1) t)
by the formulas for the cosine and sine of the sum of two angles. 
Corollary 1.5.5 Let z be a non zero complex number. Then there are always exactly k k th
roots of z in C.
Proof: Let z = x + iy and let z = |z| (cos t + i sin t) be the polar form of the complex
number. By De Moivreâ€™s theorem, a complex number,
r (cos Î± + i sin Î±) ,
is a k th root of z if and only if
rk (cos kÎ± + i sin kÎ±) = |z| (cos t + i sin t) .
This requires rk = |z| and so r = |z|
This can only happen if

1/k

and also both cos (kÎ±) = cos t and sin (kÎ±) = sin t.

kÎ± = t + 2lÏ€
for l an integer. Thus
Î±=

t + 2lÏ€
,l âˆˆ Z
k

and so the k th roots of z are of the form
(
(
)
(
))
t + 2lÏ€
t + 2lÏ€
1/k
|z|
cos
+ i sin
, l âˆˆ Z.
k
k
Since the cosine and sine are periodic of period 2Ï€, there are exactly k distinct numbers
which result from this formula. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

18

PRELIMINARIES

Example 1.5.6 Find the three cube roots of i.
(
( )
( ))
First note that i = 1 cos Ï€2 + i sin Ï€2 . Using the formula in the proof of the above
corollary, the cube roots of i are
(
(
)
(
))
(Ï€/2) + 2lÏ€
(Ï€/2) + 2lÏ€
+ i sin
1 cos
3
3
where l = 0, 1, 2. Therefore, the roots are
cos

(Ï€ )
6

+ i sin

(Ï€ )
6

(
, cos

)
( )
5
5
Ï€ + i sin
Ï€ ,
6
6

(

)
( )
3
3
cos
Ï€ + i sin
Ï€ .
2
2
âˆš
( ) âˆš
( )
Thus the cube roots of i are 23 + i 12 , âˆ’2 3 + i 12 , and âˆ’i.
The ability to ï¬nd k th roots can also be used to factor some polynomials.
and

Example 1.5.7 Factor the polynomial x3 âˆ’ 27.
First ï¬nd the cube roots
of 27.
By the (above procedure
using De Moivreâ€™s theorem,
(
âˆš )
âˆš )
3
3
âˆ’1
these cube roots are 3, 3 âˆ’1
, and 3 2 âˆ’ i 2 . Therefore, x3 + 27 =
2 +i 2
(

(

(x âˆ’ 3) x âˆ’ 3

(
âˆš )) (
âˆš ))
âˆ’1
3
3
âˆ’1
+i
xâˆ’3
âˆ’i
.
2
2
2
2

(
(
(
âˆš )) (
âˆš ))
3
3
âˆ’1
Note also x âˆ’ 3 âˆ’1
+
i
x
âˆ’
3
âˆ’
i
= x2 + 3x + 9 and so
2
2
2
2
(
)
x3 âˆ’ 27 = (x âˆ’ 3) x2 + 3x + 9
where the quadratic polynomial, x2 + 3x + 9 cannot be factored without using complex
numbers.
The real and complex numbers both are ï¬elds satisfying the axioms on Page 13 and it is
usually one of these two ï¬elds which is used in linear algebra. The numbers are often called
scalars. However, it turns out that all algebraic notions work for any ï¬eld and there are
many others. For this reason, I will often refer to the ï¬eld of scalars as F although F will
usually be either the real or complex numbers. If there is any doubt, assume it is the ï¬eld
of complex numbers which is meant. The reason the complex numbers are so signiï¬cant in
linear
is that they are algebraically complete. This means that every polynomial
âˆ‘n algebra
k
a
z
,
n
â‰¥
1, an Ì¸= 0, having coeï¬ƒcients ak in C has a root in in C.
k=0 k
Later in the book, proofs of the fundamental theorem of algebra are given. However, here
is a simple explanation of why you should believe this theorem. The issue is whether there
exists z âˆˆ C such that p (z) = 0 for p (z) a polynomial having coeï¬ƒcients in C. Dividing by
the leading coeï¬ƒcient, we can assume that p (z) is of the form
p (z) = z n + anâˆ’1 z nâˆ’1 + Â· Â· Â· + a1 z + a0 , a0 Ì¸= 0.
If a0 = 0, there is nothing to prove. Denote by Cr the circle of radius r in the complex plane
which is centered at 0. Then if r is suï¬ƒciently large and |z| = r, the term z n is far larger
than the rest of the polynomial. Thus, for r large enough, Ar = {p (z) : z âˆˆ Cr } describes
a closed curve which misses the inside of some circle having 0 as its center. Now shrink r.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

1.6. EXERCISES

19

Eventually, for r small enough, the non constant terms are negligible and so Ar is a curve
which is contained in some circle centered at a0 which has 0 in its outside.
Ar
a0
Ar

r large

0
r small
Thus it is reasonable to believe that for some r during this shrinking process, the set
Ar must hit 0. It follows that p (z) = 0 for some z. This is one of those arguments which
seems all right until you think about it too much. Nevertheless, it will suï¬ƒce to see that
the fundamental theorem of algebra is at least very plausible. A complete proof is in an
appendix.

1.6

Exercises

1. Let z = 5 + i9. Find z âˆ’1 .
2. Let z = 2 + i7 and let w = 3 âˆ’ i8. Find zw, z + w, z 2 , and w/z.
3. Give the complete solution to x4 + 16 = 0.
4. Graph the complex cube roots of 8 in the complex plane. Do the same for the four
fourth roots of 16.
5. If z is a complex number, show there exists Ï‰ a complex number with |Ï‰| = 1 and
Ï‰z = |z| .
n

6. De Moivreâ€™s theorem says [r (cos t + i sin t)] = rn (cos nt + i sin nt) for n a positive
integer. Does this formula continue to hold for all integers, n, even negative integers?
Explain.
7. You already know formulas for cos (x + y) and sin (x + y) and these were used to prove
De Moivreâ€™s theorem. Now using De Moivreâ€™s theorem, derive a formula for sin (5x)
and one for cos (5x). Hint: Use the binomial theorem.
8. If z and w are two complex numbers and the polar form of z involves the angle Î¸ while
the polar form of w involves the angle Ï•, show that in the polar form for zw the angle
involved is Î¸ + Ï•. Also, show that in the polar form of a complex number, z, r = |z| .
9. Factor x3 + 8 as a product of linear factors.
(
)
10. Write x3 + 27 in the form (x + 3) x2 + ax + b where x2 + ax + b cannot be factored
any more using only real numbers.
11. Completely factor x4 + 16 as a product of linear factors.
12. Factor x4 + 16 as the product of two quadratic polynomials each of which cannot be
factored further without using complex numbers.
13. If z, w are complex numbersâˆ‘
prove zw =âˆ‘zw and then show by induction that z1 Â· Â· Â· zm =
m
m
z1 Â· Â· Â· zm . Also verify that k=1 zk = k=1 zk . In words this says the conjugate of a
product equals the product of the conjugates and the conjugate of a sum equals the
sum of the conjugates.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

20

PRELIMINARIES

14. Suppose p (x) = an xn + anâˆ’1 xnâˆ’1 + Â· Â· Â· + a1 x + a0 where all the ak are real numbers.
Suppose also that p (z) = 0 for some z âˆˆ C. Show it follows that p (z) = 0 also.
15. I claim that 1 = âˆ’1. Here is why.
âˆ’1 = i2 =

âˆš

âˆš
âˆš
âˆš
2
âˆ’1 âˆ’1 = (âˆ’1) = 1 = 1.

This is clearly a remarkable result but is there something wrong with it? If so, what
is wrong?
16. De Moivreâ€™s theorem is really a grand thing. I plan to use it now for rational exponents,
not just integers.
1/4

1 = 1(1/4) = (cos 2Ï€ + i sin 2Ï€)

= cos (Ï€/2) + i sin (Ï€/2) = i.

Therefore, squaring both sides it follows 1 = âˆ’1 as in the previous problem. What
does this tell you about De Moivreâ€™s theorem? Is there a profound diï¬€erence between
raising numbers to integer powers and raising numbers to non integer powers?
17. Show that C cannot be considered an ordered ï¬eld. Hint: Consider i2 = âˆ’1. Recall
that 1 > 0 by Proposition 1.4.2.
18. Say a + ib < x + iy if a < x or if a = x, then b < y. This is called the lexicographic
order. Show that any two diï¬€erent complex numbers can be compared with this order.
What goes wrong in terms of the other requirements for an ordered ï¬eld.
19. With the order of Problem 18, consider for n âˆˆ N the complex number 1 âˆ’ n1 . Show
that with the lexicographic order just described, each of 1 âˆ’ in is an upper bound to
all these numbers. Therefore, this is a set which is â€œbounded aboveâ€ but has no least
upper bound with respect to the lexicographic order on C.

1.7

Completeness of R

Recall the following important deï¬nition from calculus, completeness of R.
Deï¬nition 1.7.1 A non empty set, S âŠ† R is bounded above (below) if there exists x âˆˆ R
such that x â‰¥ (â‰¤) s for all s âˆˆ S. If S is a nonempty set in R which is bounded above,
then a number, l which has the property that l is an upper bound and that every other upper
bound is no smaller than l is called a least upper bound, l.u.b. (S) or often sup (S) . If S is a
nonempty set bounded below, deï¬ne the greatest lower bound, g.l.b. (S) or inf (S) similarly.
Thus g is the g.l.b. (S) means g is a lower bound for S and it is the largest of all lower
bounds. If S is a nonempty subset of R which is not bounded above, this information is
expressed by saying sup (S) = +âˆ and if S is not bounded below, inf (S) = âˆ’âˆ.
Every existence theorem in calculus depends on some form of the completeness axiom.
Axiom 1.7.2 (completeness) Every nonempty set of real numbers which is bounded above
has a least upper bound and every nonempty set of real numbers which is bounded below has
a greatest lower bound.
It is this axiom which distinguishes Calculus from Algebra. A fundamental result about
sup and inf is the following.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

1.8. WELL ORDERING AND ARCHIMEDEAN PROPERTY

21

Proposition 1.7.3 Let S be a nonempty set and suppose sup (S) exists. Then for every
Î´ > 0,
S âˆ© (sup (S) âˆ’ Î´, sup (S)] Ì¸= âˆ….
If inf (S) exists, then for every Î´ > 0,
S âˆ© [inf (S) , inf (S) + Î´) Ì¸= âˆ….
Proof: Consider the ï¬rst claim. If the indicated set equals âˆ…, then sup (S) âˆ’ Î´ is an
upper bound for S which is smaller than sup (S) , contrary to the deï¬nition of sup (S) as
the least upper bound. In the second claim, if the indicated set equals âˆ…, then inf (S) + Î´
would be a lower bound which is larger than inf (S) contrary to the deï¬nition of inf (S). 

1.8

Well Ordering And Archimedean Property

Deï¬nition 1.8.1 A set is well ordered if every nonempty subset S, contains a smallest
element z having the property that z â‰¤ x for all x âˆˆ S.
Axiom 1.8.2 Any set of integers larger than a given number is well ordered.
In particular, the natural numbers deï¬ned as
N â‰¡ {1, 2, Â· Â· Â· }
is well ordered.
The above axiom implies the principle of mathematical induction.
Theorem 1.8.3 (Mathematical induction) A set S âŠ† Z, having the property that a âˆˆ S
and n + 1 âˆˆ S whenever n âˆˆ S contains all integers x âˆˆ Z such that x â‰¥ a.
Proof: Let T â‰¡ ([a, âˆ) âˆ© Z) \ S. Thus T consists of all integers larger than or equal
to a which are not in S. The theorem will be proved if T = âˆ…. If T Ì¸= âˆ… then by the well
ordering principle, there would have to exist a smallest element of T, denoted as b. It must
be the case that b > a since by deï¬nition, a âˆˆ
/ T. Then the integer, b âˆ’ 1 â‰¥ a and b âˆ’ 1 âˆˆ
/S
because if b âˆ’ 1 âˆˆ S, then b âˆ’ 1 + 1 = b âˆˆ S by the assumed property of S. Therefore,
b âˆ’ 1 âˆˆ ([a, âˆ) âˆ© Z) \ S = T which contradicts the choice of b as the smallest element of T.
(b âˆ’ 1 is smaller.) Since a contradiction is obtained by assuming T Ì¸= âˆ…, it must be the case
that T = âˆ… and this says that everything in [a, âˆ) âˆ© Z is also in S. 
Example 1.8.4 Show that for all n âˆˆ N,

1
2

Â·

3
4

Â· Â· Â· 2nâˆ’1
2n <

If n = 1 this reduces to the statement that
then that the inequality holds for n. Then
1 3
2n âˆ’ 1 2n + 1
Â· Â·Â·Â·
Â·
2 4
2n
2n + 2

1
2

<
=

<

âˆš1
3

âˆš 1
.
2n+1

which is obviously true. Suppose

1
2n + 1
âˆš
2n + 1 2n + 2
âˆš
2n + 1
.
2n + 2

1
The theorem will be proved if this last expression is less than âˆš2n+3
. This happens if and
only if
)2
(
1
1
2n + 1
âˆš
=
>
2
2n + 3
2n + 3
(2n + 2)
2

which occurs if and only if (2n + 2) > (2n + 3) (2n + 1) and this is clearly true which may
be seen from expanding both sides. This proves the inequality.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

22

PRELIMINARIES

Deï¬nition 1.8.5 The Archimedean property states that whenever x âˆˆ R, and a > 0, there
exists n âˆˆ N such that na > x.
Proposition 1.8.6 R has the Archimedean property.
Proof: Suppose it is not true. Then there exists x âˆˆ R and a > 0 such that na â‰¤ x
for all n âˆˆ N. Let S = {na : n âˆˆ N} . By assumption, this is bounded above by x. By
completeness, it has a least upper bound y. By Proposition 1.7.3 there exists n âˆˆ N such
that
y âˆ’ a < na â‰¤ y.
Then y = y âˆ’ a + a < na + a = (n + 1) a â‰¤ y, a contradiction. 
Theorem 1.8.7 Suppose x < y and y âˆ’ x > 1. Then there exists an integer l âˆˆ Z, such
that x < l < y. If x is an integer, there is no integer y satisfying x < y < x + 1.
Proof: Let x be the smallest positive integer. Not surprisingly, x = 1 but this can be
proved. If x < 1 then x2 < x contradicting the assertion that x is the smallest natural
number. Therefore, 1 is the smallest natural number. This shows there is no integer, y,
satisfying x < y < x + 1 since otherwise, you could subtract x and conclude 0 < y âˆ’ x < 1
for some integer y âˆ’ x.
Now suppose y âˆ’ x > 1 and let
S â‰¡ {w âˆˆ N : w â‰¥ y} .
The set S is nonempty by the Archimedean property. Let k be the smallest element of S.
Therefore, k âˆ’ 1 < y. Either k âˆ’ 1 â‰¤ x or k âˆ’ 1 > x. If k âˆ’ 1 â‰¤ x, then
â‰¤0

z }| {
y âˆ’ x â‰¤ y âˆ’ (k âˆ’ 1) = y âˆ’ k + 1 â‰¤ 1
contrary to the assumption that y âˆ’ x > 1. Therefore, x < k âˆ’ 1 < y. Let l = k âˆ’ 1. 
It is the next theorem which gives the density of the rational numbers. This means that
for any real number, there exists a rational number arbitrarily close to it.
Theorem 1.8.8 If x < y then there exists a rational number r such that x < r < y.
Proof: Let n âˆˆ N be large enough that
n (y âˆ’ x) > 1.
Thus (y âˆ’ x) added to itself n times is larger than 1. Therefore,
n (y âˆ’ x) = ny + n (âˆ’x) = ny âˆ’ nx > 1.
It follows from Theorem 1.8.7 there exists m âˆˆ Z such that
nx < m < ny
and so take r = m/n. 
Deï¬nition 1.8.9 A set, S âŠ† R is dense in R if whenever a < b, S âˆ© (a, b) Ì¸= âˆ….
Thus the above theorem says Q is â€œdenseâ€ in R.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

1.9. DIVISION AND NUMBERS

23

Theorem 1.8.10 Suppose 0 < a and let b â‰¥ 0. Then there exists a unique integer p and
real number r such that 0 â‰¤ r < a and b = pa + r.
Proof: Let S â‰¡ {n âˆˆ N : an > b} . By the Archimedean property this set is nonempty.
Let p + 1 be the smallest element of S. Then pa â‰¤ b because p + 1 is the smallest in S.
Therefore,
r â‰¡ b âˆ’ pa â‰¥ 0.
If r â‰¥ a then b âˆ’ pa â‰¥ a and so b â‰¥ (p + 1) a contradicting p + 1 âˆˆ S. Therefore, r < a as
desired.
To verify uniqueness of p and r, suppose pi and ri , i = 1, 2, both work and r2 > r1 . Then
a little algebra shows
r2 âˆ’ r1
âˆˆ (0, 1) .
p1 âˆ’ p2 =
a
Thus p1 âˆ’ p2 is an integer between 0 and 1, contradicting Theorem 1.8.7. The case that
r1 > r2 cannot occur either by similar reasoning. Thus r1 = r2 and it follows that p1 = p2 .

This theorem is called the Euclidean algorithm when a and b are integers.

1.9

Division And Numbers

First recall Theorem 1.8.10, the Euclidean algorithm.
Theorem 1.9.1 Suppose 0 < a and let b â‰¥ 0. Then there exists a unique integer p and real
number r such that 0 â‰¤ r < a and b = pa + r.
The following deï¬nition describes what is meant by a prime number and also what is
meant by the word â€œdividesâ€.
Deï¬nition 1.9.2 The number, a divides the number, b if in Theorem 1.8.10, r = 0. That
is there is zero remainder. The notation for this is a|b, read a divides b and a is called a
factor of b. A prime number is one which has the property that the only numbers which
divide it are itself and 1. The greatest common divisor of two positive integers, m, n is that
number, p which has the property that p divides both m and n and also if q divides both m
and n, then q divides p. Two integers are relatively prime if their greatest common divisor
is one. The greatest common divisor of m and n is denoted as (m, n) .
There is a phenomenal and amazing theorem which relates the greatest common divisor
to the smallest number in a certain set. Suppose m, n are two positive integers. Then if x, y
are integers, so is xm + yn. Consider all integers which are of this form. Some are positive
such as 1m + 1n and some are not. The set S in the following theorem consists of exactly
those integers of this form which are positive. Then the greatest common divisor of m and
n will be the smallest number in S. This is what the following theorem says.
Theorem 1.9.3 Let m, n be two positive integers and deï¬ne
S â‰¡ {xm + yn âˆˆ N : x, y âˆˆ Z } .
Then the smallest number in S is the greatest common divisor, denoted by (m, n) .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

24

PRELIMINARIES

Proof: First note that both m and n are in S so it is a nonempty set of positive integers.
By well ordering, there is a smallest element of S, called p = x0 m + y0 n. Either p divides m
or it does not. If p does not divide m, then by Theorem 1.8.10,
m = pq + r
where 0 < r < p. Thus m = (x0 m + y0 n) q + r and so, solving for r,
r = m (1 âˆ’ x0 ) + (âˆ’y0 q) n âˆˆ S.
However, this is a contradiction because p was the smallest element of S. Thus p|m. Similarly
p|n.
Now suppose q divides both m and n. Then m = qx and n = qy for integers, x and y.
Therefore,
p = mx0 + ny0 = x0 qx + y0 qy = q (x0 x + y0 y)
showing q|p. Therefore, p = (m, n) . 
There is a relatively simple algorithm for ï¬nding (m, n) which will be discussed now.
Suppose 0 < m < n where m, n are integers. Also suppose the greatest common divisor is
(m, n) = d. Then by the Euclidean algorithm, there exist integers q, r such that
n = qm + r, r < m

(1.1)

Now d divides n and m so there are numbers k, l such that dk = m, dl = n. From the above
equation,
r = n âˆ’ qm = dl âˆ’ qdk = d (l âˆ’ qk)
Thus d divides both m and r. If k divides both m and r, then from the equation of (1.1)
it follows k also divides n. Therefore, k divides d by the deï¬nition of the greatest common
divisor. Thus d is the greatest common divisor of m and r but m + r < m + n. This yields
another pair of positive integers for which d is still the greatest common divisor but the
sum of these integers is strictly smaller than the sum of the ï¬rst two. Now you can do the
same thing to these integers. Eventually the process must end because the sum gets strictly
smaller each time it is done. It ends when there are not two positive integers produced.
That is, one is a multiple of the other. At this point, the greatest common divisor is the
smaller of the two numbers.
Procedure 1.9.4 To ï¬nd the greatest common divisor of m, n where 0 < m < n, replace
the pair {m, n} with {m, r} where n = qm + r for r < m. This new pair of numbers has
the same greatest common divisor. Do the process to this pair and continue doing this till
you obtain a pair of numbers where one is a multiple of the other. Then the smaller is the
sought for greatest common divisor.
Example 1.9.5 Find the greatest common divisor of 165 and 385.
Use the Euclidean algorithm to write
385 = 2 (165) + 55
Thus the next two numbers are 55 and 165. Then
165 = 3 Ã— 55
and so the greatest common divisor of the ï¬rst two numbers is 55.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

1.9. DIVISION AND NUMBERS

25

Example 1.9.6 Find the greatest common divisor of 1237 and 4322.
Use the Euclidean algorithm
4322 = 3 (1237) + 611
Now the two new numbers are 1237,611. Then
1237 = 2 (611) + 15
The two new numbers are 15,611. Then
611 = 40 (15) + 11
The two new numbers are 15,11. Then
15 = 1 (11) + 4
The two new numbers are 11,4
2 (4) + 3
The two new numbers are 4, 3. Then
4 = 1 (3) + 1
The two new numbers are 3, 1. Then
3=3Ã—1
and so 1 is the greatest common divisor. Of course you could see this right away when the
two new numbers were 15 and 11. Recall the process delivers numbers which have the same
greatest common divisor.
This amazing theorem will now be used to prove a fundamental property of prime numbers which leads to the fundamental theorem of arithmetic, the major theorem which says
every integer can be factored as a product of primes.
Theorem 1.9.7 If p is a prime and p|ab then either p|a or p|b.
Proof: Suppose p does not divide a. Then since p is prime, the only factors of p are 1
and p so follows (p, a) = 1 and therefore, there exists integers, x and y such that
1 = ax + yp.
Multiplying this equation by b yields
b = abx + ybp.
Since p|ab, ab = pz for some integer z. Therefore,
b = abx + ybp = pzx + ybp = p (xz + yb)
and this shows p divides b. 
âˆn
Theorem 1.9.8 (Fundamental theorem of arithmetic) Let a âˆˆ N\ {1}. Then a = i=1 pi
where pi are all prime numbers. Furthermore, this prime factorization is unique except for
the order of the factors.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

26

PRELIMINARIES

Proof: If a equals a prime number, the prime factorization clearly exists. In particular
the prime factorization exists for the prime number 2. Assume this theorem is true for all
a â‰¤ n âˆ’ 1. If n is a prime, then it has a prime factorization. On the other hand, if n is not
a prime, then there exist two integers k and m such that n = km where each of k and m
are less than n. Therefore, each of these is no larger than n âˆ’ 1 and consequently, each has
a prime factorization. Thus so does n. It remains to argue the prime factorization is unique
except for order of the factors.
Suppose
n
m
âˆ
âˆ
pi =
qj
i=1

j=1

where the pi and qj are all prime, there is no way to reorder the qk such that m = n and
pi = qi for all i, and n + m is the smallest positive integer such that this happens. Then
by Theorem 1.9.7, p1 |qj for some j. Since these are prime numbers this requires p1 = qj .
Reordering if necessary it can be assumed that qj = q1 . Then dividing both sides by p1 = q1 ,
nâˆ’1
âˆ

pi+1 =

i=1

mâˆ’1
âˆ

qj+1 .

j=1

Since n + m was as small as possible for the theorem to fail, it follows that n âˆ’ 1 = m âˆ’ 1
and the prime numbers, q2 , Â· Â· Â· , qm can be reordered in such a way that pk = qk for all
k = 2, Â· Â· Â· , n. Hence pi = qi for all i because it was already argued that p1 = q1 , and this
results in a contradiction. 

1.10

Systems Of Equations

Sometimes it is necessary to solve systems of equations. For example the problem could be
to ï¬nd x and y such that
x + y = 7 and 2x âˆ’ y = 8.
(1.2)
The set of ordered pairs, (x, y) which solve both equations is called the solution set. For
example, you can see that (5, 2) = (x, y) is a solution to the above system. To solve this,
note that the solution set does not change if any equation is replaced by a non zero multiple
of itself. It also does not change if one equation is replaced by itself added to a multiple
of the other equation. For example, x and y solve the above system if and only if x and y
solve the system
âˆ’3y=âˆ’6

z
}|
{
x + y = 7, 2x âˆ’ y + (âˆ’2) (x + y) = 8 + (âˆ’2) (7).

(1.3)

The second equation was replaced by âˆ’2 times the ï¬rst equation added to the second. Thus
the solution is y = 2, from âˆ’3y = âˆ’6 and now, knowing y = 2, it follows from the other
equation that x + 2 = 7 and so x = 5.
Why exactly does the replacement of one equation with a multiple of another added to
it not change the solution set? The two equations of (1.2) are of the form
E 1 = f1 , E 2 = f2

(1.4)

where E1 and E2 are expressions involving the variables. The claim is that if a is a number,
then (1.4) has the same solution set as
E1 = f1 , E2 + aE1 = f2 + af1 .

Saylor URL: http://www.saylor.org/courses/ma212/

(1.5)

The Saylor Foundation

1.10. SYSTEMS OF EQUATIONS

27

Why is this?
If (x, y) solves (1.4) then it solves the ï¬rst equation in (1.5). Also, it satisï¬es aE1 = af1
and so, since it also solves E2 = f2 it must solve the second equation in (1.5). If (x, y)
solves (1.5) then it solves the ï¬rst equation of (1.4). Also aE1 = af1 and it is given that
the second equation of (1.5) is veriï¬ed. Therefore, E2 = f2 and it follows (x, y) is a solution
of the second equation in (1.4). This shows the solutions to (1.4) and (1.5) are exactly the
same which means they have the same solution set. Of course the same reasoning applies
with no change if there are many more variables than two and many more equations than
two. It is still the case that when one equation is replaced with a multiple of another one
added to itself, the solution set of the whole system does not change.
The other thing which does not change the solution set of a system of equations consists
of listing the equations in a diï¬€erent order. Here is another example.
Example 1.10.1 Find the solutions to the system,
x + 3y + 6z = 25
2x + 7y + 14z = 58
2y + 5z = 19

(1.6)

To solve this system replace the second equation by (âˆ’2) times the ï¬rst equation added
to the second. This yields. the system
x + 3y + 6z = 25
y + 2z = 8
2y + 5z = 19

(1.7)

Now take (âˆ’2) times the second and add to the third. More precisely, replace the third
equation with (âˆ’2) times the second added to the third. This yields the system
x + 3y + 6z = 25
y + 2z = 8
z=3

(1.8)

At this point, you can tell what the solution is. This system has the same solution as the
original system and in the above, z = 3. Then using this in the second equation, it follows
y + 6 = 8 and so y = 2. Now using this in the top equation yields x + 6 + 18 = 25 and so
x = 1.
This process is not really much diï¬€erent from what you have always done in solving a
single equation. For example, suppose you wanted to solve 2x + 5 = 3x âˆ’ 6. You did the
same thing to both sides of the equation thus preserving the solution set until you obtained
an equation which was simple enough to give the answer. In this case, you would add âˆ’2x
to both sides and then add 6 to both sides. This yields x = 11.
In (1.8) you could have continued as follows. Add (âˆ’2) times the bottom equation to
the middle and then add (âˆ’6) times the bottom to the top. This yields
x + 3y = 19
y=6
z=3
Now add (âˆ’3) times the second to the top. This yields
x=1
y=6 ,
z=3

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

28

PRELIMINARIES

a system which has the same solution set as the original system.
It is foolish to write the variables every time you do these operations. It is easier to
write the system (1.6) as the following â€œaugmented matrixâ€
ï£«
ï£¶
1 3 6 25
ï£­ 2 7 14 58 ï£¸ .
0 2 5 19
It has exactly ï£«
the same
it is understood there is
ï£¶ informationï£«as the
ï£¶original system but
ï£« hereï£¶
1
3
6
an x column, ï£­ 2 ï£¸ , a y column, ï£­ 7 ï£¸ and a z column, ï£­ 14 ï£¸ . The rows correspond
0
2
5
to the equations in the system. Thus the top row in the augmented matrix corresponds to
the equation,
x + 3y + 6z = 25.
Now when you replace an equation with a multiple of another equation added to itself, you
are just taking a row of this augmented matrix and replacing it with a multiple of another
row added to it. Thus the ï¬rst step in solving (1.6) would be to take (âˆ’2) times the ï¬rst
row of the augmented matrix above and add it to the second row,
ï£«
ï£¶
1 3 6 25
ï£­ 0 1 2 8 ï£¸.
0 2 5 19
Note how this corresponds to (1.7). Next take (âˆ’2) times the second row and add to the
third,
ï£«
ï£¶
1 3 6 25
ï£­ 0 1 2 8 ï£¸
0 0 1 3
which is the same as (1.8). You get the idea I hope. Write the system as an augmented
matrix and follow the procedure of either switching rows, multiplying a row by a non zero
number, or replacing a row by a multiple of another row added to it. Each of these operations
leaves the solution set unchanged. These operations are called row operations.
Deï¬nition 1.10.2 The row operations consist of the following
1. Switch two rows.
2. Multiply a row by a nonzero number.
3. Replace a row by a multiple of another row added to it.
It is important to observe that any row operation can be â€œundoneâ€ by another inverse
row operation. For example, if r1 , r2 are two rows, and r2 is replaced with râ€²2 = Î±r1 + r2
using row operation 3, then you could get back to where you started by replacing the row râ€²2
with âˆ’Î± times r1 and adding to râ€²2 . In the case of operation 2, you would simply multiply
the row that was changed by the inverse of the scalar which multiplied it in the ï¬rst place,
and in the case of row operation 1, you would just make the same switch again and you
would be back to where you started. In each case, the row operation which undoes what
was done is called the inverse row operation.
Example 1.10.3 Give the complete solution to the system of equations, 5x+10yâˆ’7z = âˆ’2,
2x + 4y âˆ’ 3z = âˆ’1, and 3x + 6y + 5z = 9.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

1.10. SYSTEMS OF EQUATIONS

29

The augmented matrix for this system is
ï£«
2 4 âˆ’3
ï£­ 5 10 âˆ’7
3 6
5

ï£¶
âˆ’1
âˆ’2 ï£¸
9

Multiply the second row by 2, the ï¬rst row by 5, and then take (âˆ’1) times the ï¬rst row and
add to the second. Then multiply the ï¬rst row by 1/5. This yields
ï£«
ï£¶
2 4 âˆ’3 âˆ’1
ï£­ 0 0 1
1 ï£¸
3 6 5
9
Now, combining some row operations, take (âˆ’3) times the ï¬rst row and add this to 2 times
the last row and replace the last row with this. This yields.
ï£«
ï£¶
2 4 âˆ’3 âˆ’1
ï£­ 0 0 1
1 ï£¸.
0 0 1
21
Putting in the variables, the last two rows say z = 1 and z = 21. This is impossible so
the last system of equations determined by the above augmented matrix has no solution.
However, it has the same solution set as the ï¬rst system of equations. This shows there is no
solution to the three given equations. When this happens, the system is called inconsistent.
This should not be surprising that something like this can take place. It can even happen
for one equation in one variable. Consider for example, x = x+1. There is clearly no solution
to this.
Example 1.10.4 Give the complete solution to the system of equations, 3x âˆ’ y âˆ’ 5z = 9,
y âˆ’ 10z = 0, and âˆ’2x + y = âˆ’6.
The augmented matrix of this system is
ï£«
ï£¶
3 âˆ’1 âˆ’5
9
ï£­ 0
1 âˆ’10 0 ï£¸
âˆ’2 1
0
âˆ’6
Replace the last row with 2 times the top row added to 3 times the bottom row. This gives
ï£¶
ï£«
3 âˆ’1 âˆ’5 9
ï£­ 0 1 âˆ’10 0 ï£¸
0 1 âˆ’10 0
Next take âˆ’1 times the middle row and
ï£«
3
ï£­ 0
0

add to the bottom.
ï£¶
âˆ’1 âˆ’5 9
1 âˆ’10 0 ï£¸
0
0
0

Take the middle row and add to the top
ï£«
1
ï£­ 0
0

and then divide the top row which results by 3.
ï£¶
0 âˆ’5 3
1 âˆ’10 0 ï£¸ .
0
0
0

This says y = 10z and x = 3 + 5z. Apparently z can equal any number. Therefore, the
solution set of this system is x = 3 + 5t, y = 10t, and z = t where t is completely arbitrary.
The system has an inï¬nite set of solutions and this is a good description of the solutions.
This is what it is all about, ï¬nding the solutions to the system.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

30

PRELIMINARIES

Deï¬nition 1.10.5 Since z = t where t is arbitrary, the variable z is called a free variable.
The phenomenon of an inï¬nite solution set occurs in equations having only one variable
also. For example, consider the equation x = x. It doesnâ€™t matter what x equals.
Deï¬nition 1.10.6 A system of linear equations is a list of equations,
n
âˆ‘

aij xj = fj , i = 1, 2, 3, Â· Â· Â· , m

j=1

where aij are numbers, fj is a number, and it is desired to ï¬nd (x1 , Â· Â· Â· , xn ) solving each of
the equations listed.
As illustrated above, such a system of linear equations may have a unique solution, no
solution, or inï¬nitely many solutions. It turns out these are the only three cases which can
occur for linear systems. Furthermore, you do exactly the same things to solve any linear
system. You write the augmented matrix and do row operations until you get a simpler
system in which it is possible to see the solution. All is based on the observation that the
row operations do not change the solution set. You can have more equations than variables,
fewer equations than variables, etc. It doesnâ€™t matter. You always set up the augmented
matrix and go to work on it. These things are all the same.
Example 1.10.7 Give the complete solution to the system of equations, âˆ’41x + 15y = 168,
109x âˆ’ 40y = âˆ’447, âˆ’3x + y = 12, and 2x + z = âˆ’1.
The augmented matrix is

ï£«

âˆ’41
ï£¬ 109
ï£¬
ï£­ âˆ’3
2

15
âˆ’40
1
0

0
0
0
1

ï£¶
168
âˆ’447 ï£·
ï£·.
12 ï£¸
âˆ’1

To solve this multiply the top row by 109, the second row by 41, add the top row to the
second row, and multiply the top row by 1/109. Note how this process combined several
row operations. This yields
ï£«
ï£¶
âˆ’41 15 0 168
ï£¬ 0
âˆ’5 0 âˆ’15 ï£·
ï£¬
ï£·.
ï£­ âˆ’3
1 0 12 ï£¸
2
0 1 âˆ’1
Next take 2 times the third row and replace the fourth row by this added to 3 times the
fourth row. Then take (âˆ’41) times the third row and replace the ï¬rst row by this added to
3 times the ï¬rst row. Then switch the third and the ï¬rst rows. This yields
ï£«
ï£¶
123 âˆ’41 0 âˆ’492
ï£¬ 0
âˆ’5 0 âˆ’15 ï£·
ï£¬
ï£·.
ï£­ 0
4
0
12 ï£¸
0
2
3
21
Take âˆ’1/2 times the third row and add to the bottom row. Then take 5 times the third
row and add to four times the second. Finally take 41 times the third row and add to 4
times the top row. This yields
ï£«
ï£¶
492 0 0 âˆ’1476
ï£¬ 0 0 0
ï£·
0
ï£¬
ï£·
ï£­ 0 4 0
12 ï£¸
0 0 3
15

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

1.11. EXERCISES

31

It follows x = âˆ’1476
492 = âˆ’3, y = 3 and z = 5.
You should practice solving systems of equations. Here are some exercises.

1.11

Exercises

1. Give the complete solution to the system of equations, 3x âˆ’ y + 4z = 6, y + 8z = 0,
and âˆ’2x + y = âˆ’4.
2. Give the complete solution to the system of equations, x+3y +3z = 3, 3x+2y +z = 9,
and âˆ’4x + z = âˆ’9.
3. Consider the system âˆ’5x + 2y âˆ’ z = 0 and âˆ’5x âˆ’ 2y âˆ’ z = 0. Both equations equal
zero and so âˆ’5x + 2y âˆ’ z = âˆ’5x âˆ’ 2y âˆ’ z which is equivalent to y = 0. Thus x and
z can equal anything. But when x = 1, z = âˆ’4, and y = 0 are plugged in to the
equations, it doesnâ€™t work. Why?
4. Give the complete solution to the system of equations, x+2y +6z = 5, 3x+2y +6z = 7
,âˆ’4x + 5y + 15z = âˆ’7.
5. Give the complete solution to the system of equations
x + 2y + 3z
âˆ’4x + 5y + z

= 5, 3x + 2y + z = 7,
= âˆ’7, x + 3z = 5.

6. Give the complete solution of the system of equations,
x + 2y + 3z
âˆ’4x + 5y + 5z

= 5, 3x + 2y + 2z = 7
= âˆ’7, x = 5

7. Give the complete solution of the system of equations
x + y + 3z
âˆ’4x + 9y + z

= 2, 3x âˆ’ y + 5z = 6
= âˆ’8, x + 5y + 7z = 2

8. Determine a such that there are inï¬nitely many solutions and then ï¬nd them. Next
determine a such that there are no solutions. Finally determine which values of a
correspond to a unique solution. The system of equations for the unknown variables
x, y, z is
3za2 âˆ’ 3a + (x + y +) 1 = 0
3x âˆ’ a âˆ’ y + z a2 + 4 âˆ’ 5 = 0
za2 âˆ’ a âˆ’ 4x + 9y + 9 = 0
9. Find the solutions to the following system of equations for x, y, z, w.
y + z = 2, z + w = 0, y âˆ’ 4z âˆ’ 5w = 2, 2y + z âˆ’ w = 4
10. Find all solutions to the following equations.
x+y+z
2x + 2y + z âˆ’ w

Saylor URL: http://www.saylor.org/courses/ma212/

= 2, z + w = 0,
= 4, x + y âˆ’ 4z âˆ’ 5z = 2

The Saylor Foundation

32

1.12

PRELIMINARIES

Fn

The notation, Cn refers to the collection of ordered lists of n complex numbers. Since every
real number is also a complex number, this simply generalizes the usual notion of Rn , the
collection of all ordered lists of n real numbers. In order to avoid worrying about whether
it is real or complex numbers which are being referred to, the symbol F will be used. If it is
not clear, always pick C. More generally, Fn refers to the ordered lists of n elements of Fn .
Deï¬nition 1.12.1 Deï¬ne Fn â‰¡ {(x1 , Â· Â· Â· , xn ) : xj âˆˆ F for j = 1, Â· Â· Â· , n} . (x1 , Â· Â· Â· , xn ) =
(y1 , Â· Â· Â· , yn ) if and only if for all j = 1, Â· Â· Â· , n, xj = yj . When (x1 , Â· Â· Â· , xn ) âˆˆ Fn , it is
conventional to denote (x1 , Â· Â· Â· , xn ) by the single bold face letter x. The numbers xj are
called the coordinates. The set
{(0, Â· Â· Â· , 0, t, 0, Â· Â· Â· , 0) : t âˆˆ F}
for t in the ith slot is called the ith coordinate axis. The point 0 â‰¡ (0, Â· Â· Â· , 0) is called the
origin.
Thus (1, 2, 4i) âˆˆ F3 and (2, 1, 4i) âˆˆ F3 but (1, 2, 4i) Ì¸= (2, 1, 4i) because, even though the
same numbers are involved, they donâ€™t match up. In particular, the ï¬rst entries are not
equal.

1.13

Algebra in Fn

There are two algebraic operations done with elements of Fn . One is addition and the other
is multiplication by numbers, called scalars. In the case of Cn the scalars are complex
numbers while in the case of Rn the only allowed scalars are real numbers. Thus, the scalars
always come from F in either case.
Deï¬nition 1.13.1 If x âˆˆ Fn and a âˆˆ F, also called a scalar, then ax âˆˆ Fn is deï¬ned by
ax = a (x1 , Â· Â· Â· , xn ) â‰¡ (ax1 , Â· Â· Â· , axn ) .

(1.9)

This is known as scalar multiplication. If x, y âˆˆ Fn then x + y âˆˆ Fn and is deï¬ned by
x + y = (x1 , Â· Â· Â· , xn ) + (y1 , Â· Â· Â· , yn )
â‰¡ (x1 + y1 , Â· Â· Â· , xn + yn )

(1.10)

With this deï¬nition, the algebraic properties satisfy the conclusions of the following
theorem.
Theorem 1.13.2 For v, w âˆˆ Fn and Î±, Î² scalars, (real numbers), the following hold.
v + w = w + v,

(1.11)

(v + w) + z = v+ (w + z) ,

(1.12)

v + 0 = v,

(1.13)

v+ (âˆ’v) = 0,

(1.14)

the commutative law of addition,

the associative law for addition,
the existence of an additive identity,

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

1.14. EXERCISES

33

the existence of an additive inverse, Also
Î± (v + w) = Î±v+Î±w,

(1.15)

(Î± + Î²) v =Î±v+Î²v,

(1.16)

Î± (Î²v) = Î±Î² (v) ,

(1.17)

1v = v.

(1.18)

In the above 0 = (0, Â· Â· Â· , 0).
You should verify that these properties all hold. As usual subtraction is deï¬ned as
x âˆ’ y â‰¡ x+ (âˆ’y) . The conclusions of the above theorem are called the vector space axioms.

1.14

Exercises

1. Verify all the properties (1.11)-(1.18).
2. Compute 5 (1, 2 + 3i, 3, âˆ’2) + 6 (2 âˆ’ i, 1, âˆ’2, 7) .
3. Draw a picture of the points in R2 which are determined by the following ordered
pairs.
(a) (1, 2)
(b) (âˆ’2, âˆ’2)
(c) (âˆ’2, 3)
(d) (2, âˆ’5)
4. Does it make sense to write (1, 2) + (2, 3, 1)? Explain.
5. Draw a picture of the points in R3 which are determined by the following ordered
triples. If you have trouble drawing this, describe it in words.
(a) (1, 2, 0)
(b) (âˆ’2, âˆ’2, 1)
(c) (âˆ’2, 3, âˆ’2)

1.15

The Inner Product In Fn

When F = R or C, there is something called an inner product. In case of R it is also called
the dot product. This is also often referred to as the scalar product.
Deï¬nition 1.15.1 Let a, b âˆˆ Fn deï¬ne a Â· b as
aÂ·bâ‰¡

n
âˆ‘

ak bk .

k=1

With this deï¬nition, there are several important properties satisï¬ed by the inner product.
In the statement of these properties, Î± and Î² will denote scalars and a, b, c will denote
vectors or in other words, points in Fn .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

34

PRELIMINARIES

Proposition 1.15.2 The inner product satisï¬es the following properties.
a Â· b =b Â· a

(1.19)

a Â· a â‰¥ 0 and equals zero if and only if a = 0

(1.20)

(Î±a + Î²b) Â· c =Î± (a Â· c) + Î² (b Â· c)

(1.21)

c Â· (Î±a + Î²b) = Î± (c Â· a) + Î² (c Â· b)

(1.22)

2

|a| = a Â· a

(1.23)

You should verify these properties. Also be sure you understand that (1.22) follows from
the ï¬rst three and is therefore redundant. It is listed here for the sake of convenience.
Example 1.15.3 Find (1, 2, 0, âˆ’1) Â· (0, i, 2, 3) .
This equals 0 + 2 (âˆ’i) + 0 + âˆ’3 = âˆ’3 âˆ’ 2i
The Cauchy Schwarz inequality takes the following form in terms of the inner product.
I will prove it using only the above axioms for the inner product.
Theorem 1.15.4 The inner product satisï¬es the inequality
|a Â· b| â‰¤ |a| |b| .

(1.24)

Furthermore equality is obtained if and only if one of a or b is a scalar multiple of the other.
Proof: First deï¬ne Î¸ âˆˆ C such that
Î¸ (a Â· b) = |a Â· b| , |Î¸| = 1,
and deï¬ne a function of t âˆˆ R
f (t) = (a + tÎ¸b) Â· (a + tÎ¸b) .
Then by (1.20), f (t) â‰¥ 0 for all t âˆˆ R. Also from (1.21),(1.22),(1.19), and (1.23)
f (t) = a Â· (a + tÎ¸b) + tÎ¸b Â· (a + tÎ¸b)
2

= a Â· a + tÎ¸ (a Â· b) + tÎ¸ (b Â· a) + t2 |Î¸| b Â· b
2

2

2

2

= |a| + 2t Re Î¸ (a Â· b) + |b| t2 = |a| + 2t |a Â· b| + |b| t2
2

Now if |b| = 0 it must be the case that a Â· b = 0 because otherwise, you could pick large
negative values of t and violate f (t) â‰¥ 0. Therefore, in this case, the Cauchy Schwarz
inequality holds. In the case that |b| Ì¸= 0, y = f (t) is a polynomial which opens up and
therefore, if it is always nonnegative, its graph is like that illustrated in the following picture

t

t

Then the quadratic formula requires that
The discriminant

z
}|
{
2
2
2
4 |a Â· b| âˆ’ 4 |a| |b| â‰¤ 0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

1.15. THE INNER PRODUCT IN FN

35

since otherwise the function, f (t) would have two real zeros and would necessarily have a
graph which dips below the t axis. This proves (1.24).
It is clear from the axioms of the inner product that equality holds in (1.24) whenever
one of the vectors is a scalar multiple of the other. It only remains to verify this is the only
way equality can occur. If either vector equals zero, then equality is obtained in (1.24) so
it can be assumed both vectors are non zero. Then if equality is achieved, it follows f (t)
has exactly one real zero because the discriminant vanishes. Therefore, for some value of
t, a + tÎ¸b = 0 showing that a is a multiple of b. 
You should note that the entire argument was based only on the properties of the inner product listed in (1.19) - (1.23). This means that whenever something satisï¬es these
properties, the Cauchy Schwartz inequality holds. There are many other instances of these
properties besides vectors in Fn . Also note that (1.24) holds if (1.20) is simpliï¬ed to aÂ·a â‰¥ 0.
The Cauchy Schwartz inequality allows a proof of the triangle inequality for distances
in Fn in much the same way as the triangle inequality for the absolute value.
Theorem 1.15.5 (Triangle inequality) For a, b âˆˆ Fn
|a + b| â‰¤ |a| + |b|

(1.25)

and equality holds if and only if one of the vectors is a nonnegative scalar multiple of the
other. Also
||a| âˆ’ |b|| â‰¤ |a âˆ’ b|
(1.26)
Proof : By properties of the inner product and the Cauchy Schwartz inequality,
2

|a + b| = (a + b) Â· (a + b) = (a Â· a) + (a Â· b) + (b Â· a) + (b Â· b)
2

2

2

= |a| + 2 Re (a Â· b) + |b| â‰¤ |a| + 2 |a Â· b| + |b|
2

2

2

2

â‰¤ |a| + 2 |a| |b| + |b| = (|a| + |b|) .
Taking square roots of both sides you obtain (1.25).
It remains to consider when equality occurs. If either vector equals zero, then that
vector equals zero times the other vector and the claim about when equality occurs is
veriï¬ed. Therefore, it can be assumed both vectors are nonzero. To get equality in the
second inequality above, Theorem 1.15.4 implies one of the vectors must be a multiple of
the other. Say b = Î±a. Also, to get equality in the ï¬rst inequality, (a Â· b) must be a
nonnegative real number. Thus
2

0 â‰¤ (a Â· b) = (aÂ·Î±a) = Î± |a| .
Therefore, Î± must be a real number which is nonnegative.
To get the other form of the triangle inequality,
a=aâˆ’b+b
so
|a| = |a âˆ’ b + b| â‰¤ |a âˆ’ b| + |b| .
Therefore,
|a| âˆ’ |b| â‰¤ |a âˆ’ b|

(1.27)

|b| âˆ’ |a| â‰¤ |b âˆ’ a| = |a âˆ’ b| .

(1.28)

Similarly,
It follows from (1.27) and (1.28) that (1.26) holds. This is because ||a| âˆ’ |b|| equals the left
side of either (1.27) or (1.28) and either way, ||a| âˆ’ |b|| â‰¤ |a âˆ’ b| . 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

36

PRELIMINARIES

1.16

What Is Linear Algebra?

The above preliminary considerations form the necessary scaï¬€olding upon which linear algebra is built. Linear algebra is the study of a certain algebraic structure called a vector
space described in a special case in Theorem 1.13.2 and in more generality below along with
special functions known as linear transformations. These linear transformations preserve
certain algebraic properties.
A good argument could be made that linear algebra is the most useful subject in all
of mathematics and that it exceeds even courses like calculus in its signiï¬cance. It is used
extensively in applied mathematics and engineering. Continuum mechanics, for example,
makes use of topics from linear algebra in deï¬ning things like the strain and in determining
appropriate constitutive laws. It is fundamental in the study of statistics. For example,
principal component analysis is really based on the singular value decomposition discussed
in this book. It is also fundamental in pure mathematics areas like number theory, functional
analysis, geometric measure theory, and diï¬€erential geometry. Even calculus cannot be
correctly understood without it. For example, the derivative of a function of many variables
is an example of a linear transformation, and this is the way it must be understood as soon
as you consider functions of more than one variable.

1.17

Exercises

1. Show that (a Â· b) =

1
4

]
[
2
2
|a + b| âˆ’ |a âˆ’ b| .
2

2. Prove from the axioms of the inner product the parallelogram identity, |a + b| +
2
2
2
|a âˆ’ b| = 2 |a| + 2 |b| .
âˆ‘n
3. For a, b âˆˆ Rn , deï¬ne a Â· b â‰¡ k=1 Î² k ak bk where Î² k > 0 for each k. Show this satisï¬es
the axioms of the inner product. What does the Cauchy Schwarz inequality say in
this case.
4. In Problem 3 above, suppose you only know Î² k â‰¥ 0. Does the Cauchy Schwarz inequality still hold? If so, prove it.
5. Let f, g be continuous functions and deï¬ne
âˆ« 1
f Â·g â‰¡
f (t) g (t)dt
0

show this satisï¬es the axioms of a inner product if you think of continuous functions
in the place of a vector in Fn . What does the Cauchy Schwarz inequality say in this
case?
6. Show that if f is a real valued continuous function,
(âˆ«

b

)2
f (t) dt

a

Saylor URL: http://www.saylor.org/courses/ma212/

âˆ«

b

â‰¤ (b âˆ’ a)

2

f (t) dt.
a

The Saylor Foundation

Matrices And Linear
Transformations
2.1

Matrices

You have now solved systems of equations by writing them in terms of an augmented matrix
and then doing row operations on this augmented matrix. It turns out that such rectangular
arrays of numbers are important from many other diï¬€erent points of view. Numbers are
also called scalars. In general, scalars are just elements of some ï¬eld. However, in the ï¬rst
part of this book, the ï¬eld will typically be either the real numbers or the complex numbers.
A matrix is a rectangular array of numbers. Several of them are referred to as matrices.
For example, here is a matrix.
ï£«
ï£¶
1 2 3 4
ï£­ 5 2 8 7 ï£¸
6 âˆ’9 1 2
This matrix is a 3 Ã— 4 matrix because there are three rows and four columns.
ï£¶ ï¬rst
ï£« The
1
row is (1 2 3 4) , the second row is (5 2 8 7) and so forth. The ï¬rst column is ï£­ 5 ï£¸ . The
6
convention in dealing with matrices is to always list the rows ï¬rst and then the columns.
Also, you can remember the columns are like columns in a Greek temple. They stand up
right while the rows just lay there like rows made by a tractor in a plowed ï¬eld. Elements of
the matrix are identiï¬ed according to position in the matrix. For example, 8 is in position
2, 3 because it is in the second row and the third column. You might remember that you
always list the rows before the columns by using the phrase Rowman Catholic. The symbol,
(aij ) refers to a matrix in which the i denotes the row and the j denotes the column. Using
this notation on the above matrix, a23 = 8, a32 = âˆ’9, a12 = 2, etc.
There are various operations which are done on matrices. They can sometimes be added,
multiplied by a scalar and sometimes multiplied. To illustrate scalar multiplication, consider
the following example.
ï£«
ï£¶ ï£«
ï£¶
1 2 3 4
3
6
9 12
6
24 21 ï£¸ .
3 ï£­ 5 2 8 7 ï£¸ = ï£­ 15
6 âˆ’9 1 2
18 âˆ’27 3
6
The new matrix is obtained by multiplying every entry of the original matrix by the given
scalar. If A is an m Ã— n matrix âˆ’A is deï¬ned to equal (âˆ’1) A.
Two matrices which are the same size can be added. When this is done, the result is the

37

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

38
matrix which is obtained by
ï£«
1
ï£­ 3
5

MATRICES AND LINEAR TRANSFORMATIONS

adding corresponding entries. Thus
ï£¶ ï£«
ï£¶ ï£«
ï£¶
2
âˆ’1 4
0
6
4 ï£¸+ï£­ 2
8 ï£¸ = ï£­ 5 12 ï£¸ .
2
6 âˆ’4
11 âˆ’2

Two matrices are equal exactly when they are the same size and the corresponding entries
are identical. Thus
ï£«
ï£¶
(
)
0 0
0 0
ï£­ 0 0 ï£¸ Ì¸=
0 0
0 0
because they are diï¬€erent sizes. As noted above, you write (cij ) for the matrix C whose
ij th entry is cij . In doing arithmetic with matrices you must deï¬ne what happens in terms
of the cij sometimes called the entries of the matrix or the components of the matrix.
The above discussion stated for general matrices is given in the following deï¬nition.
Deï¬nition 2.1.1 Let A = (aij ) and B = (bij ) be two m Ã— n matrices. Then A + B = C
where
C = (cij )
for cij = aij + bij . Also if x is a scalar,
xA = (cij )
where cij = xaij . The number Aij will typically refer to the ij th entry of the matrix A. The
zero matrix, denoted by 0 will be the matrix consisting of all zeros.
Do not be upset by the use of the subscripts, ij. The expression cij = aij + bij is just
saying that you add corresponding entries to get the result of summing two matrices as
discussed above.
Note that there are 2 Ã— 3 zero matrices, 3 Ã— 4 zero matrices, etc. In fact for every size
there is a zero matrix.
With this deï¬nition, the following properties are all obvious but you should verify all of
these properties are valid for A, B, and C, m Ã— n matrices and 0 an m Ã— n zero matrix,
A + B = B + A,

(2.1)

(A + B) + C = A + (B + C) ,

(2.2)

the commutative law of addition,

the associative law for addition,
A + 0 = A,

(2.3)

A + (âˆ’A) = 0,

(2.4)

the existence of an additive identity,

the existence of an additive inverse. Also, for Î±, Î² scalars, the following also hold.
Î± (A + B) = Î±A + Î±B,

(2.5)

(Î± + Î²) A = Î±A + Î²A,

(2.6)

Î± (Î²A) = Î±Î² (A) ,

(2.7)

1A = A.

(2.8)

The above properties, (2.1) - (2.8) are known as the vector space axioms and the fact
that the m Ã— n matrices satisfy these axioms is what is meant by saying this set of matrices
with addition and scalar multiplication as deï¬ned above forms a vector space.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.1. MATRICES

39

Deï¬nition 2.1.2 Matrices which are n Ã— 1 or 1 Ã— n are especially called vectors and are
often denoted by a bold letter. Thus
ï£«
ï£¶
x1
ï£¬
ï£·
x = ï£­ ... ï£¸
xn
is an n Ã— 1 matrix also called a column vector while a 1 Ã— n matrix of the form (x1 Â· Â· Â· xn )
is referred to as a row vector.
All the above is ï¬ne, but the real reason for considering matrices is that they can be
multiplied. This is where things quit being banal.
First consider the problem of multiplying an m Ã— n matrix by an n Ã— 1 column vector.
Consider the following example
ï£« ï£¶
(
)
7
1 2 3 ï£­ ï£¸
8
=?
4 5 6
9
It equals

(
7

1
4

)

(
+8

)

2
5

(
+9

3
6

)

Thus it is what is called a linear combination of the columns. These will be discussed
more later. Motivated by this example, here is the deï¬nition of how to multiply an m Ã— n
matrix by an n Ã— 1 matrix. (vector)
Deï¬nition 2.1.3 Let A = Aij be an m Ã— n matrix and let v be an n Ã— 1 matrix,
ï£«
ï£¶
v1
ï£¬
ï£·
v = ï£­ ... ï£¸ , A = (a1 , Â· Â· Â· , an )
vn
where ai is an m Ã— 1 vector. Then Av, written as
ï£«
(
)ï£¬
a1 Â· Â· Â· an ï£­

ï£¶
v1
.. ï£· ,
. ï£¸
vn

is the m Ã— 1 column vector which equals the following linear combination of the columns.
v 1 a1 + v 2 a2 + Â· Â· Â· + v n an â‰¡

n
âˆ‘

v j aj

(2.9)

j=1

If the j th column of A is

ï£«
ï£¬
ï£¬
ï£¬
ï£­

ï£¶

A1j
A2j
..
.

ï£·
ï£·
ï£·
ï£¸

Amj
then (2.9) takes the form
ï£«
ï£¬
ï£¬
v1 ï£¬
ï£­

A11
A21
..
.
Am1

ï£¶

ï£«

ï£·
ï£¬
ï£·
ï£¬
ï£· + v2 ï£¬
ï£¸
ï£­

A12
A22
..
.
Am2

Saylor URL: http://www.saylor.org/courses/ma212/

ï£¶

ï£«

ï£·
ï£¬
ï£·
ï£¬
ï£· + Â· Â· Â· + vn ï£¬
ï£¸
ï£­

A1n
A2n
..
.

ï£¶
ï£·
ï£·
ï£·
ï£¸

Amn

The Saylor Foundation

40

MATRICES AND LINEAR TRANSFORMATIONS

âˆ‘n
Thus the ith entry of Av is j=1 Aij vj . Note that multiplication by an m Ã— n matrix takes
an n Ã— 1 matrix, and produces an m Ã— 1 matrix (vector).
Here is another example.
Example 2.1.4 Compute

ï£«

1 2 1
ï£­ 0 2 1
2 1 4

ï£¶
1
3
ï£¬ 2 ï£·
ï£·
âˆ’2 ï£¸ ï£¬
ï£­ 0 ï£¸.
1
1
ï£¶

ï£«

First of all, this is of the form (3 Ã— 4) (4 Ã— 1) and so the result should be a (3 Ã— 1) .
Note how the inside numbers cancel. To get the entry in the second row and ï¬rst and only
column, compute
4
âˆ‘

a2k vk

= a21 v1 + a22 v2 + a23 v3 + a24 v4

k=1

= 0 Ã— 1 + 2 Ã— 2 + 1 Ã— 0 + (âˆ’2) Ã— 1 = 2.
You should do the rest of the problem and verify
ï£«
ï£«
ï£¶
1
1 2 1 3
ï£¬ 2
ï£­ 0 2 1 âˆ’2 ï£¸ ï£¬
ï£­ 0
2 1 4 1
1

ï£¶

ï£« ï£¶
8
ï£·
ï£· = ï£­ 2 ï£¸.
ï£¸
5

With this done, the next task is to multiply an m Ã— n matrix times an n Ã— p matrix.
Before doing so, the following may be helpful.
these must match

(m Ã—

[
n)
(n Ã— p

)=mÃ—p

If the two middle numbers donâ€™t match, you canâ€™t multiply the matrices!
Deï¬nition 2.1.5 Let A be an m Ã— n matrix and let B be an n Ã— p matrix. Then B is of
the form
B = (b1 , Â· Â· Â· , bp )
where bk is an n Ã— 1 matrix. Then an m Ã— p matrix AB is deï¬ned as follows:
AB â‰¡ (Ab1 , Â· Â· Â· , Abp )

(2.10)

where Abk is an m Ã— 1 matrix. Hence AB as just deï¬ned is an m Ã— p matrix. For example,
Example 2.1.6 Multiply the following.
(

1 2
0 2

1
1

)

ï£«

ï£¶
1 2 0
ï£­ 0 3 1 ï£¸
âˆ’2 1 1

The ï¬rst thing you need to check before doing anything else is whether it is possible to
do the multiplication. The ï¬rst matrix is a 2 Ã— 3 and the second matrix is a 3 Ã— 3. Therefore,

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.1. MATRICES

41

is it possible to multiply these matrices. According to the above discussion it should be a
2 Ã— 3 matrix of the form
ï£«
ï£¶
First column
Second column
Third column
z
z
z
}| ï£«
{
}|
{
}|
{
ï£¶
ï£«
ï£¶
ï£« ï£¶ï£·
ï£¬(
)
)
)
(
(
1
2
0 ï£·
ï£¬
1
1
1
2
1
2
ï£¬ 1 2 1 ï£­
ï£­ 3 ï£¸,
ï£­ 1 ï£¸ï£·
0 ï£¸,
ï£·
ï£¬
0 2 1
0 2 1
ï£¬ 0 2 1
ï£·
âˆ’2
1
1 ï£¸
ï£­
You know how to multiply a matrix times a
three columns. Thus
ï£«
(
)
1 2
1 2 1 ï£­
0 3
0 2 1
âˆ’2 1

vector and so you do so to obtain each of the
ï£¶
(
0
âˆ’1 9
1 ï£¸=
âˆ’2 7
1

3
3

)
.

Here is another example.
Example 2.1.7 Multiply the following.
ï£«
ï£¶
(
1 2 0
ï£­ 0 3 1 ï£¸ 1
0
âˆ’2 1 1

2
2

1
1

)

First check if it is possible. This is of the form (3 Ã— 3) (2 Ã— 3) . The inside numbers do not
match and so you canâ€™t do this multiplication. This means that anything you write will be
absolute nonsense because it is impossible to multiply these matrices in this order. Arenâ€™t
they the same two matrices considered in the previous example? Yes they are. It is just
that here they are in a diï¬€erent order. This shows something you must always remember
about matrix multiplication.
Order Matters!
Matrix multiplication is not commutative. This is very diï¬€erent than multiplication of
numbers!

2.1.1

The ij th Entry Of A Product

It is important to describe matrix multiplication in terms of entries of the matrices. What
is the ij th entry of AB? It would be the ith entry of the j th column of AB. Thus it would
be the ith entry of Abj . Now
ï£«
ï£¶
B1j
ï£¬
ï£·
bj = ï£­ ... ï£¸
Bnj
th

and from the above deï¬nition, the i

entry is
n
âˆ‘

Aik Bkj .

(2.11)

k=1

In terms of pictures of the matrix, you are
ï£«
A11 A12 Â· Â· Â· A1n
ï£¬ A21 A22 Â· Â· Â· A2n
ï£¬
ï£¬ ..
..
..
ï£­ .
.
.
Am1

Am2

Â·Â·Â·

Amn

Saylor URL: http://www.saylor.org/courses/ma212/

doing
ï£¶ï£«
ï£·ï£¬
ï£·ï£¬
ï£·ï£¬
ï£¸ï£­

B11
B21
..
.

B12
B22
..
.

Â·Â·Â·
Â·Â·Â·

B1p
B2p
..
.

Bn1

Bn2

Â·Â·Â·

Bnp

ï£¶
ï£·
ï£·
ï£·
ï£¸

The Saylor Foundation

42

MATRICES AND LINEAR TRANSFORMATIONS

Then as explained above, the j th column is of
ï£«
A11 A12 Â· Â· Â·
ï£¬ A21 A22 Â· Â· Â·
ï£¬
ï£¬ ..
..
ï£­ .
.
Am1 Am2 Â· Â· Â·

the form
ï£¶ï£«
A1n
B1j
ï£¬ B2j
A2n ï£·
ï£·ï£¬
.. ï£· ï£¬ ..
. ï£¸ï£­ .
Amn
Bnj

ï£¶
ï£·
ï£·
ï£·
ï£¸

which is a m Ã— 1 matrix or column vector which equals
ï£«
ï£«
ï£«
ï£¶
ï£¶
A11
A12
A1n
ï£¬ A21 ï£·
ï£¬ A22 ï£·
ï£¬ A2n
ï£¬
ï£¬
ï£¬
ï£·
ï£·
ï£¬ .. ï£· B1j + ï£¬ .. ï£· B2j + Â· Â· Â· + ï£¬ ..
ï£­ . ï£¸
ï£­ . ï£¸
ï£­ .
Am1
Am2
Amn

ï£¶
ï£·
ï£·
ï£· Bnj .
ï£¸

The ith entry of this m Ã— 1 matrix is
Ai1 B1j + Ai2 B2j + Â· Â· Â· + Ain Bnj =

m
âˆ‘

Aik Bkj .

k=1

This shows the following deï¬nition for matrix multiplication in terms of the ij th entries of
the product harmonizes with Deï¬nition 2.1.3.
This motivates the deï¬nition for matrix multiplication which identiï¬es the ij th entries
of the product.
Deï¬nition 2.1.8 Let A = (Aij ) be an m Ã— n matrix and let B = (Bij ) be an n Ã— p matrix.
Then AB is an m Ã— p matrix and
(AB)ij =

n
âˆ‘

Aik Bkj .

(2.12)

k=1

Two matrices, A and B are said to be conformable in a particular order if they can be
multiplied in that order. Thus if A is an r Ã— s matrix and B is a s Ã— p then A and B are
conformable in the order AB. The above formula for (AB)ij says that it equals the ith row
of A times the j th column of B.
ï£«
ï£¶
(
)
1 2
2 3 1
ï£­
ï£¸
3 1
Example 2.1.9 Multiply if possible
.
7 6 2
2 6
First check to see if this is possible. It is of the form (3 Ã— 2) (2 Ã— 3) and since the inside
numbers match, it must be possible to do this and the result should be a 3 Ã— 3 matrix. The
answer is of the form
ï£«ï£«
ï£¶
ï£«
ï£¶
ï£«
ï£¶
ï£¶
(
( )
(
)
)
1 2
1 2
1 2
ï£­ï£­ 3 1 ï£¸ 2 , ï£­ 3 1 ï£¸ 3 , ï£­ 3 1 ï£¸ 1 ï£¸
7
6
2
2 6
2 6
2 6
where the commas separate the columns in the
equals
ï£«
16 15
ï£­ 13 15
46 42

Saylor URL: http://www.saylor.org/courses/ma212/

resulting product. Thus the above product
ï£¶
5
5 ï£¸,
14

The Saylor Foundation

2.1. MATRICES

43

a 3 Ã— 3 matrix as desired. In terms of the ij th entries and the above deï¬nition, the entry in
the third row and second column of the product should equal
âˆ‘
a3k bk2 = a31 b12 + a32 b22 = 2 Ã— 3 + 6 Ã— 6 = 42.
j

You should try a few more such examples to verify the
entries works for other entries.
ï£«
ï£¶ï£«
1 2
2
Example 2.1.10 Multiply if possible ï£­ 3 1 ï£¸ ï£­ 7
2 6
0

above deï¬nition in terms of the ij th
ï£¶
1
2 ï£¸.
0

3
6
0

This is not possible because it is of the form (3 Ã— 2) (3 Ã— 3) and the middle numbers
donâ€™t match.
ï£«
ï£¶ï£«
ï£¶
2 3 1
1 2
Example 2.1.11 Multiply if possible ï£­ 7 6 2 ï£¸ ï£­ 3 1 ï£¸ .
0 0 0
2 6
This is possible because in this case it is of the form (3 Ã— 3) (3 Ã— 2) and the middle
numbers do match. When the multiplication is done it equals
ï£¶
ï£«
13 13
ï£­ 29 32 ï£¸ .
0
0
Check this and be sure you come up with the same answer.
ï£« ï£¶
1
(
)
Example 2.1.12 Multiply if possible ï£­ 2 ï£¸ 1 2 1 0 .
1
In this case you are trying to do (3 Ã— 1) (1 Ã— 4) .
do it. Verify
ï£«
ï£« ï£¶
1
(
)
ï£­ 2 ï£¸ 1 2 1 0 =ï£­
1

2.1.2

The inside numbers match so you can
1 2
2 4
1 2

1
2
1

ï£¶
0
0 ï£¸
0

Digraphs

Consider the following graph illustrated in the picture.

1

2

3
There are three locations in this graph, labelled 1,2, and 3. The directed lines represent
a way of going from one location to another. Thus there is one way to go from location 1
to location 1. There is one way to go from location 1 to location 3. It is not possible to go

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

44

MATRICES AND LINEAR TRANSFORMATIONS

from location 2 to location 3 although it is possible to go from location 3 to location 2. Lets
refer to moving along one of these directed lines as a step. The following 3 Ã— 3 matrix is
a numerical way of writing the above graph. This is sometimes called a digraph, short for
directed graph.
ï£«
ï£¶
1 1 1
ï£­ 1 0 0 ï£¸
1 1 0
Thus aij , the entry in the ith row and j th column represents the number of ways to go from
location i to location j in one step.
Problem: Find the number of ways to go from i to j using exactly k steps.
Denote the answer to the above problem by akij . We donâ€™t know what it is right now
unless k = 1 when it equals aij described above. However, if we did know what it was, we
could ï¬nd ak+1
as follows.
ij
âˆ‘
ak+1
=
akir arj
ij
r

This is because if you go from i to j in k + 1 steps, you ï¬rst go from i to r in k steps and
then for each of these ways there are arj ways to go from there to j. Thus akir arj gives
the number of ways to go from i to j in k + 1 steps such that the k th step leaves you at
location r. Adding these gives the above sum. Now you recognize this as the ij th entry of
the product of two matrices. Thus
âˆ‘
âˆ‘
a2ij =
air arj , a3ij =
a2ir arj
r

r

and so forth. From the above deï¬nition of matrix multiplication, this shows that if A is the
matrix associated with the directed graph as above, then akij is just the ij th entry of Ak
where Ak is just what you would think it should be, A multiplied by itself k times.
Thus in the above example, to ï¬nd the number of ways of going from 1 to 3 in two steps
you would take that matrix and multiply it by itself and then take the entry in the ï¬rst row
and third column. Thus
ï£«
ï£¶2 ï£«
ï£¶
1 1 1
3 2 1
ï£­ 1 0 0 ï£¸ =ï£­ 1 1 1 ï£¸
1 1 0
2 1 1
and you see there is exactly one way to go from 1 to 3 in two steps. You can easily see this
is true from looking at the graph also. Note there are three ways to go from 1 to 1 in 2
steps. Can you ï¬nd them from the graph? What would you do if you wanted to consider 5
steps?
ï£¶5 ï£«
ï£¶
ï£«
28 19 13
1 1 1
ï£­ 1 0 0 ï£¸ = ï£­ 13 9
6 ï£¸
1 1 0
19 13 9
There are 19 ways to go from 1 to 2 in ï¬ve steps. Do you think you could list them all by
looking at the graph? I donâ€™t think you could do it without wasting a lot of time.
Of course there is nothing sacred about having only three locations. Everything works
just as well with any number of locations. In general if you have n locations, you would
need to use a n Ã— n matrix.
Example 2.1.13 Consider the following directed graph.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.1. MATRICES

45

1

2
3

4

Write the matrix which is associated with this directed graph and ï¬nd the number of ways
to go from 2 to 4 in three steps.
Here you need to use a 4Ã—4 matrix. The one you need is
ï£«
ï£¶
0 1 1 0
ï£¬ 1 0 0 0 ï£·
ï£¬
ï£·
ï£­ 1 1 0 1 ï£¸
0 1 0 1
Then to ï¬nd the answer, you just need to multiply this matrix by itself three times and look
at the entry in the second row and fourth column.
ï£«
ï£¶3 ï£«
ï£¶
0 1 1 0
1 3 2 1
ï£¬ 1 0 0 0 ï£·
ï£¬ 2 1 0 1 ï£·
ï£¬
ï£·
ï£¬
ï£·
ï£­ 1 1 0 1 ï£¸ =ï£­ 3 3 1 2 ï£¸
0 1 0 1
1 2 1 1
There is exactly one way to go from 2 to 4 in three steps.
How many ways would there be of going from 2 to 4 in ï¬ve steps?
ï£¶5 ï£«
ï£¶
ï£«
0 1 1 0
5 9 5 4
ï£¬ 1 0 0 0 ï£·
ï£¬ 5 4 1 3 ï£·
ï£¬
ï£·
ï£¬
ï£·
ï£­ 1 1 0 1 ï£¸ = ï£­ 9 10 4 6 ï£¸
0 1 0 1
4 6 3 3
There are three ways. Note there are 10 ways to go from 3 to 2 in ï¬ve steps.
This is an interesting application of the concept of the ij th entry of the product matrices.

2.1.3

Properties Of Matrix Multiplication

As pointed out above, sometimes it is possible to multiply matrices in one order but not
in the other order. What if it makes sense to multiply them in either order? Will they be
equal then?
(
)(
)
(
)(
)
1 2
0 1
0 1
1 2
Example 2.1.14 Compare
and
.
3 4
1 0
1 0
3 4
The ï¬rst product is

the second product is

(

(

1 2
3 4
0 1
1 0

)(

)(

0
1

1
0

1
3

2
4

)

(
=

)

(
=

2
4

1
3

3
1

4
2

)
,
)
,

and you see these are not equal. Therefore, you cannot conclude that AB = BA for matrix
multiplication. However, there are some properties which do hold.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

46

MATRICES AND LINEAR TRANSFORMATIONS

Proposition 2.1.15 If all multiplications and additions make sense, the following hold for
matrices, A, B, C and a, b scalars.
A (aB + bC) = a (AB) + b (AC)

(2.13)

(B + C) A = BA + CA

(2.14)

A (BC) = (AB) C

(2.15)

Proof: Using the above deï¬nition of matrix multiplication,
âˆ‘
(A (aB + bC))ij =
Aik (aB + bC)kj
k

âˆ‘

=

Aik (aBkj + bCkj )

k

= a

âˆ‘

Aik Bkj + b

k

âˆ‘

Aik Ckj

k

= a (AB)ij + b (AC)ij
= (a (AB) + b (AC))ij
showing that A (B + C) = AB + AC as claimed. Formula (2.14) is entirely similar.
Consider (2.15), the associative law of multiplication. Before reading this, review the
deï¬nition of matrix multiplication in terms of entries of the matrices.
âˆ‘
Aik (BC)kj
(A (BC))ij =
k

=

âˆ‘

Aik

âˆ‘

k

=

âˆ‘

Bkl Clj

l

(AB)il Clj

l

= ((AB) C)ij .
Another important operation on matrices is that of taking the transpose. The following
example shows what is meant by this operation, denoted by placing a T as an exponent on
the matrix.
ï£¶T
ï£«
(
)
1 1 + 2i
1
3 2
ï£¸ =
ï£­ 3
1
1 + 2i 1 6
2
6
What happened? The ï¬rst column became the ï¬rst row and the second column became
the second row. Thus the 3 Ã— 2 matrix became a 2 Ã— 3 matrix. The number 3 was in the
second row and the ï¬rst column and it ended up in the ï¬rst row and second column. This
motivates the following deï¬nition of the transpose of a matrix.
Deï¬nition 2.1.16 Let A be an m Ã— n matrix. Then AT denotes the n Ã— m matrix which
is deï¬ned as follows.
( T)
A ij = Aji
The transpose of a matrix has the following important property.
Lemma 2.1.17 Let A be an m Ã— n matrix and let B be a n Ã— p matrix. Then
T

(2.16)

T

(2.17)

(AB) = B T AT
and if Î± and Î² are scalars,
(Î±A + Î²B) = Î±AT + Î²B T

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.1. MATRICES

47

Proof: From the deï¬nition,
(
)
T
(AB)

=

(AB)ji
âˆ‘
=
Ajk Bki

ij

k

âˆ‘(

=

(

=

BT

) (
ik

AT

)
kj

k

)
B T AT ij

(2.17) is left as an exercise. 
Deï¬nition 2.1.18 An n Ã— n matrix A is said to be symmetric if A = AT . It is said to be
skew symmetric if AT = âˆ’A.
Example 2.1.19 Let

ï£«

ï£¶
3
âˆ’3 ï£¸ .
7

ï£«

ï£¶
1 3
0 2 ï£¸
âˆ’2 0

2 1
A=ï£­ 1 5
3 âˆ’3
Then A is symmetric.
Example 2.1.20 Let

0
A = ï£­ âˆ’1
âˆ’3
Then A is skew symmetric.

There is a special matrix called I and deï¬ned by
Iij = Î´ ij
where Î´ ij is the Kronecker symbol deï¬ned by
{
1 if i = j
Î´ ij =
0 if i Ì¸= j
It is called the identity matrix because it is a multiplicative identity in the following sense.
Lemma 2.1.21 Suppose A is an m Ã— n matrix and In is the n Ã— n identity matrix. Then
AIn = A. If Im is the m Ã— m identity matrix, it also follows that Im A = A.
Proof:
(AIn )ij

=

âˆ‘

Aik Î´ kj

k

= Aij
and so AIn = A. The other case is left as an exercise for you.
Deï¬nition 2.1.22 An n Ã— n matrix A has an inverse Aâˆ’1 if and only if there exists a
matrix, denoted as Aâˆ’1 such that AAâˆ’1 = Aâˆ’1 A = I where I = (Î´ ij ) for
{
1 if i = j
Î´ ij â‰¡
0 if i Ì¸= j
Such a matrix is called invertible.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

48

MATRICES AND LINEAR TRANSFORMATIONS

If it acts like an inverse, then it is the inverse. This is the message of the following
proposition.
Proposition 2.1.23 Suppose AB = BA = I. Then B = Aâˆ’1 .
Proof: From the deï¬nition B is an inverse for A. Could there be another one B â€² ?
B â€² = B â€² I = B â€² (AB) = (B â€² A) B = IB = B.
Thus, the inverse, if it exists, is unique. 

2.1.4

Finding The Inverse Of A Matrix

A little later a formula is given for the inverse of a matrix. However, it is not a good way
to ï¬nd the inverse for a matrix. There is a much easier way and it is this which is presented
here. It is also important to note that not all matrices have inverses.
(
)
1 1
Example 2.1.24 Let A =
. Does A have an inverse?
1 1
One might think A would have an inverse because it does not equal zero. However,
(
)(
) (
)
1 1
âˆ’1
0
=
1 1
1
0
and if Aâˆ’1 existed, this could not happen because you could multiply on the left by the
T
T
inverse A and conclude the vector (âˆ’1, 1) = (0, 0) . Thus the answer is that A does not
have an inverse.
Suppose you want to ï¬nd B such that AB = I. Let
(
)
B = b1 Â· Â· Â· bn
Also the ith column of I is
ei =

(

0

Â·Â·Â·

0

1

0 Â·Â·Â·

0

)T

Thus, if AB = I, bi , the ith column of B must satisfy the equation Abi = ei . The augmented
matrix for ï¬nding bi is (A|ei ) . Thus, by doing row operations till A becomes I, you end up
with (I|bi ) where bi is the solution to Abi = ei . Now the same sequence of row operations
works regardless of the right side of the agumented matrix (A|ei ) and so you can save trouble
by simply doing the following.
(A|I)

row operations

â†’

(I|B)

and the ith column of B is bi , the solution to Abi = ei . Thus AB = I.
This is the reason for the following simple procedure for ï¬nding the inverse of a matrix.
This procedure is called the Gauss Jordan procedure. It produces the inverse if the matrix
has one. Actually, it produces the right inverse.
Procedure 2.1.25 Suppose A is an n Ã— n matrix. To ï¬nd Aâˆ’1 if it exists, form the
augmented n Ã— 2n matrix,
(A|I)
and then do row operations until you obtain an n Ã— 2n matrix of the form
(I|B)

(2.18)

if possible. When this has been done, B = Aâˆ’1 . The matrix A has an inverse exactly when
it is possible to do row operations and end up with one like (2.18).

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.1. MATRICES

49

As described above, the following is a description of what you have just done.
A
I

Rq Rqâˆ’1 Â·Â·Â·R1

â†’

I

â†’

B

Rq Rqâˆ’1 Â·Â·Â·R1

where those Ri sympolize row operations. It follows that you could undo what you did by
doing the inverse of these row operations in the opposite order. Thus
I
B

âˆ’1
R1âˆ’1 Â·Â·Â·Rqâˆ’1
Rqâˆ’1

â†’

âˆ’1
R1âˆ’1 Â·Â·Â·Rqâˆ’1
Rqâˆ’1

â†’

A
I

Here Râˆ’1 is the row operation which undoes the row operation R. Therefore, if you form
(B|I) and do the inverse of the row operations which produced I from A in the reverse
order, you would obtain (I|A) . By the same reasoning above, it follows that A is a right
inverse of B and so BA = I also. It follows from Proposition 2.1.23 that B = Aâˆ’1 . Thus
the procedure produces the inverse whenever it works.
row operations
If it is possible to do row operations and end up with A
â†’
I, then the above
argument shows that A has an inverse. Conversely, if A has an inverse, can it be found by
the above procedure? In this case there exists a unique solution x to the equation Ax = y.
In fact it is just x = Ix = Aâˆ’1 y. Thus in terms of augmented matrices, you would expect
to obtain
(
)
(A|y) â†’ I|Aâˆ’1 y
That is, you would expect to be able to do row operations to A and end up with I.
The details will be explained fully when a more careful discussion is given which is based
on more fundamental considerations. For now, it suï¬ƒces to observe that whenever the above
procedure works, it ï¬nds the inverse.
ï£«
ï£¶
1 0
1
Example 2.1.26 Let A = ï£­ 1 âˆ’1 1 ï£¸. Find Aâˆ’1 .
1 1 âˆ’1
Form the augmented matrix
ï£«

ï£¶
1 0
1 1 0 0
ï£­ 1 âˆ’1 1 0 1 0 ï£¸ .
1 1 âˆ’1 0 0 1

Now do row operations until the n Ã— n matrix on the left becomes the identity matrix. This
yields after some computations,
ï£«
ï£¶
1
1
1 0 0 0
2
2
ï£­ 0 1 0 1 âˆ’1
0 ï£¸
0 0 1 1 âˆ’ 12 âˆ’ 12
and so the inverse of A is the matrix on the right,
ï£«
ï£¶
1
1
0
2
2
ï£­ 1 âˆ’1
0 ï£¸.
1 âˆ’ 12 âˆ’ 12
Checking the answer is easy. Just multiply the matrices and see if
ï£¶ ï£«
ï£«
ï£¶ï£«
1
1
1 0
1
0
1 0
2
2
ï£­ 1 âˆ’1 1 ï£¸ ï£­ 1 âˆ’1
0 ï£¸=ï£­ 0 1
1 1 âˆ’1
0 0
1 âˆ’ 12 âˆ’ 12

Saylor URL: http://www.saylor.org/courses/ma212/

it works.
ï£¶
0
0 ï£¸.
1

The Saylor Foundation

50

MATRICES AND LINEAR TRANSFORMATIONS

Always check your answer because if
mistake.
ï£«
1 2
Example 2.1.27 Let A = ï£­ 1 0
3 1

you are like some of us, you will usually have made a
ï£¶
2
2 ï£¸. Find Aâˆ’1 .
âˆ’1

Set up the augmented matrix (A|I)
ï£«
1 2
ï£­ 1 0
3 1

ï£¶
1 0 0
0 1 0 ï£¸
0 0 1

2
2
âˆ’1

Next take (âˆ’1) times the ï¬rst row and add to the
row added to the last. This yields
ï£«
1 2
2
1
ï£­ 0 âˆ’2 0 âˆ’1
0 âˆ’5 âˆ’7 âˆ’3
Then take 5 times the second row and add to
ï£«
1
2
2
ï£­ 0 âˆ’10 0
0
0
14

second followed by (âˆ’3) times the ï¬rst
ï£¶
0 0
1 0 ï£¸.
0 1

âˆ’2 times the last row.
ï£¶
1 0 0
âˆ’5 5 0 ï£¸
1 5 âˆ’2

Next take the last row and add to (âˆ’7) times the top row. This yields
ï£¶
ï£«
âˆ’7 âˆ’14 0 âˆ’6 5 âˆ’2
ï£­ 0 âˆ’10 0 âˆ’5 5 0 ï£¸ .
0
0
14 1 5 âˆ’2
Now take (âˆ’7/5) times the second row and add to
ï£«
âˆ’7
0
0
1
ï£­ 0 âˆ’10 0 âˆ’5
0
0
14 1

the top.
ï£¶
âˆ’2 âˆ’2
5
0 ï£¸.
5 âˆ’2

Finally divide the top row by âˆ’7,
yields
ï£«
1
ï£­ 0
0

the second row by -10 and the bottom row by 14 which

Therefore, the inverse is

ï£«

0 0 âˆ’ 17
1
1 0
2
1
0 1 14
ï£­

ï£«

1 2
Example 2.1.28 Let A = ï£­ 1 0
2 2

âˆ’ 17
1
2
1
14

2
7
âˆ’ 12
5
14

2
7
âˆ’ 21
5
14
2
7

2
7

ï£¶

0 ï£¸.
âˆ’ 17
ï£¶

0 ï£¸
âˆ’ 17

ï£¶
2
2 ï£¸. Find Aâˆ’1 .
4

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.2. EXERCISES
Write the augmented matrix (A|I)
ï£«
1 2
ï£­ 1 0
2 2

51

ï£¶
0
0 ï£¸
1
(
)
and proceed to do row operations attempting to obtain I|Aâˆ’1 . Take (âˆ’1) times the top
row and add to the second. Then take (âˆ’2) times the top row and add to the bottom.
ï£«
ï£¶
1 2 2 1 0 0
ï£­ 0 âˆ’2 0 âˆ’1 1 0 ï£¸
0 âˆ’2 0 âˆ’2 0 1
2
2
4

1
0
0

0
1
0

Next add (âˆ’1) times the second row to the bottom row.
ï£«
ï£¶
1 2 2 1
0 0
ï£­ 0 âˆ’2 0 âˆ’1 1 0 ï£¸
0 0 0 âˆ’1 âˆ’1 1
At this point, you can see there will be no inverse because you have obtained a row of zeros
in the left half of the augmented matrix (A|I) . Thus there will be no way to obtain I on
the left. In other words, the three systems of equations you must solve to ï¬nd the inverse
have no solution. In particular, there is no solution for the ï¬rst column of Aâˆ’1 which must
solve
ï£¶
ï£¶ ï£«
ï£«
1
x
Aï£­ y ï£¸ = ï£­ 0 ï£¸
0
z
because a sequence of row operations leads to the impossible equation, 0x + 0y + 0z = âˆ’1.

2.2

Exercises

1. In (2.1) - (2.8) describe âˆ’A and 0.
2. Let A be an nÃ—n matrix. Show A equals the sum of a symmetric and a skew symmetric
matrix.
3. Show every skew symmetric matrix has all zeros down the main diagonal. The main
diagonal consists of every entry of the matrix which is of the form aii . It runs from
the upper left down to the lower right.
4. Using only the properties (2.1) - (2.8) show âˆ’A is unique.
5. Using only the properties (2.1) - (2.8) show 0 is unique.
6. Using only the properties (2.1) - (2.8) show 0A = 0. Here the 0 on the left is the scalar
0 and the 0 on the right is the zero for m Ã— n matrices.
7. Using only the properties (2.1) - (2.8) and previous problems show (âˆ’1) A = âˆ’A.
8. Prove (2.17).
9. Prove that Im A = A where A is an m Ã— n matrix.
n
10. (Let A and
y âˆˆ Rm . Show (Ax, y)Rm =
) be a real m Ã— n matrix and let x âˆˆ R and
k
T
x,A y Rn where (Â·, Â·)Rk denotes the dot product in R .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

52

MATRICES AND LINEAR TRANSFORMATIONS
T

11. Use the result of Problem 10 to verify directly that (AB) = B T AT without making
any reference to subscripts.
12. Let x = (âˆ’1, âˆ’1, 1) and y = (0, 1, 2) . Find xT y and xyT if possible.
13. Give an example of matrices, A, B, C such that B Ì¸= C, A Ì¸= 0, and yet AB = AC.
ï£«
ï£¶
ï£«
ï£¶
(
)
1
1
1
1 âˆ’3
1 âˆ’1 âˆ’2
0 ï£¸ . Find
14. Let A = ï£­ âˆ’2 âˆ’1 ï£¸, B =
, and C = ï£­ âˆ’1 2
2 1 âˆ’2
1
2
âˆ’3 âˆ’1 0
if possible the following products. AB, BA, AC, CA, CB, BC.
15. Consider the following digraph.

1

2

3

4

Write the matrix associated with this digraph and ï¬nd the number of ways to go from
3 to 4 in three steps.
16. Show that if Aâˆ’1 exists for an n Ã— n matrix, then it is unique. That is, if BA = I and
AB = I, then B = Aâˆ’1 .
âˆ’1

17. Show (AB)

= B âˆ’1 Aâˆ’1 .

( )âˆ’1 ( âˆ’1 )T
18. Show that if A is an invertible n Ã— n matrix, then so is AT and AT
= A
.
19. Show that if A is an n Ã— n invertible matrix and x is a n Ã— 1 matrix such that Ax = b
for b an n Ã— 1 matrix, then x = Aâˆ’1 b.
20. Give an example of a matrix A such that A2 = I and yet A Ì¸= I and A Ì¸= âˆ’I.
21. Give an example of matrices, A, B such that neither A nor B equals zero and yet
AB = 0.
ï£«
ï£¶
ï£«
ï£¶
x1 âˆ’ x2 + 2x3
x1
ï£¬
ï£·
ï£¬
ï£·
2x3 + x1
ï£· in the form A ï£¬ x2 ï£· where A is an appropriate matrix.
22. Write ï£¬
ï£­
ï£¸
ï£­
3x3
x3 ï£¸
3x4 + 3x2 + x1
x4
23. Give another example other than the one given in this section of two square matrices,
A and B such that AB Ì¸= BA.
24. Suppose A and B are square matrices of the same size. Which of the following are
correct?
2

(a) (A âˆ’ B) = A2 âˆ’ 2AB + B 2
2

(b) (AB) = A2 B 2
2

(c) (A + B) = A2 + 2AB + B 2
2

(d) (A + B) = A2 + AB + BA + B 2

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.3. LINEAR TRANSFORMATIONS

53

(e) A2 B 2 = A (AB) B
3

(f) (A + B) = A3 + 3A2 B + 3AB 2 + B 3
(g) (A + B) (A âˆ’ B) = A2 âˆ’ B 2
(h) None of the above. They are all wrong.
(i) All of the above. They are all right.
(
)
âˆ’1 âˆ’1
25. Let A =
. Find all 2 Ã— 2 matrices, B such that AB = 0.
3
3
26. Prove that if Aâˆ’1 exists and Ax = 0 then x = 0.
ï£«

27. Let

1
A=ï£­ 2
1

2
1
0

ï£¶
3
4 ï£¸.
2

Find Aâˆ’1 if possible. If Aâˆ’1 does not exist, determine why.
ï£«

28. Let

1
A=ï£­ 2
1

0
3
0

ï£¶
3
4 ï£¸.
2

Find Aâˆ’1 if possible. If Aâˆ’1 does not exist, determine why.
29. Let

ï£«

1
A=ï£­ 2
4

2
1
5

ï£¶
3
4 ï£¸.
10

Find Aâˆ’1 if possible. If Aâˆ’1 does not exist, determine why.
30. Let

ï£«

1
ï£¬ 1
A=ï£¬
ï£­ 2
1

2
1
1
2

0
2
âˆ’3
1

ï£¶
2
0 ï£·
ï£·
2 ï£¸
2

Find Aâˆ’1 if possible. If Aâˆ’1 does not exist, determine why.

2.3

Linear Transformations

By (2.13), if A is an m Ã— n matrix, then for v, u vectors in Fn and a, b scalars,
ï£« âˆˆFn ï£¶
z }| {
A ï£­au + bvï£¸ = aAu + bAv âˆˆ Fm

(2.19)

Deï¬nition 2.3.1 A function, A : Fn â†’ Fm is called a linear transformation if for all
u, v âˆˆ Fn and a, b scalars, (2.19) holds.
From (2.19), matrix multiplication deï¬nes a linear transformation as just deï¬ned. It
turns out this is the only type of linear transformation available. Thus if A is a linear
transformation from Fn to Fm , there is always a matrix which produces A. Before showing
this, here is a simple deï¬nition.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

54

MATRICES AND LINEAR TRANSFORMATIONS

Deï¬nition 2.3.2 A vector, ei âˆˆ Fn is deï¬ned as follows:
ï£« ï£¶
0
ï£¬ .. ï£·
ï£¬ . ï£·
ï£¬ ï£·
ï£·
ei â‰¡ ï£¬
ï£¬ 1 ï£·,
ï£¬ . ï£·
ï£­ .. ï£¸
0
where the 1 is in the ith position and there are zeros everywhere else. Thus
T

ei = (0, Â· Â· Â· , 0, 1, 0, Â· Â· Â· , 0) .
Of course the ei for a particular value of i in Fn would be diï¬€erent than the ei for that
same value of i in Fm for m Ì¸= n. One of them is longer than the other. However, which one
is meant will be determined by the context in which they occur.
These vectors have a signiï¬cant property.
Lemma 2.3.3 Let v âˆˆ Fn . Thus v is a list of numbers arranged vertically, v1 , Â· Â· Â· , vn . Then
eTi v = vi .

(2.20)

Also, if A is an m Ã— n matrix, then letting ei âˆˆ Fm and ej âˆˆ Fn ,
eTi Aej = Aij

(2.21)

Proof: First note that eTi is a 1 Ã— n matrix and v is an n Ã— 1 matrix so the above
multiplication in (2.20) makes perfect sense. It equals
ï£«
ï£¶
v1
ï£¬ .. ï£·
ï£¬ . ï£·
ï£¬
ï£·
ï£·
(0, Â· Â· Â· , 1, Â· Â· Â· 0) ï£¬
ï£¬ vi ï£· = vi
ï£¬ . ï£·
ï£­ .. ï£¸
vn
as claimed.
Consider (2.21). From the deï¬nition of matrix multiplication, and noting that (ej )k =
Î´ kj
ï£« âˆ‘
ï£¶
ï£«
ï£¶
A1j
k A1k (ej )k
ï£¬
ï£·
ï£¬ .. ï£·
..
ï£¬
ï£·
ï£¬ . ï£·
.
ï£¬
ï£·
ï£¬
ï£·
âˆ‘
T ï£¬
ï£·
ï£·
A
(e
)
eTi Aej = eTi ï£¬
=
e
j k ï£·
i ï£¬ Aij ï£· = Aij
k ik
ï£¬
ï£¬
ï£·
ï£¬ . ï£·
..
ï£­
ï£¸
ï£­ .. ï£¸
.
âˆ‘
Amj
k Amk (ej )k
by the ï¬rst part of the lemma. 
Theorem 2.3.4 Let L : Fn â†’ Fm be a linear transformation. Then there exists a unique
m Ã— n matrix A such that
Ax = Lx
for all x âˆˆ Fn . The ik th entry of this matrix is given by
eTi Lek

(2.22)

Stated in another way, the k th column of A equals Lek .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.3. LINEAR TRANSFORMATIONS

55

Proof: By the lemma,
(
)
(Lx)i = eTi Lx = eTi xk Lek = eTi Lek xk .
Let Aik = eTi Lek , to prove the existence part of the theorem.
To verify uniqueness, suppose Bx = Ax = Lx for all x âˆˆ Fn . Then in particular, this is
true for x = ej and then multiply on the left by eTi to obtain
Bij = eTi Bej = eTi Aej = Aij
showing A = B. 
Corollary 2.3.5 A linear transformation, L : Fn â†’ Fm is completely determined by the
vectors {Le1 , Â· Â· Â· , Len } .
Proof: This follows immediately from the above theorem. The unique matrix determining the linear transformation which is given in (2.22) depends only on these vectors.

This theorem shows that any linear transformation deï¬ned on Fn can always be considered as a matrix. Therefore, the terms â€œlinear transformationâ€ and â€œmatrixâ€ are often
used interchangeably. For example, to say that a matrix is one to one, means the linear
transformation determined by the matrix is one to one.
2
2
Example
Find the (linear
( 2.3.6
)
) transformation, L : R â†’ R which has the property that
2
1
Le1 =
and Le2 =
. From the above theorem and corollary, this linear trans1
3
formation is that determined by matrix multiplication by the matrix
(
)
2 1
.
1 3

Deï¬nition 2.3.7 Let L : Fn â†’ Fm be a linear transformation and let its matrix be the
m Ã— n matrix A. Then ker (L) â‰¡ {x âˆˆ Fn : Lx = 0} . Sometimes people also write this as
N (A) , the null space of A.
Then there is a fundamental result in the case where m < n. In this case, the matrix A
of the linear transformation looks like the following.

Theorem 2.3.8 Let A be an m Ã— n matrix where m < n. Then N (A) contains nonzero
vectors.
Proof: First consider the case where A is a 1 Ã— n matrix for n > 1. Say
(
)
A = a1 Â· Â· Â· an
If a1 = 0, consider the vector x = e1 . If a1 Ì¸= 0, let
ï£« ï£¶
b
ï£¬ 1 ï£·
ï£¬ ï£·
x =ï£¬ . ï£·
ï£­ .. ï£¸
1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

56

MATRICES AND LINEAR TRANSFORMATIONS

where b is chosen to satisfy the equation
a1 b +

n
âˆ‘

ak = 0

k=2

Suppose now that the theorem is true for any m Ã— n matrix with n > m and consider an
(m Ã— 1) Ã— n matrix A where n > m + 1. If the ï¬rst column of A is 0, then you could let
x = e1 as above. If the ï¬rst column is not the zero vector, then by doing row operations,
the equation Ax = 0 can be reduced to the equivalent system
A1 x = 0
where A1 is of the form

(
A1 =

1
0

aT
B

)

where B is an m Ã— (n âˆ’ 1) matrix. Since n > m + 1, it follows that (n âˆ’ 1) > m and so
by induction, there exists a nonzero vector y âˆˆ Fnâˆ’1 such that By = 0. Then consider the
vector
(
)
b
x=
y
ï£« T ï£¶
b1
ï£¬
ï£·
A1 x has for its top entry the expression b + aT y. Letting B = ï£­ ... ï£¸ , the ith entry of
bTm
A1 x for i > 1 is of the form bTi y = 0. Thus if b is chosen to satisfy the equation b+aT y = 0,
then A1 x = 0.

2.4

Subspaces And Spans

Deï¬nition 2.4.1 Let {x1 , Â· Â· Â· , xp } be vectors in Fn . A linear combination is any expression
of the form
p
âˆ‘
ci xi
i=1

where the ci are scalars. The set of all linear combinations of these vectors is called
span (x1 , Â· Â· Â· , xn ) . If V âŠ† Fn , then V is called a subspace if whenever Î±, Î² are scalars
and u and v are vectors of V, it follows Î±u + Î²v âˆˆ V . That is, it is â€œclosed under the
algebraic operations of vector addition and scalar multiplicationâ€. A linear combination
of vectors is said to be trivial if all the scalars in the linear combination equal zero. A set
of vectors is said to be linearly independent if the only linear combination of these vectors
which equals the zero vector is the trivial linear combination. Thus {x1 , Â· Â· Â· , xn } is called
linearly independent if whenever
p
âˆ‘
ck xk = 0
k=1

it follows that all the scalars ck equal zero. A set of vectors, {x1 , Â· Â· Â· , xp } , is called linearly
dependent if it is not linearly independent. Thus the setâˆ‘
of vectors is linearly dependent if
p
there exist scalars ci , i = 1, Â· Â· Â· , n, not all zero such that k=1 ck xk = 0.
Proposition 2.4.2 Let V âŠ† Fn . Then V is a subspace if and only if it is a vector space
itself with respect to the same operations of scalar multiplication and vector addition.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.4. SUBSPACES AND SPANS

57

Proof: Suppose ï¬rst that V is a subspace. All algebraic properties involving scalar
multiplication and vector addition hold for V because these things hold for Fn . Is 0 âˆˆ V ? Yes
it is. This is because 0v âˆˆ V and 0v = 0. By assumption, for Î± a scalar and v âˆˆ V, Î±v âˆˆ V.
Therefore, âˆ’v = (âˆ’1) v âˆˆ V . Thus V has the additive identity and additive inverse. By
assumption, V is closed with respect to the two operations. Thus V is a vector space. If
V âŠ† Fn is a vector space, then by deï¬nition, if Î±, Î² are scalars and u, v vectors in V, it
follows that Î±v + Î²u âˆˆ V . 
Thus, from the above, subspaces of Fn are just subsets of Fn which are themselves vector
spaces.
Lemma 2.4.3 A set of vectors {x1 , Â· Â· Â· , xp } is linearly independent if and only if none of
the vectors can be obtained as a linear combination of the others.
âˆ‘
Proof: Suppose ï¬rst that {x1 , Â· Â· Â· , xp } is linearly independent. If xk = jÌ¸=k cj xj , then
0 = 1xk +

âˆ‘

(âˆ’cj ) xj ,

jÌ¸=k

a nontrivial linear combination, contrary to assumption. This shows that if the set is linearly
independent, then none of the vectors is a linear combination of the others.
Now suppose no vector is a linear combination of the others. Is {x1 , Â· Â· Â· , xp } linearly
independent? If it is not, there exist scalars ci , not all zero such that
p
âˆ‘

ci xi = 0.

i=1

Say ck Ì¸= 0. Then you can solve for xk as
âˆ‘
xk =
(âˆ’cj ) /ck xj
jÌ¸=k

contrary to assumption. 
The following is called the exchange theorem.
Theorem 2.4.4 (Exchange Theorem) Let {x1 , Â· Â· Â· , xr } be a linearly independent set of vectors such that each xi is in span(y1 , Â· Â· Â· , ys ) . Then r â‰¤ s.
Proof 1: Suppose not. Then r > s. By assumption, there exist scalars aji such that
xi =

s
âˆ‘

aji yj

j=1

The matrix whose jith entry is aji has more columns than rows. Therefore, by Theorem
2.3.8 there exists a nonzero vector b âˆˆ Fr such that Ab = 0. Thus
0=

r
âˆ‘

aji bi , each j.

i=1

Then

r
âˆ‘
i=1

bi xi =

r
âˆ‘
i=1

bi

s
âˆ‘

aji yj =

j=1

( r
s
âˆ‘
âˆ‘
j=1

)
aji bi

yj = 0

i=1

contradicting the assumption that {x1 , Â· Â· Â· , xr } is linearly independent.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

58
Proof 2:
that

MATRICES AND LINEAR TRANSFORMATIONS

Deï¬ne span{y1 , Â· Â· Â· , ys } â‰¡ V, it follows there exist scalars c1 , Â· Â· Â· , cs such
x1 =

s
âˆ‘

ci yi .

(2.23)

i=1

Not all of these scalars can equal zero because if this were the case, it would follow that
x
âˆ‘1 r= 0 and so {x1 , Â· Â· Â· , xr } would not be linearly independent. Indeed, if x1 = 0, 1x1 +
i=2 0xi = x1 = 0 and so there would exist a nontrivial linear combination of the vectors
{x1 , Â· Â· Â· , xr } which equals zero.
Say ck Ì¸= 0. Then solve ((2.23)) for yk and obtain
ï£«
ï£¶
s-1 vectors here
z
}|
{
yk âˆˆ span ï£­x1 , y1 , Â· Â· Â· , ykâˆ’1 , yk+1 , Â· Â· Â· , ys ï£¸ .
Deï¬ne {z1 , Â· Â· Â· , zsâˆ’1 } by
{z1 , Â· Â· Â· , zsâˆ’1 } â‰¡ {y1 , Â· Â· Â· , ykâˆ’1 , yk+1 , Â· Â· Â· , ys }
Therefore, span {x1 , z1 , Â· Â· Â· , zsâˆ’1 } = V because if v âˆˆ V, there exist constants c1 , Â· Â· Â· , cs
such that
sâˆ’1
âˆ‘
v=
ci zi + cs yk .
i=1

Now replace the yk in the above with a linear combination of the vectors, {x1 , z1 , Â· Â· Â· , zsâˆ’1 }
to obtain v âˆˆ span {x1 , z1 , Â· Â· Â· , zsâˆ’1 } . The vector yk , in the list {y1 , Â· Â· Â· , ys } , has now been
replaced with the vector x1 and the resulting modiï¬ed list of vectors has the same span as
the original list of vectors, {y1 , Â· Â· Â· , ys } .
Now suppose that r > s and that span {x1 , Â· Â· Â· , xl , z1 , Â· Â· Â· , zp } = V where the vectors,
z1 , Â· Â· Â· , zp are each taken from the set, {y1 , Â· Â· Â· , ys } and l + p = s. This has now been done
for l = 1 above. Then since r > s, it follows that l â‰¤ s < r and so l + 1 â‰¤ r. Therefore, xl+1
is a vector not in the list, {x1 , Â· Â· Â· , xl } and since span {x1 , Â· Â· Â· , xl , z1 , Â· Â· Â· , zp } = V, there
exist scalars ci and dj such that
xl+1 =

l
âˆ‘
i=1

ci xi +

p
âˆ‘

dj zj .

(2.24)

j=1

Now not all the dj can equal zero because if this were so, it would follow that {x1 , Â· Â· Â· , xr }
would be a linearly dependent set because one of the vectors would equal a linear combination
of the others. Therefore, ((2.24)) can be solved for one of the zi , say zk , in terms of xl+1
and the other zi and just as in the above argument, replace that zi with xl+1 to obtain
ï£±
ï£¼
p-1 vectors here
ï£²
z
}|
{ï£½
span x1 , Â· Â· Â· xl , xl+1 , z1 , Â· Â· Â· zkâˆ’1 , zk+1 , Â· Â· Â· , zp = V.
ï£³
ï£¾
Continue this way, eventually obtaining
span {x1 , Â· Â· Â· , xs } = V.
But then xr âˆˆ span {x1 , Â· Â· Â· , xs } contrary to the assumption that {x1 , Â· Â· Â· , xr } is linearly
independent. Therefore, r â‰¤ s as claimed.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.4. SUBSPACES AND SPANS

59

Proof 3: Suppose r > s. Let zk denote a vector of {y1 , Â· Â· Â· , ys } . Thus there exists j as
small as possible such that
span (y1 , Â· Â· Â· , ys ) = span (x1 , Â· Â· Â· , xm , z1 , Â· Â· Â· , zj )
where m + j = s. It is given that m = 0, corresponding to no vectors of {x1 , Â· Â· Â· , xm } and
j = s, corresponding to all the yk results in the above equation holding. If j > 0 then m < s
and so
j
m
âˆ‘
âˆ‘
xm+1 =
ak xk +
bi zi
i=1

k=1

Not all the bi can equal 0 and so you can solve for one of them in terms of xm+1 , xm , Â· Â· Â· , x1 ,
and the other zk . Therefore, there exists
{z1 , Â· Â· Â· , zjâˆ’1 } âŠ† {y1 , Â· Â· Â· , ys }
such that
span (y1 , Â· Â· Â· , ys ) = span (x1 , Â· Â· Â· , xm+1 , z1 , Â· Â· Â· , zjâˆ’1 )
contradicting the choice of j. Hence j = 0 and
span (y1 , Â· Â· Â· , ys ) = span (x1 , Â· Â· Â· , xs )
It follows that
xs+1 âˆˆ span (x1 , Â· Â· Â· , xs )
contrary to the assumption the xk are linearly independent. Therefore, r â‰¤ s as claimed. 
Deï¬nition 2.4.5 A ï¬nite set of vectors, {x1 , Â· Â· Â· , xr } is a basis for Fn if span (x1 , Â· Â· Â· , xr ) =
Fn and {x1 , Â· Â· Â· , xr } is linearly independent.
Corollary 2.4.6 Let {x1 , Â· Â· Â· , xr } and {y1 , Â· Â· Â· , ys } be two bases1 of Fn . Then r = s = n.
Proof: From the exchange theorem, r â‰¤ s and s â‰¤ r. Now note the vectors,
1 is in the ith slot

z
}|
{
ei = (0, Â· Â· Â· , 0, 1, 0 Â· Â· Â· , 0)
for i = 1, 2, Â· Â· Â· , n are a basis for Fn . 
Lemma 2.4.7 Let {v1 , Â· Â· Â· , vr } be a set of vectors. Then V â‰¡ span (v1 , Â· Â· Â· , vr ) is a subspace.
âˆ‘r
âˆ‘r
Proof: Suppose Î±, Î² are two scalars and let k=1 ck vk and k=1 dk vk are two elements
of V. What about
r
r
âˆ‘
âˆ‘
Î±
ck vk + Î²
dk vk ?
k=1

Is it also in V ?
Î±

r
âˆ‘
k=1

ck vk + Î²

r
âˆ‘

k=1

dk vk =

k=1

r
âˆ‘

(Î±ck + Î²dk ) vk âˆˆ V

k=1

so the answer is yes. 
1 This is the plural form of basis. We could say basiss but it would involve an inordinate amount of
hissing as in â€œThe sixth shiekâ€™s sixth sheep is sickâ€. This is the reason that bases is used instead of basiss.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

60

MATRICES AND LINEAR TRANSFORMATIONS

Deï¬nition 2.4.8 A ï¬nite set of vectors, {x1 , Â· Â· Â· , xr } is a basis for a subspace V of Fn if
span (x1 , Â· Â· Â· , xr ) = V and {x1 , Â· Â· Â· , xr } is linearly independent.
Corollary 2.4.9 Let {x1 , Â· Â· Â· , xr } and {y1 , Â· Â· Â· , ys } be two bases for V . Then r = s.
Proof: From the exchange theorem, r â‰¤ s and s â‰¤ r. 
Deï¬nition 2.4.10 Let V be a subspace of Fn . Then dim (V ) read as the dimension of V
is the number of vectors in a basis.
Of course you should wonder right now whether an arbitrary subspace even has a basis.
In fact it does and this is in the next theorem. First, here is an interesting lemma.
Lemma 2.4.11 Suppose v âˆˆ
/ span (u1 , Â· Â· Â· , uk ) and {u1 , Â· Â· Â· , uk } is linearly independent.
Then {u1 , Â· Â· Â· , uk , v} is also linearly independent.
âˆ‘k
Proof: Suppose
i=1 ci ui + dv = 0. It is required to verify that each ci = 0 and
that d = 0. But if d Ì¸= 0, then you can solve for v as a linear combination of the vectors,
{u1 , Â· Â· Â· , uk },
k ( )
âˆ‘
ci
v=âˆ’
ui
d
i=1
âˆ‘k
contrary to assumption. Therefore, d = 0. But then i=1 ci ui = 0 and the linear independence of {u1 , Â· Â· Â· , uk } implies each ci = 0 also. 
Theorem 2.4.12 Let V be a nonzero subspace of Fn . Then V has a basis.
Proof: Let v1 âˆˆ V where v1 Ì¸= 0. If span {v1 } = V, stop. {v1 } is a basis for V .
Otherwise, there exists v2 âˆˆ V which is not in span {v1 } . By Lemma 2.4.11 {v1 , v2 } is a
linearly independent set of vectors. If span {v1 , v2 } = V stop, {v1 , v2 } is a basis for V. If
span {v1 , v2 } Ì¸= V, then there exists v3 âˆˆ
/ span {v1 , v2 } and {v1 , v2 , v3 } is a larger linearly
independent set of vectors. Continuing this way, the process must stop before n + 1 steps
because if not, it would be possible to obtain n + 1 linearly independent vectors contrary to
the exchange theorem. 
In words the following corollary states that any linearly independent set of vectors can
be enlarged to form a basis.
Corollary 2.4.13 Let V be a subspace of Fn and let {v1 , Â· Â· Â· , vr } be a linearly independent
set of vectors in V . Then either it is a basis for V or there exist vectors, vr+1 , Â· Â· Â· , vs such
that {v1 , Â· Â· Â· , vr , vr+1 , Â· Â· Â· , vs } is a basis for V.
Proof: This follows immediately from the proof of Theorem 2.4.12. You do exactly the
same argument except you start with {v1 , Â· Â· Â· , vr } rather than {v1 }. 
It is also true that any spanning set of vectors can be restricted to obtain a basis.
Theorem 2.4.14 Let V be a subspace of Fn and suppose span (u1 Â· Â· Â· , up ) = V where
the ui are nonzero vectors. Then there exist vectors {v1 Â· Â· Â· , vr } such that {v1 Â· Â· Â· , vr } âŠ†
{u1 Â· Â· Â· , up } and {v1 Â· Â· Â· , vr } is a basis for V .
Proof: Let r be the smallest positive integer with the property that for some set
{v1 Â· Â· Â· , vr } âŠ† {u1 Â· Â· Â· , up } ,
span (v1 Â· Â· Â· , vr ) = V.
Then r â‰¤ p and it must be the case that {v1 Â· Â· Â· , vr } is linearly independent because if it
were not so, one of the vectors, say vk would be a linear combination of the others. But
then you could delete this vector from {v1 Â· Â· Â· , vr } and the resulting list of r âˆ’ 1 vectors
would still span V contrary to the deï¬nition of r. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.5. AN APPLICATION TO MATRICES

2.5

61

An Application To Matrices

The following is a theorem of major signiï¬cance.
Theorem 2.5.1 Suppose A is an n Ã— n matrix. Then A is one to one (injective) if and
only if A is onto (surjective). Also, if B is an n Ã— n matrix and AB = I, then it follows
BA = I.
Proof: First suppose A is one to one. Consider the vectors, {Ae1 , Â· Â· Â· , Aen } where ek
is the column vector which is all zeros except for a 1 in the k th position. This set of vectors
is linearly independent because if
n
âˆ‘
ck Aek = 0,
k=1

then since A is linear,

(
A

n
âˆ‘

)
ck ek

=0

k=1

and since A is one to one, it follows
n
âˆ‘

ck ek = 0

k=1

which implies each ck = 0 because the ek are clearly linearly independent.
Therefore, {Ae1 , Â· Â· Â· , Aen } must be a basis for Fn because if not there would exist a
vector, y âˆˆ
/ span (Ae1 , Â· Â· Â· , Aen ) and then by Lemma 2.4.11, {Ae1 , Â· Â· Â· , Aen , y} would be
an independent set of vectors having n + 1 vectors in it, contrary to the exchange theorem.
It follows that for y âˆˆ Fn there exist constants, ci such that
)
( n
n
âˆ‘
âˆ‘
ck ek
y=
ck Aek = A
k=1

k=1

showing that, since y was arbitrary, A is onto.
Next suppose A is onto. This means the span of the columns of A equals Fn . If these
columns are not linearly independent, then by Lemma 2.4.3 on Page 57, one of the columns
is a linear combination of the others and so the span of the columns of A equals the span of
the n âˆ’ 1 other columns. This violates the exchange theorem because {e1 , Â· Â· Â· , en } would be
a linearly independent set of vectors contained in the span of only n âˆ’ 1 vectors. Therefore,
the columns of A must be independent and this is equivalent to saying that Ax = 0 if and
only if x = 0. This implies A is one to one because if Ax = Ay, then A (x âˆ’ y) = 0 and so
x âˆ’ y = 0.
Now suppose AB = I. Why is BA = I? Since AB = I it follows B is one to one since
otherwise, there would exist, x Ì¸= 0 such that Bx = 0 and then ABx = A0 = 0 Ì¸= Ix.
Therefore, from what was just shown, B is also onto. In addition to this, A must be one
to one because if Ay = 0, then y = Bx for some x and then x = ABx = Ay = 0 showing
y = 0. Now from what is given to be so, it follows (AB) A = A and so using the associative
law for matrix multiplication,
A (BA) âˆ’ A = A (BA âˆ’ I) = 0.
But this means (BA âˆ’ I) x = 0 for all x since otherwise, A would not be one to one. Hence
BA = I as claimed. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

62

MATRICES AND LINEAR TRANSFORMATIONS

This theorem shows that if an n Ã— n matrix B acts like an inverse when multiplied on
one side of A, it follows that B = Aâˆ’1 and it will act like an inverse on both sides of A.
The conclusion of this theorem pertains to square matrices only. For example, let
ï£«
ï£¶
(
)
1 0
1 0 0
ï£­
ï£¸
0 1
A=
, B=
(2.25)
1 1 âˆ’1
1 0
(

Then
BA =
ï£«

but

1
AB = ï£­ 1
1

2.6

)

1
0

0
1

0
1
0

ï£¶
0
âˆ’1 ï£¸ .
0

Matrices And Calculus

The study of moving coordinate systems gives a non trivial example of the usefulness of the
ideas involving linear transformations and matrices. To begin with, here is the concept of
the product rule extended to matrix multiplication.
Deï¬nition 2.6.1 Let A (t) be an m Ã— n matrix. Say A (t) = (Aij( (t)) . Suppose
also that
)
Aij (t) is a diï¬€erentiable function for all i, j. Then deï¬ne Aâ€² (t) â‰¡ Aâ€²ij (t) . That is, Aâ€² (t)
is the matrix which consists of replacing each entry by its derivative. Such an m Ã— n matrix
in which the entries are diï¬€erentiable functions is called a diï¬€erentiable matrix.
The next lemma is just a version of the product rule.
Lemma 2.6.2 Let A (t) be an m Ã— n matrix and let B (t) be an n Ã— p matrix with the
property that all the entries of these matrices are diï¬€erentiable functions. Then
â€²

(A (t) B (t)) = Aâ€² (t) B (t) + A (t) B â€² (t) .
Proof: This is like the usual proof.
1
(A (t + h) B (t + h) âˆ’ A (t) B (t)) =
h
1
1
(A (t + h) B (t + h) âˆ’ A (t + h) B (t)) + (A (t + h) B (t) âˆ’ A (t) B (t))
h
h
B (t + h) âˆ’ B (t) A (t + h) âˆ’ A (t)
= A (t + h)
+
B (t)
h
h
and now, using the fact that the entries of the matrices are all diï¬€erentiable, one can pass
to a limit in both sides as h â†’ 0 and conclude that
â€²

(A (t) B (t)) = Aâ€² (t) B (t) + A (t) B â€² (t) 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.6. MATRICES AND CALCULUS

2.6.1

63

The Coriolis Acceleration

Imagine a point on the surface of the earth. Now consider unit vectors, one pointing South,
one pointing East and one pointing directly away from the center of the earth.

k


j
j
i

Denote the ï¬rst as i, the second as j, and the third as k. If you are standing on the earth
you will consider these vectors as ï¬xed, but of course they are not. As the earth turns, they
change direction and so each is in reality a function of t. Nevertheless, it is with respect
to these apparently ï¬xed vectors that you wish to understand acceleration, velocities, and
displacements.
In general, let iâˆ— , jâˆ— , kâˆ— be the usual ï¬xed vectors in space and let i (t) , j (t) , k (t) be an
orthonormal basis of vectors for each t, like the vectors described in the ï¬rst paragraph.
It is assumed these vectors are C 1 functions of t. Letting the positive x axis extend in the
direction of i (t) , the positive y axis extend in the direction of j (t), and the positive z axis
extend in the direction of k (t) , yields a moving coordinate system. Now let u be a vector
and let t0 be some reference time. For example you could let t0 = 0. Then deï¬ne the
components of u with respect to these vectors, i, j, k at time t0 as
u â‰¡ u1 i (t0 ) + u2 j (t0 ) + u3 k (t0 ) .
Let u (t) be deï¬ned as the vector which has the same components with respect to i, j, k but
at time t. Thus
u (t) â‰¡ u1 i (t) + u2 j (t) + u3 k (t) .
and the vector has changed although the components have not.
This is exactly the situation in the case of the apparently ï¬xed basis vectors on the earth
if u is a position vector from the given spot on the earthâ€™s surface to a point regarded as
ï¬xed with the earth due to its keeping the same coordinates relative to the coordinate axes
which are ï¬xed with the earth. Now deï¬ne a linear transformation Q (t) mapping R3 to R3
by
Q (t) u â‰¡ u1 i (t) + u2 j (t) + u3 k (t)
where
u â‰¡ u1 i (t0 ) + u2 j (t0 ) + u3 k (t0 )
Thus letting v be a vector deï¬ned in the same manner as u and Î±, Î², scalars,
(
)
(
)
(
)
Q (t) (Î±u + Î²v) â‰¡ Î±u1 + Î²v 1 i (t) + Î±u2 + Î²v 2 j (t) + Î±u3 + Î²v 3 k (t)
) (
)
Î±u1 i (t) + Î±u2 j (t) + Î±u3 k (t) + Î²v 1 i (t) + Î²v 2 j (t) + Î²v 3 k (t)
(
)
(
)
= Î± u1 i (t) + u2 j (t) + u3 k (t) + Î² v 1 i (t) + v 2 j (t) + v 3 k (t)

=

(

â‰¡ Î±Q (t) u + Î²Q (t) v

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

64

MATRICES AND LINEAR TRANSFORMATIONS

showing that Q (t) is a linear transformation. Also, Q (t) preserves all distances because,
since the vectors, i (t) , j (t) , k (t) form an orthonormal set,
(
|Q (t) u| =

3
âˆ‘
( i )2
u

)1/2
= |u| .

i=1

Lemma 2.6.3 Suppose Q (t) is a real, diï¬€erentiable nÃ—n matrix which preserves distances.
T
T
Then Q (t) Q (t) = Q (t) Q (t) = I. Also, if u (t) â‰¡ Q (t) u, then there exists a vector, â„¦ (t)
such that
uâ€² (t) = â„¦ (t) Ã— u (t) .
The symbol Ã— refers to the cross product.
(
)
2
2
Proof: Recall that (z Â· w) = 14 |z + w| âˆ’ |z âˆ’ w| . Therefore,
(Q (t) uÂ·Q (t) w)

=
=
=

This implies

(

)
1(
2
2
|Q (t) (u + w)| âˆ’ |Q (t) (u âˆ’ w)|
4
)
1(
2
2
|u + w| âˆ’ |u âˆ’ w|
4
(u Â· w) .

)
T
Q (t) Q (t) u Â· w = (u Â· w)

T

T

T

for all u, w. Therefore, Q (t) Q (t) u = u and so Q (t) Q (t) = Q (t) Q (t) = I. This proves
the ï¬rst part of the lemma.
It follows from the product rule, Lemma 2.6.2 that
Qâ€² (t) Q (t) + Q (t) Qâ€² (t) = 0
T

and so

T

(
)T
T
T
Qâ€² (t) Q (t) = âˆ’ Qâ€² (t) Q (t)
.

(2.26)

From the deï¬nition, Q (t) u = u (t) ,
=u

z
}|
{
T
uâ€² (t) = Qâ€² (t) u =Qâ€² (t) Q (t) u (t).
Then writing the matrix of Qâ€² (t) Q (t) with respect to ï¬xed in space orthonormal basis
vectors, iâˆ— , jâˆ— , kâˆ— , where these are the usual basis vectors for R3 , it follows from (2.26) that
T
the matrix of Qâ€² (t) Q (t) is of the form
ï£«
ï£¶
0
âˆ’Ï‰ 3 (t) Ï‰ 2 (t)
ï£­ Ï‰ 3 (t)
0
âˆ’Ï‰ 1 (t) ï£¸
âˆ’Ï‰ 2 (t) Ï‰ 1 (t)
0
T

for some time dependent scalars Ï‰ i . Therefore,
ï£« 1 ï£¶â€²
ï£«
u
0
âˆ’Ï‰ 3 (t)
ï£­ u2 ï£¸ (t) = ï£­ Ï‰ 3 (t)
0
u3
âˆ’Ï‰ 2 (t) Ï‰ 1 (t)

Saylor URL: http://www.saylor.org/courses/ma212/

ï£¶ï£« 1 ï£¶
Ï‰ 2 (t)
u
âˆ’Ï‰ 1 (t) ï£¸ ï£­ u2 ï£¸ (t)
0
u3

The Saylor Foundation

2.6. MATRICES AND CALCULUS

65

where the ui are the components of the vector u (t) in terms of the ï¬xed vectors iâˆ— , jâˆ— , kâˆ— .
Therefore,
T
uâ€² (t) = â„¦ (t) Ã— u (t) = Qâ€² (t) Q (t) u (t)
(2.27)
where
because

â„¦ (t) = Ï‰ 1 (t) iâˆ— +Ï‰ 2 (t) jâˆ— +Ï‰ 3 (t) kâˆ— .
iâˆ—
w1
u1

jâˆ— kâˆ—
w2 w3 â‰¡
â„¦ (t) Ã— u (t) â‰¡
u2 u3
(
)
(
)
(
)
iâˆ— w2 u3 âˆ’ w3 u2 + jâˆ— w3 u1 âˆ’ w13 + kâˆ— w1 u2 âˆ’ w2 u1 .

This proves the lemma and yields the existence part of the following theorem. 
Theorem 2.6.4 Let i (t) , j (t) , k (t) be as described. Then there exists a unique vector â„¦ (t)
such that if u (t) is a vector whose components are constant with respect to i (t) , j (t) , k (t) ,
then
uâ€² (t) = â„¦ (t) Ã— u (t) .
Proof: It only remains to prove uniqueness. Suppose â„¦1 also works. Then u (t) = Q (t) u
and so uâ€² (t) = Qâ€² (t) u and
Qâ€² (t) u = â„¦ Ã— Q (t) u = â„¦1 Ã— Q (t) u
for all u. Therefore,
(â„¦ âˆ’ â„¦1 ) Ã— Q (t) u = 0
for all u and since Q (t) is one to one and onto, this implies (â„¦ âˆ’ â„¦1 ) Ã—w = 0 for all w and
thus â„¦ âˆ’ â„¦1 = 0. 
Now let R (t) be a position vector and let
r (t) = R (t) + rB (t)
where
rB (t) â‰¡ x (t) i (t) +y (t) j (t) +z (t) k (t) .

R(t)

rB (t)
 R

r(t)

In the example of the earth, R (t) is the position vector of a point p (t) on the earthâ€™s
surface and rB (t) is the position vector of another point from p (t) , thus regarding p (t)
as the origin. rB (t) is the position vector of a point as perceived by the observer on the
earth with respect to the vectors he thinks of as ï¬xed. Similarly, vB (t) and aB (t) will be
the velocity and acceleration relative to i (t) , j (t) , k (t), and so vB = xâ€² i + y â€² j + z â€² k and
aB = xâ€²â€² i + y â€²â€² j + z â€²â€² k. Then
v â‰¡ râ€² = Râ€² + xâ€² i + y â€² j + z â€² k+xiâ€² + yjâ€² + zkâ€² .
By , (2.27), if e âˆˆ {i, j, k} , eâ€² = â„¦ Ã— e because the components of these vectors with respect
to i, j, k are constant. Therefore,
xiâ€² + yjâ€² + zkâ€²

= xâ„¦ Ã— i + yâ„¦ Ã— j + zâ„¦ Ã— k
= â„¦Ã— (xi + yj + zk)

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

66

MATRICES AND LINEAR TRANSFORMATIONS

and consequently,
v = Râ€² + xâ€² i + y â€² j + z â€² k + â„¦ Ã— rB = Râ€² + xâ€² i + y â€² j + z â€² k + â„¦Ã— (xi + yj + zk) .
Now consider the acceleration. Quantities which are relative to the moving coordinate
system and quantities which are relative to a ï¬xed coordinate system are distinguished by
using the subscript B on those relative to the moving coordinate system.
â„¦Ã—vB

z
}|
{
a = v = R + x i + y j + z k+xâ€² iâ€² + y â€² jâ€² + z â€² kâ€² + â„¦â€² Ã— rB
ï£«
ï£¶
â„¦Ã—rB (t)
vB
z
}|
{ z
}|
{
ï£¬
ï£·
+â„¦Ã— ï£­xâ€² i + y â€² j + z â€² k+xiâ€² + yjâ€² + zkâ€² ï£¸
â€²

â€²â€²

â€²â€²

â€²â€²

â€²â€²

= Râ€²â€² + aB + â„¦â€² Ã— rB + 2â„¦ Ã— vB + â„¦Ã— (â„¦ Ã— rB ) .
The acceleration aB is that perceived by an observer who is moving with the moving coordinate system and for whom the moving coordinate system is ï¬xed. The term â„¦Ã— (â„¦ Ã— rB )
is called the centripetal acceleration. Solving for aB ,
aB = a âˆ’ Râ€²â€² âˆ’ â„¦â€² Ã— rB âˆ’ 2â„¦ Ã— vB âˆ’ â„¦Ã— (â„¦ Ã— rB ) .

(2.28)

Here the term âˆ’ (â„¦Ã— (â„¦ Ã— rB )) is called the centrifugal acceleration, it being an acceleration
felt by the observer relative to the moving coordinate system which he regards as ï¬xed, and
the term âˆ’2â„¦ Ã— vB is called the Coriolis acceleration, an acceleration experienced by the
observer as he moves relative to the moving coordinate system. The mass multiplied by the
Coriolis acceleration deï¬nes the Coriolis force.
There is a ride found in some amusement parks in which the victims stand next to
a circular wall covered with a carpet or some rough material. Then the whole circular
room begins to revolve faster and faster. At some point, the bottom drops out and the
victims are held in place by friction. The force they feel is called centrifugal force and it
causes centrifugal acceleration. It is not necessary to move relative to coordinates ï¬xed with
the revolving wall in order to feel this force and it is pretty predictable. However, if the
nauseated victim moves relative to the rotating wall, he will feel the eï¬€ects of the Coriolis
force and this force is really strange. The diï¬€erence between these forces is that the Coriolis
force is caused by movement relative to the moving coordinate system and the centrifugal
force is not.

2.6.2

The Coriolis Acceleration On The Rotating Earth

Now consider the earth. Let iâˆ— , jâˆ— , kâˆ— , be the usual basis vectors ï¬xed in space with kâˆ—
pointing in the direction of the north pole from the center of the earth and let i, j, k be the
unit vectors described earlier with i pointing South, j pointing East, and k pointing away
from the center of the earth at some point of the rotating earthâ€™s surface p. Letting R (t) be
the position vector of the point p, from the center of the earth, observe the coordinates of
R (t) are constant with respect to i (t) , j (t) , k (t) . Also, since the earth rotates from West
to East and the speed of a point on the surface of the earth relative to an observer ï¬xed in
space is Ï‰ |R| sin Ï• where Ï‰ is the angular speed of the earth about an axis through the poles
and Ï• is the polar angle measured from the positive z axis down as in spherical coordinates.
It follows from the geometric deï¬nition of the cross product that
Râ€² = Ï‰kâˆ— Ã— R

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.6. MATRICES AND CALCULUS

67

Therefore, the vector of Theorem 2.6.4 is â„¦ = Ï‰kâˆ— and so
=0

z }| {
R = â„¦â€² Ã— R + â„¦ Ã— Râ€² = â„¦Ã— (â„¦ Ã— R)
â€²â€²

since â„¦ does not depend on t. Formula (2.28) implies
aB = a âˆ’ â„¦Ã— (â„¦ Ã— R) âˆ’ 2â„¦ Ã— vB âˆ’ â„¦Ã— (â„¦ Ã— rB ) .

(2.29)

In this formula, you can totally ignore the term â„¦Ã— (â„¦ Ã— rB ) because it is so small whenever you are considering motion near some point on the earthâ€™s surface. To see this, note
seconds in a day

z }| {
Ï‰ (24) (3600) = 2Ï€, and so Ï‰ = 7.2722 Ã— 10âˆ’5 in radians per second. If you are using
seconds to measure time and feet to measure distance, this term is therefore, no larger than
(
)2
7.2722 Ã— 10âˆ’5 |rB | .
Clearly this is not worth considering in the presence of the acceleration due to gravity which
is approximately 32 feet per second squared near the surface of the earth.
If the acceleration a is due to gravity, then
aB = a âˆ’ â„¦Ã— (â„¦ Ã— R) âˆ’ 2â„¦ Ã— vB =
z

â‰¡g

}|
{
GM (R + rB )
âˆ’
âˆ’ â„¦Ã— (â„¦ Ã— R) âˆ’ 2â„¦ Ã— vB â‰¡ g âˆ’ 2â„¦ Ã— vB .
3
|R + rB |
Note that
2

â„¦Ã— (â„¦ Ã— R) = (â„¦ Â· R) â„¦âˆ’ |â„¦| R
and so g, the acceleration relative to the moving coordinate system on the earth is not
directed exactly toward the center of the earth except at the poles and at the equator,
although the components of acceleration which are in other directions are very small when
compared with the acceleration due to the force of gravity and are often neglected. Therefore, if the only force acting on an object is due to gravity, the following formula describes
the acceleration relative to a coordinate system moving with the earthâ€™s surface.
aB = gâˆ’2 (â„¦ Ã— vB )
While the vector â„¦ is quite small, if the relative velocity, vB is large, the Coriolis acceleration
could be signiï¬cant. This is described in terms of the vectors i (t) , j (t) , k (t) next.
Letting (Ï, Î¸, Ï•) be the usual spherical coordinates of the point p (t) on the surface
taken with respect to iâˆ— , jâˆ— , kâˆ— the usual way with Ï• the polar angle, it follows the iâˆ— , jâˆ— , kâˆ—
coordinates of this point are
ï£«
ï£¶
Ï sin (Ï•) cos (Î¸)
ï£­ Ï sin (Ï•) sin (Î¸) ï£¸ .
Ï cos (Ï•)
It follows,

i = cos (Ï•) cos (Î¸) iâˆ— + cos (Ï•) sin (Î¸) jâˆ— âˆ’ sin (Ï•) kâˆ—
j = âˆ’ sin (Î¸) iâˆ— + cos (Î¸) jâˆ— + 0kâˆ—

and

k = sin (Ï•) cos (Î¸) iâˆ— + sin (Ï•) sin (Î¸) jâˆ— + cos (Ï•) kâˆ— .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

68

MATRICES AND LINEAR TRANSFORMATIONS

It is necessary to obtain kâˆ— in terms of the vectors, i, j, k. Thus the following equation
needs to be solved for a, b, c to ï¬nd kâˆ— = ai+bj+ck
kâˆ—

z }| ï£¶{ ï£«
ï£«
0
cos (Ï•) cos (Î¸)
ï£­ 0 ï£¸ = ï£­ cos (Ï•) sin (Î¸)
1
âˆ’ sin (Ï•)

âˆ’ sin (Î¸)
cos (Î¸)
0

ï£¶ï£«
ï£¶
sin (Ï•) cos (Î¸)
a
sin (Ï•) sin (Î¸) ï£¸ ï£­ b ï£¸
cos (Ï•)
c

(2.30)

The ï¬rst column is i, the second is j and the third is k in the above matrix. The solution
is a = âˆ’ sin (Ï•) , b = 0, and c = cos (Ï•) .
Now the Coriolis acceleration on the earth equals
ï£«
ï£¶
kâˆ—
z
}|
{
ï£¬
ï£·
2 (â„¦ Ã— vB ) = 2Ï‰ ï£­âˆ’ sin (Ï•) i+0j+ cos (Ï•) kï£¸ Ã— (xâ€² i+y â€² j+z â€² k) .
This equals

2Ï‰ [(âˆ’y â€² cos Ï•) i+ (xâ€² cos Ï• + z â€² sin Ï•) j âˆ’ (y â€² sin Ï•) k] .

(2.31)

Remember Ï• is ï¬xed and pertains to the ï¬xed point, p (t) on the earthâ€™s surface. Therefore,
if the acceleration a is due to gravity,
aB = gâˆ’2Ï‰ [(âˆ’y â€² cos Ï•) i+ (xâ€² cos Ï• + z â€² sin Ï•) j âˆ’ (y â€² sin Ï•) k]
(R+rB )
where g = âˆ’ GM
âˆ’ â„¦Ã— (â„¦ Ã— R) as explained above. The term â„¦Ã— (â„¦ Ã— R) is pretty
|R+rB |3
small and so it will be neglected. However, the Coriolis force will not be neglected.

Example 2.6.5 Suppose a rock is dropped from a tall building. Where will it strike?
Assume a = âˆ’gk and the j component of aB is approximately
âˆ’2Ï‰ (xâ€² cos Ï• + z â€² sin Ï•) .
The dominant term in this expression is clearly the second one because xâ€² will be small.
Also, the i and k contributions will be very small. Therefore, the following equation is
descriptive of the situation.
aB = âˆ’gkâˆ’2z â€² Ï‰ sin Ï•j.
z â€² = âˆ’gt approximately. Therefore, considering the j component, this is
2gtÏ‰ sin Ï•.
)
Two integrations give Ï‰gt3 /3 sin Ï• for the j component of the relative displacement at
time t.
This shows the rock does not fall directly towards the center of the earth as expected
but slightly to the east.
(

Example 2.6.6 In 1851 Foucault set a pendulum vibrating and observed the earth rotate
out from under it. It was a very long pendulum with a heavy weight at the end so that it
would vibrate for a long time without stopping2 . This is what allowed him to observe the
earth rotate out from under it. Clearly such a pendulum will take 24 hours for the plane of
vibration to appear to make one complete revolution at the north pole. It is also reasonable
to expect that no such observed rotation would take place on the equator. Is it possible to
predict what will take place at various latitudes?
2 There is such a pendulum in the Eyring building at BYU and to keep people from touching it, there is
a little sign which says Warning! 1000 ohms.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.6. MATRICES AND CALCULUS

69

Using (2.31), in (2.29),
aB = a âˆ’ â„¦Ã— (â„¦ Ã— R)
âˆ’2Ï‰ [(âˆ’y â€² cos Ï•) i+ (xâ€² cos Ï• + z â€² sin Ï•) j âˆ’ (y â€² sin Ï•) k] .
Neglecting the small term, â„¦Ã— (â„¦ Ã— R) , this becomes
= âˆ’gk + T/mâˆ’2Ï‰ [(âˆ’y â€² cos Ï•) i+ (xâ€² cos Ï• + z â€² sin Ï•) j âˆ’ (y â€² sin Ï•) k]
where T, the tension in the string of the pendulum, is directed towards the point at which
the pendulum is supported, and m is the mass of the pendulum bob. The pendulum can be
2
thought of as the position vector from (0, 0, l) to the surface of the sphere x2 +y 2 +(z âˆ’ l) =
2
l . Therefore,
x
y
lâˆ’z
T = âˆ’T iâˆ’T j+T
k
l
l
l
and consequently, the diï¬€erential equations of relative motion are
xâ€²â€² = âˆ’T
y â€²â€² = âˆ’T

x
+ 2Ï‰y â€² cos Ï•
ml

y
âˆ’ 2Ï‰ (xâ€² cos Ï• + z â€² sin Ï•)
ml

and

lâˆ’z
âˆ’ g + 2Ï‰y â€² sin Ï•.
ml
If the vibrations of the pendulum are small so that for practical purposes, z â€²â€² = z = 0, the
last equation may be solved for T to get
z â€²â€² = T

gm âˆ’ 2Ï‰y â€² sin (Ï•) m = T.
Therefore, the ï¬rst two equations become
xâ€²â€² = âˆ’ (gm âˆ’ 2Ï‰my â€² sin Ï•)
and

x
+ 2Ï‰y â€² cos Ï•
ml

y
âˆ’ 2Ï‰ (xâ€² cos Ï• + z â€² sin Ï•) .
ml
All terms of the form xy â€² or y â€² y can be neglected because it is assumed x and y remain
small. Also, the pendulum is assumed to be long with a heavy weight so that xâ€² and y â€² are
also small. With these simplifying assumptions, the equations of motion become
y â€²â€² = âˆ’ (gm âˆ’ 2Ï‰my â€² sin Ï•)

xâ€²â€² + g
and

y â€²â€² + g

x
= 2Ï‰y â€² cos Ï•
l

y
= âˆ’2Ï‰xâ€² cos Ï•.
l

These equations are of the form
xâ€²â€² + a2 x = by â€² , y â€²â€² + a2 y = âˆ’bxâ€²
where a2 =
constant, c,

g
l

(2.32)

and b = 2Ï‰ cos Ï•. Then it is fairly tedious but routine to verify that for each
(

x = c sin

bt
2

(âˆš

)
sin

)
(âˆš
)
( )
b2 + 4a2
bt
b2 + 4a2
t , y = c cos
sin
t
2
2
2

Saylor URL: http://www.saylor.org/courses/ma212/

(2.33)

The Saylor Foundation

70

MATRICES AND LINEAR TRANSFORMATIONS

yields a solution to (2.32) along with the initial conditions,
x (0) = 0, y (0) = 0, xâ€² (0) = 0, y â€² (0) =

âˆš
c b2 + 4a2
.
2

(2.34)

It is clear from experiments with the pendulum that the earth does indeed rotate out from
under it causing the plane of vibration of the pendulum to appear to rotate. The purpose
of this discussion is not to establish these self evident facts but to predict how long it takes
for the plane of vibration to make one revolution. Therefore, there will be some instant in
time at which the pendulum will be vibrating in a plane determined by k and j. (Recall
k points away from the center of the earth and j points East. ) At this instant in time,
deï¬ned as t = 0, the conditions of (2.34) will hold for some value of c and so the solution
to (2.32) having these initial conditions will be those of (2.33) by uniqueness of the initial
value problem. Writing these solutions diï¬€erently,
(âˆš
)
( ) )
(
)
(
2 + 4a2
b
x (t)
sin ( bt
2 )
=c
sin
t
y (t)
cos bt
2
2
( ) )
sin ( bt
2 )
This is very interesting! The vector, c
always has magnitude equal to |c|
cos bt
2
but its direction changes very slowly because b is very (small.
The
) plane of vibration is
âˆš
b2 +4a2
determined by this vector and the vector k. The term sin
t changes relatively fast
2
and takes values between âˆ’1 and 1. This is what describes the actual observed vibrations
of the pendulum. Thus the plane of vibration will have made one complete revolution when
t = T for
bT
â‰¡ 2Ï€.
2
Therefore, the time it takes for the earth to turn out from under the pendulum is
(

T =

4Ï€
2Ï€
=
sec Ï•.
2Ï‰ cos Ï•
Ï‰

Since Ï‰ is the angular speed of the rotating earth, it follows Ï‰ =
hour. Therefore, the above formula implies

2Ï€
24

=

Ï€
12

in radians per

T = 24 sec Ï•.
I think this is really amazing. You could actually determine latitude, not by taking readings
with instruments using the North Star but by doing an experiment with a big pendulum.
You would set it vibrating, observe T in hours, and then solve the above equation for Ï•.
Also note the pendulum would not appear to change its plane of vibration at the equator
because limÏ•â†’Ï€/2 sec Ï• = âˆ.
The Coriolis acceleration is also responsible for the phenomenon of the next example.
Example 2.6.7 It is known that low pressure areas rotate counterclockwise as seen from
above in the Northern hemisphere but clockwise in the Southern hemisphere. Why?
Neglect accelerations other than the Coriolis acceleration and the following acceleration
which comes from an assumption that the point p (t) is the location of the lowest pressure.
a = âˆ’a (rB ) rB
where rB = r will denote the distance from the ï¬xed point p (t) on the earthâ€™s surface which
is also the lowest pressure point. Of course the situation could be more complicated but

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.7.

71

EXERCISES

this will suï¬ƒce to explain the above question. Then the acceleration observed by a person
on the earth relative to the apparently ï¬xed vectors, i, k, j, is
aB = âˆ’a (rB ) (xi+yj+zk) âˆ’ 2Ï‰ [âˆ’y â€² cos (Ï•) i+ (xâ€² cos (Ï•) + z â€² sin (Ï•)) jâˆ’ (y â€² sin (Ï•) k)]
Therefore, one obtains some diï¬€erential equations from aB = xâ€²â€² i + y â€²â€² j + z â€²â€² k by matching
the components. These are
xâ€²â€² + a (rB ) x
y â€²â€² + a (rB ) y
z â€²â€² + a (rB ) z

= 2Ï‰y â€² cos Ï•
= âˆ’2Ï‰xâ€² cos Ï• âˆ’ 2Ï‰z â€² sin (Ï•)
= 2Ï‰y â€² sin Ï•

Now remember, the vectors, i, j, k are ï¬xed relative to the earth and so are constant vectors.
Therefore, from the properties of the determinant and the above diï¬€erential equations,
â€²

(râ€²B Ã— rB ) =

=

i
âˆ’a (rB ) x + 2Ï‰y â€² cos Ï•
x

i
xâ€²
x

j
yâ€²
y

k
zâ€²
z

â€²

=

i
xâ€²â€²
x

j
y â€²â€²
y

k
z â€²â€²
z

j
âˆ’a (rB ) y âˆ’ 2Ï‰xâ€² cos Ï• âˆ’ 2Ï‰z â€² sin (Ï•)
y

k
âˆ’a (rB ) z + 2Ï‰y â€² sin Ï•
z

Then the kth component of this cross product equals
(
)â€²
Ï‰ cos (Ï•) y 2 + x2 + 2Ï‰xz â€² sin (Ï•) .
The ï¬rst term will be negative because it is assumed p (t) is the location of low pressure
causing y 2 +x2 to be a decreasing function. If it is assumed there is not a substantial motion
in the k direction, so that z is fairly constant and the last
( term
( the
) can be neglected, then
)
â€²
kth component of (râ€²B Ã— rB ) is negative provided Ï• âˆˆ 0, Ï€2 and positive if Ï• âˆˆ Ï€2 , Ï€ .
Beginning with a point at rest, this implies râ€²B Ã— rB = 0 initially and then the above implies
its kth component is negative in the upper hemisphere when Ï• < Ï€/2 and positive in the
lower hemisphere when Ï• > Ï€/2. Using the right hand and the geometric deï¬nition of the
cross product, this shows clockwise rotation in the lower hemisphere and counter clockwise
rotation in the upper hemisphere.
Note also that as Ï• gets close to Ï€/2 near the equator, the above reasoning tends to
break down because cos (Ï•) becomes close to zero. Therefore, the motion towards the low
pressure has to be more pronounced in comparison with the motion in the k direction in
order to draw this conclusion.

2.7

Exercises

1. Show the map T : Rn â†’ Rm deï¬ned by T (x) = Ax where A is an m Ã— n matrix and
x is an m Ã— 1 column vector is a linear transformation.
2. Find the matrix for the linear transformation which rotates every vector in R2 through
an angle of Ï€/3.
3. Find the matrix for the linear transformation which rotates every vector in R2 through
an angle of Ï€/4.
4. Find the matrix for the linear transformation which rotates every vector in R2 through
an angle of âˆ’Ï€/3.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

72

MATRICES AND LINEAR TRANSFORMATIONS

5. Find the matrix for the linear transformation which rotates every vector in R2 through
an angle of 2Ï€/3.
6. Find the matrix for the linear transformation which rotates every vector in R2 through
an angle of Ï€/12. Hint: Note that Ï€/12 = Ï€/3 âˆ’ Ï€/4.
7. Find the matrix for the linear transformation which rotates every vector in R2 through
an angle of 2Ï€/3 and then reï¬‚ects across the x axis.
8. Find the matrix for the linear transformation which rotates every vector in R2 through
an angle of Ï€/3 and then reï¬‚ects across the x axis.
9. Find the matrix for the linear transformation which rotates every vector in R2 through
an angle of Ï€/4 and then reï¬‚ects across the x axis.
10. Find the matrix for the linear transformation which rotates every vector in R2 through
an angle of Ï€/6 and then reï¬‚ects across the x axis followed by a reï¬‚ection across the
y axis.
11. Find the matrix for the linear transformation which reï¬‚ects every vector in R2 across
the x axis and then rotates every vector through an angle of Ï€/4.
12. Find the matrix for the linear transformation which rotates every vector in R2 through
an angle of Ï€/4 and next reï¬‚ects every vector across the x axis. Compare with the
above problem.
13. Find the matrix for the linear transformation which reï¬‚ects every vector in R2 across
the x axis and then rotates every vector through an angle of Ï€/6.
14. Find the matrix for the linear transformation which reï¬‚ects every vector in R2 across
the y axis and then rotates every vector through an angle of Ï€/6.
15. Find the matrix for the linear transformation which rotates every vector in R2 through
an angle of 5Ï€/12. Hint: Note that 5Ï€/12 = 2Ï€/3 âˆ’ Ï€/4.
T

16. Find the matrix for proju (v) where u = (1, âˆ’2, 3) .
T

17. Find the matrix for proju (v) where u = (1, 5, 3) .
T

18. Find the matrix for proju (v) where u = (1, 0, 3) .
19. Give an example of a 2 Ã— 2 matrix A which has all its entries nonzero and satisï¬es
A2 = A. A matrix which satisï¬es A2 = A is called idempotent.
20. Let A be an m Ã— n matrix and let B be an n Ã— m matrix where n < m. Show that
AB cannot have an inverse.
21. Find ker (A) for

ï£«

1
ï£¬ 0
A=ï£¬
ï£­ 1
0

2
2
4
2

3
1
4
1

2
1
3
1

ï£¶
1
2 ï£·
ï£·.
3 ï£¸
2

Recall ker (A) is just the set of solutions to Ax = 0.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.7.

73

EXERCISES

22. If A is a linear transformation, and Axp = b, show that the general solution to the
equation Ax = b is of the form xp + y where y âˆˆ ker (A). By this I mean to show that
whenever Az = b there exists y âˆˆ ker (A) such that xp + y = z. For the deï¬nition of
ker (A) see Problem 21.
23. Using Problem 21, ï¬nd the general solution to the following linear system.
ï£«

1
ï£¬ 0
ï£¬
ï£­ 1
0

2
2
4
2

3
1
4
1

2
1
3
1

ï£«
ï£¶
ï£¶
ï£«
x1
1
ï£¬ x2 ï£·
ï£¬
ï£· ï£¬
2 ï£·
ï£· ï£¬ x3 ï£· = ï£¬
ï£· ï£­
3 ï£¸ï£¬
ï£­ x4 ï£¸
2
x5

ï£¶
11
7 ï£·
ï£·
18 ï£¸
7

24. Using Problem 21, ï¬nd the general solution to the following linear system.
ï£«

1
ï£¬ 0
ï£¬
ï£­ 1
0

2
2
4
2

3
1
4
1

2
1
3
1

ï£¶

ï£«

1
ï£¬
ï£¬
2 ï£·
ï£·ï£¬
ï£¬
ï£¸
3
ï£­
2

x1
x2
x3
x4
x5

ï£¶

ï£«
ï£¶
6
ï£·
ï£· ï£¬ 7 ï£·
ï£·=ï£¬
ï£·
ï£· ï£­ 13 ï£¸
ï£¸
7

25. Show that the function Tu deï¬ned by Tu (v) â‰¡ v âˆ’ proju (v) is also a linear transformation.
T

26. If u = (1, 2, 3) , as in Example 9.3.22 and Tu is given in the above problem, ï¬nd the
matrix Au which satisï¬es Au x = Tu (x).
27. Suppose V is a subspace of Fn and T : V â†’ Fp is a nonzero linear transformation.
Show that there exists a basis for Im (T ) â‰¡ T (V )
{T v1 , Â· Â· Â· , T vm }
and that in this situation,
{v1 , Â· Â· Â· , vm }
is linearly independent.
28. â†‘In the situation of Problem 27 where V is a subspace of Fn , show that there exists
{z1 , Â· Â· Â· , zr } a basis for ker (T ) . (Recall Theorem 2.4.12. Since ker (T ) is a subspace,
it has a basis.) Now for an arbitrary T v âˆˆ T (V ) , explain why
T v = a1 T v1 + Â· Â· Â· + am T vm
and why this implies
v âˆ’ (a1 v1 + Â· Â· Â· + am vm ) âˆˆ ker (T ) .
Then explain why V = span (v1 , Â· Â· Â· , vm , z1 , Â· Â· Â· , zr ) .
29. â†‘In the situation of the above problem, show {v1 , Â· Â· Â· , vm , z1 , Â· Â· Â· , zr } is a basis for V
and therefore, dim (V ) = dim (ker (T )) + dim (T (V )) .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

74

MATRICES AND LINEAR TRANSFORMATIONS

30. â†‘Let A be a linear transformation from V to W and let B be a linear transformation
from W to U where V, W, U are all subspaces of some Fp . Explain why
A (ker (BA)) âŠ† ker (B) , ker (A) âŠ† ker (BA) .

ker(BA)

ker(B)
A

ker(A)

-

A(ker(BA))

31. â†‘Let {x1 , Â· Â· Â· , xn } be a basis of ker (A) and let {Ay1 , Â· Â· Â· , Aym } be a basis of A (ker (BA)).
Let z âˆˆ ker (BA) . Explain why
Az âˆˆ span {Ay1 , Â· Â· Â· , Aym }
and why there exist scalars ai such that
A (z âˆ’ (a1 y1 + Â· Â· Â· + am ym )) = 0
and why it follows z âˆ’ (a1 y1 + Â· Â· Â· + am ym ) âˆˆ span {x1 , Â· Â· Â· , xn }. Now explain why
ker (BA) âŠ† span {x1 , Â· Â· Â· , xn , y1 , Â· Â· Â· , ym }
and so
dim (ker (BA)) â‰¤ dim (ker (B)) + dim (ker (A)) .
This important inequality is due to Sylvester. Show that equality holds if and only if
A(ker BA) = ker(B).
32. Generalize the result of the previous problem to any ï¬nite product of linear mappings.
33. If W âŠ† V for W, V two subspaces of Fn and if dim (W ) = dim (V ) , show W = V .
34. Let V be a subspace of Fn and let V1 , Â· Â· Â· , Vm be subspaces, each contained in V . Then
V = V1 âŠ• Â· Â· Â· âŠ• Vm

(2.35)

if every v âˆˆ V can be written in a unique way in the form
v = v1 + Â· Â· Â· + vm
where each vi âˆˆ Vi . This is called a direct sum. If this uniqueness condition does not
hold, then one writes
V = V1 + Â· Â· Â· + Vm
and this symbol means all vectors of the form
v1 + Â· Â· Â· + vm , vj âˆˆ Vj for each j.
Show (2.35) is equivalent to saying that if
0 = v1 + Â· Â· Â· + vm , vj âˆˆ Vj for each j,
{
}
then each vj = 0. Next show that in the situation of (2.35), if Î² i = ui1 , Â· Â· Â· , uimi is
a basis for Vi , then {Î² 1 , Â· Â· Â· , Î² m } is a basis for V .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

2.7.

75

EXERCISES

35. â†‘Suppose you have ï¬nitely many linear mappings L1 , L2 , Â· Â· Â· , Lm which map V to V
where V is a subspace of Fn and suppose they commute. That is, Li Lj = Lj Li for all
i, j. Also suppose Lk is one to one on ker (Lj ) whenever j Ì¸= k. Letting P denote the
product of these linear transformations, P = L1 L2 Â· Â· Â· Lm , ï¬rst show
ker (L1 ) + Â· Â· Â· + ker (Lm ) âŠ† ker (P )
Next show Lj : ker (Li ) â†’ ker (Li ) . Then show
ker (L1 ) + Â· Â· Â· + ker (Lm ) = ker (L1 ) âŠ• Â· Â· Â· âŠ• ker (Lm ) .
Using Sylvesterâ€™s theorem, and the result of Problem 33, show
ker (P ) = ker (L1 ) âŠ• Â· Â· Â· âŠ• ker (Lm )
Hint: By Sylvesterâ€™s theorem and the above problem,
âˆ‘
dim (ker (P )) â‰¤
dim (ker (Li ))
i

=

dim (ker (L1 ) âŠ• Â· Â· Â· âŠ• ker (Lm )) â‰¤ dim (ker (P ))

Now consider Problem 33.
36. Let M (Fn , Fn ) denote the set of all n Ã— n matrices having entries in F. With the usual
operations of matrix addition and scalar multiplications, explain why M (Fn , Fn ) can
2
be considered as Fn . Give a basis for M (Fn , Fn ) . If A âˆˆ M (Fn , Fn ) , explain why
there exists a monic (leading coeï¬ƒcient equals 1) polynomial of the form
Î»k + akâˆ’1 Î»kâˆ’1 + Â· Â· Â· + a1 Î» + a0
such that
Ak + akâˆ’1 Akâˆ’1 + Â· Â· Â· + a1 A + a0 I = 0
The minimal polynomial of A is the polynomial like the above, for which p (A) = 0
which has smallest degree. I will discuss the uniqueness of this polynomial later. Hint:
2
Consider the matrices I, A, A2 , Â· Â· Â· , An . There are n2 + 1 of these matrices. Can they
be linearly independent? Now consider all polynomials and pick one of smallest degree
and then divide by the leading coeï¬ƒcient.
37. â†‘Suppose the ï¬eld of scalars is C and A is an n Ã— n matrix. From the preceding
problem, and the fundamental theorem of algebra, this minimal polynomial factors
r

r

(Î» âˆ’ Î»1 ) 1 (Î» âˆ’ Î»2 ) 2 Â· Â· Â· (Î» âˆ’ Î»k )

rk

where rj is the algebraic multiplicity of Î»j , and the Î»j are distinct. Thus
r

r

(A âˆ’ Î»1 I) 1 (A âˆ’ Î»2 I) 2 Â· Â· Â· (A âˆ’ Î»k I)
r

r

rk

=0

rk

and so, letting P = (A âˆ’ Î»1 I) 1 (A âˆ’ Î»2 I) 2 Â· Â· Â· (A âˆ’ Î»k I)
apply the result of Problem 35 to verify that

rj

and Lj = (A âˆ’ Î»j I)

Cn = ker (L1 ) âŠ• Â· Â· Â· âŠ• ker (Lk )
and that A : ker (Lj ) â†’ ker (Lj ). In this context, ker (Lj ) is called the generalized
eigenspace for Î»j . You need to verify the conditions of the result of this problem hold.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

76

MATRICES AND LINEAR TRANSFORMATIONS

38. In the context of Problem 37, show there exists a nonzero vector x such that
(A âˆ’ Î»j I) x = 0.
This is called an eigenvector and the Î»j is called an eigenvalue. Hint:There must exist
a vector y such that
r

r

rj âˆ’1

(A âˆ’ Î»1 I) 1 (A âˆ’ Î»2 I) 2 Â· Â· Â· (A âˆ’ Î»j I)

Â· Â· Â· (A âˆ’ Î»k I)

rk

y = z Ì¸= 0

Why? Now what happens if you do (A âˆ’ Î»j I) to z?
39. Suppose Q (t) is an orthogonal matrix. This means Q (t) is a real n Ã— n matrix which
satisï¬es
T
Q (t) Q (t) = I
( )â€²
Suppose also the entries of Q (t) are diï¬€erentiable. Show QT = âˆ’QT Qâ€² QT .
40. Remember the Coriolis force was 2â„¦ Ã— vB where â„¦ was a particular vector which
came from the matrix Q (t) as described above. Show that
ï£¶
ï£«
i (t) Â· i (t0 ) j (t) Â· i (t0 ) k (t) Â· i (t0 )
Q (t) = ï£­ i (t) Â· j (t0 ) j (t) Â· j (t0 ) k (t) Â· j (t0 ) ï£¸ .
i (t) Â· k (t0 ) j (t) Â· k (t0 ) k (t) Â· k (t0 )
There will be no Coriolis force exactly when â„¦ = 0 which corresponds to Qâ€² (t) = 0.
When will Qâ€² (t) = 0?
41. An illustration used in many beginning physics books is that of ï¬ring a riï¬‚e horizontally and dropping an identical bullet from the same height above the perfectly
ï¬‚at ground followed by an assertion that the two bullets will hit the ground at exactly the same time. Is this true on the rotating earth assuming the experiment
takes place over a large perfectly ï¬‚at ï¬eld so the curvature of the earth is not an
issue? Explain. What other irregularities will occur? Recall the Coriolis acceleration
is 2Ï‰ [(âˆ’y â€² cos Ï•) i+ (xâ€² cos Ï• + z â€² sin Ï•) j âˆ’ (y â€² sin Ï•) k] where k points away from the
center of the earth, j points East, and i points South.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Determinants
3.1

Basic Techniques And Properties

Let A be an n Ã— n matrix. The determinant of A, denoted as det (A) is a number. If the
matrix is a 2Ã—2 matrix, this number is very easy to ï¬nd.
(
)
a b
Deï¬nition 3.1.1 Let A =
. Then
c d
det (A) â‰¡ ad âˆ’ cb.
The determinant is also often denoted by enclosing the matrix with two vertical lines. Thus
(
)
a b
a b
det
=
.
c d
c d
(
)
2 4
Example 3.1.2 Find det
.
âˆ’1 6
From the deï¬nition this is just (2) (6) âˆ’ (âˆ’1) (4) = 16.
Assuming the determinant has been deï¬ned for k Ã— k matrices for k â‰¤ n âˆ’ 1, it is now
time to deï¬ne it for n Ã— n matrices.
Deï¬nition 3.1.3 Let A = (aij ) be an n Ã— n matrix. Then a new matrix called the cofactor
matrix, cof (A) is deï¬ned by cof (A) = (cij ) where to obtain cij delete the ith row and the
j th column of A, take the determinant of the (n âˆ’ 1) Ã— (n âˆ’ 1) matrix which results, (This
i+j
is called the ij th minor of A. ) and then multiply this number by (âˆ’1) . To make the
th
formulas easier to remember, cof (A)ij will denote the ij entry of the cofactor matrix.
Now here is the deï¬nition of the determinant given recursively.
Theorem 3.1.4 Let A be an n Ã— n matrix where n â‰¥ 2. Then
det (A) =

n
âˆ‘

aij cof (A)ij =

j=1

n
âˆ‘

aij cof (A)ij .

(3.1)

i=1

The ï¬rst formula consists of expanding the determinant along the ith row and the second
expands the determinant along the j th column.

77

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

78

DETERMINANTS

Note that for a n Ã— n matrix, you will need n! terms to evaluate the determinant in this
way. If n = 10, this is 10! = 3, 628 , 800 terms. This is a lot of terms.
In addition to the diï¬ƒculties just discussed, why is the determinant well deï¬ned? Why
should you get the same thing when you expand along any row or column? I think you
should regard this claim that you always get the same answer by picking any row or column
with considerable skepticism. It is incredible and not at all obvious. However, it requires
a little eï¬€ort to establish it. This is done in the section on the theory of the determinant
which follows.
Notwithstanding the diï¬ƒculties involved in using the method of Laplace expansion,
certain types of matrices are very easy to deal with.
Deï¬nition 3.1.5 A matrix M , is upper triangular if Mij = 0 whenever i > j. Thus such
a matrix equals zero below the main diagonal, the entries of the form Mii , as shown.
ï£«
ï£¶
âˆ— âˆ— Â·Â·Â· âˆ—
ï£¬
. ï£·
..
ï£¬ 0 âˆ—
. .. ï£·
ï£¬
ï£·
ï£¬ . .
ï£·
.. ... âˆ— ï£¸
ï£­ ..
0 Â·Â·Â·
0 âˆ—
A lower triangular matrix is deï¬ned similarly as a matrix for which all entries above the
main diagonal are equal to zero.
You should verify the following using the above theorem on Laplace expansion.
Corollary 3.1.6 Let M be an upper (lower) triangular matrix. Then det (M ) is obtained
by taking the product of the entries on the main diagonal.
Proof: The corollary is true if the matrix is one to one. Suppose it is n Ã— n. Then the
matrix is of the form
(
)
m11 a
0
M1
where M1 is (n âˆ’ 1)Ã—(n âˆ’ 1) . Then expanding along the ï¬rst row,
âˆn you get m11 det (M1 )+0.
Then use the induction hypothesis to obtain that det (M1 ) = i=2 mii . 
Example 3.1.7 Let

ï£«

1
ï£¬ 0
A=ï£¬
ï£­ 0
0

2
2
0
0

3
6
3
0

ï£¶
77
7 ï£·
ï£·
33.7 ï£¸
âˆ’1

Find det (A) .
From the above corollary, this is âˆ’6.
There are many properties satisï¬ed by determinants. Some of the most important are
listed in the following theorem.
Theorem 3.1.8 If two rows or two columns in an n Ã— n matrix A are switched, the determinant of the resulting matrix equals (âˆ’1) times the determinant of the original matrix. If
A is an n Ã— n matrix in which two rows are equal or two columns are equal then det (A) = 0.
Suppose the ith row of A equals (xa1 + yb1 , Â· Â· Â· , xan + ybn ). Then
det (A) = x det (A1 ) + y det (A2 )

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

3.1. BASIC TECHNIQUES AND PROPERTIES

79

where the ith row of A1 is (a1 , Â· Â· Â· , an ) and the ith row of A2 is (b1 , Â· Â· Â· , bn ) , all other rows
of A1 and A2 coinciding with those of A. In other words, det is a linear function of each
row A. The same is true with the word â€œrowâ€ replaced with the word â€œcolumnâ€. In addition
to this, if A and B are n Ã— n matrices, then
det (AB) = det (A) det (B) ,
and if A is an n Ã— n matrix, then
( )
det (A) = det AT .
This theorem implies the following corollary which gives a way to ï¬nd determinants. As
I pointed out above, the method of Laplace expansion will not be practical for any matrix
of large size.
Corollary 3.1.9 Let A be an nÃ—n matrix and let B be the matrix obtained by replacing the
ith row (column) of A with the sum of the ith row (column) added to a multiple of another
row (column). Then det (A) = det (B) . If B is the matrix obtained from A be replacing the
ith row (column) of A by a times the ith row (column) then a det (A) = det (B) .
Here is an example which shows how to use this corollary to ï¬nd a determinant.
Example 3.1.10 Find the determinant of
ï£«
1
ï£¬ 5
A=ï£¬
ï£­ 4
2

the matrix
2
1
5
2

3
2
4
âˆ’4

ï£¶
4
3 ï£·
ï£·
3 ï£¸
5

Replace the second row by (âˆ’5) times the ï¬rst row added to it. Then replace the third
row by (âˆ’4) times the ï¬rst row added to it. Finally, replace the fourth row by (âˆ’2) times
the ï¬rst row added to it. This yields the matrix
ï£«
ï£¶
1 2
3
4
ï£¬ 0 âˆ’9 âˆ’13 âˆ’17 ï£·
ï£·
B=ï£¬
ï£­ 0 âˆ’3 âˆ’8 âˆ’13 ï£¸
0 âˆ’2 âˆ’10 âˆ’3
and from the above corollary,
it has the same determinant as A. Now using the corollary
( )
det
(C) where
some more, det (B) = âˆ’1
3
ï£«
ï£¶
1 2
3
4
ï£¬ 0 0
11
22 ï£·
ï£·
C=ï£¬
ï£­ 0 âˆ’3 âˆ’8 âˆ’13 ï£¸ .
0 6
30
9
The second row was replaced by (âˆ’3) times the third row added to the second row and then
the last row was multiplied by (âˆ’3) . Now replace the last row with 2 times the third added
to it and then switch the third and second rows. Then det (C) = âˆ’ det (D) where
ï£«
ï£¶
1 2
3
4
ï£¬ 0 âˆ’3 âˆ’8 âˆ’13 ï£·
ï£·
D=ï£¬
ï£­ 0 0
11
22 ï£¸
0 0
14 âˆ’17

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

80

DETERMINANTS

You could do more row operations or you could note that this can be easily expanded along
the ï¬rst column followed by expanding the 3 Ã— 3 matrix which results along its ï¬rst column.
Thus
11 22
det (D) = 1 (âˆ’3)
= 1485
14 âˆ’17
( )
and so det (C) = âˆ’1485 and det (A) = det (B) = âˆ’1
(âˆ’1485) = 495.
3
The theorem about expanding a matrix along any row or column also provides a way to
give a formula for the inverse of a matrix. Recall the deï¬nition of the inverse of a matrix
in Deï¬nition 2.1.22 on Page 47. The following theorem gives a formula for the inverse of a
matrix. It is proved in the next section.
)
(
Theorem 3.1.11 Aâˆ’1 exists if and only if det(A) Ì¸= 0. If det(A) Ì¸= 0, then Aâˆ’1 = aâˆ’1
ij
where
âˆ’1
aâˆ’1
cof (A)ji
ij = det(A)
for cof (A)ij the ij th cofactor of A.
Theorem 3.1.11 says that to ï¬nd the inverse, take the transpose of the cofactor matrix
and divide by the determinant. The transpose of the cofactor matrix is called the adjugate
or sometimes the classical adjoint of the matrix A. It is an abomination to call it the adjoint
although you do sometimes see it referred to in this way. In words, Aâˆ’1 is equal to one over
the determinant of A times the adjugate matrix of A.
Example 3.1.12 Find the inverse of the matrix
ï£¶
ï£«
1 2 3
A=ï£­ 3 0 1 ï£¸
1 2 1
First ï¬nd the determinant of this matrix. This is seen to be 12. The cofactor matrix of
A is
ï£«
ï£¶
âˆ’2 âˆ’2 6
ï£­ 4 âˆ’2 0 ï£¸ .
2
8 âˆ’6
Each entry of A was replaced by its cofactor. Therefore, from the above theorem, the inverse
of A should equal
ï£«
ï£¶T ï£« 1
âˆ’6
âˆ’2 âˆ’2 6
1 ï£­
4 âˆ’2 0 ï£¸ = ï£­ âˆ’ 16
12
1
2
8 âˆ’6
2

1
3
âˆ’ 61

0

1
6
2
3
âˆ’ 12

ï£¶
ï£¸.

This way of ï¬nding inverses is especially useful in the case where it is desired to ï¬nd the
inverse of a matrix whose entries are functions.
Example 3.1.13 Suppose
ï£«

et
A (t) = ï£­ 0
0
âˆ’1

Find A (t)

ï£¶
0
0
cos t sin t ï£¸
âˆ’ sin t cos t

.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

3.2. EXERCISES

81

First note det (A (t)) = et . A routine computation using the above theorem shows that
this inverse is
ï£«
ï£¶T ï£« âˆ’t
ï£¶
1
0
0
e
0
0
1 ï£­
0 et cos t et sin t ï£¸ = ï£­ 0 cos t âˆ’ sin t ï£¸ .
et
0 âˆ’et sin t et cos t
0
sin t cos t
This formula for the inverse also implies a famous procedure known as Cramerâ€™s rule.
Cramerâ€™s rule gives a formula for the solutions, x, to a system of equations, Ax = y.
In case you are solving a system of equations, Ax = y for x, it follows that if Aâˆ’1 exists,
(
)
x = Aâˆ’1 A x = Aâˆ’1 (Ax) = Aâˆ’1 y
thus solving the system. Now in the case that Aâˆ’1 exists, there is a formula for Aâˆ’1 given
above. Using this formula,
xi =

n
âˆ‘
j=1

aâˆ’1
ij yj =

n
âˆ‘
j=1

1
cof (A)ji yj .
det (A)

By the formula for the expansion of a determinant along a column,
ï£«
ï£¶
âˆ— Â· Â· Â· y1 Â· Â· Â· âˆ—
1
ï£¬
..
.. ï£· ,
xi =
det ï£­ ...
.
. ï£¸
det (A)
âˆ— Â· Â· Â· yn Â· Â· Â· âˆ—
T

where here the ith column of A is replaced with the column vector, (y1 Â· Â· Â· Â·, yn ) , and the
determinant of this modiï¬ed matrix is taken and divided by det (A). This formula is known
as Cramerâ€™s rule.
Procedure 3.1.14 Suppose A is an n Ã— n matrix and it is desired to solve the system
T
T
Ax = y, y = (y1 , Â· Â· Â· , yn ) for x = (x1 , Â· Â· Â· , xn ) . Then Cramerâ€™s rule says
xi =

det Ai
det A
T

where Ai is obtained from A by replacing the ith column of A with the column (y1 , Â· Â· Â· , yn ) .
The following theorem is of fundamental importance and ties together many of the ideas
presented above. It is proved in the next section.
Theorem 3.1.15 Let A be an n Ã— n matrix. Then the following are equivalent.
1. A is one to one.
2. A is onto.
3. det (A) Ì¸= 0.

3.2

Exercises

1. Find the determinants of the following matrices.
ï£«
ï£¶
1 2 3
(a) ï£­ 3 2 2 ï£¸ (The answer is 31.)
0 9 8

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

82

DETERMINANTS

ï£«

4
(b) ï£­ 1
3
ï£«
1
ï£¬ 1
(c) ï£¬
ï£­ 4
1

ï£¶
3 2
7 8 ï£¸(The answer is 375.)
âˆ’9 3
ï£¶
2 3 2
3 2 3 ï£·
ï£·, (The answer is âˆ’2.)
1 5 0 ï£¸
2 1 2

)
(
2. If Aâˆ’1 exist, what is the relationship between det (A) and det Aâˆ’1 . Explain your
answer.
3. Let A be an n Ã— n matrix where n is odd. Suppose also that A is skew symmetric.
This means AT = âˆ’A. Show that det(A) = 0.
4. Is it true that det (A + B) = det (A) + det (B)? If this is so, explain why it is so and
if it is not so, give a counter example.
5. Let A be an r Ã— r matrix and suppose there are r âˆ’ 1 rows (columns) such that all rows
(columns) are linear combinations of these r âˆ’ 1 rows (columns). Show det (A) = 0.
6. Show det (aA) = an det (A) where here A is an n Ã— n matrix and a is a scalar.
7. Suppose A is an upper triangular matrix. Show that Aâˆ’1 exists if and only if all
elements of the main diagonal are non zero. Is it true that Aâˆ’1 will also be upper
triangular? Explain. Is everything the same for lower triangular matrices?
8. Let A and B be two n Ã— n matrices. A âˆ¼ B (A is similar to B) means there exists an
invertible matrix S such that A = S âˆ’1 BS. Show that if A âˆ¼ B, then B âˆ¼ A. Show
also that A âˆ¼ A and that if A âˆ¼ B and B âˆ¼ C, then A âˆ¼ C.
9. In the context of Problem 8 show that if A âˆ¼ B, then det (A) = det (B) .
10. Let A be an n Ã— n matrix and let x be a nonzero vector such that Ax = Î»x for some
scalar, Î». When this occurs, the vector, x is called an eigenvector and the scalar, Î»
is called an eigenvalue. It turns out that not every number is an eigenvalue. Only
certain ones are. Why? Hint: Show that if Ax = Î»x, then (Î»I âˆ’ A) x = 0. Explain
why this shows that (Î»I âˆ’ A) is not one to one and not onto. Now use Theorem 3.1.15
to argue det (Î»I âˆ’ A) = 0. What sort of equation is this? How many solutions does it
have?
11. Suppose det (Î»I âˆ’ A) = 0. Show using Theorem 3.1.15 there exists x Ì¸= 0 such that
(Î»I âˆ’ A) x = 0.
(
)
a (t) b (t)
12. Let F (t) = det
. Verify
c (t) d (t)
( â€²
)
(
)
a (t) bâ€² (t)
a (t) b (t)
â€²
F (t) = det
+ det
.
c (t) d (t)
câ€² (t) dâ€² (t)
Now suppose

ï£«

a (t)
F (t) = det ï£­ d (t)
g (t)

Saylor URL: http://www.saylor.org/courses/ma212/

b (t)
e (t)
h (t)

ï£¶
c (t)
f (t) ï£¸ .
i (t)

The Saylor Foundation

3.3. THE MATHEMATICAL THEORY OF DETERMINANTS
Use Laplace expansion and the ï¬rst part to verify F â€² (t) =
ï£« â€²
ï£¶
ï£«
a (t) bâ€² (t) câ€² (t)
a (t) b (t)
det ï£­ d (t) e (t) f (t) ï£¸ + det ï£­ dâ€² (t) eâ€² (t)
g (t) h (t) i (t)
g (t) h (t)
ï£«
ï£¶
a (t) b (t) c (t)
+ det ï£­ d (t) e (t) f (t) ï£¸ .
g â€² (t) hâ€² (t) iâ€² (t)

83

ï£¶
c (t)
f â€² (t) ï£¸
i (t)

Conjecture a general result valid for n Ã— n matrices and explain why it will be true.
Can a similar thing be done with the columns?
13. Use the formula for the inverse in terms of the cofactor matrix to ï¬nd the inverse of
the matrix
ï£« t
ï£¶
e
0
0
ï£¸.
et cos t
et sin t
A=ï£­ 0
t
t
t
t
0 e cos t âˆ’ e sin t e cos t + e sin t
14. Let A be an r Ã— r matrix and let B be an m Ã— m matrix such that r + m = n. Consider
the following n Ã— n block matrix
(
)
A 0
C=
.
D B
where the D is an m Ã— r matrix, and the 0 is a r Ã— m matrix. Letting Ik denote the
k Ã— k identity matrix, tell why
(
)(
)
A 0
Ir 0
C=
.
D Im
0 B
Now explain why det (C) = det (A) det (B) . Hint: Part of this will require an explanation of why
(
)
A 0
det
= det (A) .
D Im
See Corollary 3.1.9.
15. Suppose Q is an orthogonal matrix. This means Q is a real nÃ—n matrix which satisï¬es
QQT = I
Find the possible values for det (Q).
16. Suppose Q (t) is an orthogonal matrix. This means Q (t) is a real n Ã— n matrix which
satisï¬es
T
Q (t) Q (t) = I
Suppose Q (t) is continuous for t âˆˆ [a, b] , some interval. Also suppose det (Q (t)) = 1.
Show that it follows det (Q (t)) = 1 for all t âˆˆ [a, b].

3.3

The Mathematical Theory Of Determinants

It is easiest to give a diï¬€erent deï¬nition of the determinant which is clearly well deï¬ned
and then prove the earlier one in terms of Laplace expansion. Let (i1 , Â· Â· Â· , in ) be an ordered
list of numbers from {1, Â· Â· Â· , n} . This means the order is important so (1, 2, 3) and (2, 1, 3)
are diï¬€erent. There will be some repetition between this section and the earlier section on
determinants. The main purpose is to give all the missing proofs. Two books which give
a good introduction to determinants are Apostol [1] and Rudin [22]. A recent book which
also has a good introduction is Baker [3]

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

84

3.3.1

DETERMINANTS

The Function sgn

The following Lemma will be essential in the deï¬nition of the determinant.
Lemma 3.3.1 There exists a unique function, sgnn which maps each ordered list of numbers from {1, Â· Â· Â· , n} to one of the three numbers, 0, 1, or âˆ’1 which also has the following
properties.
sgnn (1, Â· Â· Â· , n) = 1
(3.2)
sgnn (i1 , Â· Â· Â· , p, Â· Â· Â· , q, Â· Â· Â· , in ) = âˆ’ sgnn (i1 , Â· Â· Â· , q, Â· Â· Â· , p, Â· Â· Â· , in )

(3.3)

In words, the second property states that if two of the numbers are switched, the value of the
function is multiplied by âˆ’1. Also, in the case where n > 1 and {i1 , Â· Â· Â· , in } = {1, Â· Â· Â· , n} so
that every number from {1, Â· Â· Â· , n} appears in the ordered list, (i1 , Â· Â· Â· , in ) ,
sgnn (i1 , Â· Â· Â· , iÎ¸âˆ’1 , n, iÎ¸+1 , Â· Â· Â· , in ) â‰¡
(âˆ’1)

nâˆ’Î¸

sgnnâˆ’1 (i1 , Â· Â· Â· , iÎ¸âˆ’1 , iÎ¸+1 , Â· Â· Â· , in )

(3.4)

where n = iÎ¸ in the ordered list, (i1 , Â· Â· Â· , in ) .
Proof: To begin with, it is necessary to show the existence of such a function. This is
clearly true if n = 1. Deï¬ne sgn1 (1) â‰¡ 1 and observe that it works. No switching is possible.
In the case where n = 2, it is also clearly true. Let sgn2 (1, 2) = 1 and sgn2 (2, 1) = âˆ’1
while sgn2 (2, 2) = sgn2 (1, 1) = 0 and verify it works. Assuming such a function exists for n,
sgnn+1 will be deï¬ned in terms of sgnn . If there are any repeated numbers in (i1 , Â· Â· Â· , in+1 ) ,
sgnn+1 (i1 , Â· Â· Â· , in+1 ) â‰¡ 0. If there are no repeats, then n + 1 appears somewhere in the
ordered list. Let Î¸ be the position of the number n + 1 in the list. Thus, the list is of the
form (i1 , Â· Â· Â· , iÎ¸âˆ’1 , n + 1, iÎ¸+1 , Â· Â· Â· , in+1 ) . From (3.4) it must be that
sgnn+1 (i1 , Â· Â· Â· , iÎ¸âˆ’1 , n + 1, iÎ¸+1 , Â· Â· Â· , in+1 ) â‰¡
(âˆ’1)

n+1âˆ’Î¸

sgnn (i1 , Â· Â· Â· , iÎ¸âˆ’1 , iÎ¸+1 , Â· Â· Â· , in+1 ) .

It is necessary to verify this satisï¬es (3.2) and (3.3) with n replaced with n + 1. The ï¬rst of
these is obviously true because
sgnn+1 (1, Â· Â· Â· , n, n + 1) â‰¡ (âˆ’1)

n+1âˆ’(n+1)

sgnn (1, Â· Â· Â· , n) = 1.

If there are repeated numbers in (i1 , Â· Â· Â· , in+1 ) , then it is obvious (3.3) holds because both
sides would equal zero from the above deï¬nition. It remains to verify (3.3) in the case where
there are no numbers repeated in (i1 , Â· Â· Â· , in+1 ) . Consider
(
)
r
s
sgnn+1 i1 , Â· Â· Â· , p, Â· Â· Â· , q, Â· Â· Â· , in+1 ,
where the r above the p indicates the number p is in the rth position and the s above the q
indicates that the number, q is in the sth position. Suppose ï¬rst that r < Î¸ < s. Then
(
)
Î¸
r
s
sgnn+1 i1 , Â· Â· Â· , p, Â· Â· Â· , n + 1, Â· Â· Â· , q, Â· Â· Â· , in+1 â‰¡
(âˆ’1)
while

n+1âˆ’Î¸

(
)
r
sâˆ’1
sgnn i1 , Â· Â· Â· , p, Â· Â· Â· , q , Â· Â· Â· , in+1

(
)
Î¸
r
s
sgnn+1 i1 , Â· Â· Â· , q, Â· Â· Â· , n + 1, Â· Â· Â· , p, Â· Â· Â· , in+1 â‰¡

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

3.3. THE MATHEMATICAL THEORY OF DETERMINANTS
(âˆ’1)

n+1âˆ’Î¸

85

(
)
r
sâˆ’1
sgnn i1 , Â· Â· Â· , q, Â· Â· Â· , p , Â· Â· Â· , in+1

and so, by induction, a switch of p and q introduces a minus sign in the result. Similarly,
if Î¸ > s or if Î¸ < r it also follows that (3.3) holds. The interesting case is when Î¸ = r or
Î¸ = s. Consider the case where Î¸ = r and note the other case is entirely similar.
(
)
r
s
sgnn+1 i1 , Â· Â· Â· , n + 1, Â· Â· Â· , q, Â· Â· Â· , in+1 â‰¡
(âˆ’1)
while

n+1âˆ’r

(
)
sâˆ’1
sgnn i1 , Â· Â· Â· , q , Â· Â· Â· , in+1

(3.5)

(
)
s
r
sgnn+1 i1 , Â· Â· Â· , q, Â· Â· Â· , n + 1, Â· Â· Â· , in+1 =
(
)
r
n+1âˆ’s
(âˆ’1)
sgnn i1 , Â· Â· Â· , q, Â· Â· Â· , in+1 .

(3.6)

By making s âˆ’ 1 âˆ’ r switches, move the q which is in the s âˆ’ 1th position in (3.5) to the rth
position in (3.6). By induction, each of these switches introduces a factor of âˆ’1 and so
(
)
(
)
sâˆ’1
r
sâˆ’1âˆ’r
sgnn i1 , Â· Â· Â· , q , Â· Â· Â· , in+1 = (âˆ’1)
sgnn i1 , Â· Â· Â· , q, Â· Â· Â· , in+1 .
Therefore,
(
)
(
)
r
s
sâˆ’1
n+1âˆ’r
sgnn+1 i1 , Â· Â· Â· , n + 1, Â· Â· Â· , q, Â· Â· Â· , in+1 = (âˆ’1)
sgnn i1 , Â· Â· Â· , q , Â· Â· Â· , in+1
(
)
r
sâˆ’1âˆ’r
(âˆ’1)
sgnn i1 , Â· Â· Â· , q, Â· Â· Â· , in+1
(
)
(
)
r
r
n+s
2sâˆ’1
n+1âˆ’s
= (âˆ’1)
sgnn i1 , Â· Â· Â· , q, Â· Â· Â· , in+1 = (âˆ’1)
(âˆ’1)
sgnn i1 , Â· Â· Â· , q, Â· Â· Â· , in+1
(
)
s
r
= âˆ’ sgnn+1 i1 , Â· Â· Â· , q, Â· Â· Â· , n + 1, Â· Â· Â· , in+1 .
= (âˆ’1)

n+1âˆ’r

This proves the existence of the desired function. Uniqueness follows easily from the following lemma.
Lemma 3.3.2 Every ordered list of {1, 2, Â· Â· Â· , n} can be obtained from every other ordered
list by a ï¬nite number of switches. Also, sgn is unique.
Proof: This is obvious if n = 1 or 2. Suppose then that it is true for sets of n âˆ’ 1
elements. Take two ordered lists of numbers, P1 , P2 . To get from P1 to P2 using switches,
ï¬rst make a switch to obtain the last element in the list coinciding with the last element of
P2 . By induction, there are switches which will arrange the ï¬rst n âˆ’ 1 to the right order.
To see sgnn is unique, if there exist two functions, f and g both satisfying (3.2) and
(3.3), you could start with f (1, Â· Â· Â· , n) = g (1, Â· Â· Â· , n) and applying the same sequence of
switches, eventually arrive at f (i1 , Â· Â· Â· , in ) = g (i1 , Â· Â· Â· , in ) . If any numbers are repeated,
then (3.3) gives both functions are equal to zero for that ordered list. 
Deï¬nition 3.3.3 When you have an ordered list of distinct numbers from {1, 2, Â· Â· Â· , n} , say
(i1 , Â· Â· Â· , in ) , this ordered list is called a permutation. The symbol for all such permutations
is Sn . The number sgnn (i1 , Â· Â· Â· , in ) is called the sign of the permutation.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

86

DETERMINANTS

A permutation can also be considered as a function from the set
{1, 2, Â· Â· Â· , n} to {1, 2, Â· Â· Â· , n}
as follows. Let f (k) = ik . Permutations are of fundamental importance in certain areas
of math. For example, it was by considering permutations that Galois was able to give a
criterion for solution of polynomial equations by radicals, but this is a diï¬€erent direction
than what is being attempted here.
In what follows sgn will often be used rather than sgnn because the context supplies the
appropriate n.

3.3.2

The Deï¬nition Of The Determinant

Deï¬nition 3.3.4 Let f be a real valued function which has the set of ordered lists of numbers
from {1, Â· Â· Â· , n} as its domain. Deï¬ne
âˆ‘
f (k1 Â· Â· Â· kn )
(k1 ,Â·Â·Â· ,kn )

to be the sum of all the f (k1 Â· Â· Â· kn ) for all possible choices of ordered lists (k1 , Â· Â· Â· , kn ) of
numbers of {1, Â· Â· Â· , n} . For example,
âˆ‘
f (k1 , k2 ) = f (1, 2) + f (2, 1) + f (1, 1) + f (2, 2) .
(k1 ,k2 )

Deï¬nition 3.3.5 Let (aij ) = A denote an n Ã— n matrix. The determinant of A, denoted
by det (A) is deï¬ned by
âˆ‘
sgn (k1 , Â· Â· Â· , kn ) a1k1 Â· Â· Â· ankn
det (A) â‰¡
(k1 ,Â·Â·Â· ,kn )

where the sum is taken over all ordered lists of numbers from {1, Â· Â· Â· , n}. Note it suï¬ƒces to
take the sum over only those ordered lists in which there are no repeats because if there are,
sgn (k1 , Â· Â· Â· , kn ) = 0 and so that term contributes 0 to the sum.
Let A be an n Ã— n matrix A = (aij ) and let (r1 , Â· Â· Â· , rn ) denote an ordered list of n
numbers from {1, Â· Â· Â· , n}. Let A (r1 , Â· Â· Â· , rn ) denote the matrix whose k th row is the rk row
of the matrix A. Thus
âˆ‘
det (A (r1 , Â· Â· Â· , rn )) =
sgn (k1 , Â· Â· Â· , kn ) ar1 k1 Â· Â· Â· arn kn
(3.7)
(k1 ,Â·Â·Â· ,kn )

and A (1, Â· Â· Â· , n) = A.
Proposition 3.3.6 Let (r1 , Â· Â· Â· , rn ) be an ordered list of numbers from {1, Â· Â· Â· , n}. Then
âˆ‘
sgn (r1 , Â· Â· Â· , rn ) det (A) =
sgn (k1 , Â· Â· Â· , kn ) ar1 k1 Â· Â· Â· arn kn
(3.8)
(k1 ,Â·Â·Â· ,kn )

= det (A (r1 , Â· Â· Â· , rn )) .

(3.9)

Proof: Let (1, Â· Â· Â· , n) = (1, Â· Â· Â· , r, Â· Â· Â· s, Â· Â· Â· , n) so r < s.
det (A (1, Â· Â· Â· , r, Â· Â· Â· , s, Â· Â· Â· , n)) =

Saylor URL: http://www.saylor.org/courses/ma212/

(3.10)

The Saylor Foundation

3.3. THE MATHEMATICAL THEORY OF DETERMINANTS
âˆ‘

87

sgn (k1 , Â· Â· Â· , kr , Â· Â· Â· , ks , Â· Â· Â· , kn ) a1k1 Â· Â· Â· arkr Â· Â· Â· asks Â· Â· Â· ankn ,

(k1 ,Â·Â·Â· ,kn )

and renaming the variables, calling ks , kr and kr , ks , this equals
âˆ‘
=
sgn (k1 , Â· Â· Â· , ks , Â· Â· Â· , kr , Â· Â· Â· , kn ) a1k1 Â· Â· Â· arks Â· Â· Â· askr Â· Â· Â· ankn
(k1 ,Â·Â·Â· ,kn )

=

ï£«

âˆ‘

These got switched

âˆ’ sgn ï£­k1 , Â· Â· Â· ,

z }| {
kr , Â· Â· Â· , ks

ï£¶

, Â· Â· Â· , kn ï£¸ a1k1 Â· Â· Â· askr Â· Â· Â· arks Â· Â· Â· ankn

(k1 ,Â·Â·Â· ,kn )

= âˆ’ det (A (1, Â· Â· Â· , s, Â· Â· Â· , r, Â· Â· Â· , n)) .

(3.11)

Consequently,
det (A (1, Â· Â· Â· , s, Â· Â· Â· , r, Â· Â· Â· , n)) = âˆ’ det (A (1, Â· Â· Â· , r, Â· Â· Â· , s, Â· Â· Â· , n)) = âˆ’ det (A)
Now letting A (1, Â· Â· Â· , s, Â· Â· Â· , r, Â· Â· Â· , n) play the role of A, and continuing in this way, switching pairs of numbers,
p
det (A (r1 , Â· Â· Â· , rn )) = (âˆ’1) det (A)
where it took p switches to obtain(r1 , Â· Â· Â· , rn ) from (1, Â· Â· Â· , n). By Lemma 3.3.1, this implies
p

det (A (r1 , Â· Â· Â· , rn )) = (âˆ’1) det (A) = sgn (r1 , Â· Â· Â· , rn ) det (A)
and proves the proposition in the case when there are no repeated numbers in the ordered
list, (r1 , Â· Â· Â· , rn ). However, if there is a repeat, say the rth row equals the sth row, then the
reasoning of (3.10) -(3.11) shows that det(A (r1 , Â· Â· Â· , rn )) = 0 and also sgn (r1 , Â· Â· Â· , rn ) = 0
so the formula holds in this case also. 
Observation 3.3.7 There are n! ordered lists of distinct numbers from {1, Â· Â· Â· , n} .
To see this, consider n slots placed in order. There are n choices for the ï¬rst slot. For
each of these choices, there are n âˆ’ 1 choices for the second. Thus there are n (n âˆ’ 1) ways
to ï¬ll the ï¬rst two slots. Then for each of these ways there are n âˆ’ 2 choices left for the third
slot. Continuing this way, there are n! ordered lists of distinct numbers from {1, Â· Â· Â· , n} as
stated in the observation.

3.3.3

A Symmetric Deï¬nition

With the above, it is possible to give a (more
) symmetric description of the determinant from
which it will follow that det (A) = det AT .
Corollary 3.3.8 The following formula for det (A) is valid.
det (A) =

1
Â·
n!

âˆ‘

âˆ‘

sgn (r1 , Â· Â· Â· , rn ) sgn (k1 , Â· Â· Â· , kn ) ar1 k1 Â· Â· Â· arn kn .

(3.12)

(r1 ,Â·Â·Â· ,rn ) (k1 ,Â·Â·Â· ,kn )

( )
( )
And also det AT = det (A) where AT is the transpose of A. (Recall that for AT = aTij ,
aTij = aji .)

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

88

DETERMINANTS

Proof: From Proposition 3.3.6, if the ri are distinct,
âˆ‘
det (A) =
sgn (r1 , Â· Â· Â· , rn ) sgn (k1 , Â· Â· Â· , kn ) ar1 k1 Â· Â· Â· arn kn .
(k1 ,Â·Â·Â· ,kn )

Summing over all ordered lists, (r1 , Â· Â· Â· , rn ) where the ri are distinct, (If the ri are not
distinct, sgn (r1 , Â· Â· Â· , rn ) = 0 and so there is no contribution to the sum.)
âˆ‘
âˆ‘
n! det (A) =
sgn (r1 , Â· Â· Â· , rn ) sgn (k1 , Â· Â· Â· , kn ) ar1 k1 Â· Â· Â· arn kn .
(r1 ,Â·Â·Â· ,rn ) (k1 ,Â·Â·Â· ,kn )

This proves the corollary since the formula gives the same number for A as it does for AT .

Corollary 3.3.9 If two rows or two columns in an n Ã— n matrix A, are switched, the
determinant of the resulting matrix equals (âˆ’1) times the determinant of the original matrix.
If A is an nÃ—n matrix in which two rows are equal or two columns are equal then det (A) = 0.
Suppose the ith row of A equals (xa1 + yb1 , Â· Â· Â· , xan + ybn ). Then
det (A) = x det (A1 ) + y det (A2 )
where the ith row of A1 is (a1 , Â· Â· Â· , an ) and the ith row of A2 is (b1 , Â· Â· Â· , bn ) , all other rows
of A1 and A2 coinciding with those of A. In other words, det is a linear function of each
row A. The same is true with the word â€œrowâ€ replaced with the word â€œcolumnâ€.
Proof: By Proposition 3.3.6 when two rows are switched, the determinant of the resulting matrix is (âˆ’1) times the determinant of the original matrix. By Corollary 3.3.8 the
same holds for columns because the columns of the matrix equal the rows of the transposed
matrix. Thus if A1 is the matrix obtained from A by switching two columns,
( )
( )
det (A) = det AT = âˆ’ det AT1 = âˆ’ det (A1 ) .
If A has two equal columns or two equal rows, then switching them results in the same
matrix. Therefore, det (A) = âˆ’ det (A) and so det (A) = 0.
It remains to verify the last assertion.
âˆ‘
det (A) â‰¡
sgn (k1 , Â· Â· Â· , kn ) a1k1 Â· Â· Â· (xarki + ybrki ) Â· Â· Â· ankn
(k1 ,Â·Â·Â· ,kn )

=x
+y

âˆ‘

âˆ‘

sgn (k1 , Â· Â· Â· , kn ) a1k1 Â· Â· Â· arki Â· Â· Â· ankn

(k1 ,Â·Â·Â· ,kn )

sgn (k1 , Â· Â· Â· , kn ) a1k1 Â· Â· Â· brki Â· Â· Â· ankn â‰¡ x det (A1 ) + y det (A2 ) .

(k1 ,Â·Â·Â· ,kn )

( )
The same is true of columns because det AT = det (A) and the rows of AT are the columns
of A. 

3.3.4

Basic Properties Of The Determinant

Deï¬nition 3.3.10 A vector, w, is aâˆ‘
linear combination of the vectors {v1 , Â· Â· Â· , vr } if there
r
exist scalars c1 , Â· Â· Â· cr such that w = k=1 ck vk . This is the same as saying
w âˆˆ span (v1 , Â· Â· Â· , vr ) .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

3.3. THE MATHEMATICAL THEORY OF DETERMINANTS

89

The following corollary is also of great use.
Corollary 3.3.11 Suppose A is an n Ã— n matrix and some column (row) is a linear combination of r other columns (rows). Then det (A) = 0.
(
)
Proof: Let A = a1 Â· Â· Â· an be the columns of A and suppose the condition that
one column is a linear combination of r of the others is satisï¬ed. Then by using Corollary
3.3.9 you may rearrange theâˆ‘columns to have the nth column a linear combination of the
r
ï¬rst r columns. Thus an = k=1 ck ak and so
(
)
âˆ‘r
det (A) = det a1 Â· Â· Â· ar Â· Â· Â· anâˆ’1
.
k=1 ck ak
By Corollary 3.3.9
r
âˆ‘

det (A) =

ck det

(

a1

Â·Â·Â·

ar

Â·Â·Â·

anâˆ’1

ak

)

= 0.

k=1

( )
The case for rows follows from the fact that det (A) = det AT . 
Recall the following deï¬nition of matrix multiplication.
Deï¬nition âˆ‘
3.3.12 If A and B are n Ã— n matrices, A = (aij ) and B = (bij ), AB = (cij )
n
where cij â‰¡ k=1 aik bkj .
One of the most important rules about determinants is that the determinant of a product
equals the product of the determinants.
Theorem 3.3.13 Let A and B be n Ã— n matrices. Then
det (AB) = det (A) det (B) .
Proof: Let cij be the ij th entry of AB. Then by Proposition 3.3.6,
âˆ‘
det (AB) =
sgn (k1 , Â· Â· Â· , kn ) c1k1 Â· Â· Â· cnkn
(k1 ,Â·Â·Â· ,kn )

=

âˆ‘

sgn (k1 , Â· Â· Â· , kn )

(k1 ,Â·Â·Â· ,kn )

=

âˆ‘

âˆ‘

(
âˆ‘
r1

)
a1r1 br1 k1

(
Â·Â·Â·

âˆ‘

)
anrn brn kn

rn

sgn (k1 , Â· Â· Â· , kn ) br1 k1 Â· Â· Â· brn kn (a1r1 Â· Â· Â· anrn )

(r1 Â·Â·Â· ,rn ) (k1 ,Â·Â·Â· ,kn )

=

âˆ‘

sgn (r1 Â· Â· Â· rn ) a1r1 Â· Â· Â· anrn det (B) = det (A) det (B) .

(r1 Â·Â·Â· ,rn )

The Binet Cauchy formula is a generalization of the theorem which says the determinant
of a product is the product of the determinants. The situation is illustrated in the following
picture where A, B are matrices.
B

A

Theorem 3.3.14 Let A be an n Ã— m matrix with n â‰¥ m and let B be a m Ã— n matrix. Also
let Ai
i = 1, Â· Â· Â· , C (n, m)

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

90

DETERMINANTS

be the m Ã— m submatrices of A which are obtained by deleting n âˆ’ m rows and let Bi be the
m Ã— m submatrices of B which are obtained by deleting corresponding n âˆ’ m columns. Then
âˆ‘

C(n,m)

det (BA) =

det (Bk ) det (Ak )

k=1

Proof: This follows from a computation. By Corollary 3.3.8 on Page 87, det (BA) =
âˆ‘
1 âˆ‘
sgn (i1 Â· Â· Â· im ) sgn (j1 Â· Â· Â· jm ) (BA)i1 j1 (BA)i2 j2 Â· Â· Â· (BA)im jm
m!
(i1 Â·Â·Â·im ) (j1 Â·Â·Â·jm )

1
m!

âˆ‘

âˆ‘

sgn (i1 Â· Â· Â· im ) sgn (j1 Â· Â· Â· jm ) Â·

(i1 Â·Â·Â·im ) (j1 Â·Â·Â·jm )

n
âˆ‘

Bi1 r1 Ar1 j1

r1 =1

n
âˆ‘

B i2 r2 A r2 j 2 Â· Â· Â·

r2 =1

n
âˆ‘

Bim rm Arm jm

rm =1

Now denote by Ik one of the r subsets of {1, Â· Â· Â· , n} . Thus there are C (n, m) of these.
=

C(n,m)

âˆ‘

âˆ‘

k=1

{r1 ,Â·Â·Â· ,rm }=Ik

1
m!

âˆ‘

âˆ‘

sgn (i1 Â· Â· Â· im ) sgn (j1 Â· Â· Â· jm ) Â·

(i1 Â·Â·Â·im ) (j1 Â·Â·Â·jm )

Bi1 r1 Ar1 j1 Bi2 r2 Ar2 j2 Â· Â· Â· Bim rm Arm jm

=

C(n,m)

âˆ‘

âˆ‘

k=1

{r1 ,Â·Â·Â· ,rm }=Ik

âˆ‘

1
m!

âˆ‘

sgn (i1 Â· Â· Â· im ) Bi1 r1 Bi2 r2 Â· Â· Â· Bim rm Â·

(i1 Â·Â·Â·im )

sgn (j1 Â· Â· Â· jm ) Ar1 j1 Ar2 j2 Â· Â· Â· Arm jm

(j1 Â·Â·Â·jm )

=

C(n,m)

âˆ‘

âˆ‘

k=1

{r1 ,Â·Â·Â· ,rm }=Ik

âˆ‘

1
2
sgn (r1 Â· Â· Â· rm ) det (Bk ) det (Ak ) B
m!

C(n,m)

=

det (Bk ) det (Ak )

k=1

since there are m! ways of arranging the indices {r1 , Â· Â· Â· , rm }. 

3.3.5

Expansion Using Cofactors

Lemma 3.3.15 Suppose a matrix is of the form
(
)
A âˆ—
M=
0 a
(

or
M=

A 0
âˆ— a

(3.13)

)
(3.14)

where a is a number and A is an (n âˆ’ 1) Ã— (n âˆ’ 1) matrix and âˆ— denotes either a column
or a row having length n âˆ’ 1 and the 0 denotes either a column or a row of length n âˆ’ 1
consisting entirely of zeros. Then det (M ) = a det (A) .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

3.3. THE MATHEMATICAL THEORY OF DETERMINANTS

91

Proof: Denote M by (mij ) . Thus in the ï¬rst case, mnn = a and mni = 0 if i Ì¸= n while
in the second case, mnn = a and min = 0 if i Ì¸= n. From the deï¬nition of the determinant,
âˆ‘
det (M ) â‰¡
sgnn (k1 , Â· Â· Â· , kn ) m1k1 Â· Â· Â· mnkn
(k1 ,Â·Â·Â· ,kn )

Letting Î¸ denote the position of n in the ordered list, (k1 , Â· Â· Â· , kn ) then using the earlier
conventions used to prove Lemma 3.3.1, det (M ) equals
(
)
âˆ‘
Î¸
nâˆ’1
nâˆ’Î¸
(âˆ’1)
sgnnâˆ’1 k1 , Â· Â· Â· , kÎ¸âˆ’1 , kÎ¸+1 , Â· Â· Â· , kn m1k1 Â· Â· Â· mnkn
(k1 ,Â·Â·Â· ,kn )

Now suppose (3.14). Then if kn Ì¸= n, the term involving mnkn in the above expression
equals zero. Therefore, the only terms which survive are those for which Î¸ = n or in other
words, those for which kn = n. Therefore, the above expression reduces to
âˆ‘
a
sgnnâˆ’1 (k1 , Â· Â· Â· knâˆ’1 ) m1k1 Â· Â· Â· m(nâˆ’1)knâˆ’1 = a det (A) .
(k1 ,Â·Â·Â· ,knâˆ’1 )

To get the assertion in the situation of (3.13) use Corollary 3.3.8 and (3.14) to write
(( T
))
(
)
( )
A
0
det (M ) = det M T = det
= a det AT = a det (A) .
âˆ— a
In terms of the theory of determinants, arguably the most important idea is that of
Laplace expansion along a row or a column. This will follow from the above deï¬nition of a
determinant.
Deï¬nition 3.3.16 Let A = (aij ) be an nÃ—n matrix. Then a new matrix called the cofactor
matrix cof (A) is deï¬ned by cof (A) = (cij ) where to obtain cij delete the ith row and the
j th column of A, take the determinant of the (n âˆ’ 1) Ã— (n âˆ’ 1) matrix which results, (This
i+j
is called the ij th minor of A. ) and then multiply this number by (âˆ’1) . To make the
th
formulas easier to remember, cof (A)ij will denote the ij entry of the cofactor matrix.
The following is the main result. Earlier this was given as a deï¬nition and the outrageous
totally unjustiï¬ed assertion was made that the same number would be obtained by expanding
the determinant along any row or column. The following theorem proves this assertion.
Theorem 3.3.17 Let A be an n Ã— n matrix where n â‰¥ 2. Then
det (A) =

n
âˆ‘

aij cof (A)ij =

j=1

n
âˆ‘

aij cof (A)ij .

(3.15)

i=1

The ï¬rst formula consists of expanding the determinant along the ith row and the second
expands the determinant along the j th column.
Proof: Let (ai1 , Â· Â· Â· , ain ) be the ith row of A. Let Bj be the matrix obtained from A by
leaving every row the same except the ith row which in Bj equals (0, Â· Â· Â· , 0, aij , 0, Â· Â· Â· , 0) .
Then by Corollary 3.3.9,
n
âˆ‘
det (A) =
det (Bj )
j=1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

92

DETERMINANTS

For example if

ï£«

a b
A=ï£­ d e
h i

ï£¶
c
f ï£¸
j

and i = 2, then
ï£«

a
B1 = ï£­ d
h

ï£¶
ï£«
ï£¶
ï£«
b c
a b c
a b
0 0 ï£¸ , B2 = ï£­ 0 e 0 ï£¸ , B3 = ï£­ 0 0
i j
h i j
h i

ï£¶
c
f ï£¸
j

Denote by Aij the (n âˆ’ 1) Ã— (n âˆ’ 1) matrix obtained by deleting the ith row and the j th
( )
i+j
column of A. Thus cof (A)ij â‰¡ (âˆ’1) det Aij . At this point, recall that from Proposition
3.3.6, when two rows or two columns in a matrix M, are switched, this results in multiplying
the determinant of the old matrix by âˆ’1 to get the determinant of the new matrix. Therefore,
by Lemma 3.3.15,
(( ij
))
A
âˆ—
nâˆ’j
nâˆ’i
det (Bj ) = (âˆ’1)
(âˆ’1)
det
0 aij
(( ij
))
A
âˆ—
i+j
= (âˆ’1) det
= aij cof (A)ij .
0 aij
Therefore,
det (A) =

n
âˆ‘

aij cof (A)ij

j=1

which is the formula for expanding det (A) along the ith row. Also,
n
n
âˆ‘
( )
( ) âˆ‘
det (A) = det AT =
aTij cof AT ij =
aji cof (A)ji
j=1

j=1

which is the formula for expanding det (A) along the ith column. 

3.3.6

A Formula For The Inverse

Note that this gives an easy way to write a formula for the inverse of an n Ã—n matrix. Recall
the deï¬nition of the inverse of a matrix in Deï¬nition 2.1.22 on Page 47.
(
)
Theorem 3.3.18 Aâˆ’1 exists if and only if det(A) Ì¸= 0. If det(A) Ì¸= 0, then Aâˆ’1 = aâˆ’1
ij
where
âˆ’1
aâˆ’1
cof (A)ji
ij = det(A)
for cof (A)ij the ij th cofactor of A.
Proof: By Theorem 3.3.17 and letting (air ) = A, if det (A) Ì¸= 0,
n
âˆ‘

air cof (A)ir det(A)âˆ’1 = det(A) det(A)âˆ’1 = 1.

i=1

Now in the matrix A, replace the k th column with the rth column and then expand along
the k th column. This yields for k Ì¸= r,
n
âˆ‘

air cof (A)ik det(A)âˆ’1 = 0

i=1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

3.3. THE MATHEMATICAL THEORY OF DETERMINANTS

93

because there are two equal columns by Corollary 3.3.9. Summarizing,
n
âˆ‘

âˆ’1

air cof (A)ik det (A)

= Î´ rk .

i=1

Using the other formula in Theorem 3.3.17, and similar reasoning,
n
âˆ‘

arj cof (A)kj det (A)

âˆ’1

= Î´ rk

j=1

(
)
This proves that if det (A) Ì¸= 0, then Aâˆ’1 exists with Aâˆ’1 = aâˆ’1
ij , where
âˆ’1

aâˆ’1
ij = cof (A)ji det (A)

.

Now suppose Aâˆ’1 exists. Then by Theorem 3.3.13,
(
)
(
)
1 = det (I) = det AAâˆ’1 = det (A) det Aâˆ’1
so det (A) Ì¸= 0. 
The next corollary points out that if an n Ã— n matrix A has a right or a left inverse, then
it has an inverse.
Corollary 3.3.19 Let A be an n Ã— n matrix and suppose there exists an n Ã— n matrix B
such that BA = I. Then Aâˆ’1 exists and Aâˆ’1 = B. Also, if there exists C an n Ã— n matrix
such that AC = I, then Aâˆ’1 exists and Aâˆ’1 = C.
Proof: Since BA = I, Theorem 3.3.13 implies
det B det A = 1
and so det A Ì¸= 0. Therefore from Theorem 3.3.18, Aâˆ’1 exists. Therefore,
(
)
Aâˆ’1 = (BA) Aâˆ’1 = B AAâˆ’1 = BI = B.
The case where CA = I is handled similarly. 
The conclusion of this corollary is that left inverses, right inverses and inverses are all
the same in the context of n Ã— n matrices.
Theorem 3.3.18 says that to ï¬nd the inverse, take the transpose of the cofactor matrix
and divide by the determinant. The transpose of the cofactor matrix is called the adjugate
or sometimes the classical adjoint of the matrix A. It is an abomination to call it the adjoint
although you do sometimes see it referred to in this way. In words, Aâˆ’1 is equal to one over
the determinant of A times the adjugate matrix of A.
In case you are solving a system of equations, Ax = y for x, it follows that if Aâˆ’1 exists,
(
)
x = Aâˆ’1 A x = Aâˆ’1 (Ax) = Aâˆ’1 y
thus solving the system. Now in the case that Aâˆ’1 exists, there is a formula for Aâˆ’1 given
above. Using this formula,
xi =

n
âˆ‘
j=1

aâˆ’1
ij yj =

n
âˆ‘
j=1

Saylor URL: http://www.saylor.org/courses/ma212/

1
cof (A)ji yj .
det (A)

The Saylor Foundation

94

DETERMINANTS

By the formula for the expansion of a determinant along a column,
ï£«
ï£¶
âˆ— Â· Â· Â· y1 Â· Â· Â· âˆ—
1
ï£¬
..
.. ï£· ,
xi =
det ï£­ ...
.
. ï£¸
det (A)
âˆ— Â· Â· Â· yn Â· Â· Â· âˆ—
T

where here the ith column of A is replaced with the column vector, (y1 Â· Â· Â· Â·, yn ) , and the
determinant of this modiï¬ed matrix is taken and divided by det (A). This formula is known
as Cramerâ€™s rule.
Deï¬nition 3.3.20 A matrix M , is upper triangular if Mij = 0 whenever i > j. Thus such
a matrix equals zero below the main diagonal, the entries of the form Mii as shown.
ï£«
ï£¶
âˆ— âˆ— Â·Â·Â· âˆ—
ï£¬
. ï£·
..
ï£¬ 0 âˆ—
. .. ï£·
ï£·
ï£¬
ï£¬ . .
ï£·
.. ... âˆ— ï£¸
ï£­ ..
0 Â·Â·Â·
0 âˆ—
A lower triangular matrix is deï¬ned similarly as a matrix for which all entries above the
main diagonal are equal to zero.
With this deï¬nition, here is a simple corollary of Theorem 3.3.17.
Corollary 3.3.21 Let M be an upper (lower) triangular matrix. Then det (M ) is obtained
by taking the product of the entries on the main diagonal.

3.3.7

Rank Of A Matrix

Deï¬nition 3.3.22 A submatrix of a matrix A is the rectangular array of numbers obtained
by deleting some rows and columns of A. Let A be an m Ã— n matrix. The determinant
rank of the matrix equals r where r is the largest number such that some r Ã— r submatrix
of A has a non zero determinant. The row rank is deï¬ned to be the dimension of the span
of the rows. The column rank is deï¬ned to be the dimension of the span of the columns.
Theorem 3.3.23 If A, an m Ã— n matrix has determinant rank r, then there exist r rows of
the matrix such that every other row is a linear combination of these r rows.
Proof: Suppose the determinant rank of A = (aij ) equals r. Thus some r Ã— r submatrix
has non zero determinant and there is no larger square submatrix which has non zero
determinant. Suppose such a submatrix is determined by the r columns whose indices are
j1 < Â· Â· Â· < jr
and the r rows whose indices are
i1 < Â· Â· Â· < ir
I want to show that every row is a linear combination of these rows. Consider the lth row
and let p be an index between 1 and n. Form the following (r + 1) Ã— (r + 1) matrix
ï£«
ï£¶
ai1 j1 Â· Â· Â· ai1 jr ai1 p
ï£¬ ..
ï£·
..
..
ï£¬ .
ï£·
.
.
ï£¬
ï£·
ï£­ air j1 Â· Â· Â· air jr air p ï£¸
alj1
Â· Â· Â· aljr
alp

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

3.3. THE MATHEMATICAL THEORY OF DETERMINANTS

95

Of course you can assume l âˆˆ
/ {i1 , Â· Â· Â· , ir } because there is nothing to prove if the lth
row is one of the chosen ones. The above matrix has determinant 0. This is because if
pâˆˆ
/ {j1 , Â· Â· Â· , jr } then the above would be a submatrix of A which is too large to have non
zero determinant. On the other hand, if p âˆˆ {j1 , Â· Â· Â· , jr } then the above matrix has two
columns which are equal so its determinant is still 0.
Expand the determinant of the above matrix along the last column. Let Ck denote the
cofactor associated with the entry aik p . This is not dependent on the choice of p. Remember,
you delete the column and the row the entry is in and take the determinant of what is left
and multiply by âˆ’1 raised to an appropriate power. Let C denote the cofactor associated
with alp . This is given to be nonzero, it being the determinant of the matrix
ï£«
ï£¶
ai1 j1 Â· Â· Â· ai1 jr
ï£¬ ..
ï£·
..
ï£¸
ï£­ .
.
air j1

Â·Â·Â·

Thus
0 = alp C +

a ir j r

r
âˆ‘

Ck aik p

k=1

which implies
alp =

r
âˆ‘
âˆ’Ck
k=1

C

aik p â‰¡

r
âˆ‘

mk aik p

k=1

Since this is true for every p and since mk does not depend on p, this has shown the lth row
is a linear combination of the i1 , i2 , Â· Â· Â· , ir rows. 
Corollary 3.3.24 The determinant rank equals the row rank.
Proof: From Theorem 3.3.23, every row is in the span of r rows where r is the determinant rank. Therefore, the row rank (dimension of the span of the rows) is no larger than
the determinant rank. Could the row rank be smaller than the determinant rank? If so,
it follows from Theorem 3.3.23 that there exist p rows for p < r â‰¡ determinant rank, such
that the span of these p rows equals the row space. But then you could consider the r Ã— r
sub matrix which determines the determinant rank and it would follow that each of these
rows would be in the span of the restrictions of the p rows just mentioned. By Theorem
2.4.4, the exchange theorem, the rows of this sub matrix would not be linearly independent
and so some row is a linear combination of the others. By Corollary 3.3.11 the determinant
would be 0, a contradiction. 
Corollary 3.3.25 If A has determinant rank r, then there exist r columns of the matrix
such that every other column is a linear combination of these r columns. Also the column
rank equals the determinant rank.
Proof: This follows from the above by considering AT . The rows of AT are the columns
of A and the determinant rank of AT and A are the same. Therefore, from Corollary 3.3.24,
column rank of A = row rank of AT = determinant rank of AT = determinant rank of A.
The following theorem is of fundamental importance and ties together many of the ideas
presented above.
Theorem 3.3.26 Let A be an n Ã— n matrix. Then the following are equivalent.
1. det (A) = 0.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

96

DETERMINANTS

2. A, AT are not one to one.
3. A is not onto.
Proof: Suppose det (A) = 0. Then the determinant rank of A = r < n. Therefore,
there exist r columns such that every other column is a linear combination of these columns
th
by Theorem 3.3.23. In particular, it follows that for
( some m, the m column
) is a linear
combination of all the others. Thus letting A = a1 Â· Â· Â· am Â· Â· Â· an where the
columns are denoted by ai , there exists scalars Î±i such that
âˆ‘
am =
Î± k ak .
kÌ¸=m

Now consider the column vector, x â‰¡

(

Î±1

Ax = âˆ’am +

Â·Â·Â·
âˆ‘

âˆ’1 Â· Â· Â·

Î±n

)T

. Then

Î±k ak = 0.

kÌ¸=m

Since also A0 = 0, it follows A is not one to one. Similarly, AT is not one to one by the
same argument applied to AT . This veriï¬es that 1.) implies 2.).
Now suppose 2.). Then since AT is not one to one, it follows there exists x Ì¸= 0 such that
AT x = 0.
Taking the transpose of both sides yields
x T A = 0T
where the 0T is a 1 Ã— n matrix or row vector. Now if Ay = x, then
(
)
2
|x| = xT (Ay) = xT A y = 0y = 0
contrary to x Ì¸= 0. Consequently there can be no y such that Ay = x and so A is not onto.
This shows that 2.) implies 3.).
Finally, suppose 3.). If 1.) does not hold, then det (A) Ì¸= 0 but then from Theorem 3.3.18
Aâˆ’1 exists and so for every y âˆˆ Fn there exists a unique x âˆˆ Fn such that Ax = y. In fact
x = Aâˆ’1 y. Thus A would be onto contrary to 3.). This shows 3.) implies 1.). 
Corollary 3.3.27 Let A be an n Ã— n matrix. Then the following are equivalent.
1. det(A) Ì¸= 0.
2. A and AT are one to one.
3. A is onto.
Proof: This follows immediately from the above theorem.

3.3.8

Summary Of Determinants

In all the following A, B are n Ã— n matrices
1. det (A) is a number.
2. det (A) is linear in each row and in each column.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

3.4. THE CAYLEY HAMILTON THEOREM

97

3. If you switch two rows or two columns, the determinant of the resulting matrix is âˆ’1
times the determinant of the unswitched matrix. (This and the previous one say
(a1 Â· Â· Â· an ) â†’ det (a1 Â· Â· Â· an )
is an alternating multilinear function or alternating tensor.
4. det (e1 , Â· Â· Â· , en ) = 1.
5. det (AB) = det (A) det (B)
6. det (A) can be expanded along any row or any column and the same result is obtained.
( )
7. det (A) = det AT
8. Aâˆ’1 exists if and only if det (A) Ì¸= 0 and in this case
( âˆ’1 )
A
=
ij

1
cof (A)ji
det (A)

(3.16)

9. Determinant rank, row rank and column rank are all the same number for any m Ã— n
matrix.

3.4

The Cayley Hamilton Theorem

Deï¬nition 3.4.1 Let A be an n Ã— n matrix. The characteristic polynomial is deï¬ned as
pA (t) â‰¡ det (tI âˆ’ A)
and the solutions to pA (t) = 0 are called eigenvalues. For A a matrix and p (t) = tn +
anâˆ’1 tnâˆ’1 + Â· Â· Â· + a1 t + a0 , denote by p (A) the matrix deï¬ned by
p (A) â‰¡ An + anâˆ’1 Anâˆ’1 + Â· Â· Â· + a1 A + a0 I.
The explanation for the last term is that A0 is interpreted as I, the identity matrix.
The Cayley Hamilton theorem states that every matrix satisï¬es its characteristic equation, that equation deï¬ned by pA (t) = 0. It is one of the most important theorems in linear
algebra1 . The following lemma will help with its proof.
Lemma 3.4.2 Suppose for all |Î»| large enough,
A0 + A1 Î» + Â· Â· Â· + Am Î»m = 0,
where the Ai are n Ã— n matrices. Then each Ai = 0.
Proof: Multiply by Î»âˆ’m to obtain
A0 Î»âˆ’m + A1 Î»âˆ’m+1 + Â· Â· Â· + Amâˆ’1 Î»âˆ’1 + Am = 0.
Now let |Î»| â†’ âˆ to obtain Am = 0. With this, multiply by Î» to obtain
A0 Î»âˆ’m+1 + A1 Î»âˆ’m+2 + Â· Â· Â· + Amâˆ’1 = 0.
Now let |Î»| â†’ âˆ to obtain Amâˆ’1 = 0. Continue multiplying by Î» and letting Î» â†’ âˆ to
obtain that all the Ai = 0. 
With the lemma, here is a simple corollary.
1 A special case was ï¬rst proved by Hamilton in 1853. The general case was announced by Cayley some
time later and a proof was given by Frobenius in 1878.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

98

DETERMINANTS

Corollary 3.4.3 Let Ai and Bi be n Ã— n matrices and suppose
A0 + A1 Î» + Â· Â· Â· + Am Î»m = B0 + B1 Î» + Â· Â· Â· + Bm Î»m
for all |Î»| large enough. Then Ai = Bi for all i. Consequently if Î» is replaced by any n Ã— n
matrix, the two sides will be equal. That is, for C any n Ã— n matrix,
A0 + A1 C + Â· Â· Â· + Am C m = B0 + B1 C + Â· Â· Â· + Bm C m .
Proof: Subtract and use the result of the lemma. 
With this preparation, here is a relatively easy proof of the Cayley Hamilton theorem.
Theorem 3.4.4 Let A be an n Ã— n matrix and let p (Î») â‰¡ det (Î»I âˆ’ A) be the characteristic
polynomial. Then p (A) = 0.
Proof: Let C (Î») equal the transpose of the cofactor matrix of (Î»I âˆ’ A) for |Î»| large.
(If |Î»| is large enough, then Î» cannot be in the ï¬nite list of eigenvalues of A and so for such
âˆ’1
Î», (Î»I âˆ’ A) exists.) Therefore, by Theorem 3.3.18
C (Î») = p (Î») (Î»I âˆ’ A)

âˆ’1

.

Note that each entry in C (Î») is a polynomial in Î» having degree no more than n âˆ’ 1.
Therefore, collecting the terms,
C (Î») = C0 + C1 Î» + Â· Â· Â· + Cnâˆ’1 Î»nâˆ’1
for Cj some n Ã— n matrix. It follows that for all |Î»| large enough,
(
)
(Î»I âˆ’ A) C0 + C1 Î» + Â· Â· Â· + Cnâˆ’1 Î»nâˆ’1 = p (Î») I
and so Corollary 3.4.3 may be used. It follows the matrix coeï¬ƒcients corresponding to equal
powers of Î» are equal on both sides of this equation. Therefore, if Î» is replaced with A, the
two sides will be equal. Thus
(
)
0 = (A âˆ’ A) C0 + C1 A + Â· Â· Â· + Cnâˆ’1 Anâˆ’1 = p (A) I = p (A) .

3.5

Block Multiplication Of Matrices

Consider the following problem
(

A
C

B
D

)(

You know how to do this. You get
(
AE + BG
CE + DG

E
G

F
H

)

AF + BH
CF + DH

)
.

Now what if instead of numbers, the entries, A, B, C, D, E, F, G are matrices of a size such
that the multiplications and additions needed in the above formula all make sense. Would
the formula be true in this case? I will show below that this is true.
Suppose A is a matrix of the form
ï£«
ï£¶
A11 Â· Â· Â· A1m
ï£¬
.. ï£·
..
A = ï£­ ...
(3.17)
.
. ï£¸
Ar1

Saylor URL: http://www.saylor.org/courses/ma212/

Â·Â·Â·

Arm

The Saylor Foundation

3.5. BLOCK MULTIPLICATION OF MATRICES

99

where Aij is a si Ã— pj matrix where si is constant for j = 1, Â· Â· Â· , m for each i = 1, Â· Â· Â· , r.
Such a matrix is called a block matrix, also a partitioned matrix. How do you get the
block Aij ? Here is how for A an m Ã— n matrix:

z(

si Ã—m

}|

0 Isi Ã—si

){

zï£«

nÃ—pj

}|
0

ï£¶{

0 Aï£­ Ipj Ã—pj ï£¸.
0

(3.18)

In the block column matrix on the right, you need to have cj âˆ’ 1 rows of zeros above the
small pj Ã— pj identity matrix where the columns of A involved in Aij are cj , Â· Â· Â· , cj + pj âˆ’ 1
and in the block row matrix on the left, you need to have ri âˆ’ 1 columns of zeros to the left
of the si Ã— si identity matrix where the rows of A involved in Aij are ri , Â· Â· Â· , ri + si . An
important observation to make is that the matrix on the right speciï¬es columns to use in
the block and the one on the left speciï¬es the rows used. Thus the block Aij in this case
is a matrix of size si Ã— pj . There is no overlap between the blocks of A. Thus the identity
n Ã— n identity matrix corresponding to multiplication on the right of A is of the form
ï£«
ï£¶
Ip1 Ã—p1
0
ï£·
ï£¬
..
ï£¸
ï£­
.
0

Ipm Ã—pm

where these little identity matrices donâ€™t overlap. A similar conclusion follows from consideration of the matrices Isi Ã—si . Note that in (3.18) the matrix on the right is a block column
matrix for the above block diagonal matrix and the matrix on the left in (3.18) is a block
row matrix taken from a similar block diagonal matrix consisting of the Isi Ã—si .
Next consider the question of multiplication of two block matrices. Let B be a block
matrix of the form
ï£«
ï£¶
B11 Â· Â· Â· B1p
ï£¬ ..
.. ï£·
..
(3.19)
ï£­ .
.
. ï£¸
Br1

Â·Â·Â·

Brp

A11
ï£¬ ..
ï£­ .
Ap1

Â·Â·Â·
..
.
Â·Â·Â·

ï£¶
A1m
.. ï£·
. ï£¸
Apm

and A is a block matrix of the form
ï£«

(3.20)

and that for all i, j, it makes sense to multiply Bis Asj for all s âˆˆ {1, Â· Â· Â· , p}. (That is the
two matrices, Bis and Asj are conformable.) and that
âˆ‘ for ï¬xed ij, it follows Bis Asj is the
same size for each s so that it makes sense to write s Bis Asj .
The following theorem says essentially that when you take the product of two matrices,
you can do it two ways. One way is to simply multiply them forming BA. The other way
is to partition both matrices, formally multiply the blocks to get another block matrix and
this one will be BA partitioned. Before presenting this theorem, here is a simple lemma
which is really a special case of the theorem.
Lemma 3.5.1 Consider the following product.
ï£«
ï£¶
0
(
ï£­ I ï£¸ 0 I
0

Saylor URL: http://www.saylor.org/courses/ma212/

0

)

The Saylor Foundation

100

DETERMINANTS

where the ï¬rst is n Ã— r and the second is r Ã— n. The small identity matrix I is an r Ã— r matrix
and there are l zero rows above I and l zero columns to the left of I in the right matrix.
Then the product of these matrices is a block matrix of the form
ï£«
ï£¶
0 0 0
ï£­ 0 I 0 ï£¸
0 0 0
Proof: From the deï¬nition of the way you multiply matrices, the product is
ï£« ï£«
ï£¶
ï£«
ï£¶
ï£«
ï£¶
ï£«
ï£¶
ï£«
ï£¶
ï£«
ï£¶ ï£¶
0
0
0
0
0
0
ï£­ ï£­ I ï£¸ 0 Â· Â· Â· ï£­ I ï£¸ 0 ï£­ I ï£¸ e1 Â· Â· Â· ï£­ I ï£¸ er ï£­ I ï£¸ 0 Â· Â· Â· ï£­ I ï£¸ 0 ï£¸
0
0
0
0
0
0
which yields the claimed result. In the formula ej refers to the column vector of length r
which has a 1 in the j th position. 
Theorem 3.5.2 Let B be a q Ã— p block matrix as in (3.19) and let A be a p Ã— n block matrix
as in (3.20) such that Bis is conformable with Asj and each product, Bis Asj for s = 1, Â· Â· Â· , p
is of the same size so they can be added. Then BA can be obtained as a block matrix such
that the ij th block is of the form
âˆ‘
Bis Asj .
(3.21)
s

Proof: From (3.18)

Bis Asj =

(

0 Iri Ã—ri

0

)

ï£«

0

ï£¶

B ï£­ Ips Ã—ps ï£¸
0

(

0 Ips Ã—ps

0

)

ï£«

0

ï£¶

A ï£­ Iqj Ã—qj ï£¸
0

where here it is assumed Bis is ri Ã— ps and Asj is ps Ã— qj . The product involves the sth
block in the ith row of blocks for B and the sth block in the j th column of A. Thus there
are the same number of rows above the Ips Ã—ps as there are columns to the left of Ips Ã—ps in
those two inside matrices. Then from Lemma 3.5.1
ï£«
ï£¶
ï£«
ï£¶
0
0
0
0
(
)
ï£­ Ips Ã—ps ï£¸ 0 Ips Ã—ps 0 = ï£­ 0 Ips Ã—ps 0 ï£¸
0
0
0
0
Since the blocks of small identity matrices do not overlap,
ï£¶
ï£«
ï£¶ ï£« I
0
p1 Ã—p1
0
0
0
âˆ‘
ï£·
..
ï£­ 0 Ips Ã—ps 0 ï£¸ = ï£¬
ï£­
ï£¸=I
.
s
0
0
0
0
Ipp Ã—pp
and so
âˆ‘

Bis Asj =

âˆ‘(
s

s

=

(

0 Iri Ã—ri

)

ï£«

0

ï£¶

(

)

ï£«

0

ï£¶

B ï£­ Ips Ã—ps ï£¸ 0 Ips Ã—ps 0 A ï£­ Iqj Ã—qj ï£¸
0
0
ï£«
ï£¶
ï£«
ï£¶
0
0
(
)
) âˆ‘
ï£­ Ips Ã—ps ï£¸ 0 Ips Ã—ps 0 A ï£­ Iqj Ã—qj ï£¸
0 B
s
0
0

0 Iri Ã—ri

0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

3.5. BLOCK MULTIPLICATION OF MATRICES

=

(

0 Iri Ã—ri

0

)

ï£«

ï£¶

0

BIA ï£­ Iqj Ã—qj ï£¸ =
0

101
(

0 Iri Ã—ri

0

)

ï£«

0

ï£¶

BA ï£­ Iqj Ã—qj ï£¸
0

th
which equals the ij th block of BA. Hence the
âˆ‘ ij block of BA equals the formal multiplication according to matrix multiplication,
B
s is Asj . 
)
(
a b
Example 3.5.3 Let an nÃ—n matrix have the form A =
where P is nâˆ’1Ã—nâˆ’1.
c P
(
)
p q
Multiply it by B =
where B is also an n Ã— n matrix and Q is n âˆ’ 1 Ã— n âˆ’ 1.
r Q

You use block multiplication
(
)(
a b
p
c P
r

q
Q

)

(
=

ap + br aq + bQ
pc + P r cq + P Q

)

Note that this all makes sense. For example, b = 1 Ã— n âˆ’ 1 and r = n âˆ’ 1 Ã— 1 so br is a
1 Ã— 1. Similar considerations apply to the other blocks.
Here is an interesting and signiï¬cant application of block multiplication. In this theorem,
pM (t) denotes the characteristic polynomial, det (tI âˆ’ M ) . The zeros of this polynomial will
be shown later to be eigenvalues of the matrix M . First note that from block multiplication,
for the following block matrices consisting of square blocks of an appropriate size,
(
) (
)(
)
A 0
A 0
I 0
=
so
B C
B I
0 C
(
)
(
)
(
)
A 0
A 0
I 0
det
= det
det
= det (A) det (C)
B C
B I
0 C
Theorem 3.5.4 Let A be an m Ã— n matrix and let B be an n Ã— m matrix for m â‰¤ n. Then
pBA (t) = tnâˆ’m pAB (t) ,
so the eigenvalues of BA and AB are the same including multiplicities except that BA has
nâˆ’m extra zero eigenvalues. Here pA (t) denotes the characteristic polynomial of the matrix
A.
Proof: Use block multiplication to write
(
)(
) (
)
AB 0
I A
AB ABA
=
B 0
0 I
B
BA
(
)(
) (
)
I A
0
0
AB ABA
=
.
0 I
B BA
B
BA
Therefore,

(

)âˆ’1 (

)(

)

(

)
0
BA
(
)
(
)
0
0
AB 0
Since the two matrices above are similar, it follows that
and
have
B BA
B 0
the same characteristic polynomials. See Problem 8 on Page 82. Therefore, noting that BA
is an n Ã— n matrix and AB is an m Ã— m matrix,
I
0

A
I

AB
B

0
0

I
0

A
I

=

0
B

tm det (tI âˆ’ BA) = tn det (tI âˆ’ AB)
and so det (tI âˆ’ BA) = pBA (t) = tnâˆ’m det (tI âˆ’ AB) = tnâˆ’m pAB (t) . 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

102

3.6

DETERMINANTS

Exercises

1. Let m < n and let A be an m Ã— n matrix. Show that A is not one to one. Hint:
Consider the n Ã— n matrix A1 which is of the form
(
)
A
A1 â‰¡
0
where the 0 denotes an (n âˆ’ m) Ã— n matrix of zeros. Thus det A1 = 0 and so A1 is
not one to one. Now observe that A1 x is the vector,
(
)
Ax
A1 x =
0
which equals zero if and only if Ax = 0.
2. Let v1 , Â· Â· Â· , vn be vectors in Fn and let M (v1 , Â· Â· Â· , vn ) denote the matrix whose ith
column equals vi . Deï¬ne
d (v1 , Â· Â· Â· , vn ) â‰¡ det (M (v1 , Â· Â· Â· , vn )) .
Prove that d is linear in each variable, (multilinear), that
d (v1 , Â· Â· Â· , vi , Â· Â· Â· , vj , Â· Â· Â· , vn ) = âˆ’d (v1 , Â· Â· Â· , vj , Â· Â· Â· , vi , Â· Â· Â· , vn ) ,

(3.22)

and
d (e1 , Â· Â· Â· , en ) = 1

(3.23)

where here ej is the vector in Fn which has a zero in every position except the j th
position in which it has a one.
3. Suppose f : Fn Ã— Â· Â· Â· Ã— Fn â†’ F satisï¬es (3.22) and (3.23) and is linear in each variable.
Show that f = d.
4. Show that if you replace a row (column) of an n Ã— n matrix A with itself added to
some multiple of another row (column) then the new matrix has the same determinant
as the original one.
5. Use the result of Problem 4 to evaluate by hand
ï£«
1 2 3
ï£¬ âˆ’6 3 2
det ï£¬
ï£­ 5 2 2
3 4 6

the determinant
ï£¶
2
3 ï£·
ï£·.
3 ï£¸
4

6. Find the inverse if it exists of the matrix
ï£« t
ï£¶
e
cos t
sin t
ï£­ et âˆ’ sin t cos t ï£¸ .
et âˆ’ cos t âˆ’ sin t
7. Let Ly = y (n) + anâˆ’1 (x) y (nâˆ’1) + Â· Â· Â· + a1 (x) y â€² + a0 (x) y where the ai are given
continuous functions deï¬ned on an interval, (a, b) and y is some function which has n

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

3.6. EXERCISES

103

derivatives so it makes sense to write Ly. Suppose Lyk = 0 for k =
Wronskian of these functions, yi is deï¬ned as
ï£«
y1 (x)
Â·Â·Â·
yn (x)
ï£¬ y1â€² (x)
Â·
Â·
Â·
ynâ€² (x)
ï£¬
W (y1 , Â· Â· Â· , yn ) (x) â‰¡ det ï£¬
..
..
ï£­
.
.
(nâˆ’1)
(nâˆ’1)
y1
(x) Â· Â· Â· yn
(x)

1, 2, Â· Â· Â· , n. The
ï£¶
ï£·
ï£·
ï£·
ï£¸

Show that for W (x) = W (y1 , Â· Â· Â· , yn ) (x) to save space,
ï£«

y1 (x)
..
.

Â·Â·Â·

ï£¬
ï£¬
Â·Â·Â·
W â€² (x) = det ï£¬ (nâˆ’2)
ï£­ y1
(x)
(n)
y1 (x)
Â·Â·Â·

yn (x)
..
.

ï£¶

ï£·
ï£·
ï£·.
(nâˆ’2)
yn
(x) ï£¸
(n)
yn (x)

Now use the diï¬€erential equation, Ly = 0 which is satisï¬ed by each of these functions,
yi and properties of determinants presented above to verify that W â€² + anâˆ’1 (x) W = 0.
Give an explicit solution of this linear diï¬€erential equation, Abelâ€™s formula, and use
your answer to verify that the Wronskian of these solutions to the equation, Ly = 0
either vanishes identically on (a, b) or never.
8. Two n Ã— n matrices, A and B, are similar if B = S âˆ’1 AS for some invertible n Ã— n
matrix S. Show that if two matrices are similar, they have the same characteristic
polynomials. The characteristic polynomial of A is det (Î»I âˆ’ A) .
9. Suppose the characteristic polynomial of an n Ã— n matrix A is of the form
tn + anâˆ’1 tnâˆ’1 + Â· Â· Â· + a1 t + a0
and that a0 Ì¸= 0. Find a formula Aâˆ’1 in terms of powers of the matrix A. Show that
n
Aâˆ’1 exists if and only if a0 Ì¸= 0. In fact, show that a0 = (âˆ’1) det (A) .
10. â†‘Letting p (t) denote the characteristic polynomial of A, show that pÎµ (t) â‰¡ p (t âˆ’ Îµ)
is the characteristic polynomial of A + ÎµI. Then show that if det (A) = 0, it follows
that det (A + ÎµI) Ì¸= 0 whenever |Îµ| is suï¬ƒciently small.
11. In constitutive
modeling of the stress and strain tensors, one sometimes considers sums
âˆ‘âˆ
of the form k=0 ak Ak where A is a 3Ã—3 matrix. Show using the Cayley Hamilton
theorem that if such a thing makes any sense, you can always obtain it as a ï¬nite sum
having no more than n terms.
12. Recall you can ï¬nd the determinant from expanding along the j th column.
âˆ‘
det (A) =
Aij (cof (A))ij
i

Think of det (A) as a function of the entries, Aij . Explain why the ij th cofactor is
really just
âˆ‚ det (A)
.
âˆ‚Aij

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

104

DETERMINANTS

13. Let U be an open set in Rn and let g :U â†’ Rn be such that all the ï¬rst partial
derivatives of all components of g exist and are continuous. Under these conditions
form the matrix Dg (x) given by
Dg (x)ij â‰¡

âˆ‚gi (x)
â‰¡ gi,j (x)
âˆ‚xj

The best kept secret in calculus courses is that the linear transformation determined
by this matrix Dg (x) is called the derivative of g and is the correct generalization
of the concept of derivative of a function of one variable. Suppose the second partial
derivatives also exist and are continuous. Then show that
âˆ‘
(cof (Dg))ij,j = 0.
âˆ‘

j

Hint: First explain why
i gi,k cof (Dg)ij = Î´ jk det (Dg) . Next diï¬€erentiate with
respect to xj and sum on j using the equality of mixed partial derivatives. Assume
det (Dg) Ì¸= 0 to prove the identity in this special case. Then explain using Problem 10
why there exists a sequence Îµk â†’ 0 such that for gÎµk (x) â‰¡ g (x) + Îµk x, det (DgÎµk ) Ì¸= 0
and so the identity holds for gÎµk . Then take a limit to get the desired result in general.
This is an extremely important identity which has surprising implications. One can
build degree theory on it for example. It also leads to simple proofs of the Brouwer
ï¬xed point theorem from topology.
14. A determinant of the form
1
a0
a20
..
.

1
a1
a21
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·

1
an
a2n
..
.

anâˆ’1
0
an0

anâˆ’1
1
an1

Â·Â·Â·
Â·Â·Â·

anâˆ’1
n
ann

is called a Vandermonde determinant. Show this determinant equals
âˆ
(aj âˆ’ ai )
0â‰¤i<jâ‰¤n

By this is meant to take the product of all terms of the form (aj âˆ’ ai ) such that j > i.
Hint: Show it works if n = 1 so you are looking at
1
a0

1
a1

Then suppose it holds for n âˆ’ 1 and consider the case n. Consider the polynomial in
t, p (t) which is obtained from the above by replacing the last column with the column
(
)T
1 t Â· Â· Â· tn
.
Explain why p (aj ) = 0 for i = 0, Â· Â· Â· , n âˆ’ 1. Explain why
p (t) = c

nâˆ’1
âˆ

(t âˆ’ ai ) .

i=0

Of course c is the coeï¬ƒcient of tn . Find this coeï¬ƒcient from the above description of
p (t) and the induction hypothesis. Then plug in t = an and observe you have the
formula valid for n.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Row Operations
4.1

Elementary Matrices

The elementary matrices result from doing a row operation to the identity matrix.
Deï¬nition 4.1.1 The row operations consist of the following
1. Switch two rows.
2. Multiply a row by a nonzero number.
3. Replace a row by a multiple of another row added to it.
The elementary matrices are given in the following deï¬nition.
Deï¬nition 4.1.2 The elementary matrices consist of those matrices which result by applying a row operation to an identity matrix. Those which involve switching rows of the identity
are called permutation matrices. More generally, if (i1 , i2 , Â· Â· Â· , in ) is a permutation, a matrix which has a 1 in the ik position in row k and zero in every other position of that row is
called a permutation matrix. Thus each permutation corresponds to a unique permutation
matrix.
As an example of why these elementary matrices are interesting, consider the following.
ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
0 1 0
a b c d
x y z w
ï£­ 1 0 0 ï£¸ï£­ x y z w ï£¸ = ï£­ a b c d ï£¸
0 0 1
f g h i
f g h i
A 3 Ã— 4 matrix was multiplied on the left by an elementary matrix which was obtained from
row operation 1 applied to the identity matrix. This resulted in applying the operation 1
to the given matrix. This is what happens in general.
Now consider what these elementary matrices look like. First consider the one which
involves switching row i and row j where i < j. This matrix is of the form
ï£«
ï£¶
1
0
ï£¬
ï£·
..
ï£¬
ï£·
.
ï£¬
ï£·
ï£¬
ï£·
0
Â·
Â·
Â·
1
ï£¬
ï£·
ï£¬
ï£·
..
..
ï£¬
ï£·
.
.
ï£¬
ï£·
ï£¬
ï£·
1 Â·Â·Â· 0
ï£¬
ï£·
ï£¬
ï£·
..
ï£­
ï£¸
.
0
1
105

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

106

ROW OPERATIONS

The two exceptional rows are shown. The ith row was the j th and the j th row was the ith
in the identity matrix. Now consider what this does to a column vector.
ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
1
0
v1
v1
ï£· ï£¬ .. ï£· ï£¬ .. ï£·
ï£¬
..
ï£¬
ï£·ï£¬ . ï£· ï£¬ . ï£·
.
ï£¬
ï£·ï£¬
ï£· ï£¬
ï£·
ï£¬
ï£· ï£¬ vi ï£· ï£¬ vj ï£·
0
Â·
Â·
Â·
1
ï£· ï£¬
ï£·
ï£¬
ï£·ï£¬
ï£¬
ï£· ï£¬ .. ï£· ï£¬ .. ï£·
..
..
=
ï£·
ï£¬
ï£·
ï£¬
ï£·
ï£¬
.
.
ï£¬
ï£·ï£¬ . ï£· ï£¬ . ï£·
ï£¬
ï£·
ï£¬
ï£·
ï£¬
ï£·
1 Â·Â·Â· 0
ï£¬
ï£· ï£¬ vj ï£· ï£¬ vi ï£·
ï£·
ï£¬
ï£¬
ï£·
ï£¬
.
. ï£·
..
ï£¸ ï£­ .. ï£¸ ï£­ .. ï£¸
ï£­
.
0
1
vn
vn
Now denote by P ij the elementary matrix which comes from the identity from switching
rows i and j. From what was just explained consider multiplication on the left by this
elementary matrix.
ï£«
ï£¶
a11 a12 Â· Â· Â· a1p
ï£¬ ..
..
.. ï£·
ï£¬ .
.
. ï£·
ï£¬
ï£·
ï£¬ ai1 ai2 Â· Â· Â· aip ï£·
ï£¬
ï£·
ï£¬
..
.. ï£·
P ij ï£¬ ...
ï£·
.
.
ï£¬
ï£·
ï£¬ aj1 aj2 Â· Â· Â· ajp ï£·
ï£¬
ï£·
ï£¬ .
..
.. ï£·
ï£­ ..
.
. ï£¸
an1 an2 Â· Â· Â· anp
From the way you multiply matrices this is a matrix which has the indicated columns.
ï£«
ï£«
ï£¶
ï£«
ï£¶
ï£«
ï£¶ï£¶
a11
a12
a1p
ï£¬
ï£¬ .. ï£·
ï£¬ .. ï£·
ï£¬ .. ï£·ï£·
ï£¬
ï£¬ . ï£·
ï£¬ . ï£·
ï£¬ . ï£·ï£·
ï£¬
ï£¬
ï£·
ï£¬
ï£·
ï£¬
ï£·ï£·
ï£¬
ï£¬ ai1 ï£·
ï£¬ ai2 ï£·
ï£¬ aip ï£·ï£·
ï£¬
ï£¬
ï£·
ï£¬
ï£·
ï£¬
ï£·ï£·
ï£¬ ij ï£¬ .. ï£· ij ï£¬ .. ï£·
ï£¬ . ï£·ï£·
ï£¬P ï£¬ . ï£· , P ï£¬ . ï£· , Â· Â· Â· , P ij ï£¬ .. ï£·ï£·
ï£¬
ï£¬
ï£·
ï£¬
ï£·
ï£¬
ï£·ï£·
ï£¬
ï£¬ aj1 ï£·
ï£¬ aj2 ï£·
ï£¬ ajp ï£·ï£·
ï£¬
ï£¬
ï£·
ï£¬
ï£·
ï£¬
ï£·ï£·
ï£¬
ï£¬ . ï£·
ï£¬ . ï£·
ï£¬ . ï£·ï£·
ï£­
ï£­ .. ï£¸
ï£­ .. ï£¸
ï£­ .. ï£¸ï£¸
an1
an2
anp
ï£«ï£«

ï£¶ ï£«
a11
ï£¬ï£¬ .. ï£· ï£¬
ï£¬ï£¬ . ï£· ï£¬
ï£¬ï£¬
ï£· ï£¬
ï£¬ï£¬ aj1 ï£· ï£¬
ï£¬ï£¬
ï£· ï£¬
ï£¬ï£¬
ï£· ï£¬
= ï£¬ï£¬ ... ï£· , ï£¬
ï£¬ï£¬
ï£· ï£¬
ï£¬ï£¬ ai1 ï£· ï£¬
ï£¬ï£¬
ï£· ï£¬
ï£¬ï£¬ . ï£· ï£¬
.
ï£­ï£­ . ï£¸ ï£­
an1

ï£¶
a12
.. ï£·
. ï£·
ï£·
aj2 ï£·
ï£·
.. ï£· , Â· Â· Â·
. ï£·
ï£·
ai2 ï£·
ï£·
.. ï£·
. ï£¸
an2

Saylor URL: http://www.saylor.org/courses/ma212/

ï£«

ï£¶ï£¶
a1p
ï£¬ .. ï£·ï£·
ï£¬ . ï£·ï£·
ï£¬
ï£·ï£·
ï£¬ ajp ï£·ï£·
ï£¬
ï£·ï£·
ï£¬
ï£·ï£·
, ï£¬ ... ï£·ï£·
ï£¬
ï£·ï£·
ï£¬ aip ï£·ï£·
ï£¬
ï£·ï£·
ï£¬ . ï£·ï£·
.
ï£­ . ï£¸ï£¸
anp

The Saylor Foundation

4.1. ELEMENTARY MATRICES

107
ï£«

a11
ï£¬ ..
ï£¬ .
ï£¬
ï£¬ aj1
ï£¬
ï£¬
= ï£¬ ...
ï£¬
ï£¬ ai1
ï£¬
ï£¬ .
ï£­ ..

a12
..
.

Â·Â·Â·

aj2
..
.

Â·Â·Â·

ai2
..
.

Â·Â·Â·

an1

an2

Â·Â·Â·

ï£¶
a1p
.. ï£·
. ï£·
ï£·
ajp ï£·
ï£·
.. ï£·
. ï£·
ï£·
aip ï£·
ï£·
.. ï£·
. ï£¸
anp

This has established the following lemma.
Lemma 4.1.3 Let P ij denote the elementary matrix which involves switching the ith and
the j th rows. Then
P ij A = B
where B is obtained from A by switching the ith and the j th rows.
As a consequence of the above lemma, if you have any permutation (i1 , Â· Â· Â· , in ), it
follows from Lemma 3.3.2 that the corresponding permutation matrix can be obtained by
multiplying ï¬nitely many permutation matrices, each of which switch only two rows. Now
every such permutation matrix in which only two rows are switched has determinant âˆ’1.
p
Therefore, the determinant of the permutation matrix for (i1 , Â· Â· Â· , in ) equals (âˆ’1) where
the given permutation can be obtained by making p switches. Now p is not unique. There
are many ways to make switches and end up with a given permutation, but what this shows
is that the total number of switches is either always odd or always even. That is, you could
not obtain a given permutation by making 2m switches and 2k + 1 switches. A permutation
is said to be even if p is even and odd if p is odd. This is an interesting result in abstract
algebra which is obtained very easily from a consideration of elementary matrices and of
course the theory of the determinant. Also, this shows that the composition of permutations
corresponds to the product of the corresponding permutation matrices.
To see permutations considered more directly in the context of group theory, you should
see a good abstract algebra book such as [17] or [13].
Next consider the row operation which involves multiplying the ith row by a nonzero
constant, c. The elementary matrix which results from applying this operation to the ith
row of the identity matrix is of the form
ï£«
ï£¶
1
0
ï£¬
ï£·
..
ï£¬
ï£·
.
ï£¬
ï£·
ï£¬
ï£·
c
ï£¬
ï£·
ï£¬
ï£·
.
.
ï£­
ï£¸
.
0
1
Now consider what this does to a column vector.
ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
1
0
v1
v1
ï£¬
ï£· ï£¬ .. ï£· ï£¬ .. ï£·
..
ï£¬
ï£·ï£¬ . ï£· ï£¬ . ï£·
.
ï£¬
ï£·ï£¬
ï£· ï£¬
ï£·
ï£¬
ï£· ï£¬ vi ï£· = ï£¬ cvi ï£·
c
ï£¬
ï£·ï£¬
ï£· ï£¬
ï£·
ï£¬
ï£·ï£¬ . ï£· ï£¬ . ï£·
..
ï£­
ï£¸ ï£­ .. ï£¸ ï£­ .. ï£¸
.
0
1
vn
vn

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

108

ROW OPERATIONS

Denote by E (c, i) this elementary matrix which multiplies the ith row of the identity by the
nonzero constant, c. Then from what was just discussed and the way matrices are multiplied,
ï£«
ï£¶
a11 a12 Â· Â· Â· Â· Â· Â· a1p
ï£¬ ..
..
.. ï£·
ï£¬ .
.
. ï£·
ï£¬
ï£·
ï£·
a
a
Â·
Â·
Â·
Â·
Â·
Â·
a
E (c, i) ï£¬
i2
ip ï£·
ï£¬ i1
ï£¬ .
ï£·
.
.
..
.. ï£¸
ï£­ ..
an1 an2 Â· Â· Â· Â· Â· Â· anp
equals a matrix having the columns indicated below.
ï£«
ï£¶
ï£«
ï£¶
ï£«
ï£«
a12
a1p
a11
ï£¬ .. ï£·
ï£¬ ..
ï£¬
ï£¬ .. ï£·
ï£¬ . ï£·
ï£¬ . ï£·
ï£¬ .
ï£¬
ï£¬
ï£¬
ï£·
ï£¬
ï£·
ï£¬
ï£¬ ai1 ï£· , E (c, i) ï£¬ ai2 ï£· , Â· Â· Â· , E (c, i) ï£¬ aip
E
(c,
i)
= ï£¬
ï£¬
ï£·
ï£¬
ï£¬
ï£¬
ï£·
ï£¬ .
ï£¬ . ï£·
ï£¬ . ï£·
ï£¬
ï£­ ..
ï£­ .. ï£¸
ï£­ .. ï£¸
ï£­
anp
an2
an1
ï£«
ï£¶
a11 a12 Â· Â· Â· Â· Â· Â· a1p
ï£¬ ..
..
.. ï£·
ï£¬ .
.
. ï£·
ï£¬
ï£·
ï£¬
= ï£¬ cai1 cai2 Â· Â· Â· Â· Â· Â· caip ï£·
ï£·
ï£¬ .
..
.. ï£·
ï£­ ..
.
. ï£¸
an1 an2 Â· Â· Â· Â· Â· Â· anp

ï£¶ï£¶
ï£·ï£·
ï£·ï£·
ï£·ï£·
ï£·ï£·
ï£·ï£·
ï£·ï£·
ï£¸ï£¸

This proves the following lemma.
Lemma 4.1.4 Let E (c, i) denote the elementary matrix corresponding to the row operation in which the ith row is multiplied by the nonzero constant, c. Thus E (c, i) involves
multiplying the ith row of the identity matrix by c. Then
E (c, i) A = B
where B is obtained from A by multiplying the ith row of A by c.
Finally consider the third of these row operations.
matrix which replaces the j th row with itself added
case i < j this will be of the form
ï£«
1
ï£¬
..
ï£¬
.
ï£¬
ï£¬
1
ï£¬
ï£¬
.. . .
ï£¬
.
.
ï£¬
ï£¬
c
Â·
Â·
Â·
1
ï£¬
ï£¬
ï£­
0

Saylor URL: http://www.saylor.org/courses/ma212/

Denote by E (c Ã— i + j) the elementary
to c times the ith row added to it. In
0

..

.

ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

1

The Saylor Foundation

4.1. ELEMENTARY MATRICES

109

Now consider what this does to a column vector.
ï£«
ï£¶ï£«
1
0
ï£¬
ï£·ï£¬
..
ï£¬
ï£·ï£¬
.
ï£¬
ï£·ï£¬
ï£¬
ï£·ï£¬
1
ï£¬
ï£·ï£¬
ï£¬
ï£·ï£¬
.. . .
ï£¬
ï£·ï£¬
.
.
ï£¬
ï£·ï£¬
ï£¬
ï£·ï£¬
c Â·Â·Â· 1
ï£¬
ï£·ï£¬
ï£¬
ï£·ï£¬
.
..
ï£­
ï£¸ï£­
0
1

ï£¶ ï£«
v1
v1
.. ï£· ï£¬
..
ï£¬
. ï£·
.
ï£· ï£¬
ï£¬
vi ï£·
v
i
ï£· ï£¬
.. ï£· = ï£¬
..
ï£¬
. ï£·
.
ï£· ï£¬
ï£·
vj ï£· ï£¬
ï£¬ cvi + vj
.. ï£· ï£¬
..
. ï£¸ ï£­
.
vn
vn

Now from this and the way matrices are multiplied,
ï£«
a11 a12 Â· Â· Â· Â· Â· Â·
ï£¬ ..
..
ï£¬ .
.
ï£¬
ï£¬ ai1 ai2 Â· Â· Â· Â· Â· Â·
ï£¬
ï£¬
..
E (c Ã— i + j) ï£¬ ...
.
ï£¬
ï£¬ aj2 aj2 Â· Â· Â· Â· Â· Â·
ï£¬
ï£¬ .
..
ï£­ ..
.
an1

an2

Â·Â·Â·

Â·Â·Â·

Â·Â·Â·

Â·Â·Â·

Â·Â·Â·

Â·Â·Â·

Â·Â·Â·

Â·Â·Â·

Â·Â·Â·

Â·Â·Â·

ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

ï£¶
a1p
.. ï£·
. ï£·
ï£·
aip ï£·
ï£·
.. ï£·
. ï£·
ï£·
ajp ï£·
ï£·
.. ï£·
. ï£¸
anp

equals a matrix of the following form having the indicated columns.
ï£«
ï£«
ï£¶
ï£«
ï£¶
ï£«
a11
a12
ï£¬
ï£¬ .. ï£·
ï£¬ .. ï£·
ï£¬
ï£¬
ï£¬ . ï£·
ï£¬ . ï£·
ï£¬
ï£¬
ï£¬
ï£·
ï£¬
ï£·
ï£¬
ï£¬
ï£¬ ai1 ï£·
ï£¬ ai2 ï£·
ï£¬
ï£¬
ï£¬
ï£·
ï£¬
ï£·
ï£¬
ï£¬
ï£¬ .. ï£·
ï£¬ .. ï£·
ï£¬
ï£¬E (c Ã— i + j) ï£¬ . ï£· , E (c Ã— i + j) ï£¬ . ï£· , Â· Â· Â· E (c Ã— i + j) ï£¬
ï£¬
ï£¬
ï£·
ï£¬
ï£·
ï£¬
ï£¬
ï£¬ aj2 ï£·
ï£¬ aj2 ï£·
ï£¬
ï£¬
ï£¬
ï£·
ï£¬
ï£·
ï£¬
ï£¬
ï£¬ . ï£·
ï£¬ . ï£·
ï£¬
ï£­
ï£­ .. ï£¸
ï£­ .. ï£¸
ï£­
an1
an2
ï£«

a11
..
.

ï£¬
ï£¬
ï£¬
ï£¬
ai1
ï£¬
ï£¬
..
=ï£¬
.
ï£¬
ï£¬ aj2 + cai1
ï£¬
ï£¬
..
ï£­
.
an1

a12
..
.

Â·Â·Â·

a1p
..
.

ai2
..
.

Â·Â·Â·

aip
..
.

aj2 + cai2
..
.

Â·Â·Â·

ajp + caip
..
.

an2

Â·Â·Â·

anp

ï£¶ï£¶
a1p
.. ï£·ï£·
ï£·
. ï£·
ï£·ï£·
ï£·
aip ï£·
ï£·ï£·
.. ï£·ï£·
ï£·
. ï£·
ï£·ï£·
ï£·
ajp ï£·ï£·
ï£·
.. ï£·ï£·
. ï£¸ï£¸
anp

ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

The case where i > j is handled similarly. This proves the following lemma.
Lemma 4.1.5 Let E (c Ã— i + j) denote the elementary matrix obtained from I by replacing
the j th row with c times the ith row added to it. Then
E (c Ã— i + j) A = B
where B is obtained from A by replacing the j th row of A with itself added to c times the
ith row of A.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

110

ROW OPERATIONS

The next theorem is the main result.
Theorem 4.1.6 To perform any of the three row operations on a matrix A it suï¬ƒces to do
the row operation on the identity matrix obtaining an elementary matrix E and then take
the product, EA. Furthermore, each elementary matrix is invertible and its inverse is an
elementary matrix.
Proof: The ï¬rst part of this theorem has been proved in Lemmas 4.1.3 - 4.1.5. It
only remains to verify the claim about the inverses. Consider ï¬rst the elementary matrices
corresponding to row operation of type three.
E (âˆ’c Ã— i + j) E (c Ã— i + j) = I
This follows because the ï¬rst matrix takes c times row i in the identity and adds it to row j.
When multiplied on the left by E (âˆ’c Ã— i + j) it follows from the ï¬rst part of this theorem
that you take the ith row of E (c Ã— i + j) which coincides with the ith row of I since that
row was not changed, multiply it by âˆ’c and add to the j th row of E (c Ã— i + j) which was
the j th row of I added to c times the ith row of I. Thus E (âˆ’c Ã— i + j) multiplied on the
left, undoes the row operation which resulted in E (c Ã— i + j). The same argument applied
to the product
E (c Ã— i + j) E (âˆ’c Ã— i + j)
replacing c with âˆ’c in the argument yields that this product is also equal to I. Therefore,
âˆ’1
E (c Ã— i + j) = E (âˆ’c Ã— i + j) .
Similar reasoning shows that for E (c, i) the elementary matrix which comes from multiplying the ith row by the nonzero constant, c,
(
)
âˆ’1
E (c, i) = E câˆ’1 , i .
Finally, consider P ij which involves switching the ith and the j th rows.
P ij P ij = I
because by the ï¬rst part of this theorem, multiplying on the left by P ij switches the ith
and j th rows of P ij which was obtained from switching the ith and j th rows of the identity.
First you switch them to get P ij and then you multiply on the left by P ij which switches
( )âˆ’1
these rows again and restores the identity matrix. Thus P ij
= P ij . 

4.2

The Rank Of A Matrix

Recall the following deï¬nition of rank of a matrix.
Deï¬nition 4.2.1 A submatrix of a matrix A is the rectangular array of numbers obtained
by deleting some rows and columns of A. Let A be an m Ã— n matrix. The determinant
rank of the matrix equals r where r is the largest number such that some r Ã— r submatrix
of A has a non zero determinant. The row rank is deï¬ned to be the dimension of the span
of the rows. The column rank is deï¬ned to be the dimension of the span of the columns.
The rank of A is denoted as rank (A).
The following theorem is proved in the section on the theory of the determinant and is
restated here for convenience.
Theorem 4.2.2 Let A be an m Ã— n matrix. Then the row rank, column rank and determinant rank are all the same.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

4.2. THE RANK OF A MATRIX

111

So how do you ï¬nd the rank? It turns out that row operations are the key to the practical
computation of the rank of a matrix.
In rough terms, the following lemma states that linear relationships between columns
in a matrix are preserved by row operations.
Lemma 4.2.3 Let B and A be two m Ã— n matrices and suppose B results from a row
operation applied to A. Then the k th column of B is a linear combination of the i1 , Â· Â· Â· , ir
columns of B if and only if the k th column of A is a linear combination of the i1 , Â· Â· Â· , ir
columns of A. Furthermore, the scalars in the linear combination are the same. (The linear
relationship between the k th column of A and the i1 , Â· Â· Â· , ir columns of A is the same as the
linear relationship between the k th column of B and the i1 , Â· Â· Â· , ir columns of B.)
Proof: Let A equal the following matrix in which the ak are the columns
(
)
a1 a2 Â· Â· Â· an
and let B equal the following matrix in which the columns are given by the bk
(
)
b1 b2 Â· Â· Â· bn
Then by Theorem 4.1.6 on Page 110 bk = Eak where E is an elementary matrix. Suppose
then that one of the columns of A is a linear combination of some other columns of A. Say
âˆ‘
ak =
cr ar .
râˆˆS

Then multiplying by E,
bk = Eak =

âˆ‘

cr Ear =

râˆˆS

âˆ‘

cr br .

râˆˆS

Corollary 4.2.4 Let A and B be two m Ã— n matrices such that B is obtained by applying
a row operation to A. Then the two matrices have the same rank.
Proof: Lemma 4.2.3 says the linear relationships are the same between the columns of
A and those of B. Therefore, the column rank of the two matrices is the same. 
This suggests that to ï¬nd the rank of a matrix, one should do row operations until a
matrix is obtained in which its rank is obvious.
Example 4.2.5 Find the rank of the following matrix and identify columns whose linear
combinations yield all the other columns.
ï£«
ï£¶
1 2 1 3 2
ï£­ 1 3 6 0 2 ï£¸
(4.1)
3 7 8 6 6
Take (âˆ’1) times the ï¬rst row and add
row and add to the third. This yields
ï£«
1 2
ï£­ 0 1
0 1

to the second and then take (âˆ’3) times the ï¬rst
1
5
5

ï£¶
3 2
âˆ’3 0 ï£¸
âˆ’3 0

By the above corollary, this matrix has the same rank as the ï¬rst matrix. Now take (âˆ’1)
times the second row and add to the third row yielding
ï£«
ï£¶
1 2 1 3 2
ï£­ 0 1 5 âˆ’3 0 ï£¸
0 0 0 0 0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

112

ROW OPERATIONS

At this point it is clear the rank is 2. This is because every column is in the span of the
ï¬rst two and these ï¬rst two columns are linearly independent.
Example 4.2.6 Find the rank of the following matrix and identify columns whose linear
combinations yield all the other columns.
ï£«
ï£¶
1 2 1 3 2
ï£­ 1 2 6 0 2 ï£¸
(4.2)
3 6 8 6 6
Take (âˆ’1) times the ï¬rst row and add
row and add to the last row. This yields
ï£«
1 2
ï£­ 0 0
0 0
Now multiply the second row by 1/5 and
ï£«
1 2
ï£­ 0 0
0 0
Add (âˆ’1) times the second row to the
ï£«
1
ï£­ 0
0

to the second and then take (âˆ’3) times the ï¬rst
ï£¶
3 2
âˆ’3 0 ï£¸
âˆ’3 0

1
5
5

add 5 times it to the last row.
ï£¶
1
3
2
1 âˆ’3/5 0 ï£¸
0
0
0

ï¬rst.
2 0
0 1
0 0

ï£¶
2
âˆ’3/5 0 ï£¸
0
0
18
5

(4.3)

It is now clear the rank of this matrix is 2 because the ï¬rst and third columns form a
basis for the column space.
The matrix (4.3) is the row reduced echelon form for the matrix (4.2).

4.3

The Row Reduced Echelon Form

The following deï¬nition is for the row reduced echelon form of a matrix.
Deï¬nition 4.3.1 Let ei denote the column vector which has all zero entries except for the
ith slot which is one. An mÃ—n matrix is said to be in row reduced echelon form if, in viewing
successive columns from left to right, the ï¬rst nonzero column encountered is e1 and if you
have encountered e1 , e2 , Â· Â· Â· , ek , the next column is either ek+1 or is a linear combination
of the vectors, e1 , e2 , Â· Â· Â· , ek .
For example, here are some
ï£«
0 1
ï£­ 0 0
0 0

matrices which
ï£¶ ï£«
3 0 3
0 1 5 ï£¸,ï£­
0 0 0

are in row reduced echelon form.
ï£¶
1 0 3 âˆ’11 0
0 1 4
4
0 ï£¸.
0 0 0
0
1

Theorem 4.3.2 Let A be an m Ã— n matrix. Then A has a row reduced echelon form
determined by a simple process.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

4.3. THE ROW REDUCED ECHELON FORM

113

Proof: Viewing the columns of A from left to right take the ï¬rst nonzero column. Pick
a nonzero entry in this column and switch the row containing this entry with the top row of
A. Now divide this new top row by the value of this nonzero entry to get a 1 in this position
and then use row operations to make all entries below this entry equal to zero. Thus the
ï¬rst nonzero column is now e1 . Denote the resulting matrix by A1 . Consider the submatrix
of A1 to the right of this column and below the ï¬rst row. Do exactly the same thing for it
that was done for A. This time the e1 will refer to Fmâˆ’1 . Use this 1 and row operations
to zero out every entry above it in the rows of A1 . Call the resulting matrix A2 . Thus A2
satisï¬es the conditions of the above deï¬nition up to the column just encountered. Continue
this way till every column has been dealt with and the result must be in row reduced echelon
form. 
The following diagram illustrates the above procedure. Say the matrix looked something
like the following.
ï£«
ï£¶
0 âˆ— âˆ— âˆ— âˆ— âˆ— âˆ—
ï£¬ 0 âˆ— âˆ— âˆ— âˆ— âˆ— âˆ— ï£·
ï£¬
ï£·
ï£¬ .. .. .. .. .. .. .. ï£·
ï£­ . . . . . . . ï£¸
0 âˆ— âˆ— âˆ— âˆ— âˆ— âˆ—
First step would yield something like
ï£«
0
ï£¬ 0
ï£¬
ï£¬ ..
ï£­ .

1
0
..
.

0 0

âˆ— âˆ—
âˆ— âˆ—
.. ..
. .
âˆ— âˆ—

For the second step you look at the lower right
ï£«
âˆ— âˆ— âˆ—
ï£¬ .. .. ..
ï£­ . . .
âˆ—

âˆ—

âˆ—

âˆ—
âˆ—
..
.

âˆ—
âˆ—
..
.

âˆ—
âˆ—
..
.

âˆ—

âˆ—

âˆ—

ï£¶
ï£·
ï£·
ï£·
ï£¸

corner as described,
ï£¶
âˆ— âˆ—
.. .. ï£·
. . ï£¸
âˆ—

and if the ï¬rst column consists of all zeros but the
something like this.
ï£«
0 1 âˆ— âˆ—
ï£¬ .. .. .. ..
ï£­ . . . .
0 0 âˆ— âˆ—

âˆ—
next one is not all zeros, you would get
ï£¶
âˆ—
.. ï£·
. ï£¸
âˆ—

Thus, after zeroing out the term in the top row above the 1, you get the following for the
next step in the computation of the row reduced echelon form for the original matrix.
ï£«
ï£¶
0 1 âˆ— 0 âˆ— âˆ— âˆ—
ï£¬ 0 0 0 1 âˆ— âˆ— âˆ— ï£·
ï£¬
ï£·
ï£¬ .. .. .. .. .. .. .. ï£· .
ï£­ . . . . . . . ï£¸
0

0

0

0

âˆ— âˆ—

âˆ—

Next you look at the lower right matrix below the top two rows and to the right of the ï¬rst
four columns and repeat the process.
Deï¬nition 4.3.3 The ï¬rst pivot column of A is the ï¬rst nonzero column of A. The next
pivot column is the ï¬rst column after this which is not a linear combination of the columns to
its left. The third pivot column is the next column after this which is not a linear combination

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

114

ROW OPERATIONS

of those columns to its left, and so forth. Thus by Lemma 4.2.3 if a pivot column occurs
as the j th column from the left, it follows that in the row reduced echelon form there will be
one of the ek as the j th column.
There are three choices for row operations at each step in the above theorem. A natural
question is whether the same row reduced echelon matrix always results in the end from
following the above algorithm applied in any way. The next corollary says this is the case.
Deï¬nition 4.3.4 Two matrices are said to be row equivalent if one can be obtained from
the other by a sequence of row operations.
Since every row operation can be obtained by multiplication on the left by an elementary
matrix and since each of these elementary matrices has an inverse which is also an elementary
matrix, it follows that row equivalence is a similarity relation. Thus one can classify matrices
according to which similarity class they are in. Later in the book, another more profound
way of classifying matrices will be presented.
It has been shown above that every matrix is row equivalent to one which is in row
reduced echelon form. Note
ï£«
ï£¶
x1
ï£¬ .. ï£·
ï£­ . ï£¸ = x1 e1 + Â· Â· Â· + xn en
xn
so to say two column vectors are equal is to say they are the same linear combination of the
special vectors ej .
Corollary 4.3.5 The row reduced echelon form is unique. That is if B, C are two matrices
in row reduced echelon form and both are row equivalent to A, then B = C.
Proof: Suppose B and C are both row reduced echelon forms for the matrix A. Then
they clearly have the same zero columns since row operations leave zero columns unchanged.
If B has the sequence e1 , e2 , Â· Â· Â· , er occurring for the ï¬rst time in the positions, i1 , i2 , Â· Â· Â· , ir ,
the description of the row reduced echelon form means that each of these columns is not a
linear combination of the preceding columns. Therefore, by Lemma 4.2.3, the same is true of
the columns in positions i1 , i2 , Â· Â· Â· , ir for C. It follows from the description of the row reduced
echelon form, that e1 , Â· Â· Â· , er occur respectively for the ï¬rst time in columns i1 , i2 , Â· Â· Â· , ir
for C. Thus B, C have the same columns in these positions. By Lemma 4.2.3, the other
columns in the two matrices are linear combinations, involving the same scalars, of the
columns in the i1 , Â· Â· Â· , ik position. Thus each column of B is identical to the corresponding
column in C. 
The above corollary shows that you can determine whether two matrices are row equivalent by simply checking their row reduced echelon forms. The matrices are row equivalent
if and only if they have the same row reduced echelon form.
The following corollary follows.
Corollary 4.3.6 Let A be an m Ã— n matrix and let R denote the row reduced echelon form
obtained from A by row operations. Then there exists a sequence of elementary matrices,
E1 , Â· Â· Â· , Ep such that
(Ep Epâˆ’1 Â· Â· Â· E1 ) A = R.
Proof: This follows from the fact that row operations are equivalent to multiplication
on the left by an elementary matrix. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

4.3. THE ROW REDUCED ECHELON FORM

115

Corollary 4.3.7 Let A be an invertible n Ã— n matrix. Then A equals a ï¬nite product of
elementary matrices.
Proof: Since Aâˆ’1 is given to exist, it follows A must have rank n because by Theorem
3.3.18 det(A) Ì¸= 0 which says the determinant rank and hence the column rank of A is n
and so the row reduced echelon form of A is I because the columns of A form a linearly
independent set. Therefore, by Corollary 4.3.6 there is a sequence of elementary matrices,
E1 , Â· Â· Â· , Ep such that
(Ep Epâˆ’1 Â· Â· Â· E1 ) A = I.
âˆ’1
âˆ’1
But now multiply on the left on both sides by Epâˆ’1 then by Epâˆ’1
and then by Epâˆ’2
etc.
until you get
âˆ’1
A = E1âˆ’1 E2âˆ’1 Â· Â· Â· Epâˆ’1
Epâˆ’1

and by Theorem 4.1.6 each of these in this product is an elementary matrix.
Corollary 4.3.8 The rank of a matrix equals the number of nonzero pivot columns. Furthermore, every column is contained in the span of the pivot columns.
Proof: Write the row reduced echelon form for the matrix. From Corollary 4.2.4 this
row reduced matrix has the same rank as the original matrix. Deleting all the zero rows
and all the columns in the row reduced echelon form which do not correspond to a pivot
column, yields an r Ã— r identity submatrix in which r is the number of pivot columns. Thus
the rank is at least r.
From Lemma 4.2.3 every column of A is a linear combination of the pivot columns since
this is true by deï¬nition for the row reduced echelon form. Therefore, the rank is no more
than r. 
Here is a fundamental observation related to the above.
Corollary 4.3.9 Suppose A is an mÃ—n matrix and that m < n. That is, the number of rows
is less than the number of columns. Then one of the columns of A is a linear combination
of the preceding columns of A.
Proof: Since m < n, not all the columns of A can be pivot columns. That is, in the
row reduced echelon form say ei occurs for the ï¬rst time at ri where r1 < r2 < Â· Â· Â· < rp
where p â‰¤ m. It follows since m < n, there exists some column in the row reduced echelon
form which is a linear combination of the preceding columns. By Lemma 4.2.3 the same is
true of the columns of A. 
Deï¬nition 4.3.10 Let A be an mÃ—n matrix having rank, r. Then the nullity of A is deï¬ned
to be n âˆ’ r. Also deï¬ne ker (A) â‰¡ {x âˆˆ Fn : Ax = 0} . This is also denoted as N (A) .
Observation 4.3.11 Note that ker (A) is a subspace because if a, b are scalars and x, y are
vectors in ker (A), then
A (ax + by) = aAx + bAy = 0 + 0 = 0
Recall that the dimension of the column space of a matrix equals its rank and since the
column space is just A (Fn ) , the rank is just the dimension of A (Fn ). The next theorem
shows that the nullity equals the dimension of ker (A).
Theorem 4.3.12 Let A be an m Ã— n matrix. Then rank (A) + dim (ker (A)) = n..

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

116

ROW OPERATIONS

Proof: Since ker (A) is a subspace, there exists a basis for ker (A) , {x1 , Â· Â· Â· , xk } . Also
let {Ay1 , Â· Â· Â· , Ayl } be a basis for A (Fn ). Let u âˆˆ Fn . Then there exist unique scalars ci
such that
l
âˆ‘
Au =
ci Ayi
i=1

(

It follows that

A uâˆ’

l
âˆ‘

)
ci yi

=0

i=1

and so the vector in parenthesis is in ker (A). Thus there exist unique bj such that
u=

l
âˆ‘

ci yi +

i=1

k
âˆ‘

bj xj

j=1

Since u was arbitrary, this shows {x1 , Â· Â· Â· , xk , y1 , Â· Â· Â· , yl } spans Fn . If these vectors are
independent, then they will form a basis and the claimed equation will be obtained. Suppose
then that
l
k
âˆ‘
âˆ‘
ci yi +
bj xj = 0
i=1

j=1

Apply A to both sides. This yields
l
âˆ‘

ci Ayi = 0

i=1

and so each ci = 0. Then the independence of the xj imply each bj = 0. 

4.4

Rank And Existence Of Solutions To Linear Systems

Consider the linear system of equations,
Ax = b

(4.4)

where A is an m Ã— n matrix, x is a n Ã— 1 column vector, and b is an m Ã— 1 column vector.
Suppose
(
)
A = a1 Â· Â· Â· an
T

where the ak denote the columns of A. Then x = (x1 , Â· Â· Â· , xn ) is a solution of the system
(4.4), if and only if
x1 a1 + Â· Â· Â· + xn an = b
which says that b is a vector in span (a1 , Â· Â· Â· , an ) . This shows that there exists a solution
to the system, (4.4) if and only if b is contained in span (a1 , Â· Â· Â· , an ) . In words, there is a
solution to (4.4) if and only if b is in the column space of A. In terms of rank, the following
proposition describes the situation.
Proposition 4.4.1 Let A be an m Ã— n matrix and let b be an m Ã— 1 column vector. Then
there exists a solution to (4.4) if and only if
(
)
rank A | b = rank (A) .
(4.5)

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

4.5. FREDHOLM ALTERNATIVE

117

(
)
Proof: Place A | b and A in row reduced echelon form, respectively B and C. If
the above condition on rank is true, then both B and C have the same number of nonzero
rows. In particular, you cannot have a row of the form
(
)
0 Â·Â·Â· 0 â‹†
where â‹† Ì¸= 0 in B. Therefore, there will exist a solution to the system (4.4).
Conversely, suppose there exists a solution. This means there cannot be such a row in B
described above. Therefore, B and C must have the same number of zero rows and so they
have the same number of nonzero rows. Therefore, the rank of the two matrices in (4.5) is
the same. 

4.5

Fredholm Alternative

There is a very useful version of Proposition 4.4.1 known as the Fredholm alternative.
I will only present this for the case of real matrices here. Later a much more elegant and
general approach is presented which allows for the general case of complex matrices.
The following deï¬nition is used to state the Fredholm alternative.
Deï¬nition 4.5.1 Let S âŠ† Rm . Then S âŠ¥ â‰¡ {z âˆˆ Rm : z Â· s = 0 for every s âˆˆ S} . The funny
exponent, âŠ¥ is called â€œperpâ€.
Now note

(

T

ker A

)

{
}
â‰¡ z : AT z = 0 =

{
z:

m
âˆ‘

}
zk ak = 0

k=1

Lemma 4.5.2 Let A be a real m Ã— n matrix, let x âˆˆ Rn and y âˆˆ Rm . Then
(
)
(Ax Â· y) = xÂ·AT y
Proof: This follows right away from the deï¬nition of the inner product and matrix
multiplication.
âˆ‘
âˆ‘( )
(
)
(Ax Â· y) =
Akl xl yk =
AT lk xl yk = x Â· AT y . 
k,l

k,l

Now it is time to state the Fredholm alternative. The ï¬rst version of this is the following
theorem.
Theorem 4.5.3 Let A be a real m Ã— n matrix and let b âˆˆ Rm . There exists a solution, x
( )âŠ¥
to the equation Ax = b if and only if b âˆˆ ker AT .
( )âŠ¥
Proof: First suppose b âˆˆ ker AT . Then this says that if AT x = 0, it follows that
b Â· x = 0. In other words, taking the transpose, if
xT A = 0, then xT b = 0.
Thus, if P is a product of elementary matrices such that P A is in row reduced echelon form,
th
th
then if P A has a row
( of zeros, in
) the k position, then there is also a zero in the k position
A
|
b
of P b. Thus rank
= rank (A) .By Proposition 4.4.1, there exists a solution, x
to the system Ax
( T=) b. It remains to go the other direction.
Let z âˆˆ ker A and suppose Ax = b. I need to verify b Â· z = 0. By Lemma 4.5.2,
b Â· z = Ax Â· z = x Â· AT z = x Â· 0 = 0 
This implies the following corollary which is also called the Fredholm alternative. The
â€œalternativeâ€ becomes more clear in this corollary.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

118

ROW OPERATIONS

Corollary 4.5.4 Let A be an m Ã— n matrix. Then A maps Rn onto Rm if and only if the
only solution to AT x = 0 is x = 0.
( )
( )âŠ¥
Proof: If the only solution to AT x = 0 is x = 0, then ker AT = {0} and so ker AT
=
Rm because every b âˆˆ Rm has the property that b Â· 0 = 0. Therefore, Ax = b has a solu( )âŠ¥
tion for any b âˆˆ Rm because the b for which there is a solution are those in ker AT
by
Theorem 4.5.3. In other words, A maps Rn onto Rm .
( )âŠ¥
Conversely if A is onto, then by Theorem 4.5.3 every b âˆˆ Rm is in ker AT
and so if
AT x = 0, then b Â· x = 0 for every b. In particular, this holds for b = x. Hence if AT x = 0,
then x = 0. 
Here is an amusing example.
Example 4.5.5 Let A be an m Ã— n matrix in which m > n. Then A cannot map onto Rm .
The reason for this is that AT is an n Ã— m where m > n and so in the augmented matrix
( T )
A |0
there must be some free variables. Thus there exists a nonzero vector x such that AT x = 0.

4.6

Exercises

1. Let {u1 , Â· Â· Â· , un } be vectors in Rn . The parallelepiped determined by these vectors
P (u1 , Â· Â· Â· , un ) is deï¬ned as
}
{ n
âˆ‘
tk uk : tk âˆˆ [0, 1] for all k .
P (u1 , Â· Â· Â· , un ) â‰¡
k=1

Now let A be an n Ã— n matrix. Show that
{Ax : x âˆˆ P (u1 , Â· Â· Â· , un )}
is also a parallelepiped.
2. In the context of Problem 1, draw P (e1 , e2 ) where e1 , e2 are the standard basis vectors
for R2 . Thus e1 = (1, 0) , e2 = (0, 1) . Now suppose
(
)
1 1
E=
0 1
where E is the elementary matrix which takes the third row and adds to the ï¬rst.
Draw
{Ex : x âˆˆ P (e1 , e2 )} .
In other words, draw the result of doing E to the vectors in P (e1 , e2 ). Next draw the
results of doing the other elementary matrices to P (e1 , e2 ).
3. In the context of Problem 1, either draw or describe the result of doing elementary
matrices to P (e1 , e2 , e3 ). Describe geometrically the conclusion of Corollary 4.3.7.
4. Consider a permutation of {1, 2, Â· Â· Â· , n}. This is an ordered list of numbers taken from
this list with no repeats, {i1 , i2 , Â· Â· Â· , in }. Deï¬ne the permutation matrix P (i1 , i2 , Â· Â· Â· , in )
as the matrix which is obtained from the identity matrix by placing the j th column
th
of I as the ith
j column of P (i1 , i2 , Â· Â· Â· , in ) . Thus the 1 in the ij column of this perth
mutation matrix occurs in the j slot. What does this permutation matrix do to the
T
column vector (1, 2, Â· Â· Â· , n) ?

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

4.6. EXERCISES

119

5. â†‘Consider the 3 Ã— 3 permutation matrices. List all of them and then determine the
dimension of their span. Recall that you can consider an m Ã— n matrix as something
in Fnm .
6. Determine
(
1
(a)
0
ï£«
1
ï£­
0
(b)
0
ï£«
1
(c) ï£­ 0
0

which matrices are in row reduced echelon form.
)
2 0
1 7
ï£¶
0 0 0
0 1 2 ï£¸
0 0 0
ï£¶
1 0 0 0 5
0 1 2 0 4 ï£¸
0 0 0 1 3

7. Row reduce the following matrices to obtain the row reduced echelon form. List the
pivot columns in the original matrix.
ï£«
ï£¶
1 2 0 3
(a) ï£­ 2 1 2 2 ï£¸
1 1 0 3
ï£«
ï£¶
1 2 3
ï£¬ 2 1 âˆ’2 ï£·
ï£·
(b) ï£¬
ï£­ 3 0 0 ï£¸
3 2 1
ï£¶
ï£«
1 2 1 3
(c) ï£­ âˆ’3 2 1 0 ï£¸
3 2 1 1
8. Find the rank and nullity of the following matrices. If the rank is r, identify r columns
in the original matrix which have the property that every other column may be
written as a linear combination of these.
ï£¶
ï£«
0 1 0 2 1 2 2
ï£¬ 0 3 2 12 1 6 8 ï£·
ï£·
(a) ï£¬
ï£­ 0 1 1 5 0 2 3 ï£¸
0 2 1 7 0 3 4
ï£«
ï£¶
0 1 0 2 0 1 0
ï£¬ 0 3 2 6 0 5 4 ï£·
ï£·
(b) ï£¬
ï£­ 0 1 1 2 0 2 2 ï£¸
0 2 1 4 0 3 2
ï£«
ï£¶
0 1 0 2 1 1 2
ï£¬ 0 3 2 6 1 5 1 ï£·
ï£·
(c) ï£¬
ï£­ 0 1 1 2 0 2 1 ï£¸
0 2 1 4 0 3 1
9. Find the rank of the following matrices. If the rank is r, identify r columns in the
original matrix which have the property that every other column may be written
as a linear combination of these. Also ï¬nd a basis for the row and column spaces of
the matrices.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

120

ROW OPERATIONS

ï£«

1
ï£¬ 3
(a) ï£¬
ï£­ 2
0
ï£«
1
ï£¬ 4
(b) ï£¬
ï£­ 2
0
ï£«
0
ï£¬ 0
(c) ï£¬
ï£­ 0
0
ï£«
0
ï£¬ 0
(d) ï£¬
ï£­ 0
0
ï£«
0
ï£¬ 0
(e) ï£¬
ï£­ 0
0

0
1
1
2

ï£¶
0
1 ï£·
ï£·
0 ï£¸
1
ï£¶
0
1 ï£·
ï£·
0 ï£¸
0

1
3
1
2

0
2
1
1

2
12
5
7

1
3
1
2

0
2
1
1

2
6
2
4

1
3
1
2

0
2
1
1

2
6
2
4

2
2
1
2

ï£¶
1 2 2
1 6 8 ï£·
ï£·
0 2 3 ï£¸
0 3 4
ï£¶
0 1 0
0 5 4 ï£·
ï£·
0 2 2 ï£¸
0 3 2
ï£¶
1 1 2
1 5 1 ï£·
ï£·
0 2 1 ï£¸
0 3 1

10. Suppose A is an m Ã— n matrix. Explain why the rank of A is always no larger than
min (m, n) .
11. Suppose A is an m Ã— n matrix in which m â‰¤ n. Suppose also that the rank of A equals
m. Show that A maps Fn onto Fm . Hint: The vectors e1 , Â· Â· Â· , em occur as columns
in the row reduced echelon form for A.
12. Suppose A is an m Ã— n matrix and that m > n. Show there exists b âˆˆ Fm such that
there is no solution to the equation
Ax = b.
13. Suppose A is an m Ã— n matrix in which m â‰¥ n. Suppose also that the rank of A
equals n. Show that A is one to one. Hint: If not, there exists a vector, x Ì¸= 0 such
that Ax = 0, and this implies at least one column of A is a linear combination of the
others. Show this would require the column rank to be less than n.
14. Explain why an n Ã— n matrix A is both one to one and onto if and only if its rank is
n.
15. Suppose A is an m Ã— n matrix and {w1 , Â· Â· Â· , wk } is a linearly independent set of
vectors in A (Fn ) âŠ† Fm . Suppose also that Azi = wi . Show that {z1 , Â· Â· Â· , zk } is also
linearly independent.
16. Show rank (A + B) â‰¤ rank (A) + rank (B).
17. Suppose A is an m Ã— n matrix, m â‰¥ n and the columns of A are independent. Suppose also that {z1 , Â· Â· Â· , zk } is a linearly independent set of vectors in Fn . Show that
{Az1 , Â· Â· Â· , Azk } is linearly independent.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

4.6. EXERCISES

121

18. Suppose A is an m Ã— n matrix and B is an n Ã— p matrix. Show that
dim (ker (AB)) â‰¤ dim (ker (A)) + dim (ker (B)) .
Hint: Consider the subspace, B (Fp ) âˆ© ker (A) and suppose a basis for this subspace
is {w1 , Â· Â· Â· , wk } . Now suppose {u1 , Â· Â· Â· , ur } is a basis for ker (B) . Let {z1 , Â· Â· Â· , zk }
be such that Bzi = wi and argue that
ker (AB) âŠ† span (u1 , Â· Â· Â· , ur , z1 , Â· Â· Â· , zk ) .
19. Let m < n and let A be an m Ã— n matrix. Show that A is not one to one.
20. Let A be an m Ã— n real matrix and let b âˆˆ Rm . Show there exists a solution, x to the
system
AT Ax = AT b
Next show that if x, x1 are two solutions, then Ax = Ax1 . Hint: First show that
( T )T
(
)
A A = AT A. Next show if x âˆˆ ker AT A , then Ax = 0. Finally apply the Fredholm alternative. Show AT b âˆˆ ker(AT A)âŠ¥ . This will give existence of a solution.
21. Show that in the context of Problem 20 that if x is the solution there, then |b âˆ’ Ax| â‰¤
|b âˆ’ Ay| for every y. Thus Ax is the point of A (Rn ) which is closest to b of every
point in A (Rn ). This is a solution to the least squares problem.
ï£«ï£« ï£¶ ï£«
ï£¶ï£¶
1
0
ï£¬ï£¬ 0 ï£· ï£¬ 1 ï£·ï£·
T
ï£¬ ï£· ï£¬
ï£·ï£·
22. â†‘Here is a point in R4 : (1, 2, 3, 4) . Find the point in span ï£¬
ï£­ï£­ 2 ï£¸ , ï£­ 3 ï£¸ï£¸ which
3
2
is closest to the given point.
T

23. â†‘Here is a point in R4 : (1, 2, 3, 4) . Find the point on the plane described by x + 2y âˆ’
4z + 4w = 0 which is closest to the given point.
24. Suppose A, B are two invertible n Ã— n matrices. Show there exists a sequence of row
operations which when done to A yield B. Hint: Recall that every invertible matrix
is a product of elementary matrices.
25. If A is invertible and n Ã— n and B is n Ã— p, show that AB has the same null space as
B and also the same rank as B.
26. Here are two matrices in row reduced echelon form
ï£«
ï£¶
ï£«
1 0 1
1 0
A = ï£­ 0 1 1 ï£¸, B = ï£­ 0 1
0 0 0
0 0

ï£¶
0
1 ï£¸
0

Does there exist a sequence of row operations which when done to A will yield B?
Explain.
27. Is it true that an upper triagular matrix has rank equal to the number of nonzero
entries down the main diagonal?

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

122

ROW OPERATIONS

28. Let {v1 , Â· Â· Â· , vnâˆ’1 } be vectors in Fn . Describe a systematic way to obtain a vector vn
which is perpendicular to each of these vectors. Hint: You might consider something
like this
ï£«
ï£¶
e1
e2
Â·Â·Â·
en
ï£¬ v11
v12
Â·Â·Â·
v1n ï£·
ï£·
ï£¬
det ï£¬
ï£·
..
..
..
ï£¸
ï£­
.
.
.
v(nâˆ’1)1

v(nâˆ’1)2

Â·Â·Â·

v(nâˆ’1)n

where vij is the j th entry of the vector vi . This is a lot like the cross product.
29. Let A be an m Ã— n matrix. Then ker (A) is a subspace of Fn . Is it true that every
subspace of Fn is the kernel or null space of some matrix? Prove or disprove.
30. Let A be an nÃ—n matrix and let P ij be the permutation matrix which switches the ith
and j th rows of the identity. Show that P ij AP ij produces a matrix which is similar
to A which switches the ith and j th entries on the main diagonal.
31. Recall the procedure for ï¬nding the inverse of a matrix on Page 48. It was shown that
the procedure, when it works, ï¬nds the inverse of the matrix. Show that whenever
the matrix has an inverse, the procedure works.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Some Factorizations
5.1

LU Factorization

An LU factorization of a matrix involves writing the given matrix as the product of a
lower triangular matrix which has the main diagonal consisting entirely of ones, L, and an
upper triangular matrix U in the indicated order. The L goes with â€œlowerâ€ and the U with
â€œupperâ€. It turns out many matrices can be written in this way and when this is possible,
people get excited about slick ways of solving the system of equations, Ax = y. The method
lacks generality but is of interest just the same.
(
)
0 1
Example 5.1.1 Can you write
in the form LU as just described?
1 0
To do so you would need
(
)(
1 0
a
x 1
0

b
c

)

(
=

a
b
xa xb + c

)

(
=

0
1

1
0

)
.

Therefore, b = 1 and a = 0. Also, from the bottom rows, xa = 1 which canâ€™t happen and
have a = 0. Therefore, you canâ€™t write this matrix in the form LU. It has no LU factorization.
This is what I mean above by saying the method lacks generality.
Which matrices have an LU factorization? It turns out it is those whose row reduced
echelon form can be achieved without switching rows and which only involve row operations
of type 3 in which row j is replaced with a multiple of row i added to row j for i < j.

5.2

Finding An LU Factorization

There is a convenient procedure for ï¬nding an LU factorization. It turns out that it is
only necessary to keep track of the multipliers which are used to row reduce to upper
triangular form. This procedure is described in the following examples and is called the
multiplier method. It is due to Dolittle.
ï£«
ï£¶
1 2 3
Example 5.2.1 Find an LU factorization for A = ï£­ 2 1 âˆ’4 ï£¸
1 5 2
Write the matrix next to the identity matrix as shown.
ï£«
ï£¶ï£«
ï£¶
1 0 0
1 2 3
ï£­ 0 1 0 ï£¸ ï£­ 2 1 âˆ’4 ï£¸ .
0 0 1
1 5 2
123

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

124

SOME FACTORIZATIONS

The process involves doing row operations to the matrix on the right while simultaneously
updating successive columns of the matrix on the left. First take âˆ’2 times the ï¬rst row and
add to the second in the matrix on the right.
ï£«
ï£¶ï£«
ï£¶
1 0 0
1 2
3
ï£­ 2 1 0 ï£¸ ï£­ 0 âˆ’3 âˆ’10 ï£¸
0 0 1
1 5
2
Note the method for updating the matrix on the left. The 2 in the second entry of the ï¬rst
column is there because âˆ’2 times the ï¬rst row of A added to the second row of A produced
a 0. Now replace the third row in the matrix on the right by âˆ’1 times the ï¬rst row added
to the third. Thus the next step is
ï£«
ï£¶ï£«
ï£¶
1 0 0
1 2
3
ï£­ 2 1 0 ï£¸ ï£­ 0 âˆ’3 âˆ’10 ï£¸
1 0 1
0 3
âˆ’1
Finally, add the second row to the bottom row and make the following changes
ï£¶ï£«
ï£¶
ï£«
1 0 0
1 2
3
ï£­ 2 1 0 ï£¸ ï£­ 0 âˆ’3 âˆ’10 ï£¸ .
1 âˆ’1 1
0 0 âˆ’11
At this point, stop because the matrix on the right is upper triangular. An LU factorization
is the above.
The justiï¬cation for this gimmick will be given later.
ï£«
ï£¶
1 2 1 2 1
ï£¬ 2 0 2 1 1 ï£·
ï£·
Example 5.2.2 Find an LU factorization for A = ï£¬
ï£­ 2 3 1 3 2 ï£¸.
1 0 1 1 2
This time everything is done at once for a whole column. This saves trouble. First
multiply the ï¬rst row by (âˆ’1) and then add to the last row. Next take (âˆ’2) times the ï¬rst
and add to the second and then (âˆ’2) times the ï¬rst and add to the third.
ï£«
ï£¶ï£«
ï£¶
1 0 0 0
1 2
1
2
1
ï£¬ 2 1 0 0 ï£· ï£¬ 0 âˆ’4 0 âˆ’3 âˆ’1 ï£·
ï£¬
ï£·ï£¬
ï£·
ï£­ 2 0 1 0 ï£¸ ï£­ 0 âˆ’1 âˆ’1 âˆ’1 0 ï£¸ .
1 0 0 1
0 âˆ’2 0 âˆ’1 1
This ï¬nishes the ï¬rst column of L and the ï¬rst column of U. Now take âˆ’ (1/4) times the
second row in the matrix on the right and add to the third followed by âˆ’ (1/2) times the
second added to the last.
ï£«
ï£¶ï£«
ï£¶
1 0
0 0
1 2
1
2
1
ï£¬ 2 1
ï£¬
0 0 ï£·
âˆ’3
âˆ’1 ï£·
ï£¬
ï£· ï£¬ 0 âˆ’4 0
ï£·
ï£­ 2 1/4 1 0 ï£¸ ï£­ 0 0 âˆ’1 âˆ’1/4 1/4 ï£¸
1 1/2 0 1
0 0
0
1/2 3/2
This ï¬nishes the second column of L as well as the second column of U . Since the matrix
on the right is upper triangular, stop. The LU factorization has now been obtained. This
technique is called Dolittleâ€™s method. II
This process is entirely typical of the general case. The matrix U is just the ï¬rst upper
triangular matrix you come to in your quest for the row reduced echelon form using only

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

5.3. SOLVING LINEAR SYSTEMS USING AN LU FACTORIZATION

125

the row operation which involves replacing a row by itself added to a multiple of another
row. The matrix L is what you get by updating the identity matrix as illustrated above.
You should note that for a square matrix, the number of row operations necessary to
reduce to LU form is about half the number needed to place the matrix in row reduced
echelon form. This is why an LU factorization is of interest in solving systems of equations.

5.3

Solving Linear Systems Using An LU Factorization

The reason people care about the LU factorization is it allows the quick solution of systems
of equations. Here is an example.
ï£«
ï£¶
ï£«
ï£¶
x
1 2 3 2
ï£¬ y ï£·
ï£·
Example 5.3.1 Suppose you want to ï¬nd the solutions to ï£­ 4 3 1 1 ï£¸ ï£¬
ï£­ z ï£¸ =
1 2 3 0
w
ï£« ï£¶
1
ï£­ 2 ï£¸.
3
Of course one way is to write the augmented matrix and grind away. However, this
involves more row operations than the computation of an LU factorization and it turns out
that an LU factorization can give the solution quickly. Here is how. The following is an LU
factorization for the matrix.
ï£«
ï£¶ ï£«
ï£¶ï£«
ï£¶
1 2 3 2
1 0 0
1 2
3
2
ï£­ 4 3 1 1 ï£¸ = ï£­ 4 1 0 ï£¸ ï£­ 0 âˆ’5 âˆ’11 âˆ’7 ï£¸ .
1 2 3 0
1 0 1
0 0
0
âˆ’2
T

Let U x = y and consider Ly = b
ï£«
1
ï£­ 4
1

where in this case, b = (1, 2, 3) . Thus
ï£¶ ï£« ï£¶
ï£¶ï£«
1
y1
0 0
1 0 ï£¸ ï£­ y2 ï£¸ = ï£­ 2 ï£¸
3
y3
0 1
ï£«
ï£¶
1
which yields very quickly that y = ï£­ âˆ’2 ï£¸ . Now you can ï¬nd x by solving U x = y. Thus
2
in this case,
ï£«
ï£¶
ï£«
ï£¶
ï£«
ï£¶
x
1 2
3
2
1
ï£¬ y ï£·
ï£· ï£­ âˆ’2 ï£¸
ï£­ 0 âˆ’5 âˆ’11 âˆ’7 ï£¸ ï£¬
ï£­ z ï£¸=
0 0
0
âˆ’2
2
w
which yields

ï£«

ï£¶
âˆ’ 35 + 75 t
ï£¬ 9 âˆ’ 11 t ï£·
5
5
ï£· , t âˆˆ R.
x =ï£¬
ï£­
ï£¸
t
âˆ’1

Work this out by hand and you will see the advantage of working only with triangular
matrices.
It may seem like a trivial thing but it is used because it cuts down on the number of
operations involved in ï¬nding a solution to a system of equations enough that it makes a
diï¬€erence for large systems.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

126

5.4

SOME FACTORIZATIONS

The P LU Factorization

As indicated above, some matrices donâ€™t have an LU factorization. Here is an example.
ï£«
ï£¶
1 2 3 2
M =ï£­ 1 2 3 0 ï£¸
(5.1)
4 3 1 1
In this case, there is another factorization which is useful called a P LU factorization. Here
P is a permutation matrix.
Example 5.4.1 Find a P LU factorization for the above matrix in (5.1).
Proceed as before trying to ï¬nd the row echelon form of the matrix. First add âˆ’1 times
the ï¬rst row to the second row and then add âˆ’4 times the ï¬rst to the third. This yields
ï£«
ï£¶ï£«
ï£¶
1 0 0
1 2
3
2
ï£­ 1 1 0 ï£¸ï£­ 0 0
0
âˆ’2 ï£¸
4 0 1
0 âˆ’5 âˆ’11 âˆ’7
There is no way to do only row operations involving replacing a row with itself added to a
multiple of another row to the second matrix in such a way as to obtain an upper triangular
matrix. Therefore, consider M with the bottom two rows switched.
ï£«
ï£¶
1 2 3 2
Mâ€² = ï£­ 4 3 1 1 ï£¸ .
1 2 3 0
Now try again with this matrix. First take âˆ’1 times the ï¬rst row and add to the bottom
row and then take âˆ’4 times the ï¬rst row and add to the second row. This yields
ï£¶
ï£¶ï£«
ï£«
1 2
3
2
1 0 0
ï£­ 4 1 0 ï£¸ ï£­ 0 âˆ’5 âˆ’11 âˆ’7 ï£¸
0 0
0
âˆ’2
1 0 1
The second matrix is upper triangular and so
ï£¶ï£«
ï£«
1 0 0
1
ï£­ 4 1 0 ï£¸ï£­ 0
0
1 0 1
Thus M â€² = P M = LU where L and U
so
ï£«
ï£¶ ï£«
1 2 3 2
1 0
ï£­ 1 2 3 0 ï£¸=ï£­ 0 0
4 3 1 1
0 1

the LU factorization of the matrix M â€² is
ï£¶
2
3
2
âˆ’5 âˆ’11 âˆ’7 ï£¸ .
0
0
âˆ’2

are given
ï£¶ï£«
0
1
1 ï£¸ï£­ 4
0
1

above. Therefore, M = P 2 M = P LU and
ï£¶ï£«
ï£¶
0 0
1 2
3
2
1 0 ï£¸ ï£­ 0 âˆ’5 âˆ’11 âˆ’7 ï£¸
0 1
0 0
0
âˆ’2

This process can always be followed and so there always exists
given matrix even though there isnâ€™t always an LU factorization.
ï£«
1 2 3
Example 5.4.2 Use a P LU factorization of M â‰¡ ï£­ 1 2 3
4 3 1
T
M x = b where b = (1, 2, 3) .

Saylor URL: http://www.saylor.org/courses/ma212/

a P LU factorization of a
ï£¶
2
0 ï£¸ to solve the system
1

The Saylor Foundation

5.5. JUSTIFICATION FOR THE MULTIPLIER METHOD
Let U x = y and consider
ï£«
1 0
ï£­ 0 0
0 1

127

P Ly = b. In other words, solve,
ï£¶ï£«
ï£¶ï£«
ï£¶ ï£« ï£¶
0
1 0 0
y1
1
1 ï£¸ ï£­ 4 1 0 ï£¸ ï£­ y2 ï£¸ = ï£­ 2 ï£¸ .
y3
0
1 0 1
3

Then multiplying both sides by P
ï£«
1
ï£­ 4
1

gives
0
1
0

ï£¶ï£«
ï£¶ ï£« ï£¶
0
y1
1
0 ï£¸ ï£­ y2 ï£¸ = ï£­ 3 ï£¸
1
y3
2
ï£«

ï£¶ ï£«
ï£¶
y1
1
y = ï£­ y2 ï£¸ = ï£­ âˆ’1 ï£¸ .
y3
1

and so

Now U x = y and so it only remains to solve
ï£«

1
ï£­ 0
0
which yields

5.5

ï£«

ï£¶
ï£«
ï£¶
x1
2
3
2
1
ï£¬ x2 ï£·
ï£· ï£­ âˆ’1 ï£¸
âˆ’5 âˆ’11 âˆ’7 ï£¸ ï£¬
ï£­ x3 ï£¸ =
0
0
âˆ’2
1
x4
ï£¶

ï£¶ ï£«
x1
ï£¬ x2 ï£· ï£¬
ï£¬
ï£· ï£¬
ï£­ x3 ï£¸ = ï£­
x4
ï£«

1
5
9
10

ï£¶
+ 57 t
ï£·
âˆ’ 11
5 t ï£· : t âˆˆ R.
ï£¸
t
âˆ’ 21

Justiï¬cation For The Multiplier Method

Why does the multiplier method work for ï¬nding an LU factorization? Suppose A is a
matrix which has the property that the row reduced echelon form for A may be achieved
using only the row operations which involve replacing a row with itself added to a multiple
of another row. It is not ever necessary to switch rows. Thus every row which is replaced
using this row operation in obtaining the echelon form may be modiï¬ed by using a row
which is above it. Furthermore, in the multiplier method for ï¬nding the LU factorization,
we zero out the elements below the pivot entry in ï¬rst column and then the next and so on
when scanning from the left. In terms of elementary matrices, this means the row operations
used to reduce A to upper triangular form correspond to multiplication on the left by lower
triangular matrices having all ones down the main diagonal and the sequence of elementary
matrices which row reduces A has the property that in scanning the list of elementary
matrices from the right to the left, this list consists of several matrices which involve only
changes from the identity in the ï¬rst column, then several which involve only changes from
the identity in the second column and so forth. More precisely, Ep Â· Â· Â· E1 A = U where U
is upper triangular, each Ei is a lower triangular elementary matrix having all ones down
the main diagonal, for some ri , each of Er1 Â· Â· Â· E1 diï¬€ers from the identity only in the ï¬rst
column, each of Er2 Â· Â· Â· Er1 +1 diï¬€ers from the identity only in the second column and so
Will be L

}|
{
z
âˆ’1
Epâˆ’1 U. You multiply the inverses in the reverse order.
forth. Therefore, A = E1âˆ’1 Â· Â· Â· Epâˆ’1
Now each of the Eiâˆ’1 is also lower triangular with 1 down the main diagonal. Therefore
their product has this property. Recall also that if Ei equals the identity matrix except

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

128

SOME FACTORIZATIONS

for having an a in the j th column somewhere below the main diagonal, Eiâˆ’1 is obtained by
replacing the a in Ei with âˆ’a, thus explaining why we replace with âˆ’1 times the multiplier
âˆ’1
in computing L. In the case where A is a 3 Ã— m matrix, E1âˆ’1 Â· Â· Â· Epâˆ’1
Epâˆ’1 is of the form
ï£«
ï£¶ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
1 0 0
1 0 0
1 0 0
1 0 0
ï£­ a 1 0 ï£¸ï£­ 0 1 0 ï£¸ï£­ 0 1 0 ï£¸ = ï£­ a 1 0 ï£¸.
0 0 1
b 0 1
0 c 1
b c 1
Note that scanning from left to right, the ï¬rst two in the product involve changes in the
identity only in the ï¬rst column while in the third matrix, the change is only in the second.
If the entries in the ï¬rst column had been zeroed out in a diï¬€erent order, the following
would have resulted.
ï£«
ï£¶ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
1 0 0
1 0 0
1 0 0
1 0 0
ï£­ 0 1 0 ï£¸ï£­ a 1 0 ï£¸ï£­ 0 1 0 ï£¸ = ï£­ a 1 0 ï£¸
b 0 1
0 0 1
0 c 1
b c 1
However, it is important to be working from the left to the right, one column at a time.
A similar observation holds in any dimension. Multiplying the elementary matrices which
involve a change only in the j th column you obtain A equal to an upper triangular, n Ã— m
matrix U which is multiplied by a sequence of lower triangular matrices on its left which is
of the following form, in which the aij are negatives of multipliers used in row reducing to
an upper triangular matrix.
ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
1
0 Â·Â·Â· 0
1
0
Â·Â·Â· 0
1 0
Â·Â·Â·
0
ï£¬
.. ï£· ï£¬
.. ï£· ï£¬
.. ï£·
ï£¬ a11
ï£¬
ï£¬
1
. ï£·
1
. ï£·
. ï£·
ï£¬
ï£·ï£¬ 0
ï£·Â·Â·Â·ï£¬ 0 1
ï£·
ï£¬
ï£·
ï£¬
ï£·
ï£¬
ï£·
..
.
.
.
..
..
..
..
ï£­
. 0 ï£¸ ï£­ ..
. 0 ï£¸ ï£­ ..
.
.
0 ï£¸
a1,nâˆ’1 0 Â· Â· Â· 1
0 a2,nâˆ’2 Â· Â· Â· 1
0 Â· Â· Â· an,nâˆ’1 1
From the matrix multiplication, this product equals
ï£«
1
ï£¬ a11
1
ï£¬
ï£¬
..
..
ï£­
.
.
a1,nâˆ’1 Â· Â· Â· an,nâˆ’1

ï£¶
ï£·
ï£·
ï£·
ï£¸
1

Notice how the end result of the matrix multiplication made no change in the aij . It just
ï¬lled in the empty spaces with the aij which occurred in one of the matrices in the product.
This is why, in computing L, it is suï¬ƒcient to begin with the left column and work column
by column toward the right, replacing entries with the negative of the multiplier used in the
row operation which produces a zero in that entry.

5.6

Existence For The P LU Factorization

Here I will consider an invertible n Ã— n matrix and show that such a matrix always has
a P LU factorization. More general matrices could also be considered but this is all I will
present.
Let A be such an invertible matrix and consider the ï¬rst column of A. If A11 Ì¸= 0, use
this to zero out everything below it. The entry A11 is called the pivot. Thus in this case
there is a lower triangular matrix L1 which has all ones on the diagonal such that
(
)
âˆ— âˆ—
L1 P1 A =
(5.2)
0 A1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

5.6. EXISTENCE FOR THE P LU FACTORIZATION

129

Here P1 = I. In case A11 = 0, let r be such that Ar1 Ì¸= 0 and r is the ï¬rst entry for which
this happens. In this case, let P1 be the permutation matrix which switches the ï¬rst row
and the rth row. Then as before, there exists a lower triangular matrix L1 which has all
ones on the diagonal such that (5.2) holds in this case also. In the ï¬rst column, this L1 has
zeros between the ï¬rst row and the rth row.
Go to A1 . Following the same procedure as above, there exists a lower triangular matrix
and permutation matrix Lâ€²2 , P2â€² such that
(
)
âˆ— âˆ—
Lâ€²2 P2â€² A1 =
0 A2
(

Let
L2 =

1 0
0 Lâ€²2

)

(
, P2 =

1 0
0 P2â€²

)

Then using block multiplication, Theorem 3.5.2,
(
)(
)(
)
1 0
1 0
âˆ— âˆ—
=
0 Lâ€²2
0 P2â€²
0 A1
)
)(
) (
(
âˆ—
âˆ—
âˆ—
âˆ—
1 0
=
=
0 Lâ€²2 P2â€² A1
0 P2â€² A1
0 Lâ€²2
ï£«
ï£¶
âˆ— Â·Â·Â·
âˆ—
ï£­ 0 âˆ—
âˆ— ï£¸ = L2 P2 L1 P1 A
0 0 A2
and L2 has all the subdiagonal entries equal to 0 except possibly some nonzero entries in
the second column starting with position r2 where P2 switches rows r2 and 2. Continuing
this way, it follows there are lower triangular matrices Lj having all ones down the diagonal
and permutation matrices Pi which switch only two rows such that
Lnâˆ’1 Pnâˆ’1 Lnâˆ’2 Pnâˆ’2 Lnâˆ’3 Â· Â· Â· L2 P2 L1 P1 A = U

(5.3)

where U is upper triangular. The matrix Lj has all zeros below the main diagonal except
for the j th column and even in this column it has zeros between position j and rj where Pj
switches rows j and rj . Of course in the case where no switching is necessary, you could get
all nonzero entries below the main diagonal in the j th column for Lj .
The fact that Lj is the identity except for the j th column means that each Pk for k > j
almost commutes with Lj . Say Pk switches the k th and the q th rows for q â‰¥ k > j. When
you place Pk on the right of Lj it just switches the k th and the q th columns and leaves the
j th column unchanged. Therefore, the same result as placing Pk on the left of Lj can be
obtained by placing Pk on the right of Lj and modifying Lj by switching the k th and the q th
entries in the j th column. (Note this could possibly interchange a 0 for something nonzero.)
It follows from (5.3) there exists P, the product of permutation matrices, P = Pnâˆ’1 Â· Â· Â· P1
each of which switches two rows, and L a lower triangular matrix having all ones on the
main diagonal, L = Lâ€²nâˆ’1 Â· Â· Â· Lâ€²2 Lâ€²1 , where the Lâ€²j are obtained as just described by moving a
succession of Pk from the left to the right of Lj and modifying the j th column as indicated,
such that
LP A = U.
Then

A = P T Lâˆ’1 U

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

130

SOME FACTORIZATIONS

It is customary to write this more simply as
A = P LU
where L is an upper triangular matrix having all ones on the diagonal and P is a permutation
matrix consisting of P1 Â· Â· Â· Pnâˆ’1 as described above. This proves the following theorem.
Theorem 5.6.1 Let A be any invertible n Ã— n matrix. Then there exists a permutation
matrix P and a lower triangular matrix L having all ones on the main diagonal and an
upper triangular matrix U such that
A = P LU

5.7

The QR Factorization

As pointed out above, the LU factorization is not a mathematically respectable thing because it does not always exist. There is another factorization which does always exist.
Much more can be said about it than I will say here. At this time, I will only deal with real
matrices and so the inner product will be the usual real dot product.
Deï¬nition 5.7.1 An n Ã— n real matrix Q is called an orthogonal matrix if
QQT = QT Q = I.
Thus an orthogonal matrix is one whose inverse is equal to its transpose.
First note that if a matrix is orthogonal this says
âˆ‘
âˆ‘
QTij Qjk =
Qji Qjk = Î´ ik
j

Thus
2

|Qx| =

âˆ‘

j

ï£«
ï£­

i

=

r

ï£¶2
Qij xj ï£¸ =

i

Qis Qir xs xr =

âˆ‘âˆ‘
r

r

r

Î´ sr xs xr =

âˆ‘

s

s

Qis xs Qir xr

s

âˆ‘âˆ‘âˆ‘

s

=

âˆ‘âˆ‘âˆ‘

j

âˆ‘âˆ‘âˆ‘
i

âˆ‘

Qis Qir xs xr

i
2

x2r = |x|

r

This shows that orthogonal transformations preserve distances. You can show that if you
have a matrix which does preserve distances, then it must be orthogonal also.
Example 5.7.2 One of the most important examples of an orthogonal matrix is the so
called Householder matrix. You have v a unit vector and you form the matrix
I âˆ’ 2vvT
This is an orthogonal matrix which is also symmetric. To see this, you use the rules of
matrix operations.
(
)T
I âˆ’ 2vvT

(
)T
= I T âˆ’ 2vvT
= I âˆ’ 2vvT

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

5.7. THE QR FACTORIZATION

131

so it is symmetric. Now to show it is orthogonal,
(
)(
)
I âˆ’ 2vvT I âˆ’ 2vvT
= I âˆ’ 2vvT âˆ’ 2vvT + 4vvT vvT
= I âˆ’ 4vvT + 4vvT = I
2

because vT v = v Â· v = |v| = 1. Therefore, this is an example of an orthogonal matrix.
Consider the following problem.
Problem 5.7.3 Given two vectors x, y such that |x| = |y| Ì¸= 0 but x Ì¸= y and you want an
orthogonal matrix Q such that Qx = y and Qy = x. The thing which works is the Householder matrix
xâˆ’y
T
Qâ‰¡I âˆ’2
2 (x âˆ’ y)
|x âˆ’ y|
Here is why this works.
Q (x âˆ’ y) =

(x âˆ’ y) âˆ’ 2

= (x âˆ’ y) âˆ’ 2

Q (x + y)

=

(x + y) âˆ’ 2

= (x + y) âˆ’ 2

xâˆ’y

T

2

(x âˆ’ y) (x âˆ’ y)

2

|x âˆ’ y| = y âˆ’ x

|x âˆ’ y|
xâˆ’y
|x âˆ’ y|

xâˆ’y

2

T

2

|x âˆ’ y|
xâˆ’y

(x âˆ’ y) (x + y)

2 ((x âˆ’ y) Â· (x + y))
|x âˆ’ y|
)
xâˆ’y ( 2
2
=x+y
= (x + y) âˆ’ 2
|x|
âˆ’
|y|
2
|x âˆ’ y|

Hence
Qx + Qy

= x+y

Qx âˆ’ Qy

= yâˆ’x

Adding these equations, 2Qx = 2y and subtracting them yields 2Qy = 2x.
A picture of the geometric signiï¬cance follows.
x
y

The orthogonal matrix Q reï¬‚ects across the dotted line taking x to y and y to x.
Deï¬nition 5.7.4 Let A be an m Ã— n matrix. Then a QR factorization of A consists of two
matrices, Q orthogonal and R upper triangular (right triangular) having all the entries on
the main diagonal nonnegative such that A = QR.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

132

SOME FACTORIZATIONS

With the solution to this simple problem, here is how to obtain a QR factorization for
any matrix A. Let
A = (a1 , a2 , Â· Â· Â· , an )
where the ai are the columns. If a1 = 0, let Q1 = I. If a1 Ì¸= 0, let
ï£«
ï£¶
|a1 |
ï£¬ 0 ï£·
ï£¬
ï£·
b â‰¡ï£¬ . ï£·
ï£­ .. ï£¸
0
and form the Householder matrix
Q1 â‰¡ I âˆ’ 2

(a1 âˆ’ b)
2

|a1 âˆ’ b|

(a1 âˆ’ b)

T

As in the above problem Q1 a1 = b and so
(
)
|a1 | âˆ—
Q1 A =
0 A2
where A2 is a mâˆ’1Ã—nâˆ’1 matrix. Now ï¬nd in the same way as was just done a mâˆ’1Ã—mâˆ’1
b 2 such that
matrix Q
(
)
âˆ— âˆ—
b 2 A2 =
Q
0 A3
(

Let
Q2 â‰¡
(

Then
Q2 Q1 A =

ï£«
ï£¬
=ï£­

1 0
b2
0 Q
)(

1 0
b2
0 Q
|a1 |
..
.
0

âˆ—

âˆ—

)
.
|a1 | âˆ—
0 A2
ï£¶

)

ï£·
âˆ— âˆ— ï£¸
0 A3

Continuing this way until the result is upper triangular, you get a sequence of orthogonal
matrices Qp Qpâˆ’1 Â· Â· Â· Q1 such that
Qp Qpâˆ’1 Â· Â· Â· Q1 A = R

(5.4)

where R is upper triangular.
Now if Q1 and Q2 are orthogonal, then from properties of matrix multiplication,
T

Q1 Q2 (Q1 Q2 ) = Q1 Q2 QT2 QT1 = Q1 IQT1 = I
and similarly
T

(Q1 Q2 ) Q1 Q2 = I.
Thus the product of orthogonal matrices is orthogonal. Also the transpose of an orthogonal
matrix is orthogonal directly from the deï¬nition. Therefore, from (5.4)
T

A = (Qp Qpâˆ’1 Â· Â· Â· Q1 ) R â‰¡ QR.
This proves the following theorem.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

5.8. EXERCISES

133

Theorem 5.7.5 Let A be any real m Ã— n matrix. Then there exists an orthogonal matrix
Q and an upper triangular matrix R having nonnegative entries on the main diagonal such
that
A = QR
and this factorization can be accomplished in a systematic manner.
II

5.8

Exercises

1. Find a

2. Find a

3. Find a

4. Find a

5. Find a

ï£«

ï£¶
1 2 0
LU factorization of ï£­ 2 1 3 ï£¸ .
1 2 3
ï£«
ï£¶
1 2 3 2
LU factorization of ï£­ 1 3 2 1 ï£¸ .
5 0 1 3
ï£¶
ï£«
1 2 1
P LU factorization of ï£­ 1 2 2 ï£¸ .
2 1 1
ï£¶
ï£«
1 2 1 2 1
P LU factorization of ï£­ 2 4 2 4 1 ï£¸ .
1 2 1 3 2
ï£¶
ï£«
1 2 1
ï£¬ 1 2 2 ï£·
ï£·
P LU factorization of ï£¬
ï£­ 2 4 1 ï£¸.
3 2 1

6. Is there only one LU factorization for a given matrix? Hint: Consider the equation
(
) (
)(
)
0 1
1 0
0 1
=
.
0 1
1 1
0 0
7. Here is a matrix and
ï£«
1
A=ï£­ 1
0

an LU factorization
ï£¶ ï£«
2 5 0
1
1 4 9 ï£¸=ï£­ 1
1 2 5
0

of it.

ï£¶ï£«
0 0
1 2
1 0 ï£¸ ï£­ 0 âˆ’1
âˆ’1 1
0 0

ï£¶
5
0
âˆ’1 9 ï£¸
1 14

Use this factorization to solve the system of equations
ï£« ï£¶
1
Ax = ï£­ 2 ï£¸
3
8. Find a QR factorization for the matrix
ï£«
ï£¶
1 2 1
ï£­ 3 âˆ’2 1 ï£¸
1 0 2

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

134

SOME FACTORIZATIONS

9. Find a QR factorization for the matrix
ï£«
1 2
ï£­ 3 0
1 0

1
1
2

ï£¶
0
1 ï£¸
1

10. If you had a QR factorization, A = QR, describe how you could use it to solve the
equation Ax = b.
11. If Q is an orthogonal matrix, show the columns are an orthonormal set. That is show
that for
(
)
Q = q1 Â· Â· Â· qn
it follows that qi Â· qj = Î´ ij . Also show that any orthonormal set of vectors is linearly
independent.
12. Show you canâ€™t expect uniqueness for QR factorizations. Consider
ï£«
ï£¶
0 0 0
ï£­ 0 0 1 ï£¸
0 0 1
and verify this equals
ï£¶ï£«
0
0 0
âˆš
1
ï£¸ï£­ 0 0
2
2 âˆš
0 0
âˆ’ 12 2

ï£«

0
1
âˆš
ï£­ 1 2 0
2âˆš
1
2 2 0
ï£«

and also

1
ï£­ 0
0

0
1
0

ï£¶ï£«
0 0
0
0 ï£¸ï£­ 0 0
0 0
1

âˆš ï£¶
2
0 ï£¸
0

ï£¶
0
1 ï£¸.
1

Using Deï¬nition 5.7.4, can it be concluded that if A is an invertible matrix it will
follow there is only one QR factorization?
13. Suppose {a1 , Â· Â· Â· , an } are linearly independent vectors in Rn and let
(
)
A = a1 Â· Â· Â· an
Form a QR factorization for A.
ï£«
(

a1

Â·Â·Â·

an

)

=

(

q1

Â·Â·Â·

qn

)ï£¬
ï£¬
ï£¬
ï£­

r11
0
..
.

r12
r22

Â·Â·Â·
Â·Â·Â·
..
.

0

0

Â·Â·Â·

ï£¶
r1n
r2n ï£·
ï£·
ï£·
ï£¸
rnn

Show that for each k â‰¤ n,
span (a1 , Â· Â· Â· , ak ) = span (q1 , Â· Â· Â· , qk )
Prove that every subspace of Rn has an orthonormal basis. The procedure just described is similar to the Gram Schmidt procedure which will be presented later.
14. Suppose Qn Rn converges to an orthogonal matrix Q where Qn is orthogonal and Rn
is upper triangular having all positive entries on the diagonal. Show that then Qn
converges to Q and Rn converges to the identity.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Linear Programming
6.1

Simple Geometric Considerations

One of the most important uses of row operations is in solving linear program problems
which involve maximizing a linear function subject to inequality constraints determined
from linear equations. Here is an example. A certain hamburger store has 9000 hamburger
patties to use in one week and a limitless supply of special sauce, lettuce, tomatoes, onions,
and buns. They sell two types of hamburgers, the big stack and the basic burger. It has also
been determined that the employees cannot prepare more than 9000 of either type in one
week. The big stack, popular with the teenagers from the local high school, involves two
patties, lots of delicious sauce, condiments galore, and a divider between the two patties.
The basic burger, very popular with children, involves only one patty and some pickles
and ketchup. Demand for the basic burger is twice what it is for the big stack. What
is the maximum number of hamburgers which could be sold in one week given the above
limitations?
Let x be the number of basic burgers and y the number of big stacks which could be sold
in a week. Thus it is desired to maximize z = x + y subject to the above constraints. The
total number of patties is 9000 and so the number of patty used is x+2y. This number must
satisfy x + 2y â‰¤ 9000 because there are only 9000 patty available. Because of the limitation
on the number the employees can prepare and the demand, it follows 2x + y â‰¤ 9000.
You never sell a negative number of hamburgers and so x, y â‰¥ 0. In simpler terms the
problem reduces to maximizing z = x + y subject to the two constraints, x + 2y â‰¤ 9000 and
2x + y â‰¤ 9000. This problem is pretty easy to solve geometrically. Consider the following
picture in which R labels the region described by the above inequalities and the line z = x+y
is shown for a particular value of z.

x+y =z

2x + y = 4

R

x + 2y = 4

As you make z larger this line moves away from the origin, always having the same slope
135

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

136

LINEAR PROGRAMMING

and the desired solution would consist of a point in the region, R which makes z as large as
possible or equivalently one for which the line is as far as possible from the origin. Clearly
this point is the point of intersection of the two lines, (3000, 3000) and so the maximum
value of the given function is 6000. Of course this type of procedure is ï¬ne for a situation in
which there are only two variables but what about a similar problem in which there are very
many variables. In reality, this hamburger store makes many more types of burgers than
those two and there are many considerations other than demand and available patty. Each
will likely give you a constraint which must be considered in order to solve a more realistic
problem and the end result will likely be a problem in many dimensions, probably many
more than three so your ability to draw a picture will get you nowhere for such a problem.
Another method is needed. This method is the topic of this section. I will illustrate with
this particular problem. Let x1 = x and y = x2 . Also let x3 and x4 be nonnegative variables
such that
x1 + 2x2 + x3 = 9000, 2x1 + x2 + x4 = 9000.
To say that x3 and x4 are nonnegative is the same as saying x1 + 2x2 â‰¤ 9000 and 2x1 + x2 â‰¤
9000 and these variables are called slack variables at this point. They are called this because
they â€œtake up the slackâ€. I will discuss these more later. First a general situation is
considered.

6.2

The Simplex Tableau

Here is some notation.
Deï¬nition 6.2.1 Let x, y be vectors in Rq . Then x â‰¤ y means for each i, xi â‰¤ yi .
The problem is as follows:
Let A be an m Ã— (m + n) real matrix of rank m. It is desired to ï¬nd x âˆˆ Rn+m such
that x satisï¬es the constraints,
x â‰¥ 0, Ax = b
(6.1)
and out of all such x,
zâ‰¡

m+n
âˆ‘

ci xi

i=1

is as large (or small) as possible. This is usually referred to as maximizing or minimizing z
subject to the( above constraints.) First I will consider the constraints.
Let A = a1 Â· Â· Â· an+m . First you ï¬nd a vector, x0 â‰¥ 0, Ax0 = b such that n of
the components of this vector equal 0. Letting i1 , Â· Â· Â· , in be the positions of x0 for which
x0i = 0, suppose also that {aj1 , Â· Â· Â· , ajm } is linearly independent for ji the other positions
of x0 . Geometrically, this means that x0 is a corner of the feasible region, those x which
satisfy the constraints. This is called a basic feasible solution. Also deï¬ne
cB
xB
and

â‰¡ (cj1 . Â· Â· Â· , cjm ) , cF â‰¡ (ci1 , Â· Â· Â· , cin )
â‰¡ (xj1 , Â· Â· Â· , xjm ) , xF â‰¡ (xi1 , Â· Â· Â· , xin ) .

( ) (
z 0 â‰¡ z x0 = c B

cF

)

(

x0B
x0F

)
= cB x0B

since x0F = 0. The variables which are the components of the vector xB are called the basic
variables and the variables which are the entries of xF are called the free variables. You

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

6.2. THE SIMPLEX TABLEAU

137

(
)T
set xF = 0. Now x0 , z 0 is a solution to
(
)(
) (
)
A 0
x
b
=
âˆ’c 1
z
0
along with the constraints x â‰¥ 0. Writing the above in augmented matrix form yields
)
(
A 0 b
(6.2)
âˆ’c 1 0
Permute the columns and variables on the left if necessary to write the above in the form
ï£«
ï£¶
(
)
(
)
xB
B
F
0 ï£­
b
xF ï£¸ =
(6.3)
âˆ’cB âˆ’cF 1
0
z
or equivalently in the augmented matrix form keeping track of the variables on the bottom
as
ï£«
ï£¶
B
F
0 b
ï£­ âˆ’cB âˆ’cF 1 0 ï£¸ .
(6.4)
xB
xF
0 0
Here B pertains to the variables xi1 , Â· Â· Â· , xjm and is an m Ã— m matrix with linearly independent columns, {aj1 , Â· Â· Â· , ajm } , and F is an m Ã— n matrix. Now it is assumed that
)
(
)
(
(
) x0B
(
) x0B
B F
= B F
= Bx0B = b
x0F
0
and since B is assumed to have rank m, it follows
x0B = B âˆ’1 b â‰¥ 0.

(6.5)

This is very important to observe. B âˆ’1 b â‰¥ 0! This is by the assumption that x0 â‰¥ 0.
Do row operations on the top part of the matrix
(
)
B
F
0 b
(6.6)
âˆ’cB âˆ’cF 1 0
and obtain its row reduced echelon form. Then after these row operations the above becomes
(
)
I
B âˆ’1 F 0 B âˆ’1 b
.
(6.7)
âˆ’cB âˆ’cF
1
0
where B âˆ’1 b â‰¥ 0. Next do another
Thus
(
I
0
(
I
=
0
(
I
=
0

row operation in order to get a 0 where you see a âˆ’cB .
B âˆ’1 F
cB B âˆ’1 F â€² âˆ’ cF
B âˆ’1 F
cB B âˆ’1 F â€² âˆ’ cF
B âˆ’1 F
cB B âˆ’1 F âˆ’ cF

B âˆ’1 b
cB B âˆ’1 b
)
0 B âˆ’1 b
1 cB x0B
)
0 B âˆ’1 b
1
z0
0
1

)
(6.8)

(6.9)

(
)T
The reason there is a z 0 on the bottom right corner is that xF = 0 and x0B , x0F , z 0 is a
solution of the system of equations represented by the above augmented matrix because it is

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

138

LINEAR PROGRAMMING

a solution to the system of equations corresponding to the system of equations represented
by (6.6) and row operations leave solution sets unchanged. Note how attractive this is. The
z0 is the value of z at the point x0 . The augmented matrix of (6.9) is called the simplex
tableau and it is the beginning point for the simplex algorithm to be described a little
later. It is very convenient to express the simplex(tableau
) in the above form in which the
I
variables are possibly permuted in order to have
on the left side. However, as far
0
as the simplex algorithm is concerned it is not necessary to be permuting the variables in
this manner. Starting with (6.9) you could permute the variables and columns to obtain an
augmented matrix in which the variables are in their original order. What is really required
for the simplex tableau?
It is an augmented m + 1 Ã— m + n + 2 matrix which represents a system of equations
T
which has the same set of solutions, (x,z) as the system whose augmented matrix is
(
)
A 0 b
âˆ’c 1 0
(Possibly the variables for x are taken in another order.) There are m linearly independent
columns in the ï¬rst m + n columns for which there is only one nonzero entry, a 1 in one of
the ï¬rst m rows, the â€œsimple columnsâ€, the other ï¬rst m + n columns being the â€œnonsimple
columnsâ€. As in the above, the variables corresponding to the simple columns are xB ,
the basic variables and those corresponding to the nonsimple columns are xF , the free
variables. Also, the top m entries of the last column on the right are nonnegative. This is
the description of a simplex tableau.
In a simplex tableau it is easy to spot a basic feasible solution. You can see one quickly
by setting the variables, xF corresponding to the nonsimple columns equal to zero. Then
the other variables, corresponding to the simple columns are each equal to a nonnegative
entry in the far right column. Lets call this an â€œobvious basic feasible solutionâ€. If a
solution is obtained by setting the variables corresponding to the nonsimple columns equal
to zero and the variables corresponding to the simple columns equal to zero this will be
referred to as an â€œobviousâ€ solution. Lets also call the ï¬rst m + n entries in the bottom
row the â€œbottom left rowâ€. In a simplex tableau, the entry in the bottom right corner gives
the value of the variable being maximized or minimized when the obvious basic feasible
solution is chosen.
The following is a special case of the general theory presented above and shows how such
a special case can be ï¬t into the above framework. The following example is rather typical
of the sorts of problems considered. It involves inequality constraints instead of Ax = b.
This is handled by adding in â€œslack variablesâ€ as explained below.
The idea is to obtain an augmented matrix for the constraints such that obvious solutions
are also feasible. Then there is an algorithm, to be presented later, which takes you from
one obvious feasible solution to another until you obtain the maximum.
Example 6.2.2 Consider z = x1 âˆ’x2 subject to the constraints, x1 +2x2 â‰¤ 10, x1 +2x2 â‰¥ 2,
and 2x1 + x2 â‰¤ 6, xi â‰¥ 0. Find a simplex tableau for a problem of the form x â‰¥ 0,Ax = b
which is equivalent to the above problem.
You add in slack variables. These are positive variables, one for each of the ï¬rst three constraints, which change the ï¬rst three inequalities into equations. Thus the ï¬rst three inequalities become x1 +2x2 +x3 = 10, x1 +2x2 âˆ’x4 = 2, and 2x1 +x2 +x5 = 6, x1 , x2 , x3 , x4 , x5 â‰¥ 0.
Now it is necessary to ï¬nd a basic feasible solution. You mainly need to ï¬nd a positive so-

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

6.2. THE SIMPLEX TABLEAU

139

lution to the equations,
x1 + 2x2 + x3 = 10
x1 + 2x2 âˆ’ x4 = 2
2x1 + x2 + x5 = 6
the solution set for the above system is given by
x2 =

2
2 1
1
10 2
x4 âˆ’ + x5 , x1 = âˆ’ x4 +
âˆ’ x5 , x3 = âˆ’x4 + 8.
3
3 3
3
3
3

An easy way to get a basic feasible solution is to let x4 = 8 and x5 = 1. Then a feasible
solution is
(x1 , x2 , x3 , x4 , x5 ) = (0, 5, 0, 8, 1) .
(
)
A 0 b
It follows z 0 = âˆ’5 and the matrix (6.2),
with the variables kept track of
âˆ’c 1 0
on the bottom is
ï£¶
ï£«
1
2
1
0
0 0 10
ï£¬ 1
2
0 âˆ’1 0 0 2 ï£·
ï£·
ï£¬
ï£¬ 2
1
0
0
1 0 6 ï£·
ï£·
ï£¬
ï£­ âˆ’1 1
0
0
0 1 0 ï£¸
x1 x2 x3 x4 x5 0 0
and the ï¬rst thing to do is to permute the columns so that the list of variables on the bottom
will have x1 and x3 at the end.
ï£«
ï£¶
2
0
0
1
1 0 10
ï£¬ 2 âˆ’1 0
1
0 0 2 ï£·
ï£¬
ï£·
ï£¬ 1
0
1
2
0 0 6 ï£·
ï£¬
ï£·
ï£­ 1
0
0 âˆ’1 0 1 0 ï£¸
x2 x4 x5 x1 x3 0 0
Next, as described above, take the
above matrix. This yields
ï£«
1
ï£­ 0
0

row reduced echelon form of the top three lines of the
0 0
1 0
0 1

1
2

1
2

0

1
âˆ’ 12

3
2

ï£¶
0 5
0 8 ï£¸.
0 1

Now do row operations to
ï£«

1
ï£¬ 0
ï£¬
ï£­ 0
1
to ï¬nally obtain

ï£«

0 0
1 0
0 1
0 0

1 0 0
ï£¬ 0 1 0
ï£¬
ï£­ 0 0 1
0 0 0

1
2

1
2

0

1
âˆ’ 12
âˆ’1 0
3
2

1
2

1
2

0

1
âˆ’ 12
âˆ’ 12

3
2
âˆ’ 32

0
0
0
1
0
0
0
1

ï£¶
5
8 ï£·
ï£·
1 ï£¸
0
ï£¶
5
8 ï£·
ï£·
1 ï£¸
âˆ’5

and this is a simplex tableau. The variables are x2 , x4 , x5 , x1 , x3 , z.
It isnâ€™t as hard as it may appear from the above. Lets not permute the variables and
simply ï¬nd an acceptable simplex tableau as described above.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

140

LINEAR PROGRAMMING

Example 6.2.3 Consider z = x1 âˆ’x2 subject to the constraints, x1 +2x2 â‰¤ 10, x1 +2x2 â‰¥ 2,
and 2x1 + x2 â‰¤ 6, xi â‰¥ 0. Find a simplex tableau.
is

Adding in slack variables, an augmented
ï£«
1 2 1
ï£­ 1 2 0
2 1 0

matrix which is descriptive of the constraints
ï£¶
0 0 10
âˆ’1 0 6 ï£¸
0 1 6

The obvious solution is not feasible because of that -1 in the fourth column. When you let
x1 , x2 = 0, you end up having x4 = âˆ’6 which is negative. Consider the second column and
select the 2 as a pivot to zero out that which is above and below the 2.
ï£«
ï£¶
0 0 1 1 0 4
ï£­ 1 1 0 âˆ’1 0 3 ï£¸
2
2
3
1
0 0
1 3
2
2
This one is good. When you let x1 = x4 = 0, you ï¬nd that x2 = 3, x3 = 4, x5 = 3. The
obvious solution is now feasible. You can now assemble the simplex tableau. The ï¬rst step
is to include a column and row for z. This yields
ï£«
ï£¶
0 0 1 1 0 0 4
ï£¬ 1 1 0 âˆ’1 0 0 3 ï£·
2
ï£·
ï£¬ 32
1
ï£­
0 0
1 0 3 ï£¸
2
2
âˆ’1 0 1 0 0 1 0
Now you need to get zeros in the right places so the simple columns will be preserved as
simple columns in this larger matrix. This means you need to zero out the 1 in the third
column on the bottom. A simplex tableau is now
ï£«
ï£¶
0 0 1 1 0 0 4
ï£¬ 1 1 0 âˆ’1 0 0 3 ï£·
2
ï£¬ 32
ï£·.
1
ï£­
0 0
1 0 3 ï£¸
2
2
âˆ’1 0 0 âˆ’1 0 1 âˆ’4
Note it is not the same one obtained earlier. There is no reason a simplex tableau should
be unique. In fact, it follows from the above general description that you have one for each
basic feasible point of the region determined by the constraints.

6.3
6.3.1

The Simplex Algorithm
Maximums

The simplex algorithm takes you from one basic feasible solution to another while maximizing or minimizing the function you are trying to maximize or minimize. Algebraically,
it takes you from one simplex tableau to another in which the lower right corner either
increases in the case of maximization or decreases in the case of minimization.
I will continue writing the simplex tableau in such a way that the simple columns having
only one entry nonzero are on the left. As explained above, this amounts to permuting the
variables. I will do this because it is possible to describe what is going on without onerous
notation. However, in the examples, I wonâ€™t worry so much about it. Thus, from a basic
feasible solution, a simplex tableau of the following form has been obtained in which the
columns for the basic variables, xB are listed ï¬rst and b â‰¥ 0.
(
)
I F 0 b
(6.10)
0 c 1 z0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

6.3. THE SIMPLEX ALGORITHM

141

( 0 0)
Let x0i = bi for i = 1, Â· Â· Â· , m and x0i( = 0 for
i
>
m.
Then
x , z is a solution to the above
)
0 0
system and since b â‰¥ 0, it follows x , z is a basic feasible solution.
(
)
F
If ci < 0 for some i, and if Fji â‰¤ 0 so that a whole column of
is â‰¤ 0 with the
c
bottom entry < 0, then letting xi be the variable corresponding to that column, you could
leave all the other entries of xF equal to zero but change xi to be positive. Let the new
vector be denoted by xâ€²F and letting xâ€²B = b âˆ’ F xâ€²F it follows
âˆ‘
(xâ€²B )k = bk âˆ’
Fkj (xF )j
j

= bk âˆ’ Fki xi â‰¥ 0
Now this shows (xâ€²B , xâ€²F ) is feasible whenever xi > 0 and so you could let xi become
arbitrarily large and positive and conclude there is no maximum for z because
z = (âˆ’ci ) xi + z 0

(6.11)

If this happens in a simplex tableau, you can say there is no maximum and stop.
What if c â‰¥ 0? Then z = z 0 âˆ’ cxF and to satisfy the constraints, you need xF â‰¥ 0.
Therefore, in this case, z 0 is the largest possible value of z and so the maximum has been
found. You stop when this occurs. Next I explain what to do if neither of the above stopping
conditions hold.
(The only
) case which remains is that some ci < 0 and some Fji > 0. You pick a column
F
in
in which ci < 0, usually the one for which ci is the largest in absolute value.
c
You pick Fji > 0 as a pivot element, divide the j th row by Fji and then use to obtain
zeros above Fji and below Fji , thus obtaining a new simple column. This row operation
also makes exactly one of the other simple columns into a nonsimple column. (In terms of
variables, it is said that a free variable becomes a basic variable and a basic variable becomes
a free variable.) Now permuting the columns and variables, yields
(
)
I F â€² 0 bâ€²
0 câ€² 1 z 0â€²
( )
bj
where z 0â€² â‰¥ z 0 because z 0â€² = z 0 âˆ’ ci Fji
and ci < 0. If bâ€² â‰¥ 0, you are in the same
position you were at the beginning but now z 0 is larger. Now here is the important thing.
You donâ€™t pick just any Fji when you do these row operations. You pick the positive one
for which the row operation results in bâ€² â‰¥ 0. Otherwise the obvious basic feasible
solution obtained by letting xâ€²F = 0 will fail to satisfy the constraint that x â‰¥ 0.
How is this done? You need
bâ€²k â‰¡ bk âˆ’

Fki bj
â‰¥0
Fji

(6.12)

Fki bj
.
Fji

(6.13)

for each k = 1, Â· Â· Â· , m or equivalently,
bk â‰¥

Now if Fki â‰¤ 0 the above holds. Therefore, you only need to check Fpi for Fpi > 0. The
pivot, Fji is the one which makes the quotients of the form
bp
Fpi

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

142

LINEAR PROGRAMMING

for all positive Fpi the smallest. This will work because for Fki > 0,
bk
Fki bp
bp
â‰¤
â‡’ bk â‰¥
Fpi
Fki
Fpi
Having gotten a new simplex tableau, you do the same thing to it which was just done
and continue. As long as b > 0, so you donâ€™t encounter the degenerate case, the values
for z associated with setting xF = 0 keep getting strictly larger every time the process is
repeated. You keep going until you ï¬nd c â‰¥ 0. Then you stop. You are at a maximum.
Problems can occur in the process in the so called degenerate case when at some stage of
the process some bj = 0. In this case you can cycle through diï¬€erent values for x with no
improvement in z. This case will not be discussed here.
Example 6.3.1 Maximize 2x1 + 3x2 subject to the constraints x1 + x2 â‰¥ 1, 2x1 + x2 â‰¤
6, x1 + 2x2 â‰¤ 6, x1 , x2 â‰¥ 0.
The constraints are of the form
x1 + x2 âˆ’ x3
2x1 + x2 + x4
x1 + 2x2 + x5

= 1
= 6
= 6

where the x3 , x4 , x5 are the slack variables. An augmented matrix for these equations is of
the form
ï£¶
ï£«
1 1 âˆ’1 0 0 1
ï£­ 2 1 0 1 0 6 ï£¸
1 2 0 0 1 6
Obviously the obvious solution is not feasible.
basic variables. Lets just try something.
ï£«
1 1 âˆ’1
ï£­ 0 âˆ’1 2
0 1
1

It results in x3 < 0. We need to exchange
ï£¶
0 0 1
1 0 4 ï£¸
0 1 5

Now this one is all right because the obvious solution is feasible. Letting x2 = x3 = 0,
it follows that the obvious solution is feasible. Now we add in the objective function as
described above.
ï£«
ï£¶
1
1 âˆ’1 0 0 0 1
ï£¬ 0 âˆ’1 2 1 0 0 4 ï£·
ï£¬
ï£·
ï£­ 0
1
1 0 1 0 5 ï£¸
âˆ’2 âˆ’3 0 0 0 1 0
Then do row operations to leave the simple columns
ï£«
1 1 âˆ’1 0 0
ï£¬ 0 âˆ’1 2 1 0
ï£¬
ï£­ 0 1
1 0 1
0 âˆ’1 âˆ’2 0 0

the same. Then
ï£¶
0 1
0 4 ï£·
ï£·
0 5 ï£¸
1 2

Now there are negative numbers on the bottom row to the left of the 1. Lets pick the ï¬rst.
(It would be more sensible to pick the second.) The ratios to look at are 5/1, 1/1 so pick for

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

6.3. THE SIMPLEX ALGORITHM

143

the pivot the 1 in the second column and ï¬rst row. This will leave the right column above
the lower right corner nonnegative. Thus the next tableau is
ï£«
ï£¶
1 1 âˆ’1 0 0 0 1
ï£¬ 1 0 1 1 0 0 5 ï£·
ï£¬
ï£·
ï£­ âˆ’1 0 2 0 1 0 4 ï£¸
1 0 âˆ’3 0 0 1 3
There is still a negative number there to the left of the 1 in the bottom row. The new ratios
are 4/2, 5/1 so the new pivot is the 2 in the third column. Thus the next tableau is
ï£¶
ï£« 1
1
1 0 0
0 3
2
2
ï£¬ 3
0 0 1 âˆ’ 12 0 3 ï£·
ï£·
ï£¬ 2
ï£­ âˆ’1 0 2 0 1 0 4 ï£¸
3
1 9
âˆ’ 21 0 0 0
2
Still, there is a negative number in the bottom row to the left of the 1 so the process does
not stop yet. The ratios are 3/ (3/2) and 3/ (1/2) and so the new pivot is that 3/2 in the
ï¬rst column. Thus the new tableau is
ï£«
ï£¶
2
0 1 0 âˆ’ 13
0 2
3
ï£¬ 3 0 0 1 âˆ’1 0 3 ï£·
2
ï£¬ 2
ï£·
2
2
ï£­ 0 0 2
0 6 ï£¸
3
3
1
4
0 0 0
1 10
3
3
Now stop. The maximum value is 10. This is an easy enough problem to do geometrically
and so you can easily verify that this is the right answer. It occurs when x4 = x5 = 0, x1 =
2, x2 = 2, x3 = 3.

6.3.2

Minimums

How does it diï¬€er if you are ï¬nding a minimum? From a basic feasible solution, a simplex
tableau of the following form has been obtained in which the simple columns for the basic
variables, xB are listed ï¬rst and b â‰¥ 0.
(
)
I F 0 b
(6.14)
0 c 1 z0
(
)
Let x0i = bi for i = 1, Â· Â· Â· , m and x0i (= 0 for) i > m. Then x0 , z 0 is a solution to the above
system and since b â‰¥ 0, it follows x0 , z 0 is a basic feasible solution. So far, there is no
change.
Suppose ï¬rst that some ci > 0 and Fji â‰¤ 0 for each j. Then let xâ€²F consist of changing xi
by making it positive but leaving the other entries of xF equal to 0. Then from the bottom
row,
z = âˆ’ci xi + z 0
and you let xâ€²B = b âˆ’ F xâ€²F â‰¥ 0. Thus the constraints continue to hold when xi is made
increasingly positive and it follows from the above equation that there is no minimum for
z. You stop when this happens.
Next suppose c â‰¤ 0. Then in this case, z = z 0 âˆ’ cxF and from the constraints, xF â‰¥ 0
and so âˆ’cxF â‰¥ 0 and so z 0 is the minimum value and you stop since this is what you are
looking for.
What do you do in the case where some ci > 0 and some Fji > 0? In this case, you use
the simplex algorithm as in the case of maximums to obtain a new simplex tableau in which

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

144

LINEAR PROGRAMMING

z 0â€² is smaller. You choose Fji the same way to be the positive entry of the ith column such
that bp /Fpi â‰¥ bj /Fji for all positive entries, Fpi and do the same row operations. Now this
time,
(
)
bj
0â€²
0
z = z âˆ’ ci
< z0
Fji
As in the case of maximums no problem can occur and the process will converge unless
you have the degenerate case in which some bj = 0. As in the earlier case, this is most
unfortunate when it occurs. You see what happens of course. z 0 does not change and the
algorithm just delivers diï¬€erent values of the variables forever with no improvement.
To summarize the geometrical signiï¬cance of the simplex algorithm, it takes you from one
corner of the feasible region to another. You go in one direction to ï¬nd the maximum and
in another to ï¬nd the minimum. For the maximum you try to get rid of negative entries of c
and for minimums you try to eliminate positive entries of c, where the method of elimination
involves the auspicious use of an appropriate pivot element and row operations.
Now return to Example 6.2.2. It will be modiï¬ed to be a maximization problem.
Example 6.3.2 Maximize z = x1 âˆ’ x2 subject to the constraints,
x1 + 2x2 â‰¤ 10, x1 + 2x2 â‰¥ 2,
and 2x1 + x2 â‰¤ 6, xi â‰¥ 0.
Recall this is the same as maximizing z = x1 âˆ’ x2 subject to
ï£«
ï£¶
x1
ï£«
ï£¶
ï£«
ï£¶
ï£¬ x2 ï£·
1 2 1 0 0
10
ï£¬
ï£·
ï£­ 1 2 0 âˆ’1 0 ï£¸ ï£¬ x3 ï£· = ï£­ 2 ï£¸ , x â‰¥ 0,
ï£¬
ï£·
ï£­ x4 ï£¸
2 1 0 0 1
6
x5
the variables, x3 , x4 , x5 being slack variables. Recall the simplex tableau was
ï£¶
ï£«
1
1
0 5
1 0 0
2
2
ï£¬ 0 1 0 0
1 0 8 ï£·
ï£·
ï£¬
3
1
ï£­ 0 0 1
âˆ’
0 1 ï£¸
2
2
0 0 0 âˆ’ 32 âˆ’ 12 1 âˆ’5
with the variables ordered as x2 , x4 , x5 , x1 , x3 and so xB = (x2 , x4 , x5 ) and
xF = (x1 , x3 ) .
Apply the simplex algorithm to the fourth column because âˆ’ 32 < 0 and this is the most
negative entry in the bottom row. The pivot is 3/2 because 1/(3/2) = 2/3 < 5/ (1/2) .
Dividing this row by 3/2 and then using this to zero out the other elements in that column,
the new simplex tableau is
ï£¶
ï£«
2
0 14
1 0 âˆ’ 31 0
3
3
ï£¬ 0 1 0 0 1 0 8 ï£·
ï£·.
ï£¬
2
ï£­ 0 0
1 âˆ’ 13 0 23 ï£¸
3
0 0 1 0 âˆ’1 1 âˆ’4
Now there is still a negative number in the bottom left row. Therefore, the process should
be continued. This time the pivot is the 2/3 in the top of the column. Dividing the top row

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

6.3. THE SIMPLEX ALGORITHM

145

by 2/3 and then using this to zero out the entries below it,
ï£« 3
ï£¶
0 âˆ’ 12 0 1 0 7
2
1
ï£¬ âˆ’3 1
0 0 0 1 ï£·
2
ï£¬ 12
ï£·.
1
ï£­
0
1 0 0 3 ï£¸
2
2
1
3
0
0 0 1 3
2
2
Now all the numbers on the bottom left row are nonnegative so the process stops. Now
recall the variables and columns were ordered as x2 , x4 , x5 , x1 , x3 . The solution in terms of
x1 and x2 is x2 = 0 and x1 = 3 and z = 3. Note that in the above, I did not worry about
permuting the columns to keep those which go with the basic variables on the left.
Here is a bucolic example.
Example 6.3.3 Consider the following table.

iron
protein
folic acid
copper
calcium

F1
1
5
1
2
1

F2
2
3
2
1
1

F3
1
2
2
1
1

F4
3
1
1
1
1

This information is available to a pig farmer and Fi denotes a particular feed. The numbers
in the table contain the number of units of a particular nutrient contained in one pound of
the given feed. Thus F2 has 2 units of iron in one pound. Now suppose the cost of each feed
in cents per pound is given in the following table.
F1
2

F2
3

F3
2

F4
3

A typical pig needs 5 units of iron, 8 of protein, 6 of folic acid, 7 of copper and 4 of calcium.
(The units may change from nutrient to nutrient.) How many pounds of each feed per pig
should the pig farmer use in order to minimize his cost?
His problem is to minimize C â‰¡ 2x1 + 3x2 + 2x3 + 3x4 subject to the constraints
x1 + 2x2 + x3 + 3x4

â‰¥

5,

5x1 + 3x2 + 2x3 + x4
x1 + 2x2 + 2x3 + x4
2x1 + x2 + x3 + x4

â‰¥
â‰¥
â‰¥

8,
6,
7,

x1 + x2 + x3 + x4

â‰¥

4.

where each xi â‰¥ 0. Add in the slack variables,
x1 + 2x2 + x3 + 3x4 âˆ’ x5

= 5

5x1 + 3x2 + 2x3 + x4 âˆ’ x6
x1 + 2x2 + 2x3 + x4 âˆ’ x7

= 8
= 6

2x1 + x2 + x3 + x4 âˆ’ x8
x1 + x2 + x3 + x4 âˆ’ x9

= 7
= 4

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

146
The augmented matrix for
ï£«
1
ï£¬ 5
ï£¬
ï£¬ 1
ï£¬
ï£­ 2
1

LINEAR PROGRAMMING

this system is
2
3
2
1
1

1
2
2
1
1

3
1
1
1
1

âˆ’1
0
0
0
0

0
0
0
âˆ’1 0
0
0 âˆ’1 0
0
0 âˆ’1
0
0
0

0
0
0
0
âˆ’1

5
8
6
7
4

ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸

How in the world can you ï¬nd a basic feasible solution? Remember the simplex algorithm
is designed to keep the entries in the right column nonnegative so you use this algorithm a
few times till the obvious solution is a basic feasible solution.
Consider the ï¬rst column. The pivot is the 5. Using the row operations described in the
algorithm, you get
ï£«
ï£¶
7
3
14
17
1
0
âˆ’1
0
0
0
5
5
5
5
5
3
2
1
8 ï£·
ï£¬ 1
0 âˆ’ 15
0
0
0
5
5
5
5 ï£·
ï£¬
7
8
4
1
22
ï£¬ 0
ï£·
0
âˆ’1 0
0
5
5
5
5
5 ï£·
ï£¬
2
19 ï£¸
ï£­ 0 âˆ’1 1 3
0
0
âˆ’1
0
5
5
5
5
5
2
3
4
1
0
0
0
0 âˆ’1 12
5
5
5
5
5
Now go to the second column. The pivot in this column is the 7/5. This is in a diï¬€erent
row than the pivot in the ï¬rst column so I will use it to zero out everything below it. This
will get rid of the zeros in the ï¬fth column and introduce zeros in the second. This yields
ï£«
ï£¶
17
1
0 1 37
0
2 âˆ’ 57
0
0
7
7
3
1 ï£·
ï£¬ 1 0 1 âˆ’1
âˆ’ 27
0
0
0
7
7
7 ï£·
ï£¬
ï£¬ 0 0 1 âˆ’2 1
0 âˆ’1 0
0
1 ï£·
ï£¬
ï£·
3
30 ï£¸
ï£­ 0 0 2
1 âˆ’ 17
0 âˆ’1 0
7
7
7
2
1
0 0 37
0
0
0 âˆ’1 10
7
7
7
Now consider another column, this time the fourth. I will pick this one because it has
some negative numbers in it so there are fewer entries to check in looking for a pivot.
Unfortunately, the pivot is the top 2 and I donâ€™t want to pivot on this because it would
destroy the zeros in the second column. Consider the ï¬fth column. It is also not a good
choice because the pivot is the second element from the top and this would destroy the zeros
in the ï¬rst column. Consider the sixth column. I can use either of the two bottom entries
as the pivot. The matrix is
ï£«
ï£¶
0 1 0
2 âˆ’1 0 0
0
1
1
ï£¬ 1 0 1 âˆ’1 1 0 0
0 âˆ’2 3 ï£·
ï£¬
ï£·
ï£¬ 0 0 1 âˆ’2 1 0 âˆ’1 0
0
1 ï£·
ï£¬
ï£·
ï£­ 0 0 âˆ’1 1 âˆ’1 0 0 âˆ’1 3
0 ï£¸
0 0 3
0
2 1 0
0 âˆ’7 10
Next consider the third column. The pivot
ï£«
0 1 0 2 âˆ’1
ï£¬ 1 0 0 1
0
ï£¬
ï£¬ 0 0 1 âˆ’2 1
ï£¬
ï£­ 0 0 0 âˆ’1 0
0 0 0 6 âˆ’1

is the 1 in the third row. This yields
ï£¶
0 0
0
1 1
0 1
0 âˆ’2 2 ï£·
ï£·
0 âˆ’1 0
0 1 ï£·
ï£·.
0 âˆ’1 âˆ’1 3 1 ï£¸
1 3
0 âˆ’7 7

There are still 5 columns which consist entirely of zeros except for one entry. Four of them
have that entry equal to 1 but one still has a -1 in it, the -1 being in the fourth column.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

6.3. THE SIMPLEX ALGORITHM

147

I need to do the row operations on a nonsimple column which has the pivot in the fourth
row. Such a column is the second to the last. The pivot is the 3. The new matrix is
ï£¶
ï£«
1
7
1
0 1 0
âˆ’1 0
0 23
3
3
3
1
1
ï£¬ 1 0 0
0 0
âˆ’ 23 0 83 ï£·
3
3
ï£·
ï£¬
ï£¬ 0 0 1 âˆ’2 1 0 âˆ’1
0 0 1 ï£·
(6.15)
ï£¬
ï£·.
1
1
1 ï£¸
ï£­ 0 0 0 âˆ’1
0
0
âˆ’
âˆ’
1
3
3
3
3
2
âˆ’1 1
âˆ’ 73 0 28
0 0 0 11
3
3
3
Now the obvious basic solution is feasible. You let x4 = 0 = x5 = x7 = x8 and x1 =
8/3, x2 = 2/3, x3 = 1, and x6 = 28/3. You donâ€™t need to worry too much about this. It is
the above matrix which is desired. Now you can assemble the simplex tableau and begin
the algorithm. Remember C â‰¡ 2x1 + 3x2 + 2x3 + 3x4 . First add the row and column which
deal with C. This yields
ï£«
ï£¶
1
7
1
0
1
0
âˆ’1 0
0 0 23
3
3
3
1
1
ï£¬ 1
0
0
0 0
âˆ’ 23 0 0 83 ï£·
3
3
ï£¬
ï£·
ï£¬ 0
0
1 âˆ’2 1 0 âˆ’1
0 0 0 1 ï£·
ï£·
ï£¬
(6.16)
ï£¬ 0
0
0 âˆ’ 31
0 0 âˆ’ 13 âˆ’ 13 1 0 13 ï£·
ï£¬
ï£·
2
7
28
11
ï£­ 0
0
0
âˆ’1 1
âˆ’3 0 0 3 ï£¸
3
3
âˆ’2 âˆ’3 âˆ’2 âˆ’3 0 0 0
0 0 1 0
Now you do row operations to keep the simple columns of (6.15) simple in (6.16). Of course
you could permute the columns if you wanted but this is not necessary.
This yields the following for a simplex tableau. Now it is a matter of getting rid of the
positive entries in the bottom row because you are trying to minimize.
ï£«
ï£¶
1
1
7
âˆ’1 0
0 0 23
0 1 0
3
3
3
1
1
ï£¬ 1 0 0
0 0
âˆ’ 23 0 0 83 ï£·
3
3
ï£¬
ï£·
ï£¬ 0 0 1 âˆ’2 1 0 âˆ’1
0 0 0 1 ï£·
ï£¬
ï£·
ï£¬ 0 0 0 âˆ’1
0 0 âˆ’ 13 âˆ’ 13 1 0 13 ï£·
3
ï£¬
ï£·
2
ï£­ 0 0 0 11 âˆ’1 1
ï£¸
âˆ’ 73 0 0 28
3
3
3
2
1
1
28
0 0 0
âˆ’1 0 âˆ’ 3 âˆ’ 3 0 1 3
3
The most positive of them is the 2/3 and so I will apply the algorithm to this one ï¬rst. The
pivot is the 7/3. After doing the row operation the next tableau is
ï£«
ï£¶
3
1
1
0
0 1 âˆ’ 37 0
0 0 27
7
7
7
1
2
ï£¬ 1 âˆ’1 0 0
ï£·
0
âˆ’ 75 0 0 18
7
7
7
7 ï£·
ï£¬
6
1
5
2
11 ï£·
ï£¬ 0
1
0
0
âˆ’
0
0
7
7
7
7
7 ï£·
ï£¬
1
ï£¬ 0
0 0 âˆ’ 17 0 âˆ’ 27 âˆ’ 27 1 0 37 ï£·
7
ï£¬
ï£·
4
1
ï£­ 0 âˆ’ 11 0 0
ï£¸
1
âˆ’ 20
0 0 58
7
7
7
7
7
0 âˆ’ 27 0 0 âˆ’ 57 0 âˆ’ 37 âˆ’ 37 0 1 64
7
and you see that all the entries are negative and so the minimum is 64/7 and it occurs when
x1 = 18/7, x2 = 0, x3 = 11/7, x4 = 2/7.
There is no maximum for the above problem. However, I will pretend I donâ€™t know this
and attempt to use the simplex algorithm. You set up the simiplex tableau the same way.
Recall it is
ï£¶
ï£«
1
1
7
âˆ’1 0
0 0 23
0 1 0
3
3
3
1
1
ï£¬ 1 0 0
0 0
âˆ’ 23 0 0 83 ï£·
3
3
ï£·
ï£¬
ï£¬ 0 0 1 âˆ’2 1 0 âˆ’1
0 0 0 1 ï£·
ï£·
ï£¬
ï£¬ 0 0 0 âˆ’1
0 0 âˆ’ 13 âˆ’ 13 1 0 13 ï£·
3
ï£·
ï£¬
2
ï£¸
ï£­ 0 0 0 11 âˆ’1 1
âˆ’ 73 0 0 28
3
3
3
2
1
1
28
0 0 0
âˆ’1
0
âˆ’
âˆ’
0
1
3
3
3
3

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

148

LINEAR PROGRAMMING

Now to maximize, you try to get rid of the negative entries in the bottom left row. The
most negative entry is the -1 in the ï¬fth column. The pivot is the 1 in the third row of this
column. The new tableau is
ï£«
ï£¶
1
1
0 1 1
0 0 âˆ’ 23
0 0 35
3
3
1
1
ï£¬ 1 0 0
0 0
âˆ’ 23 0 0 83 ï£·
3
3
ï£¬
ï£·
ï£¬ 0 0 1 âˆ’2 1 0 âˆ’1
0 0 0 1 ï£·
ï£¬
ï£·
ï£¬ 0 0 0 âˆ’1 0 0 âˆ’1 âˆ’1 1 0 1 ï£· .
3
3
3
3 ï£·
ï£¬
5
ï£­ 0 0 1
ï£¸
0 1 âˆ’ 13 âˆ’ 73 0 0 31
3
3
4
0 0 1 âˆ’ 3 0 0 âˆ’ 43 âˆ’ 13 0 1 31
3
Consider the fourth column. The pivot
ï£«
0 3
3 1
ï£¬ 1 âˆ’1 âˆ’1 0
ï£¬
ï£¬ 0 6
7 0
ï£¬
ï£¬ 0 1
1 0
ï£¬
ï£­ 0 âˆ’5 âˆ’4 0
0 4
5 0

is the top 1/3. The new tableau is
ï£¶
0 0 âˆ’2 1 0 0 5
0 0 1 âˆ’1 0 0 1 ï£·
ï£·
1 0 âˆ’5 2 0 0 11 ï£·
ï£·
0 0 âˆ’1 0 1 0 2 ï£·
ï£·
0 1 3 âˆ’4 0 0 2 ï£¸
0 0 âˆ’4 1 0 1 17

There is still a negative
algorithm yields
ï£«
0
ï£¬ 1
ï£¬
ï£¬ 0
ï£¬
ï£¬ 0
ï£¬
ï£­ 0
0

0
0
1
0
0
0

in the bottom, the -4. The pivot in that column is the 3. The
âˆ’ 13
2
3
âˆ’ 73
âˆ’ 23
âˆ’ 53
âˆ’ 83

1
3
1
3
1
3
âˆ’ 13
âˆ’ 43
âˆ’ 13

1
0
0
0
0
0

2
3
âˆ’ 31
5
3
1
3
1
3
4
3

0
0
0
0
1
0

âˆ’ 53

1
3
âˆ’ 14
3
âˆ’ 43
âˆ’ 43
âˆ’ 13
3

Note how z keeps getting larger. Consider the column
the single positive entry, 1/3. The next tableau is
ï£«
5 3 2 1 0 âˆ’1 0 0
ï£¬ 3 2 1 0 0 âˆ’1 0 1
ï£¬
ï£¬ 14 7 5 0 1 âˆ’3 0 0
ï£¬
ï£¬ 4 2 1 0 0 âˆ’1 0 0
ï£¬
ï£­ 4 1 0 0 0 âˆ’1 1 0
13 6 4 0 0 âˆ’3 0 0

0 0
0 0
0 0
1 0
0 0
0 1

19
3
1
3
43
3
8
3
2
3
59
3

ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

having the âˆ’13/3 in it. The pivot is
0
0
0
1
0
0

ï£¶
0 8
0 1 ï£·
ï£·
0 19 ï£·
ï£·.
0 4 ï£·
ï£·
0 2 ï£¸
1 24

There is a column consisting of all negative entries. There is therefore, no maximum. Note
also how there is no way to pick the pivot in that column.
Example 6.3.4 Minimize z = x1 âˆ’ 3x2 + x3 subject to the constraints x1 + x2 + x3 â‰¤
10, x1 + x2 + x3 â‰¥ 2, x1 + x2 + 3x3 â‰¤ 8 and x1 + 2x2 + x3 â‰¤ 7 with all variables nonnegative.
There exists an answer because the region deï¬ned by the constraints is closed and
bounded. Adding in slack variables you get the following augmented matrix corresponding
to the constraints.
ï£«
ï£¶
1 1 1 1 0 0 0 10
ï£¬ 1 1 1 0 âˆ’1 0 0 2 ï£·
ï£¬
ï£·
ï£­ 1 1 3 0 0 1 0 8 ï£¸
1 2 1 0 0 0 1 7

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

6.3. THE SIMPLEX ALGORITHM

149

Of course there is a problem with the obvious solution obtained by setting to zero all
variables corresponding to a nonsimple column because of the simple column which has the
âˆ’1 in it. Therefore, I will use the simplex algorithm to make this column non simple. The
third column has the 1 in the second row as the pivot so I will use this column. This yields
ï£«
ï£¶
0
0 0 1 1 0 0 8
ï£¬ 1
1 1 0 âˆ’1 0 0 2 ï£·
ï£¬
ï£·
(6.17)
ï£­ âˆ’2 âˆ’2 0 0 3 1 0 2 ï£¸
0
1 0 0 1 0 1 5
and the obvious solution is feasible. Now it is time to assemble the simplex tableau. First
add in the bottom row and second to last column corresponding to the equation for z. This
yields
ï£¶
ï£«
0
0
0 1 1 0 0 0 8
ï£¬ 1
1
1 0 âˆ’1 0 0 0 2 ï£·
ï£·
ï£¬
ï£¬ âˆ’2 âˆ’2 0 0 3 1 0 0 2 ï£·
ï£·
ï£¬
ï£­ 0
1
0 0 1 0 1 0 5 ï£¸
âˆ’1 3 âˆ’1 0 0 0 0 1 0
Next you need to zero out the entries in the bottom
columns in (6.17). This yields the simplex tableau
ï£«
0
0 0 1 1 0
ï£¬ 1
1 1 0 âˆ’1 0
ï£¬
ï£¬ âˆ’2 âˆ’2 0 0 3 1
ï£¬
ï£­ 0
1 0 0 1 0
0
4 0 0 âˆ’1 0

row which are below one of the simple
ï£¶
0 0 8
0 0 2 ï£·
ï£·
0 0 2 ï£·
ï£·.
1 0 5 ï£¸
0 1 2

The desire is to minimize this so you need to get rid of the positive entries in the left bottom
row. There is only one such entry, the 4. In that column the pivot is the 1 in the second
row of this column. Thus the next tableau is
ï£¶
ï£«
0 0 0 1 1 0 0 0 8
ï£¬ 1 1 1 0 âˆ’1 0 0 0 2 ï£·
ï£¬
ï£·
ï£¬ 0 0 2 0 1 1 0 0 6 ï£·
ï£¬
ï£·
ï£­ âˆ’1 0 âˆ’1 0 2 0 1 0 3 ï£¸
âˆ’4 0 âˆ’4 0 3 0 0 1 âˆ’6
There is still a positive number there, the 3. The
algorithm again. This yields
ï£« 1
1
0
1 0 0
2
2
1
1
ï£¬
1
0 0 0
2
ï£¬ 12
5
ï£¬
0
0 0 1
2
ï£¬ 21
1
ï£­ âˆ’
0
âˆ’
0 1 0
2
2
âˆ’ 52 0 âˆ’ 52 0 0 0

pivot in this column is the 2. Apply the
âˆ’ 12
1
2
âˆ’ 12
1
2
âˆ’ 32

0
0
0
0
1

13
2
7
2
9
2
3
2
âˆ’ 21
2

ï£¶
ï£·
ï£·
ï£·.
ï£·
ï£¸

Now all the entries in the left bottom row are nonpositive so the process has stopped. The
minimum is âˆ’21/2. It occurs when x1 = 0, x2 = 7/2, x3 = 0.
Now consider the same problem but change the word, minimize to the word, maximize.
Example 6.3.5 Maximize z = x1 âˆ’ 3x2 + x3 subject to the constraints x1 + x2 + x3 â‰¤
10, x1 + x2 + x3 â‰¥ 2, x1 + x2 + 3x3 â‰¤ 8 and x1 + 2x2 + x3 â‰¤ 7 with all variables nonnegative.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

150

LINEAR PROGRAMMING

The ï¬rst part of it is the same. You wind up with
ï£«
0
0 0 1 1 0
ï£¬ 1
1 1 0 âˆ’1 0
ï£¬
ï£¬ âˆ’2 âˆ’2 0 0 3 1
ï£¬
ï£­ 0
1 0 0 1 0
0
4 0 0 âˆ’1 0

the same simplex tableau,
ï£¶
0 0 8
0 0 2 ï£·
ï£·
0 0 2 ï£·
ï£·
1 0 5 ï£¸
0 1 2

but this time, you apply the algorithm to get rid of the negative entries in the left bottom
row. There is a âˆ’1. Use this column. The pivot is the 3. The next tableau is
ï£« 2
ï£¶
2
0 1 0 âˆ’ 13 0 0 22
3
3
3
1
1
ï£¬ 1
1 0 0
0 0 83 ï£·
3
3
ï£¬ 32
ï£·
1
2
ï£¬ âˆ’
0 0 23 ï£·
3
ï£¬ 23 âˆ’53 0 0 1
ï£·
ï£­
ï£¸
0 0 0 âˆ’ 13 1 0 13
3
3
3
2
10
1
8
âˆ’3
0 0 0
0 1 3
3
3
There is still a negative entry, the âˆ’2/3.
the 2/3 on the fourth row. This yields
ï£«
0 âˆ’1 0 1
ï£¬ 0 âˆ’1 1 0
2
ï£¬
ï£¬ 0 1 0 0
ï£¬
5
ï£­ 1
0 0
2
0 5 0 0

This will be the new pivot column. The pivot is
0 0
1
0
2
1 0
0 âˆ’ 12
0 0

âˆ’1
âˆ’ 12
1
3
2

1

0
0
0
0
1

3

ï£¶

ï£·
ï£·
5 ï£·
ï£·
13 ï£¸
2
7
1
2

and the process stops. The maximum for z is 7 and it occurs when x1 = 13/2, x2 = 0, x3 =
1/2.

6.4

Finding A Basic Feasible Solution

By now it should be fairly clear that ï¬nding a basic feasible solution can create considerable
diï¬ƒculty. Indeed, given a system of linear inequalities along with the requirement that each
variable be nonnegative, do there even exist points satisfying all these inequalities? If you
have many variables, you canâ€™t answer this by drawing a picture. Is there some other way
to do this which is more systematic than what was presented above? The answer is yes. It
is called the method of artiï¬cial variables. I will illustrate this method with an example.
Example 6.4.1 Find a basic feasible solution to the system 2x1 +x2 âˆ’x3 â‰¥ 3, x1 +x2 +x3 â‰¥
2, x1 + x2 + x3 â‰¤ 7 and x â‰¥ 0.
If you write the appropriate augmented
ï£«
2 1 âˆ’1
ï£­ 1 1 1
1 1 1

matrix with the slack variables,
ï£¶
âˆ’1 0 0 3
0 âˆ’1 0 2 ï£¸
0
0 1 7

(6.18)

The obvious solution is not feasible. This is why it would be hard to get started with
the simplex method. What is the problem? It is those âˆ’1 entries in the fourth and ï¬fth
columns. To get around this, you add in artiï¬cial variables to get an augmented matrix of
the form
ï£«
ï£¶
2 1 âˆ’1 âˆ’1 0 0 1 0 3
ï£­ 1 1 1
0 âˆ’1 0 0 1 2 ï£¸
(6.19)
1 1 1
0
0 1 0 0 7

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

6.4. FINDING A BASIC FEASIBLE SOLUTION

151

Thus the variables are x1 , x2 , x3 , x4 , x5 , x6 , x7 , x8 . Suppose you can ï¬nd a feasible solution
to the system of equations represented by the above augmented matrix. Thus all variables
are nonnegative. Suppose also that it can be done in such a way that x8 and x7 happen
to be 0. Then it will follow that x1 , Â· Â· Â· , x6 is a feasible solution for (6.18). Conversely, if
you can ï¬nd a feasible solution for (6.18), then letting x7 and x8 both equal zero, you have
obtained a feasible solution to (6.19). Since all variables are nonnegative, x7 and x8 both
equalling zero is equivalent to saying the minimum of z = x7 + x8 subject to the constraints
represented by the above augmented matrix equals zero. This has proved the following
simple observation.
Observation 6.4.2 There exists a feasible solution to the constraints represented by the
augmented matrix of (6.18) and x â‰¥ 0 if and only if the minimum of x7 + x8 subject to the
constraints of (6.19) and x â‰¥ 0 exists and equals 0.
Of course a similar observation would hold in other similar situations. Now the point of
all this is that it is trivial to see a feasible solution to (6.19), namely x6 = 7, x7 = 3, x8 = 2
and all the other variables may be set to equal zero. Therefore, it is easy to ï¬nd an initial
simplex tableau for the minimization problem just described. First add the column and row
for z
ï£¶
ï£«
2 1 âˆ’1 âˆ’1 0 0 1
0 0 3
ï£¬ 1 1 1
0 âˆ’1 0 0
1 0 2 ï£·
ï£·
ï£¬
ï£­ 1 1 1
0
0 1 0
0 0 7 ï£¸
0 0 0
0
0 0 âˆ’1 âˆ’1 1 0
Next it is necessary to make the last two columns on the bottom left row into simple columns.
Performing the row operation, this yields an initial simplex tableau,
ï£¶
ï£«
2 1 âˆ’1 âˆ’1 0 0 1 0 0 3
ï£¬ 1 1 1
0 âˆ’1 0 0 1 0 2 ï£·
ï£¬
ï£·
ï£­ 1 1 1
0
0 1 0 0 0 7 ï£¸
3 2 0 âˆ’1 âˆ’1 0 0 0 1 5
Now the algorithm involves getting rid of the positive entries on the left bottom row. Begin
with the ï¬rst column. The pivot is the 2. An application of the simplex algorithm yields
the new tableau
ï£«
ï£¶
1
1 12 âˆ’ 12 âˆ’ 12
0 0
0 0 32
2
3
1
ï£¬ 0 1
âˆ’1 0 âˆ’ 12 1 0 12 ï£·
2
2
2
ï£¬
ï£·
3
1
ï£­ 0 1
ï£¸
0 1 âˆ’ 12 0 0 11
2
2
2
2
1
3
1
3
1
0 2
âˆ’1 0 âˆ’ 2 0 1 2
2
2
Now go to the third column. The pivot is the
simplex algorithm yields
ï£«
1 23 0 âˆ’ 13 âˆ’ 13
1
ï£¬ 0 1 1
âˆ’ 23
3
3
ï£¬
ï£­ 0 0 0 0
1
0 0 0 0
0

3/2 in the second row. An application of the
0
0
1
0

1
3
âˆ’ 13

0
âˆ’1

1
3
2
3

0
0
âˆ’1 0
âˆ’1 1

5
3
1
3

ï£¶

ï£·
ï£·
5 ï£¸
0

(6.20)

and you see there are only nonpositive numbers on the bottom left column so the process
stops and yields 0 for the minimum of z = x7 +x8 . As for the other variables, x1 = 5/3, x2 =
0, x3 = 1/3, x4 = 0, x5 = 0, x6 = 5. Now as explained in the above observation, this is a
basic feasible solution for the original system (6.18).
Now consider a maximization problem associated with the above constraints.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

152

LINEAR PROGRAMMING

Example 6.4.3 Maximize x1 âˆ’ x2 + 2x3 subject to the constraints, 2x1 + x2 âˆ’ x3 â‰¥ 3, x1 +
x2 + x3 â‰¥ 2, x1 + x2 + x3 â‰¤ 7 and x â‰¥ 0.
From (6.20) you can immediately assemble an initial simplex tableau. You begin with
the ï¬rst 6 columns and top 3 rows in (6.20). Then add in the column and row for z. This
yields
ï£«
ï£¶
2
0 âˆ’ 13 âˆ’ 13 0 0 53
1
3
1
1
ï£¬ 0
1
âˆ’ 23 0 0 13 ï£·
3
3
ï£¬
ï£·
ï£­ 0 0 0
0
1 1 0 5 ï£¸
âˆ’1 1 âˆ’2 0
0 0 1 0
and you ï¬rst do row operations to make the ï¬rst and
the next simplex tableau is
ï£«
1 23 0 âˆ’ 13 âˆ’ 31 0
1
ï£¬ 0 1 1
âˆ’ 23 0
3
3
ï£¬
ï£­ 0 0 0 0
1 1
1
5
âˆ’
0
0 73 0
3
3

third columns simple columns. Thus
0
0
0
1

5
3
1
3

ï£¶

ï£·
ï£·
5 ï£¸
7
3

You are trying to get rid of negative entries in the bottom left row. There is only one, the
âˆ’5/3. The pivot is the 1. The next simplex tableau is then
ï£«
ï£¶
1 23 0 âˆ’ 13 0 13 0 10
3
1
ï£¬ 0 1 1
ï£·
0 23 0 11
3
3
3 ï£·
ï£¬
ï£­ 0 0 0 0 1 1 0 5 ï£¸
1
0 73 0
0 53 1 32
3
3
and so the maximum value of z is 32/3 and it occurs when x1 = 10/3, x2 = 0 and x3 = 11/3.

6.5

Duality

You can solve minimization problems by solving maximization problems. You can also go
the other direction and solve maximization problems by minimization problems. Sometimes
this makes things much easier. To be more speciï¬c, the two problems to be considered are
A.) Minimize z = cx subject to x â‰¥ 0 and Ax â‰¥ b and
B.) Maximize w = yb such that y â‰¥ 0 and yA â‰¤ c,
(
)
equivalently AT yT â‰¥ cT and w = bT yT .
In these problems it is assumed A is an m Ã— p matrix.
I will show how a solution of the ï¬rst yields a solution of the second and then show how
a solution of the second yields a solution of the ï¬rst. The problems, A.) and B.) are called
dual problems.
Lemma 6.5.1 Let x be a solution of the inequalities of A.) and let y be a solution of the
inequalities of B.). Then
cx â‰¥ yb.
and if equality holds in the above, then x is the solution to A.) and y is a solution to B.).
Proof: This follows immediately. Since c â‰¥ yA, cx â‰¥ yAx â‰¥ yb.
It follows from this lemma that if y satisï¬es the inequalities of B.) and x satisï¬es the
inequalities of A.) then if equality holds in the above lemma, it must be that x is a solution
of A.) and y is a solution of B.). 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

6.5. DUALITY

153

Now recall that to solve either of these problems using the simplex method, you ï¬rst
add in slack variables. Denote by xâ€² and yâ€² the enlarged list of variables. Thus xâ€² has at
least m entries and so does yâ€² and the inequalities involving A were replaced by equalities
whose augmented matrices were of the form
(
)
(
)
A âˆ’I b , and AT I cT
Then you included the row and column for z and w to obtain
(
)
(
)
A âˆ’I 0 b
AT
I 0 cT
and
.
âˆ’c 0 1 0
âˆ’bT 0 1 0

(6.21)

Then the problems have basic feasible solutions if it is possible to permute the ï¬rst p + m
columns in the above two matrices and obtain matrices of the form
(
)
(
)
B1
F1
0 cT
B
F
0 b
and
(6.22)
âˆ’bTB1 âˆ’bTF1 1 0
âˆ’cB âˆ’cF 1 0
where B, B1 are invertible m Ã— m and p Ã— p matrices and denoting the variables associated
with these columns by xB , yB and those variables associated with F or F1 by xF and yF ,
it follows
that )letting BxB = b and xF = 0, the resulting vector, xâ€² is a solution to xâ€² â‰¥ 0
(
and A âˆ’I xâ€² = b with similar constraints holding for yâ€² . In other words, it is possible
to obtain simplex tableaus,
)
(
) (
I
B1âˆ’1 F1
0
B1âˆ’1 cT
I
B âˆ’1 F
0
B âˆ’1 b
(6.23)
,
0 cB B âˆ’1 F âˆ’ cF 1 cB B âˆ’1 b
0 bTB1 B1âˆ’1 F âˆ’ bTF1 1 bTB1 B1âˆ’1 cT
Similar considerations apply to the second problem. Thus as just described, a basic feasible
solution is one which determines a simplex tableau like the above in which you get a feasible
solution by setting all but the ï¬rst m variables equal to zero. The simplex algorithm takes
you from one basic feasible solution to another till eventually, if there is no degeneracy, you
obtain a basic feasible solution which yields the solution of the problem of interest.
Theorem 6.5.2 Suppose there exists a solution x to A.) where x is a basic feasible solution
of the inequalities of A.). Then there exists a solution y to B.) and cx = by. It is also
possible to ï¬nd y from x using a simple formula.
Proof: Since the solution to A.) is basic and feasible, there exists a simplex tableau
like (6.23) such that xâ€² can be split into xB and xF such that xF = 0 and xB = B âˆ’1 b.
Now since it is a minimizer, it follows cB B âˆ’1 F âˆ’ cF â‰¤ 0 and the minimum value for cx is
cB B âˆ’1 b. Stating this again, cx = cB B âˆ’1 b. Is it possible you can take y = cB B âˆ’1 ? From
âˆ’1
Lemma 6.5.1 this will be so if cB B âˆ’1 solves the constraints of problem B.). Is
( cB B â‰¥) 0?
âˆ’1
âˆ’1
A âˆ’I
Is
â‰¤
( cB B ) A â‰¤ c? These two conditions are satisï¬ed if and only if cB B
c 0 . Referring to the process of permuting the columns of the ï¬rst augmented
matrix
(
)
of (6.21)
to )get (6.22) and doing the same permutations on the( columns) of ( A âˆ’I )
(
âˆ’1
and c 0 , the desired inequality
holds if and
(
) only
( if cB B ) B F â‰¤ cB cF
âˆ’1
which is equivalent to saying cB cB B F
â‰¤ cB cF
and this is true because
cB B âˆ’1 F âˆ’ cF â‰¤ 0 due to the assumption that x is a minimizer. The simple formula is just
y = cB B âˆ’1 . 
The proof of the following corollary is similar.
Corollary 6.5.3 Suppose there exists a solution, y to B.) where y is a basic feasible solution
of the inequalities of B.). Then there exists a solution, x to A.) and cx = by. It is also
possible to ï¬nd x from y using a simple formula. In this case, and referring to (6.23), the
simple formula is x = B1âˆ’T bB1 .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

154

LINEAR PROGRAMMING

As an example, consider the pig farmers problem. The main diï¬ƒculty in this problem
was ï¬nding an initial simplex tableau. Now consider the following example and marvel at
how all the diï¬ƒculties disappear.
Example 6.5.4 minimize C â‰¡ 2x1 + 3x2 + 2x3 + 3x4 subject to the constraints
x1 + 2x2 + x3 + 3x4
5x1 + 3x2 + 2x3 + x4

â‰¥
â‰¥

5,
8,

x1 + 2x2 + 2x3 + x4
2x1 + x2 + x3 + x4
x1 + x2 + x3 + x4

â‰¥
â‰¥
â‰¥

6,
7,
4.

where each xi â‰¥ 0.
Here the dual problem is to maximize w = 5y1 + 8y2 + 6y3 + 7y4 + 4y5 subject to the
constraints
ï£«
ï£¶
ï£« ï£¶
ï£«
ï£¶
y1
2
1 5 1 2 1
ï£¬ y2 ï£·
ï£¬ 2 3 2 1 1 ï£·ï£¬
ï£· ï£¬ 3 ï£·
ï£· ï£¬ ï£·
ï£¬
ï£·ï£¬
ï£­ 1 2 2 1 1 ï£¸ ï£¬ y3 ï£· â‰¤ ï£­ 2 ï£¸ .
ï£­ y4 ï£¸
3
3 1 1 1 1
y5
Adding in slack variables, these inequalities are equivalent to the system of equations whose
augmented matrix is
ï£«
ï£¶
1 5 1 2 1 1 0 0 0 2
ï£¬ 2 3 2 1 1 0 1 0 0 3 ï£·
ï£¬
ï£·
ï£­ 1 2 2 1 1 0 0 1 0 2 ï£¸
3 1 1 1 1 0 0 0 1 3
Now the obvious solution is feasible so there is no hunting for an initial obvious feasible
solution required. Now add in the row and column for w. This yields
ï£«
ï£¶
1
5
1
2
1 1 0 0 0 0 2
ï£¬ 2
3
2
1
1 0 1 0 0 0 3 ï£·
ï£¬
ï£·
ï£¬ 1
2
2
1
1 0 0 1 0 0 2 ï£·
ï£¬
ï£·.
ï£­ 3
1
1
1
1 0 0 0 1 0 3 ï£¸
âˆ’5 âˆ’8 âˆ’6 âˆ’7 âˆ’4 0 0 0 0 1 0
It is a maximization problem so you want to eliminate the negatives in the bottom left row.
Pick the column having the one which is most negative, the âˆ’8. The pivot is the top 5.
Then apply the simplex algorithm to obtain
ï£« 1
ï£¶
1
2
1
1
1
0 0 0 0 52
5
5
5
5
5
7
2
ï£¬ 7
0
âˆ’ 15
âˆ’ 35 1 0 0 0 59 ï£·
5
5
ï£·
ï£¬ 35
8
1
3
ï£¬
0
âˆ’ 25 0 1 0 0 56 ï£·
5
5
5
5
ï£·.
ï£¬ 14
4
3
4
1
13 ï£¸
ï£­
0
âˆ’
0
0
1
0
5
5
5
5
5
5
8
âˆ’ 17
0 âˆ’ 22
âˆ’ 19
âˆ’ 12
0 0 0 1 16
5
5
5
5
5
5
There are still negative entries in the bottom left row. Do the simplex algorithm to the
8
column which has the âˆ’ 22
5 . The pivot is the 5 . This yields
ï£« 1
ï£¶
3
1
1
1 0
0 âˆ’ 18 0 0 14
8
8
8
4
ï£¬ 7
0 0 âˆ’ 38 âˆ’ 18 âˆ’ 14 1 âˆ’ 78 0 0 34 ï£·
ï£¬ 83
ï£·
1
3
5
ï£¬
0 1
âˆ’ 14 0
0 0 34 ï£·
8
8
8
ï£·
ï£¬ 85
1
1
ï£­
0 0
0 0 âˆ’ 12 1 0 2 ï£¸
2
2
2
1
âˆ’ 74 0 0 âˆ’ 13
âˆ’ 34
0 11
0 1 13
4
2
4
2

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

6.5. DUALITY

155

and there are still negative numbers. Pick the column which
the 3/8 in the top. This yields
ï£« 1
8
2
0 1 13
0 âˆ’ 13 0
3
3
3
ï£¬ 1
1 0 0 0
0 1 âˆ’1 0
ï£¬ 1
1
1
1
2
ï£¬
âˆ’
1
0
âˆ’
0
0
3
3
3
3
ï£¬ 37
4
1
1
1
ï£­
âˆ’
0
0
âˆ’
0
âˆ’
1
3
3
3
3
3
8
5
âˆ’ 23 26
0 0 13
0
0
3
3
3

has the âˆ’13/4. The pivot is
0
0
0
0
1

2
3

ï£¶

1 ï£·
ï£·
2 ï£·
3 ï£·
5 ï£¸

3
26
3

which has only one negative entry on the bottom left. The pivot for this ï¬rst column is the
7
3 . The next tableau is
ï£«
ï£¶
5
2
0 20
0 1
0 âˆ’ 72 âˆ’ 17 0 37
7
7
7
1
ï£¬ 0 11 0 0 âˆ’ 1
1 âˆ’ 76 âˆ’ 37 0 27 ï£·
7
7
7
ï£¬
ï£·
2
2
5
ï£¬ 0 âˆ’1 1 0
âˆ’7 0
âˆ’ 17 0 37 ï£·
7
7
7
ï£¬
ï£·
1
3
ï£­ 1 âˆ’4 0 0
âˆ’ 17 0 âˆ’ 17
0 57 ï£¸
7
7
7
3
18
2
0 58
0 0
0 11
1 64
7
7
7
7
7
7
and all the entries in the left bottom row are nonnegative so the answer is 64/7. This is
the same as obtained before. So what values for x are needed? Here the basic variables are
y1 , y3 , y4 , y7 . Consider the original augmented matrix, one step before the simplex tableau.
ï£«
ï£¶
1
5
1
2
1 1 0 0 0 0 2
ï£¬ 2
3
2
1
1 0 1 0 0 0 3 ï£·
ï£¬
ï£·
ï£¬ 1
2
2
1
1 0 0 1 0 0 2 ï£·
ï£¬
ï£·.
ï£­ 3
1
1
1
1 0 0 0 1 0 3 ï£¸
âˆ’5 âˆ’8 âˆ’6 âˆ’7 âˆ’4 0 0 0 0 1 0
Permute the columns to put the columns associated
ï£«
1
1
2 0 5
1
ï£¬ 2
2
1
1
3
1
ï£¬
ï£¬ 1
2
1
0
2
1
ï£¬
ï£­ 3
1
1 0 1
1
âˆ’5 âˆ’6 âˆ’7 0 âˆ’8 âˆ’4
ï£«

The matrix B is

1
ï£¬ 2
ï£¬
ï£­ 1
3
and so B âˆ’T equals

Also bTB =

(

5

ï£«

âˆ’ 17
ï£¬ 0
ï£¬ 1
ï£­ âˆ’
7

6 7

0

)

ï£«
ï£¬
x=ï£¬
ï£­

3
7

1
2
2
1
âˆ’ 27
0
5
7
âˆ’ 17

with these basic variables ï¬rst. Thus
ï£¶
1 0 0 0 2
0 0 0 0 3 ï£·
ï£·
0 1 0 0 2 ï£·
ï£·
0 0 1 0 3 ï£¸
0 0 0 1 0

ï£¶
0
1 ï£·
ï£·
0 ï£¸
0

2
1
1
1
5
7

0
âˆ’ 27
âˆ’ 71

1
7

1 ï£·
ï£·
âˆ’ 67 ï£¸
âˆ’ 37

and so from Corollary 6.5.3,
ï£¶ï£«
5
1
âˆ’ 17 âˆ’ 72
5
7
7
ï£¬ 6
0
0
0
1 ï£·
ï£·ï£¬
5
âˆ’ 17
âˆ’ 27 âˆ’ 67 ï£¸ ï£­ 7
7
3
0
âˆ’ 71 âˆ’ 17 âˆ’ 37
7

Saylor URL: http://www.saylor.org/courses/ma212/

ï£¶

ï£¶

ï£«

18
7

ï£¶

ï£· ï£¬ 0 ï£·
ï£· = ï£¬ 11 ï£·
ï£¸
ï£¸ ï£­
7
2
7

The Saylor Foundation

156

LINEAR PROGRAMMING

which agrees with the original way of doing the problem.
Two good books which give more discussion of linear programming are Strang [25] and
Nobel and Daniels [20]. Also listed in these books are other references which may prove
useful if you are interested in seeing more on these topics. There is a great deal more which
can be said about linear programming.

6.6

Exercises

1. Maximize and minimize z = x1 âˆ’ 2x2 + x3 subject to the constraints x1 + x2 + x3 â‰¤
10, x1 + x2 + x3 â‰¥ 2, and x1 + 2x2 + x3 â‰¤ 7 if possible. All variables are nonnegative.
2. Maximize and minimize the following if possible. All variables are nonnegative.
(a) z = x1 âˆ’ 2x2 subject to the constraints x1 + x2 + x3 â‰¤ 10, x1 + x2 + x3 â‰¥ 1, and
x1 + 2x2 + x3 â‰¤ 7
(b) z = x1 âˆ’ 2x2 âˆ’ 3x3 subject to the constraints x1 + x2 + x3 â‰¤ 8, x1 + x2 + 3x3 â‰¥ 1,
and x1 + x2 + x3 â‰¤ 7
(c) z = 2x1 + x2 subject to the constraints x1 âˆ’ x2 + x3 â‰¤ 10, x1 + x2 + x3 â‰¥ 1, and
x1 + 2x2 + x3 â‰¤ 7
(d) z = x1 + 2x2 subject to the constraints x1 âˆ’ x2 + x3 â‰¤ 10, x1 + x2 + x3 â‰¥ 1, and
x1 + 2x2 + x3 â‰¤ 7
3. Consider contradictory constraints, x1 + x2 â‰¥ 12 and x1 + 2x2 â‰¤ 5, x1 â‰¥ 0, x2 â‰¥ 0.
You know these two contradict but show they contradict using the simplex algorithm.
4. Find a solution to the following inequalities for x, y â‰¥ 0 if it is possible to do so. If it
is not possible, prove it is not possible.
(a)

6x + 3y â‰¥ 4
8x + 4y â‰¤ 5

(b)

6x1 + 4x3 â‰¤ 11
5x1 + 4x2 + 4x3 â‰¥ 8
6x1 + 6x2 + 5x3 â‰¤ 11

(c)

6x1 + 4x3 â‰¤ 11
5x1 + 4x2 + 4x3 â‰¥ 9
6x1 + 6x2 + 5x3 â‰¤ 9

(d)

x1 âˆ’ x2 + x3 â‰¤ 2
x1 + 2x2 â‰¥ 4
3x1 + 2x3 â‰¤ 7

(e)

5x1 âˆ’ 2x2 + 4x3 â‰¤ 1
6x1 âˆ’ 3x2 + 5x3 â‰¥ 2
5x1 âˆ’ 2x2 + 4x3 â‰¤ 5

5. Minimize z = x1 + x2 subject to x1 + x2 â‰¥ 2, x1 + 3x2 â‰¤ 20, x1 + x2 â‰¤ 18. Change
to a maximization problem and solve as follows: Let yi = M âˆ’ xi . Formulate in terms
of y1 , y2 .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Spectral Theory
Spectral Theory refers to the study of eigenvalues and eigenvectors of a matrix. It is of
fundamental importance in many areas. Row operations will no longer be such a useful tool
in this subject.

7.1

Eigenvalues And Eigenvectors Of A Matrix

The ï¬eld of scalars in spectral theory is best taken to equal C although I will sometimes
refer to it as F when it could be either C or R.
Deï¬nition 7.1.1 Let M be an n Ã— n matrix and let x âˆˆ Cn be a nonzero vector for which
M x = Î»x

(7.1)

for some scalar, Î». Then x is called an eigenvector and Î» is called an eigenvalue (characteristic value) of the matrix M.
Eigenvectors are never equal to zero!
The set of all eigenvalues of an n Ã— n matrix M, is denoted by Ïƒ (M ) and is referred to as
the spectrum of M.
Eigenvectors are vectors which are shrunk, stretched or reï¬‚ected upon multiplication by
a matrix. How can they be identiï¬ed? Suppose x satisï¬es (7.1). Then
(Î»I âˆ’ M ) x = 0
for some x Ì¸= 0. Therefore, the matrix M âˆ’ Î»I cannot have an inverse and so by Theorem
3.3.18
det (Î»I âˆ’ M ) = 0.
(7.2)
In other words, Î» must be a zero of the characteristic polynomial. Since M is an nÃ—n matrix,
it follows from the theorem on expanding a matrix by its cofactor that this is a polynomial
equation of degree n. As such, it has a solution, Î» âˆˆ C. Is it actually an eigenvalue? The
answer is yes and this follows from Theorem 3.3.26 on Page 95. Since det (Î»I âˆ’ M ) = 0
the matrix Î»I âˆ’ M cannot be one to one and so there exists a nonzero vector, x such that
(Î»I âˆ’ M ) x = 0. This proves the following corollary.
Corollary 7.1.2 Let M be an nÃ—n matrix and det (M âˆ’ Î»I) = 0. Then there exists x âˆˆ Cn
such that (M âˆ’ Î»I) x = 0.
157

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

158

SPECTRAL THEORY

As an example, consider the following.
Example 7.1.3 Find the eigenvalues and eigenvectors for the matrix
ï£«
ï£¶
5 âˆ’10 âˆ’5
14
2 ï£¸.
A=ï£­ 2
âˆ’4 âˆ’8
6
You ï¬rst need to identify the eigenvalues. Recall this requires the solution of the equation
ï£« ï£«
ï£¶ ï£«
ï£¶ï£¶
1 0 0
5 âˆ’10 âˆ’5
14
2 ï£¸ï£¸ = 0
det ï£­Î» ï£­ 0 1 0 ï£¸ âˆ’ ï£­ 2
0 0 1
âˆ’4 âˆ’8
6
When you expand this determinant, you ï¬nd the equation is
(
)
(Î» âˆ’ 5) Î»2 âˆ’ 20Î» + 100 = 0
and so the eigenvalues are
5, 10, 10.
I have listed 10 twice because it is a zero of multiplicity two due to
2

Î»2 âˆ’ 20Î» + 100 = (Î» âˆ’ 10) .
Having found the eigenvalues, it only remains to ï¬nd the eigenvectors. First ï¬nd the
eigenvectors for Î» = 5. As explained above, this requires you to solve the equation,
ï£« ï£«
ï£¶ ï£«
ï£¶ï£¶ ï£«
ï£¶ ï£« ï£¶
1 0 0
5 âˆ’10 âˆ’5
x
0
ï£­5 ï£­ 0 1 0 ï£¸ âˆ’ ï£­ 2
14
2 ï£¸ï£¸ ï£­ y ï£¸ = ï£­ 0 ï£¸ .
0 0 1
âˆ’4 âˆ’8
6
z
0
That is you need to ï¬nd the solution to
ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
0
10
5
x
0
ï£­ âˆ’2 âˆ’9 âˆ’2 ï£¸ ï£­ y ï£¸ = ï£­ 0 ï£¸
4
8 âˆ’1
z
0
By now this is an old problem. You set up the augmented matrix and row reduce to get the
solution. Thus the matrix you must row reduce is
ï£«
ï£¶
0
10
5 0
ï£­ âˆ’2 âˆ’9 âˆ’2 0 ï£¸ .
(7.3)
4
8 âˆ’1 0
The reduced row echelon form is

ï£«

1 0
ï£­ 0 1
0 0

âˆ’ 54
1
2

0

ï£¶
0
0 ï£¸
0

and so the solution is any vector of the form
ï£« 5 ï£¶
ï£« 5 ï£¶
4z
4
ï£­ âˆ’1 z ï£¸ = z ï£­ âˆ’1 ï£¸
2
2
z
1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.1. EIGENVALUES AND EIGENVECTORS OF A MATRIX

159

where z âˆˆ F. You would obtain the same collection of vectors if you replaced z with 4z.
Thus a simpler description for the solutions to this system of equations whose augmented
matrix is in (7.3) is
ï£«
ï£¶
5
z ï£­ âˆ’2 ï£¸
(7.4)
4
where z âˆˆ F. Now you need to remember that you canâ€™t take z = 0 because this would
result in the zero vector and
Eigenvectors are never equal to zero!
Other than this value, every other choice of z in (7.4) results in an eigenvector. It is a good
idea to check your work! To do so, I will take the original matrix and multiply by this vector
and see if I get 5 times this vector.
ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
ï£«
ï£¶
5 âˆ’10 âˆ’5
5
25
5
ï£­ 2
14
2 ï£¸ ï£­ âˆ’2 ï£¸ = ï£­ âˆ’10 ï£¸ = 5 ï£­ âˆ’2 ï£¸
âˆ’4 âˆ’8
6
4
20
4
so it appears this is correct. Always check your work on these problems if you care about
getting the answer right.
The variable, z is called a free variable or sometimes a parameter. The set of vectors in
(7.4) is called the eigenspace and it equals ker (Î»I âˆ’ A) . You should observe that in this case
the eigenspace has dimension 1 because there is one vector which spans the eigenspace. In
general, you obtain the solution from the row echelon form and the number of diï¬€erent free
variables gives you the dimension of the eigenspace. Just remember that not every vector
in the eigenspace is an eigenvector. The vector, 0 is not an eigenvector although it is in the
eigenspace because
Eigenvectors are never equal to zero!
Next consider the eigenvectors for Î» = 10. These vectors are solutions to the equation,
ï£« ï£«
ï£¶ ï£«
ï£¶ï£¶ ï£«
ï£¶ ï£« ï£¶
1 0 0
5 âˆ’10 âˆ’5
x
0
ï£­10 ï£­ 0 1 0 ï£¸ âˆ’ ï£­ 2
14
2 ï£¸ï£¸ ï£­ y ï£¸ = ï£­ 0 ï£¸
0 0 1
âˆ’4 âˆ’8
6
z
0
That is you must ï¬nd the solutions to
ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
5
10
5
x
0
ï£­ âˆ’2 âˆ’4 âˆ’2 ï£¸ ï£­ y ï£¸ = ï£­ 0 ï£¸
4
8
4
z
0
which reduces to consideration of the augmented matrix
ï£«
ï£¶
5
10
5 0
ï£­ âˆ’2 âˆ’4 âˆ’2 0 ï£¸
4
8
4 0
The row reduced echelon form for this matrix
ï£«
1 2
ï£­ 0 0
0 0

Saylor URL: http://www.saylor.org/courses/ma212/

is
1
0
0

ï£¶
0
0 ï£¸
0

The Saylor Foundation

160

SPECTRAL THEORY

and so the eigenvectors are of the form
ï£«
ï£¶
ï£«
ï£¶
ï£«
ï£¶
âˆ’2y âˆ’ z
âˆ’2
âˆ’1
ï£­
ï£¸ = yï£­ 1 ï£¸ + zï£­ 0 ï£¸.
y
z
0
1
You canâ€™t pick z and y both equal to zero because this would result in the zero vector and
Eigenvectors are never equal to zero!
However, every other choice of z and y does result in an eigenvector for the eigenvalue
Î» = 10. As in the case for Î» = 5 you should check your work if you care about getting it
right.
ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
ï£«
ï£¶
5 âˆ’10 âˆ’5
âˆ’1
âˆ’10
âˆ’1
ï£­ 2
14
2 ï£¸ ï£­ 0 ï£¸ = ï£­ 0 ï£¸ = 10 ï£­ 0 ï£¸
âˆ’4 âˆ’8
6
1
10
1
so it worked. The other vector will also work. Check it.
The above example shows how to ï¬nd eigenvectors and eigenvalues algebraically. You
may have noticed it is a bit long. Sometimes students try to ï¬rst row reduce the matrix
before looking for eigenvalues. This is a terrible idea because row operations destroy the
value of the eigenvalues. The eigenvalue problem is really not about row operations. A
general rule to remember about the eigenvalue problem is this.
If it is not long and hard it is usually wrong!
The eigenvalue problem is the hardest problem in algebra and people still do research on
ways to ï¬nd eigenvalues. Now if you are so fortunate as to ï¬nd the eigenvalues as in the
above example, then ï¬nding the eigenvectors does reduce to row operations and this part
of the problem is easy. However, ï¬nding the eigenvalues is anything but easy because for
an n Ã— n matrix, it involves solving a polynomial equation of degree n and none of us are
very good at doing this. If you only ï¬nd a good approximation to the eigenvalue, it wonâ€™t
work. It either is or is not an eigenvalue and if it is not, the only solution to the equation,
(Î»I âˆ’ M ) x = 0 will be the zero solution as explained above and
Eigenvectors are never equal to zero!
Here is another example.
Example 7.1.4 Let

ï£«

2 2
A=ï£­ 1 3
âˆ’1 1
First ï¬nd the eigenvalues.
ï£« ï£«

1 0
det ï£­Î» ï£­ 0 1
0 0

ï£¶
âˆ’2
âˆ’1 ï£¸
1

ï£¶ ï£«
0
2 2
0 ï£¸âˆ’ï£­ 1 3
1
âˆ’1 1

ï£¶ï£¶
âˆ’2
âˆ’1 ï£¸ï£¸ = 0
1

This is Î»3 âˆ’ 6Î»2 + 8Î» = 0 and the solutions are 0, 2, and 4.
0 Can be an Eigenvalue!

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.1. EIGENVALUES AND EIGENVECTORS OF A MATRIX
Now ï¬nd the eigenvectors. For Î» = 0 the
ï£«
2
ï£­ 1
âˆ’1

161

augmented matrix for ï¬nding the solutions is
ï£¶
2 âˆ’2 0
3 âˆ’1 0 ï£¸
1 1 0

and the row reduced echelon form is
ï£«

1 0
ï£­ 0 1
0 0

ï£¶
âˆ’1 0
0 0 ï£¸
0 0

Therefore, the eigenvectors are of the form
ï£«

ï£¶
1
zï£­ 0 ï£¸
1

where z Ì¸= 0.
Next ï¬nd the eigenvectors for Î» = 2. The augmented matrix for the system of equations
needed to ï¬nd these eigenvectors is
ï£¶
ï£«
0 âˆ’2 2 0
ï£­ âˆ’1 âˆ’1 1 0 ï£¸
1 âˆ’1 1 0
and the row reduced echelon form is
ï£«

1 0
ï£­ 0 1
0 0

and so the eigenvectors are of the form

ï£¶
0 0
âˆ’1 0 ï£¸
0 0

ï£«

ï£¶
0
zï£­ 1 ï£¸
1

where z Ì¸= 0.
Finally ï¬nd the eigenvectors for Î» = 4. The augmented matrix for the system of equations
needed to ï¬nd these eigenvectors is
ï£¶
ï£«
2 âˆ’2 2 0
ï£­ âˆ’1 1 1 0 ï£¸
1 âˆ’1 3 0
and the row reduced echelon form is
ï£«

ï£¶
1 âˆ’1 0 0
ï£­ 0 0 1 0 ï£¸.
0 0 0 0

Therefore, the eigenvectors are of the form
ï£«

ï£¶
1
yï£­ 1 ï£¸
0

where y Ì¸= 0.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

162
Example 7.1.5 Let

SPECTRAL THEORY

ï£«

2
A = ï£­ âˆ’2
14

ï£¶
âˆ’2 âˆ’1
âˆ’1 âˆ’2 ï£¸ .
25 14

Find the eigenvectors and eigenvalues.
In this case the eigenvalues are 3, 6, 6 where I have listed 6 twice because it is a zero of
algebraic multiplicity two, the characteristic equation being
2

(Î» âˆ’ 3) (Î» âˆ’ 6) = 0.
It remains to ï¬nd the eigenvectors for these eigenvalues. First consider the eigenvectors for
Î» = 3. You must solve
ï£« ï£«
ï£¶ ï£«
ï£¶ï£¶ ï£«
ï£¶ ï£« ï£¶
1 0 0
2 âˆ’2 âˆ’1
x
0
ï£­3 ï£­ 0 1 0 ï£¸ âˆ’ ï£­ âˆ’2 âˆ’1 âˆ’2 ï£¸ï£¸ ï£­ y ï£¸ = ï£­ 0 ï£¸ .
0 0 1
14 25 14
z
0
Using routine row operations, the eigenvectors are nonzero vectors of the form
ï£«
ï£¶
ï£«
ï£¶
1
z
ï£­ âˆ’z ï£¸ = z ï£­ âˆ’1 ï£¸
1
z
Next consider the eigenvectors for Î» = 6. This requires you to solve
ï£« ï£«
ï£¶ ï£«
ï£¶ï£¶ ï£«
ï£¶ ï£«
ï£¶
1 0 0
2 âˆ’2 âˆ’1
x
0
ï£­6 ï£­ 0 1 0 ï£¸ âˆ’ ï£­ âˆ’2 âˆ’1 âˆ’2 ï£¸ï£¸ ï£­ y ï£¸ = ï£­ 0 ï£¸
0 0 1
14 25 14
z
0
and using the usual procedures yields the eigenvectors for Î» = 6 are of the form
ï£« 1 ï£¶
âˆ’8
z ï£­ âˆ’ 14 ï£¸
1
or written more simply,

ï£«

ï£¶
âˆ’1
z ï£­ âˆ’2 ï£¸
8

where z âˆˆ F.
Note that in this example the eigenspace for the eigenvalue Î» = 6 is of dimension 1
because there is only one parameter which can be chosen. However, this eigenvalue is of
multiplicity two as a root to the characteristic equation.
Deï¬nition 7.1.6 If A is an n Ã— n matrix with the property that some eigenvalue has algebraic multiplicity as a root of the characteristic equation which is greater than the dimension
of the eigenspace associated with this eigenvalue, then the matrix is called defective.
There may be repeated roots to the characteristic equation, (7.2) and it is not known
whether the dimension of the eigenspace equals the multiplicity of the eigenvalue. However,
the following theorem is available.
Theorem 7.1.7 Suppose M vi = Î»i vi , i = 1, Â· Â· Â· , r , vi Ì¸= 0, and that if i Ì¸= j, then Î»i Ì¸= Î»j .
Then the set of eigenvectors, {v1 , Â· Â· Â· , vr } is linearly independent.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.1. EIGENVALUES AND EIGENVECTORS OF A MATRIX

163

Proof. Suppose the claim of the lemma is not true. Then there exists a subset of this
set of vectors
{w1 , Â· Â· Â· , wr } âŠ† {v1 , Â· Â· Â· , vk }
such that

r
âˆ‘

cj wj = 0

(7.5)

j=1

where each cj Ì¸= 0. Say M wj = Âµj wj where
{Âµ1 , Â· Â· Â· , Âµr } âŠ† {Î»1 , Â· Â· Â· , Î»k } ,
the Âµj being distinct eigenvalues of M . Out of all such subsets, let this one be such that r
is as small as possible. Then necessarily, r > 1 because otherwise, c1 w1 = 0 which would
imply w1 = 0, which is not allowed for eigenvectors.
Now apply M to both sides of (7.5).
r
âˆ‘

cj Âµj wj = 0.

(7.6)

j=1

Next pick Âµk Ì¸= 0 and multiply both sides of (7.5) by Âµk . Such a Âµk exists because r > 1.
Thus
r
âˆ‘
cj Âµk wj = 0
(7.7)
j=1

Subtract the sum in (7.7) from the sum in (7.6) to obtain
r
âˆ‘

(
)
cj Âµk âˆ’ Âµj wj = 0

j=1

(

)
Now one of the constants cj Âµk âˆ’ Âµj equals 0, when j = k. Therefore, r was not as small
as possible after all. 
In words, this theorem says that eigenvectors associated with distinct eigenvalues are
linearly independent.
Sometimes you have to consider eigenvalues which are complex numbers. This occurs in
diï¬€erential equations for example. You do these problems exactly the same way as you do
the ones in which the eigenvalues are real. Here is an example.
Example 7.1.8 Find the eigenvalues and eigenvectors of the matrix
ï£«
ï£¶
1 0 0
A = ï£­ 0 2 âˆ’1 ï£¸ .
0 1 2
You need to ï¬nd the eigenvalues. Solve
ï£« ï£«
ï£¶ ï£«
1 0 0
1
det ï£­Î» ï£­ 0 1 0 ï£¸ âˆ’ ï£­ 0
0 0 1
0

0
2
1

ï£¶ï£¶
0
âˆ’1 ï£¸ï£¸ = 0.
2

(
)
This reduces to (Î» âˆ’ 1) Î»2 âˆ’ 4Î» + 5 = 0. The solutions are Î» = 1, Î» = 2 + i, Î» = 2 âˆ’ i.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

164

SPECTRAL THEORY

There is nothing new about ï¬nding the eigenvectors for Î» = 1 so consider the eigenvalue
Î» = 2 + i. You need to solve
ï£¶ ï£«
ï£¶ï£¶ ï£«
ï£¶ ï£«
ï£¶
ï£«
ï£«
1 0 0
1 0 0
x
0
ï£­(2 + i) ï£­ 0 1 0 ï£¸ âˆ’ ï£­ 0 2 âˆ’1 ï£¸ï£¸ ï£­ y ï£¸ = ï£­ 0 ï£¸
0 0 1
0 1 2
z
0
In other words, you must consider the augmented matrix
ï£«
ï£¶
1+i 0 0 0
ï£­ 0
i 1 0 ï£¸
0
âˆ’1 i 0
for the solution. Divide the top row by (1 + i) and then take âˆ’i times the second row and
add to the bottom. This yields
ï£«
ï£¶
1 0 0 0
ï£­ 0 i 1 0 ï£¸
0 0 0 0
Now multiply the second row by âˆ’i to obtain
ï£«
ï£¶
1 0 0 0
ï£­ 0 1 âˆ’i 0 ï£¸
0 0 0 0
Therefore, the eigenvectors are of the form
ï£«

ï£¶
0
zï£­ i ï£¸.
1

You should ï¬nd the eigenvectors for Î» = 2 âˆ’ i. These are
ï£«
ï£¶
0
z ï£­ âˆ’i ï£¸ .
1
As usual, if you want to
ï£«
1 0
ï£­ 0 2
0 1

get it right you had better check it.
ï£¶ï£«
ï£¶ ï£«
ï£¶
ï£«
ï£¶
0
0
0
0
âˆ’1 ï£¸ ï£­ âˆ’i ï£¸ = ï£­ âˆ’1 âˆ’ 2i ï£¸ = (2 âˆ’ i) ï£­ âˆ’i ï£¸
2
1
2âˆ’i
1

so it worked.

7.2

Some Applications Of Eigenvalues And Eigenvectors

Recall that n Ã— n matrices can be considered as linear transformations. If F is a 3 Ã— 3 real
matrix having positive determinant, it can be shown that F = RU where R is a rotation
matrix and U is a symmetric real matrix having positive eigenvalues. An application of
this wonderful result, known to mathematicians as the right polar decomposition, is to
continuum mechanics where a chunk of material is identiï¬ed with a set of points in three
dimensional space.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.2. SOME APPLICATIONS OF EIGENVALUES AND EIGENVECTORS

165

The linear transformation, F in this context is called the deformation gradient and
it describes the local deformation of the material. Thus it is possible to consider this
deformation in terms of two processes, one which distorts the material and the other which
just rotates it. It is the matrix U which is responsible for stretching and compressing. This
is why in continuum mechanics, the stress is often taken to depend on U which is known in
this context as the right Cauchy Green strain tensor. This process of writing a matrix as a
product of two such matrices, one of which preserves distance and the other which distorts
is also important in applications to geometric measure theory an interesting ï¬eld of study
in mathematics and to the study of quadratic forms which occur in many applications such
as statistics. Here I am emphasizing the application to mechanics in which the eigenvectors
of U determine the principle directions, those directions in which the material is stretched
or compressed to the maximum extent.
Example 7.2.1 Find the principle directions determined by the matrix
ï£« 29 6
ï£¶
6
ï£­

11
6
11
6
11

11
41
44
19
44

11
19
44
41
44

ï£¸

The eigenvalues are 3, 1, and 21 .
It is nice to be given the eigenvalues. The largest eigenvalue is 3 which means that in
the direction determined by the eigenvector associated with 3 the stretch is three times as
large. The smallest eigenvalue is 1/2 and so in the direction determined by the eigenvector
for 1/2 the material is compressed, becoming locally half as long. It remains to ï¬nd these
directions. First consider the eigenvector for 3. It is necessary to solve
ï£« ï£«
ï£¶ ï£« 29 6
ï£¶ï£¶ ï£«
ï£¶ ï£« ï£¶
6
1 0 0
x
0
11
11
11
ï£­3 ï£­ 0 1 0 ï£¸ âˆ’ ï£­ 6 41 19 ï£¸ï£¸ ï£­ y ï£¸ = ï£­ 0 ï£¸
11
44
44
6
19
41
0 0 1
z
0
11
44
44
Thus the augmented matrix for this system of equations is
ï£¶
ï£« 4
6
6
âˆ’ 11
âˆ’ 11
0
11
91
ï£­ âˆ’6
âˆ’ 19
0 ï£¸
11
44
44
6
19
91
âˆ’ 11 âˆ’ 44
0
44
The row reduced echelon form is

ï£«

1 0
ï£­ 0 1
0 0

ï£¶
âˆ’3 0
âˆ’1 0 ï£¸
0 0

and so the principle direction for the eigenvalue 3 in which the material is stretched to the
maximum extent is
ï£« ï£¶
3
ï£­ 1 ï£¸.
1
A direction vector in this direction is
âˆš
ï£¶
3/âˆš11
ï£­ 1/ 11 ï£¸ .
âˆš
1/ 11
ï£«

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

166

SPECTRAL THEORY

You should show that the direction in which the material is compressed the most is in the
direction
ï£«
ï£¶
0âˆš
ï£­ âˆ’1/ 2 ï£¸
âˆš
1/ 2
Note this is meaningful information which you would have a hard time ï¬nding without
the theory of eigenvectors and eigenvalues.
Another application is to the problem of ï¬nding solutions to systems of diï¬€erential
equations. It turns out that vibrating systems involving masses and springs can be studied
in the form
xâ€²â€² = Ax
(7.8)
where A is a real symmetric n Ã— n matrix which has nonpositive eigenvalues. This is
analogous to the case of the scalar equation for undamped oscillation, xâ€²â€² + Ï‰ 2 x = 0. The
main diï¬€erence is that here the scalar Ï‰ 2 is replaced with the matrix âˆ’A. Consider the
problem of ï¬nding solutions to (7.8). You look for a solution which is in the form
x (t) = veÎ»t

(7.9)

and substitute this into (7.8). Thus
xâ€²â€² = vÎ»2 eÎ»t = eÎ»t Av
and so
Î»2 v = Av.
Therefore, Î»2 needs to be an eigenvalue of A and v needs to be an eigenvector. Since A
has nonpositive eigenvalues, Î»2 = âˆ’a2 and so Î» = Â±ia where âˆ’a2 is an eigenvalue of A.
Corresponding to this you obtain solutions of the form
x (t) = v cos (at) , v sin (at) .
Note these solutions oscillate because of the cos (at) and sin (at) in the solutions. Here is
an example.
Example 7.2.2 Find oscillatory solutions to the system of diï¬€erential equations, xâ€²â€² = Ax
where
ï£« 5
ï£¶
âˆ’ 3 âˆ’ 13
âˆ’ 13
5
ï£¸.
A = ï£­ âˆ’ 13 âˆ’ 13
6
6
1
5
13
âˆ’3
âˆ’
6
6
The eigenvalues are âˆ’1, âˆ’2, and âˆ’3.
According to the above, you can ï¬nd solutions by looking for the eigenvectors. Consider
the eigenvectors for âˆ’3. The augmented matrix for ï¬nding the eigenvectors is
ï£¶
ï£« 4
1
1
âˆ’3
0
3
3
ï£­ 1
âˆ’ 56 âˆ’ 56 0 ï£¸
3
1
âˆ’ 56 âˆ’ 56 0
3
and its row echelon form is

ï£«

1 0
ï£­ 0 1
0 0

Saylor URL: http://www.saylor.org/courses/ma212/

0
1
0

ï£¶
0
0 ï£¸.
0

The Saylor Foundation

7.3. EXERCISES

167

Therefore, the eigenvectors are of the form
ï£«

ï£¶
0
v = z ï£­ âˆ’1 ï£¸ .
1
It follows

ï£«

ï£¶
ï£«
ï£¶
0
0
(âˆš )
(âˆš )
ï£­ âˆ’1 ï£¸ cos
3t , ï£­ âˆ’1 ï£¸ sin
3t
1
1

are both solutions to the system of diï¬€erential equations. You can ï¬nd other oscillatory
solutions in the same way by considering the other eigenvalues. You might try checking
these answers to verify they work.
This is just a special case of a procedure used in diï¬€erential equations to obtain closed
form solutions to systems of diï¬€erential equations using linear algebra. The overall philosophy is to take one of the easiest problems in analysis and change it into the eigenvalue
problem which is the most diï¬ƒcult problem in algebra. However, when it works, it gives
precise solutions in terms of known functions.

7.3

Exercises

1. If A is the matrix of a linear transformation which rotates all vectors in R2 through
30â—¦ , explain why A cannot have any real eigenvalues.
2. If A is an n Ã— n matrix and c is a nonzero constant, compare the eigenvalues of A and
cA.
3. If A is an invertible n Ã— n matrix, compare the eigenvalues of A and Aâˆ’1 . More
generally, for m an arbitrary integer, compare the eigenvalues of A and Am .
4. Let A, B be invertible n Ã— n matrices which commute. That is, AB = BA. Suppose
x is an eigenvector of B. Show that then Ax must also be an eigenvector for B.
5. Suppose A is an n Ã— n matrix and it satisï¬es Am = A for some m a positive integer
larger than 1. Show that if Î» is an eigenvalue of A then |Î»| equals either 0 or 1.
6. Show that if Ax = Î»x and Ay = Î»y, then whenever a, b are scalars,
A (ax + by) = Î» (ax + by) .
Does this imply that ax + by is an eigenvector? Explain.
ï£«

ï£¶
âˆ’1 âˆ’1 7
7. Find the eigenvalues and eigenvectors of the matrix ï£­ âˆ’1 0 4 ï£¸ . Determine
âˆ’1 âˆ’1 5
whether the matrix is defective.
ï£«
ï£¶
âˆ’3 âˆ’7 19
8. Find the eigenvalues and eigenvectors of the matrix ï£­ âˆ’2 âˆ’1 8 ï£¸ .Determine
âˆ’2 âˆ’3 10
whether the matrix is defective.
ï£«
ï£¶
âˆ’7 âˆ’12 30
9. Find the eigenvalues and eigenvectors of the matrix ï£­ âˆ’3 âˆ’7 15 ï£¸ .
âˆ’3 âˆ’6 14

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

168

SPECTRAL THEORY

ï£«

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

ï£¶
7 âˆ’2 0
Find the eigenvalues and eigenvectors of the matrix ï£­ 8 âˆ’1 0 ï£¸ . Determine
âˆ’2 4 6
whether the matrix is defective.
ï£«
ï£¶
3 âˆ’2 âˆ’1
1 ï£¸.
Find the eigenvalues and eigenvectors of the matrix ï£­ 0 5
0 2
4
ï£«
ï£¶
6 8 âˆ’23
Find the eigenvalues and eigenvectors of the matrix ï£­ 4 5 âˆ’16 ï£¸. Determine
3 4 âˆ’12
whether the matrix is defective.
ï£«
ï£¶
5 2 âˆ’5
Find the eigenvalues and eigenvectors of the matrix ï£­ 12 3 âˆ’10 ï£¸ . Determine
12 4 âˆ’11
whether the matrix is defective.
ï£«
ï£¶
20 9 âˆ’18
âˆ’6 ï£¸ . Determine
Find the eigenvalues and eigenvectors of the matrix ï£­ 6 5
30 14 âˆ’27
whether the matrix is defective.
ï£¶
ï£«
1
26 âˆ’17
âˆ’4
4 ï£¸ . Determine
Find the eigenvalues and eigenvectors of the matrix ï£­ 4
âˆ’9 âˆ’18
9
whether the matrix is defective.
ï£«
ï£¶
3 âˆ’1 âˆ’2
Find the eigenvalues and eigenvectors of the matrix ï£­ 11 3 âˆ’9 ï£¸ . Determine
8
0 âˆ’6
whether the matrix is defective.
ï£«
ï£¶
âˆ’2
1 2
Find the eigenvalues and eigenvectors of the matrix ï£­ âˆ’11 âˆ’2 9 ï£¸ . Determine
âˆ’8
0 7
whether the matrix is defective.
ï£«
ï£¶
2 1 âˆ’1
Find the eigenvalues and eigenvectors of the matrix ï£­ 2 3 âˆ’2 ï£¸ . Determine whether
2 2 âˆ’1
the matrix is defective.
ï£«
ï£¶
4 âˆ’2 âˆ’2
Find the complex eigenvalues and eigenvectors of the matrix ï£­ 0 2 âˆ’2 ï£¸ .
2 0
2
ï£«
ï£¶
9
6 âˆ’3
6
0 ï£¸ . Determine
Find the eigenvalues and eigenvectors of the matrix ï£­ 0
âˆ’3 âˆ’6 9
whether the matrix is defective.
ï£«
ï£¶
4 âˆ’2 âˆ’2
Find the complex eigenvalues and eigenvectors of the matrix ï£­ 0 2 âˆ’2 ï£¸ . De2 0
2
termine whether the matrix is defective.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.3. EXERCISES

169
ï£«

ï£¶
âˆ’4 2
0
22. Find the complex eigenvalues and eigenvectors of the matrix ï£­ 2 âˆ’4 0 ï£¸ .
âˆ’2 2 âˆ’2
Determine whether the matrix is defective.
ï£«
ï£¶
1
1 âˆ’6
23. Find the complex eigenvalues and eigenvectors of the matrix ï£­ 7 âˆ’5 âˆ’6 ï£¸ .
âˆ’1 7
2
Determine whether the matrix is defective.
ï£«
ï£¶
4 2 0
24. Find the complex eigenvalues and eigenvectors of the matrix ï£­ âˆ’2 4 0 ï£¸ . Deterâˆ’2 2 6
mine whether the matrix is defective.
25. Here is a matrix.

ï£«

1
ï£¬ 0
ï£¬
ï£­ 0
0

ï£¶
a 0 0
1 b 0 ï£·
ï£·
0 2 c ï£¸
0 0 2

Find values of a, b, c for which the matrix is defective and values of a, b, c for which it
is nondefective.
26. Here is a matrix.

ï£¶
a 1 0
ï£­ 0 b 1 ï£¸
0 0 c
ï£«

where a, b, c are numbers. Show this is sometimes defective depending on the choice
of a, b, c. What is an easy case which will ensure it is not defective?
27. Suppose A is an n Ã— n matrix consisting entirely of real entries but a + ib is a complex
eigenvalue having the eigenvector, x + iy. Here x and y are real vectors. Show that
then a âˆ’ ib is also an eigenvalue with the eigenvector, x âˆ’ iy. Hint: You should
remember that the conjugate of a product of complex numbers equals the product of
the conjugates. Here a + ib is a complex number whose conjugate equals a âˆ’ ib.
28. Recall an n Ã— n matrix is said to be symmetric if it has all real entries and if A = AT .
Show the eigenvalues of a real symmetric matrix are real and for each eigenvalue, it
has a real eigenvector.
29. Recall an n Ã— n matrix is said to be skew symmetric if it has all real entries and if
A = âˆ’AT . Show that any nonzero eigenvalues must be of the form ib where i2 = âˆ’1.
In words, the eigenvalues are either 0 or pure imaginary.
30. Is it possible for a nonzero matrix to have only 0 as an eigenvalue?
31. Show that the eigenvalues and eigenvectors of a real matrix occur in conjugate pairs.
32. Suppose A is an n Ã— n matrix having all real eigenvalues which are distinct. Show
there exists S such that S âˆ’1 AS = D, a diagonal matrix. If
ï£«
ï£¶
Î»1
0
ï£¬
ï£·
..
D=ï£­
ï£¸
.
0

Saylor URL: http://www.saylor.org/courses/ma212/

Î»n

The Saylor Foundation

170

SPECTRAL THEORY

deï¬ne eD by

ï£«
ï£¬
eD â‰¡ ï£­

eÎ» 1

0
..

.

0
and deï¬ne

ï£¶
ï£·
ï£¸

eÎ» n

eA â‰¡ SeD S âˆ’1 .

Next show that if A is as just described, so is tA where t is a real number and the
eigenvalues of At are tÎ»k . If you diï¬€erentiate a matrix of functions entry by entry so
that for the ij th entry of Aâ€² (t) you get aâ€²ij (t) where aij (t) is the ij th entry of A (t) ,
show
d ( At )
e
= AeAt
dt
( )
Next show det eAt Ì¸= 0. This is called the matrix exponential. Note I have only
deï¬ned it for the case where the eigenvalues of A are real, but the same procedure will
work even for complex eigenvalues. All you have to do is to deï¬ne what is meant by
ea+ib .
ï£« 7
ï£¶
1
âˆ’ 14
12
6
7
âˆ’ 16 ï£¸ . The
33. Find the principle directions determined by the matrix ï£­ âˆ’ 14 12
1
1
2
âˆ’6
6
3
1
1
eigenvalues are 3 , 1, and 2 listed according to multiplicity.
34. Find the principle directions determined by the matrix
ï£« 5
ï£¶
âˆ’ 13 âˆ’ 13
3
7
1 ï£¸
ï£­ âˆ’1
The eigenvalues are 1, 2, and 1. What is the physical interpreta3
6
6
1
1
7
âˆ’3
6
6
tion of the repeated eigenvalue?
35. Find oscillatory solutions to the system of diï¬€erential equations, xâ€²â€² = Ax where A =
ï£¶
ï£«
âˆ’3 âˆ’1 âˆ’1
ï£­ âˆ’1 âˆ’2 0 ï£¸ The eigenvalues are âˆ’1, âˆ’4, and âˆ’2.
âˆ’1 0 âˆ’2
36. Let A and B be n Ã— n matrices and let the columns of B be
b1 , Â· Â· Â· , bn
and the rows of A are
aT1 , Â· Â· Â· , aTn .
Show the columns of AB are
Ab1 Â· Â· Â· Abn
and the rows of AB are
aT1 B Â· Â· Â· aTn B.
37. Let M be an n Ã— n matrix. Then deï¬ne the adjoint of M , denoted by M âˆ— to be the
transpose of the conjugate of M. For example,
(
)âˆ— (
)
2
i
2 1âˆ’i
=
.
1+i 3
âˆ’i
3
A matrix M, is self adjoint if M âˆ— = M. Show the eigenvalues of a self adjoint matrix
are all real.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.3. EXERCISES

171

38. Let M be an n Ã— n matrix and suppose x1 , Â· Â· Â· , xn are n eigenvectors which form a
linearly independent set. Form the matrix S by making the columns these vectors.
Show that S âˆ’1 exists and that S âˆ’1 M S is a diagonal matrix (one having zeros everywhere except on the main diagonal) having the eigenvalues of M on the main diagonal.
When this can be done the matrix is said to be diagonalizable.
39. Show that a n Ã— n matrix M is diagonalizable if and only if Fn has a basis of eigenvectors. Hint: The ï¬rst part is done in Problem 38. It only remains to show that if the
matrix can be diagonalized by some matrix S giving D = S âˆ’1 M S for D a diagonal
matrix, then it has a basis of eigenvectors. Try using the columns of the matrix S.
ï£«

40. Let

ï£¶
2
0 ï£·
ï£¸
3

1 2
ï£¬ 3 4
A=ï£­
0 1
ï£«

and let

ï£¶

0 1
ï£¬ 1 1
B=ï£­
2 1

ï£·
ï£¸
(

Multiply AB verifying the block multiplication formula. Here A11 =
( )
(
)
2
, A21 = 0 1 and A22 = (3) .
0

1
3

2
4

)
, A12 =

41. Suppose A, B are n Ã— n matrices and Î» is a nonzero eigenvalue of AB. Show that then
it is also an eigenvalue of BA. Hint: Use the deï¬nition of what it means for Î» to be
an eigenvalue. That is,
ABx = Î»x
where x Ì¸= 0. Maybe you should multiply both sides by B.
42. Using the above problem show that if A, B are n Ã— n matrices, it is not possible that
AB âˆ’ BA = aI for any a Ì¸= 0. Hint: First show that if A is a matrix, then the
eigenvalues of A âˆ’ aI are Î» âˆ’ a where Î» is an eigenvalue of A.
43. Consider the following matrix.
ï£«

0 Â·Â·Â·
ï£¬ 1 0
ï£¬
C=ï£¬
..
ï£­
.
0

âˆ’a0
âˆ’a1
..
.

0
..
1

.

ï£¶
ï£·
ï£·
ï£·
ï£¸

âˆ’anâˆ’1

Show det (Î»I âˆ’ C) = a0 + Î»a1 + Â· Â· Â· anâˆ’1 Î»nâˆ’1 + Î»n . This matrix is called a companion
matrix for the given polynomial.
44. A discreet dynamical system is of the form
x (k + 1) = Ax (k) , x (0) = x0
where A is an n Ã— n matrix and x (k) is a vector in Rn . Show ï¬rst that
x (k) = Ak x0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

172

SPECTRAL THEORY

for all k â‰¥ 1. If A is nondefective so that it has a basis of eigenvectors, {v1 , Â· Â· Â· , vn }
where
Avj = Î»j vj
you can write the initial condition x0 in a unique way as a linear combination of these
eigenvectors. Thus
n
âˆ‘
x0 =
aj vj
j=1

Now explain why
x (k) =

n
âˆ‘
j=1

aj Ak vj =

n
âˆ‘

aj Î»kj vj

j=1

which gives a formula for x (k) , the solution of the dynamical system.
45. Suppose A is an n Ã— n matrix and let v be an eigenvector such that Av = Î»v. Also
suppose the characteristic polynomial of A is
det (Î»I âˆ’ A) = Î»n + anâˆ’1 Î»nâˆ’1 + Â· Â· Â· + a1 Î» + a0
Explain why

(

)
An + anâˆ’1 Anâˆ’1 + Â· Â· Â· + a1 A + a0 I v = 0

If A is nondefective, give a very easy proof of the Cayley Hamilton theorem based on
this. Recall this theorem says A satisï¬es its characteristic equation,
An + anâˆ’1 Anâˆ’1 + Â· Â· Â· + a1 A + a0 I = 0.
46. Suppose an n Ã— n nondefective matrix A has only 1 and âˆ’1 as eigenvalues. Find A12 .
47. Suppose the characteristic polynomial of an n Ã— n matrix A is 1 âˆ’ Î»n . Find Amn where
m is an integer. Hint: Note ï¬rst that A is nondefective. Why?
48. Sometimes sequences come in terms of a recursion formula. An example is the Fibonacci sequence.
x0 = 1 = x1 , xn+1 = xn + xnâˆ’1
Show this can be considered as a discreet dynamical system as follows.
(
) (
)(
) (
) (
)
xn+1
1 1
xn
x1
1
=
,
=
xn
1 0
xnâˆ’1
x0
1
Now use the technique of Problem 44 to ï¬nd a formula for xn .
49. Let A be an n Ã— n matrix having characteristic polynomial
det (Î»I âˆ’ A) = Î»n + anâˆ’1 Î»nâˆ’1 + Â· Â· Â· + a1 Î» + a0
n

Show that a0 = (âˆ’1) det (A).

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.4. SCHURâ€™S THEOREM

7.4

173

Schurâ€™s Theorem

Every matrix is related to an upper triangular matrix in a particularly signiï¬cant way. This
is Schurâ€™s theorem and it is the most important theorem in the spectral theory of matrices.
Lemma 7.4.1 Let {x1 , Â· Â· Â· , xn } be a basis for Fn . Then there exists an orthonormal basis for Fn , {u1 , Â· Â· Â· , un } which has the property that for each k â‰¤ n, span(x1 , Â· Â· Â· , xk ) =
span (u1 , Â· Â· Â· , uk ) .
Proof: Let {x1 , Â· Â· Â· , xn } be a basis for Fn . Let u1 â‰¡ x1 / |x1 | . Thus for k = 1,
span (u1 ) = span (x1 ) and {u1 } is an orthonormal set. Now suppose for some k < n, u1 , Â· Â· Â· ,
uk have been chosen such that (uj Â· ul ) = Î´ jl and span (x1 , Â· Â· Â· , xk ) = span (u1 , Â· Â· Â· , uk ).
Then deï¬ne
âˆ‘k
xk+1 âˆ’ j=1 (xk+1 Â· uj ) uj
uk+1 â‰¡
,
(7.10)
âˆ‘k
xk+1 âˆ’ j=1 (xk+1 Â· uj ) uj
where the denominator is not equal to zero because the xj form a basis and so
xk+1 âˆˆ
/ span (x1 , Â· Â· Â· , xk ) = span (u1 , Â· Â· Â· , uk )
Thus by induction,
uk+1 âˆˆ span (u1 , Â· Â· Â· , uk , xk+1 ) = span (x1 , Â· Â· Â· , xk , xk+1 ) .
Also, xk+1 âˆˆ span (u1 , Â· Â· Â· , uk , uk+1 ) which is seen easily by solving (7.10) for xk+1 and it
follows
span (x1 , Â· Â· Â· , xk , xk+1 ) = span (u1 , Â· Â· Â· , uk , uk+1 ) .
If l â‰¤ k,

ï£«
(uk+1 Â· ul ) = C ï£­(xk+1 Â· ul ) âˆ’

k
âˆ‘

ï£¶
(xk+1 Â· uj ) (uj Â· ul )ï£¸ =

j=1

ï£«
C ï£­(xk+1 Â· ul ) âˆ’

k
âˆ‘

ï£¶
(xk+1 Â· uj ) Î´ lj ï£¸ = C ((xk+1 Â· ul ) âˆ’ (xk+1 Â· ul )) = 0.

j=1
n

The vectors, {uj }j=1 , generated in this way are therefore an orthonormal basis because
each vector has unit length. 
The process by which these vectors were generated is called the Gram Schmidt process.
Here is a fundamental deï¬nition.
Deï¬nition 7.4.2 An n Ã— n matrix U, is unitary if U U âˆ— = I = U âˆ— U where U âˆ— is deï¬ned to
be the transpose of the conjugate of U.
Proposition 7.4.3 An nÃ—n matrix is unitary if and only if the columns are an orthonormal
set.
Proof: This follows right away from the way we multiply matrices. If U is an n Ã— n
complex matrix, then
(U âˆ— U )ij = uâˆ—i uj = (ui , uj )
and the matrix is unitary if and only if this equals Î´ ij if and only if the columns are
orthonormal. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

174

SPECTRAL THEORY

Theorem 7.4.4 Let A be an n Ã— n matrix. Then there exists a unitary matrix U such that
U âˆ— AU = T,

(7.11)

where T is an upper triangular matrix having the eigenvalues of A on the main diagonal
listed according to multiplicity as roots of the characteristic equation.
Proof: The theorem is clearly true if A is a 1 Ã— 1 matrix. Just let U = 1 the 1 Ã— 1
matrix which has 1 down the main diagonal and zeros elsewhere. Suppose it is true for
(n âˆ’ 1) Ã— (n âˆ’ 1) matrices and let A be an n Ã— n matrix. Then let v1 be a unit eigenvector
for A . Then there exists Î»1 such that
Av1 = Î»1 v1 , |v1 | = 1.
Extend {v1 } to a basis and then use Lemma 7.4.1 to obtain {v1 , Â· Â· Â· , vn }, an orthonormal
basis in Fn . Let U0 be a matrix whose ith column is vi . Then from the above, it follows U0
is unitary. Then U0âˆ— AU0 is of the form
ï£«
ï£¶
Î»1 âˆ— Â· Â· Â· âˆ—
ï£¬ 0
ï£·
ï£¬
ï£·
ï£¬ ..
ï£·
ï£­ .
ï£¸
A
1

0
where A1 is an n âˆ’ 1 Ã— n âˆ’ 1 matrix. Now by induction there exists an (n âˆ’ 1) Ã— (n âˆ’ 1)
e1 such that
unitary matrix U
e1âˆ— A1 U
e1 = Tnâˆ’1 ,
U
an upper triangular matrix. Consider
(
U1 â‰¡
This is a unitary matrix and
(
)(
1 0
Î»1
U1âˆ— U0âˆ— AU0 U1 =
eâˆ—
0
0 U
1

1 0
e1
0 U

âˆ—
A1

)(

)

1
0

0
e
U1

)

(
=

Î»1
0

)

âˆ—
Tnâˆ’1

â‰¡T

âˆ—

where T is upper triangular. Then let U = U0 U1 . Since (U0 U1 ) = U1âˆ— U0âˆ— , it follows A
is similar to T and that U0 U1 is unitary. Hence A and T have the same characteristic
polynomials and since the eigenvalues of T are the diagonal entries listed according to
algebraic multiplicity, 
As a simple consequence of the above theorem, here is an interesting lemma.
Lemma 7.4.5 Let A be of the form
ï£«

Â·Â·Â·
..
.
Â·Â·Â·

P1
ï£¬ ..
A=ï£­ .
0
where Pk is an mk Ã— mk matrix. Then
det (A) =

âˆ

ï£¶
âˆ—
.. ï£·
. ï£¸
Ps

det (Pk ) .

k

Also, the eigenvalues of A consist of the union of the eigenvalues of the Pj .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.4. SCHURâ€™S THEOREM

175

Proof: Let Uk be an mk Ã— mk unitary matrix such that
Ukâˆ— Pk Uk = Tk
where Tk is upper triangular. Then it follows that for
ï£«
ï£¶
ï£«
U1 Â· Â· Â· 0
ï£¬
.. ï£· , U âˆ— = ï£¬
..
U â‰¡ ï£­ ...
ï£­
.
. ï£¸
0 Â· Â· Â· Us
and also
ï£« âˆ—
U1
ï£¬ ..
ï£­ .
0

Â·Â·Â·
..
.
Â·Â·Â·

ï£¶ï£«
0
.. ï£· ï£¬
. ï£¸ï£­
Usâˆ—

P1
..
.
0

Â·Â·Â·
..
.
Â·Â·Â·

ï£¶ï£«
âˆ—
.. ï£· ï£¬
. ï£¸ï£­
Ps

U1
..
.
0

U1âˆ—
..
.
0

Â·Â·Â·
..
.
Â·Â·Â·

ï£¶
0
.. ï£·
. ï£¸
Usâˆ—

ï£¶ ï£«
0
.. ï£· = ï£¬
. ï£¸ ï£­
Us

Â·Â·Â·
..
.
Â·Â·Â·

T1
..
.
0

Â·Â·Â·
..
.
Â·Â·Â·

ï£¶
âˆ—
.. ï£· .
. ï£¸
Ts

Therefore, since the determinant of an upper triangular matrix is the product of the diagonal
entries,
âˆ
âˆ
det (A) =
det (Tk ) =
det (Pk ) .
k

k

From the above formula, the eigenvalues of A consist of the eigenvalues of the upper triangular matrices Tk , and each Tk has the same eigenvalues as Pk . 
What if A is a real matrix and you only want to consider real unitary matrices?
Theorem 7.4.6 Let A be a real n Ã— n matrix. Then
and a matrix T of the form
ï£«
P1 Â· Â· Â· âˆ—
ï£¬
.
..
T =ï£­
. ..
0

there exists a real unitary matrix Q
ï£¶
ï£·
ï£¸

(7.12)

Pr

where Pi equals either a real 1 Ã— 1 matrix or Pi equals a real 2 Ã— 2 matrix having as its
eigenvalues a conjugate pair of eigenvalues of A such that QT AQ = T. The matrix T is
called the real Schur form of the matrix A. Recall that a real unitary matrix is also called
an orthogonal matrix.
Proof: Suppose
Av1 = Î»1 v1 , |v1 | = 1
where Î»1 is real. Then let {v1 , Â· Â· Â· , vn } be an orthonormal basis of vectors in Rn . Let Q0
be a matrix whose ith column is vi . Then Qâˆ—0 AQ0 is of the form
ï£«
ï£¶
Î»1 âˆ— Â· Â· Â· âˆ—
ï£¬ 0
ï£·
ï£¬
ï£·
ï£¬ ..
ï£·
ï£­ .
ï£¸
A
1

0
where A1 is a real n âˆ’ 1 Ã— n âˆ’ 1 matrix. This is just like the proof of Theorem 7.4.4 up to
this point.
Now consider the case where Î»1 = Î± + iÎ² where Î² Ì¸= 0. It follows since A is real that
v1 = z1 + iw1 and that v1 = z1 âˆ’ iw1 is an eigenvector for the eigenvalue Î± âˆ’ iÎ². Here
z1 and w1 are real vectors. Since v1 and v1 are eigenvectors corresponding to distinct
eigenvalues, they form a linearly independent set. From this it follows that {z1 , w1 } is an

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

176

SPECTRAL THEORY

independent set of vectors in Cn , hence in Rn . Indeed,{v1 , v1 } is an independent set and
also span (v1 , v1 ) = span (z1 , w1 ) . Now using the Gram Schmidt theorem in Rn , there exists
{u1 , u2 } , an orthonormal set of real vectors such that span (u1 , u2 ) = span (v1 , v1 ). For
example,
2
|z1 | w1 âˆ’ (w1 Â· z1 ) z1
u1 = z1 / |z1 | , u2 =
2
|z1 | w1 âˆ’ (w1 Â· z1 ) z1
Let {u1 , u2 , Â· Â· Â· , un } be an orthonormal basis in Rn and let Q0 be a unitary matrix whose
ith column is ui so Q0 is a real orthogonal matrix. Then Auj are both in span (u1 , u2 ) for
j = 1, 2 and so uTk Auj = 0 whenever k â‰¥ 3. It follows that Qâˆ—0 AQ0 is of the form
ï£«
ï£¶
âˆ— âˆ— Â·Â·Â· âˆ—
ï£· (
ï£¬ âˆ— âˆ—
)
ï£¬
ï£·
P1 âˆ—
ï£·
ï£¬
âˆ—
0
Q0 AQ0 = ï£¬
ï£·=
0 A1
ï£¬ ..
ï£·
ï£¸
ï£­ .
A
1

0
where A1 is now an n âˆ’ 2 Ã— n âˆ’ 2 matrix and P1 is a 2 Ã— 2 matrix. Now this is similar to A
and so two of its eigenvalues are Î± + iÎ² and Î± âˆ’ iÎ².
e 1 an n âˆ’ 2 Ã— n âˆ’ 2 matrix to put A1 in an appropriate form as above and
Now ï¬nd Q
come up with A2 either an n âˆ’ 4 Ã— n âˆ’ 4 matrix or an n âˆ’ 3 Ã— n âˆ’ 3 matrix. Then the only
other diï¬€erence is to let
ï£«
ï£¶
1 0 0 Â·Â·Â· 0
ï£¬ 0 1 0 Â·Â·Â· 0 ï£·
ï£¬
ï£·
ï£¬
ï£·
Q1 = ï£¬ 0 0
ï£·
ï£¬ .. ..
ï£·
e
ï£­ . .
ï£¸
Q1
0 0
thus putting a 2Ã—2 identity matrix in the upper left corner rather than a one. Repeating this
process with the above modiï¬cation for the case of a complex eigenvalue leads eventually
to (7.12) where Q is the product of real unitary matrices Qi above. When the block Pi is
2 Ã— 2, its eigenvalues are a conjugate pair of eigenvalues of A and if it is 1 Ã— 1 it is a real
eigenvalue of A.
Here is why this last claim is true
ï£«
ï£¶
Î»I1 âˆ’ P1 Â· Â· Â·
âˆ—
ï£¬
ï£·
..
..
Î»I âˆ’ T = ï£­
ï£¸
.
.
Î»Ir âˆ’ Pr

0

where Ik is the 2 Ã— 2 identity matrix in the case that Pk is 2 Ã— 2 and is the number 1 in the
case where Pk is a 1 Ã— 1 matrix. Now by Lemma 7.4.5,
det (Î»I âˆ’ T ) =

r
âˆ

det (Î»Ik âˆ’ Pk ) .

k=1

Therefore, Î» is an eigenvalue of T if and only if it is an eigenvalue of some Pk . This proves
the theorem since the eigenvalues of T are the same as those of A including multiplicity
because they have the same characteristic polynomial due to the similarity of A and T. 
Corollary 7.4.7 Let A be a real n Ã— n matrix having only real eigenvalues. Then there
exists a real orthogonal matrix Q and an upper triangular matrix T such that
QT AQ = T

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.4. SCHURâ€™S THEOREM

177

and furthermore, if the eigenvalues of A are listed in decreasing order,
Î»1 â‰¥ Î»2 â‰¥ Â· Â· Â· â‰¥ Î»n
Q can be chosen such that T is of the form
ï£«
Î»1 âˆ—
ï£¬
ï£¬ 0 Î»2
ï£¬
ï£¬ . .
..
ï£­ ..
0 Â·Â·Â·

Â·Â·Â·
..
.
..
.
0

ï£¶
âˆ—
.. ï£·
. ï£·
ï£·
ï£·
âˆ— ï£¸
Î»n

Proof: Most of this follows right away from Theorem 7.4.6. It remains to verify the
claim that the diagonal entries can be arranged in the desired order. However, this follows
from a simple modiï¬cation of the above argument. When you ï¬nd v1 the eigenvalue of Î»1 ,
just be sure Î»1 is chosen to be the largest eigenvalue. Then observe that from Lemma 7.4.5
applied to the characteristic equation, the eigenvalues of the (n âˆ’ 1) Ã— (n âˆ’ 1) matrix A1
are {Î»1 , Â· Â· Â· , Î»n }. Then pick Î»2 to continue the process of construction with A1 . 
Of course there is a similar conclusion which can be proved exactly the same way in the
case where A has complex eigenvalues.
Corollary 7.4.8 Let A be a real n Ã— n matrix. Then there exists a real orthogonal matrix
Q and an upper triangular matrix T such that
ï£«
ï£¶
P1 Â· Â· Â· âˆ—
ï£¬
. ï£·
..
QT AQ = T = ï£­
. .. ï£¸
0
Pr
where Pi equals either a real 1 Ã— 1 matrix or Pi equals a real 2 Ã— 2 matrix having as its
eigenvalues a conjugate pair of eigenvalues of A. If Pk corresponds to the two eigenvalues
Î±k Â± iÎ² k â‰¡ Ïƒ (Pk ) , Q can be chosen such that
|Ïƒ (P1 )| â‰¥ |Ïƒ (P2 )| â‰¥ Â· Â· Â·
where
|Ïƒ (Pk )| â‰¡

âˆš
Î±2k + Î² 2k

The blocks, Pk can be arranged in any other order also.
Deï¬nition 7.4.9 When a linear transformation, A, mapping a linear space, V to V has a
basis of eigenvectors, the linear transformation is called non defective. Otherwise it is called
defective. An n Ã— n matrix A, is called normal if AAâˆ— = Aâˆ— A. An important class of normal
matrices is that of the Hermitian or self adjoint matrices. An n Ã— n matrix A is self adjoint
or Hermitian if A = Aâˆ— .
The next lemma is the basis for concluding that every normal matrix is unitarily similar
to a diagonal matrix.
Lemma 7.4.10 If T is upper triangular and normal, then T is a diagonal matrix.
Proof:This is obviously true if T is 1 Ã— 1. In fact, it canâ€™t help being diagonal in this
case. Suppose then that the lemma is true for (n âˆ’ 1) Ã— (n âˆ’ 1) matrices and let T be an
upper triangular normal n Ã— n matrix. Thus T is of the form
)
(
)
(
t11 0T
t11 aâˆ—
âˆ—
T =
, T =
0 T1
a T1âˆ—

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

178

SPECTRAL THEORY

Then
TT

âˆ—

T âˆ—T

(
=
(
=

t11
0

aâˆ—
T1

t11
a

0T
T1âˆ—

)(
)(

t11
a

0T
T1âˆ—

t11
0

aâˆ—
T1

)

(
=

)

(
=

|t11 | + aâˆ— a aâˆ— T1âˆ—
T1 a
T1 T1âˆ—
2

2

|t11 |
at11

t11 aâˆ—
âˆ—
aa + T1âˆ— T1

)
)

Since these two matrices are equal, it follows a = 0. But now it follows that T1âˆ— T1 = T1 T1âˆ—
and so by induction T1 is a diagonal matrix D1 . Therefore,
(
)
t11 0T
T =
0 D1
a diagonal matrix.
Now here is a proof which doesnâ€™t involve block multiplication. Since T is normal,
T âˆ— T = T T âˆ— . Writing this in terms of components and using the description of the adjoint
as the transpose of the conjugate, yields the following for the ik th entry of T âˆ— T = T T âˆ— .
zâˆ‘

TTâˆ—

T âˆ—T

}| âˆ‘
{ zâˆ‘
}| âˆ‘
{
âˆ—
âˆ—
tji tjk .
tij tjk =
tij tkj =
tij tjk =

j

j

j

j

Now use the fact that T is upper triangular and let i = k = 1 to obtain the following from
the above.
âˆ‘
âˆ‘
2
2
2
|t1j | =
|tj1 | = |t11 |
j

j

You see, tj1 = 0 unless j = 1 due to the
T is of the form
ï£«
âˆ—
ï£¬ 0
ï£¬
ï£¬ ..
ï£­ .

assumption that T is upper triangular. This shows
ï£¶
0 Â·Â·Â· 0
âˆ— Â·Â·Â· âˆ— ï£·
ï£·
. ï£·.
..
..
.
. .. ï£¸

0 Â·Â·Â·

âˆ—

0

Now do the same thing only this time take i = k = 2 and use the result just established.
Thus, from the above,
âˆ‘
âˆ‘
2
2
2
|t2j | =
|tj2 | = |t22 | ,
j

j

showing that t2j = 0 if j > 2 which means T has the form
ï£«
ï£¶
âˆ— 0 0 Â·Â·Â· 0
ï£¬ 0 âˆ— 0 Â·Â·Â· 0 ï£·
ï£¬
ï£·
ï£¬ 0 0 âˆ— Â·Â·Â· âˆ— ï£·
ï£¬
ï£·.
ï£¬ .. .. . .
. ï£·
..
ï£­ . .
.
. .. ï£¸
0 0

0

0

âˆ—

Next let i = k = 3 and obtain that T looks like a diagonal matrix in so far as the ï¬rst 3
rows and columns are concerned. Continuing in this way it follows T is a diagonal matrix.

Theorem 7.4.11 Let A be a normal matrix. Then there exists a unitary matrix U such
that U âˆ— AU is a diagonal matrix.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.4. SCHURâ€™S THEOREM

179

Proof: From Theorem 7.4.4 there exists a unitary matrix U such that U âˆ— AU equals
an upper triangular matrix. The theorem is now proved if it is shown that the property of
being normal is preserved under unitary similarity transformations. That is, verify that if
A is normal and if B = U âˆ— AU, then B is also normal. But this is easy.
Bâˆ—B

= U âˆ— Aâˆ— U U âˆ— AU = U âˆ— Aâˆ— AU
= U âˆ— AAâˆ— U = U âˆ— AU U âˆ— Aâˆ— U = BB âˆ— .

Therefore, U âˆ— AU is a normal and upper triangular matrix and by Lemma 7.4.10 it must be
a diagonal matrix. 
Corollary 7.4.12 If A is Hermitian, then all the eigenvalues of A are real and there exists
an orthonormal basis of eigenvectors.
Proof: Since A is normal, there exists unitary, U such that U âˆ— AU = D, a diagonal
matrix whose diagonal entries are the eigenvalues of A. Therefore, Dâˆ— = U âˆ— Aâˆ— U = U âˆ— AU =
D showing D is real.
Finally, let
(
)
U = u1 u2 Â· Â· Â· un
where the ui denote the columns of U and
ï£«
Î»1
ï£¬
D=ï£­
0
The equation, U âˆ— AU = D implies
(
Au1
AU =
= UD =

(

0
..

.

ï£¶
ï£·
ï£¸

Î»n

Â·Â·Â·

Au2
Î»1 u1

Aun

)

Â·Â·Â·

Î»2 u2

Î»n un

)

where the entries denote the columns of AU and U D respectively. Therefore, Aui = Î»i ui
and since the matrix is unitary, the ij th entry of U âˆ— U equals Î´ ij and so
Î´ ij = uâˆ—i uj â‰¡ uj Â· ui .
This proves the corollary because it shows the vectors {ui } are orthonormal. Therefore,
they form a basis because every orthonormal set of vectors is linearly independent. 
Corollary 7.4.13 If A is a real symmetric matrix, then A is Hermitian and there exists a
real unitary matrix U such that U T AU = D where D is a diagonal matrix whose diagonal
entries are the eigenvalues of A. By arranging the columns of U the diagonal entries of D
can be made to appear in any order.
Proof: This follows from Theorem 7.4.6 and Corollary 7.4.12. Let
(
)
U = u1 Â· Â· Â· un
Then AU = U D so
(
AU = Au1

Â·Â·Â·

Aun

)

=

(

u1

Â·Â·Â·

un

)

D=

(

Î»1 u1

Â·Â·Â·

Î»n un

)

Hence each column of U is an eigenvector of A. It follows that by rearranging these columns,
the entries of D on the main diagonal can be made to appear in any order. To see this,
consider such a rearrangement resulting in an orthogonal matrix U â€² given by
(
)
U â€² = ui1 Â· Â· Â· uin

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

180

SPECTRAL THEORY

Then

U â€²T AU â€² = U â€²T

ï£«

ï£¶
uTi1
ï£·(
ï£¬
= ï£­ ... ï£¸ Î»i1 ui1
uTin

7.5

(

Â·Â·Â·

Â·Â·Â·

Aui1

Î»in uin

)

ï£«

ï£¬
=ï£­

Auin

)

Î»i1

0
..

.

0

ï£¶
ï£·
ï£¸ 

Î»in

Trace And Determinant

The determinant has already been discussed. It is also clear that if A = S âˆ’1 BS so that
A, B are similar, then
(
)
(
)
det (A) = det S âˆ’1 det (S) det (B) = det S âˆ’1 S det (B)
= det (I) det (B) = det (B)
The trace is deï¬ned in the following deï¬nition.
Deï¬nition 7.5.1 Let A be an n Ã— n matrix whose ij th entry is denoted as aij . Then
âˆ‘
trace (A) â‰¡
aii
i

In other words it is the sum of the entries down the main diagonal.
With this deï¬nition, it is easy to see that if A = S âˆ’1 BS, then
trace (A) = trace (B) .
Here is why.
trace (A)

â‰¡

âˆ‘

Aii =

i

=

âˆ‘

âˆ‘(
âˆ‘
âˆ‘
)
(
)
S âˆ’1 ij Bjk Ski =
Bjk
Ski S âˆ’1 ij
i,j,k

Bjk Î´ kj =

j,k

âˆ‘

j,k

i

Bkk = trace (B) .

k

Alternatively,
trace (AB) â‰¡

âˆ‘

Aij Bji = trace (BA) .

ij

Therefore,

(
)
(
)
trace S âˆ’1 AS = trace ASS âˆ’1 = trace (A) .

Theorem 7.5.2 Let A be an nÃ—n matrix. Then trace (A) equals the sum of the eigenvalues
of A and det (A) equals the product of the eigenvalues of A.
This is proved using Schurâ€™s theorem and is in Problem 17 below. Another important
property of the trace is in the following theorem.
Theorem 7.5.3 Let A be an m Ã— n matrix and let B be an n Ã— m matrix. Then
trace (AB) = trace (BA) .
Proof:
trace (AB) â‰¡

(
âˆ‘ âˆ‘
i

)
Aik Bki

k

Saylor URL: http://www.saylor.org/courses/ma212/

=

âˆ‘âˆ‘
k

Bki Aik = trace (BA) 

i

The Saylor Foundation

7.6. QUADRATIC FORMS

7.6

181

Quadratic Forms

Deï¬nition 7.6.1 A quadratic form in three dimensions is an expression of the form
ï£«
ï£¶
x
(
)
x y z Aï£­ y ï£¸
(7.13)
z
where A is a 3 Ã— 3 symmetric matrix. In higher dimensions the idea is the same except you
use a larger symmetric matrix in place of A. In two dimensions A is a 2 Ã— 2 matrix.
For example, consider
(

x

y

z

)

ï£«

3 âˆ’4
ï£­ âˆ’4 0
1 âˆ’4

ï£¶ï£«
ï£¶
1
x
âˆ’4 ï£¸ ï£­ y ï£¸
3
z

(7.14)

which equals 3x2 âˆ’ 8xy + 2xz âˆ’ 8yz + 3z 2 . This is very awkward because of the mixed terms
such as âˆ’8xy. The idea is to pick diï¬€erent axes such that if x, y, z are taken with respect
to these axes, the quadratic form is much simpler. In other words, look for new variables,
xâ€² , y â€² , and z â€² and a unitary matrix U such that
ï£« â€² ï£¶ ï£«
ï£¶
x
x
U ï£­ yâ€² ï£¸ = ï£­ y ï£¸
(7.15)
zâ€²
z
and if you write the quadratic form in terms of the primed variables, there will be no mixed
terms. Any symmetric real matrix is Hermitian and is therefore normal. From Corollary
7.4.13, it follows there exists a real unitary matrix U, (an orthogonal matrix) such that
U T AU = D a diagonal matrix. Thus in the quadratic form, (7.13)
ï£«
ï£¶
ï£« â€² ï£¶
x
x
(
)
( â€²
)
x y z Aï£­ y ï£¸ =
x y â€² z â€² U T AU ï£­ y â€² ï£¸
z
zâ€²
ï£« â€² ï£¶
x
( â€²
)
x yâ€² zâ€² D ï£­ yâ€² ï£¸
=
zâ€²
and in terms of these new variables, the quadratic form becomes
Î»1 (xâ€² ) + Î»2 (y â€² ) + Î»3 (z â€² )
2

2

2

where D = diag (Î»1 , Î»2 , Î»3 ) . Similar considerations apply equally well in any other dimension. For the given example,
âˆš ï£¶ï£«
ï£¶
ï£« 1âˆš
1
0
âˆ’ 2âˆš 2
3 âˆ’4 1
2 âˆš2
âˆš
1
1
ï£¸ ï£­ âˆ’4 0 âˆ’4 ï£¸ Â·
ï£­ 1 6
6
6âˆš
3 âˆš
6 âˆš6
1
1
1
1 âˆ’4 3
3
âˆ’
3
3
3
3
3
ï£«
ï£¶
ï£«
ï£¶
âˆš1
âˆ’ âˆš12 âˆš16
2 0 0
3
ï£¬ 0
âˆš2
âˆ’ âˆš13 ï£·
ï£­
ï£¸ = ï£­ 0 âˆ’4 0 ï£¸
6
1
1
1
âˆš
âˆš
âˆš
0 0 8
2

6

3

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

182

SPECTRAL THEORY

and so if the new variables are given by
ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
âˆš1
âˆ’ âˆš12 âˆš16
xâ€²
x
3
ï£¬ 0
âˆš2
âˆ’ âˆš13 ï£·
ï£­
ï£¸ ï£­ yâ€² ï£¸ = ï£­ y ï£¸ ,
6
1
1
1
âˆš
âˆš
âˆš
zâ€²
z
2

6

3

it follows that in terms of the new variables the quadratic form is 2 (xâ€² ) âˆ’ 4 (y â€² ) + 8 (z â€² ) .
You can work other examples the same way.
2

7.7

2

2

Second Derivative Test

Under certain conditions the mixed partial derivatives will always be equal. This astonishing fact was ï¬rst observed by Euler around 1734. It is also called Clairautâ€™s theorem.
Theorem 7.7.1 Suppose f : U âŠ† F2 â†’ R where U is an open set on which fx , fy , fxy and
fyx exist. Then if fxy and fyx are continuous at the point (x, y) âˆˆ U , it follows
fxy (x, y) = fyx (x, y) .
Proof: Since U is open, there exists r > 0 such that B ((x, y) , r) âŠ† U. Now let |t| , |s| <
r/2, t, s real numbers and consider
h(t)

h(0)

}|
{ z
}|
{
1 z
âˆ† (s, t) â‰¡ {f (x + t, y + s) âˆ’ f (x + t, y) âˆ’ (f (x, y + s) âˆ’ f (x, y))}.
st

(7.16)

Note that (x + t, y + s) âˆˆ U because
|(x + t, y + s) âˆ’ (x, y)|

(
)1/2
= |(t, s)| = t2 + s2
( 2
)1/2
r
r2
r
â‰¤
+
= âˆš < r.
4
4
2

As implied above, h (t) â‰¡ f (x + t, y + s)âˆ’f (x + t, y). Therefore, by the mean value theorem
from calculus and the (one variable) chain rule,
âˆ† (s, t) =
=

1
1
(h (t) âˆ’ h (0)) = hâ€² (Î±t) t
st
st
1
(fx (x + Î±t, y + s) âˆ’ fx (x + Î±t, y))
s

for some Î± âˆˆ (0, 1) . Applying the mean value theorem again,
âˆ† (s, t) = fxy (x + Î±t, y + Î²s)
where Î±, Î² âˆˆ (0, 1).
If the terms f (x + t, y) and f (x, y + s) are interchanged in (7.16), âˆ† (s, t) is unchanged
and the above argument shows there exist Î³, Î´ âˆˆ (0, 1) such that
âˆ† (s, t) = fyx (x + Î³t, y + Î´s) .
Letting (s, t) â†’ (0, 0) and using the continuity of fxy and fyx at (x, y) ,
lim
(s,t)â†’(0,0)

âˆ† (s, t) = fxy (x, y) = fyx (x, y) . 

The following is obtained from the above by simply ï¬xing all the variables except for the
two of interest.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.7. SECOND DERIVATIVE TEST

183

Corollary 7.7.2 Suppose U is an open subset of Fn and f : U â†’ R has the property
that for two indices, k, l, fxk , fxl , fxl xk , and fxk xl exist on U and fxk xl and fxl xk are both
continuous at x âˆˆ U. Then fxk xl (x) = fxl xk (x) .
Thus the theorem asserts that the mixed partial derivatives are equal at x if they are
deï¬ned near x and continuous at x.
Now recall the Taylor formula with the Lagrange form of the remainder. What follows
is a proof of this important result based on the mean value theorem or Rolleâ€™s theorem.
Theorem 7.7.3 Suppose f has n + 1 derivatives on an interval, (a, b) and let c âˆˆ (a, b) .
Then if x âˆˆ (a, b) , there exists Î¾ between c and x such that
f (x) = f (c) +

n
âˆ‘
f (k) (c)

k!

k=1

âˆ‘0

(In this formula, the symbol

k=1

k

(x âˆ’ c) +

f (n+1) (Î¾)
n+1
(x âˆ’ c)
.
(n + 1)!

ak will denote the number 0.)

Proof: If n = 0 then the theorem is true because it is just the mean value theorem.
Suppose the theorem is true for n âˆ’ 1, n â‰¥ 1. It can be assumed x Ì¸= c because if x = c there
is nothing to show. Then there exists K such that
(
)
n
âˆ‘
f (k) (c)
k
n+1
f (x) âˆ’ f (c) +
(x âˆ’ c) + K (x âˆ’ c)
(7.17)
=0
k!
k=1

In fact,
K=

(
âˆ‘n
âˆ’f (x) + f (c) + k=1
(x âˆ’ c)

f (k) (c)
k!
n+1

k

(x âˆ’ c)

)
.

Now deï¬ne F (t) for t in the closed interval determined by x and c by
(
)
n
âˆ‘
f (k) (c)
k
n+1
F (t) â‰¡ f (x) âˆ’ f (t) +
(x âˆ’ t) + K (x âˆ’ t)
.
k!
k=1

The c in (7.17) got replaced by t.
Therefore, F (c) = 0 by the way K was chosen and also F (x) = 0. By the mean value
theorem or Rolleâ€™s theorem, there exists t1 between x and c such that F â€² (t1 ) = 0. Therefore,
0 =

f â€² (t1 ) âˆ’

n
âˆ‘
f (k) (c)

k!

k=1

(

k (x âˆ’ t1 )

kâˆ’1

n

âˆ’ K (n + 1) (x âˆ’ t1 )

)
f (k+1) (c)
k
n
= f (t1 ) âˆ’ f (c) +
(x âˆ’ t1 )
âˆ’ K (n + 1) (x âˆ’ t1 )
k!
k=1
(
)
nâˆ’1
âˆ‘ f â€²(k) (c)
k
n
â€²
â€²
= f (t1 ) âˆ’ f (c) +
(x âˆ’ t1 )
âˆ’ K (n + 1) (x âˆ’ t1 )
k!
â€²

â€²

nâˆ’1
âˆ‘

k=1

By induction applied to f â€² , there exists Î¾ between x and t1 such that the above simpliï¬es
to
f â€²(n) (Î¾) (x âˆ’ t1 )
n
âˆ’ K (n + 1) (x âˆ’ t1 )
n!
n
f (n+1) (Î¾) (x âˆ’ t1 )
n
âˆ’ K (n + 1) (x âˆ’ t1 )
n!
n

0

=
=

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

184

SPECTRAL THEORY

therefore,
K=

f (n+1) (Î¾)
f (n+1) (Î¾)
=
(n + 1) n!
(n + 1)!

and the formula is true for n. 
The following is a special case and is what will be used.
Theorem 7.7.4 Let h : (âˆ’Î´, 1 + Î´) â†’ R have m+1 derivatives. Then there exists t âˆˆ [0, 1]
such that
m
âˆ‘
h(k) (0) h(m+1) (t)
h (1) = h (0) +
+
.
k!
(m + 1)!
k=1

Now let f : U â†’ R where U âŠ† Rn and suppose f âˆˆ C m (U ) . Let x âˆˆ U and let r > 0 be
such that
B (x,r) âŠ† U.
Then for ||v|| < r, consider

f (x+tv) âˆ’ f (x) â‰¡ h (t)

for t âˆˆ [0, 1] . Then by the chain rule,
hâ€² (t) =

n âˆ‘
n
n
âˆ‘
âˆ‘
âˆ‚f
âˆ‚2f
(x + tv) vk , hâ€²â€² (t) =
(x + tv) vk vj 
âˆ‚xk
âˆ‚xj âˆ‚xk
j=1

k=1

k=1

Then from the Taylor formula stopping at the second derivative, the following theorem can
be obtained.
Theorem 7.7.5 Let f : U â†’ R and let f âˆˆ C 2 (U ) . Then if
B (x,r) âŠ† U,
and ||v|| < r, there exists t âˆˆ (0, 1) such that.
f (x + v) = f (x) +

n
n
n
âˆ‘
1 âˆ‘ âˆ‘ âˆ‚2f
âˆ‚f
(x) vk +
(x + tv) vk vj
âˆ‚xk
2
âˆ‚xj âˆ‚xk
j=1

(7.18)

k=1

k=1

Deï¬nition 7.7.6 Deï¬ne the following matrix.
Hij (x+tv) â‰¡

âˆ‚ 2 f (x+tv)
.
âˆ‚xj âˆ‚xi

It is called the Hessian matrix. From Corollary 7.7.2, this is a symmetric matrix. Then in
terms of this matrix, (7.18) can be written as
f (x + v) = f (x) +

n
âˆ‘
âˆ‚f
1
(x) vk + vT H (x+tv) v
âˆ‚xj
2
j=1

Then this implies f (x + v) =
f (x) +

n
âˆ‘
)
âˆ‚f
1
1(
(x) vk + vT H (x) v+ vT (H (x+tv) âˆ’H (x)) v .
âˆ‚x
2
2
j
j=1

(7.19)

Using the above formula, here is the second derivative test.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.7. SECOND DERIVATIVE TEST

185

Theorem 7.7.7 In the above situation, suppose fxj (x) = 0 for each xj . Then if H (x) has
all positive eigenvalues, x is a local minimum for f . If H (x) has all negative eigenvalues,
then x is a local maximum. If H (x) has a positive eigenvalue, then there exists a direction
in which f has a local minimum at x, while if H (x) has a negative eigenvalue, there exists
a direction in which H (x) has a local maximum at x.
Proof: Since fxj (x) = 0 for each xj , formula (7.19) implies
)
1
1(
f (x + v) = f (x) + vT H (x) v+ vT (H (x+tv) âˆ’H (x)) v
2
2
where H (x) is a symmetric matrix. Thus, by Corollary 7.4.12 H (x) has all real eigenvalues.
Suppose ï¬rst that H (x) has all positive eigenvalues and that all are larger than Î´ 2 > 0.
n
Thenâˆ‘H (x) has an orthonormal basis of eigenvectors, {vi }i=1 and if u is an arbitrary vector,
n
u = j=1 uj vj where uj = u Â· vj . Thus
(
uT H (x) u =

n
âˆ‘

)
uk vkT

ï£«
H (x) ï£­

n
âˆ‘
j=1

k=1

ï£¶
uj vj ï£¸ =

n
âˆ‘

u2j Î»j â‰¥ Î´ 2

j=1

n
âˆ‘

2

u2j = Î´ 2 |u| .

j=1

From (7.19) and the continuity of H, if v is small enough,
1
Î´2
1
2
2
2
|v| .
f (x + v) â‰¥ f (x) + Î´ 2 |v| âˆ’ Î´ 2 |v| = f (x) +
2
4
4
This shows the ï¬rst claim of the theorem. The second claim follows from similar reasoning.
Suppose H (x) has a positive eigenvalue Î»2 . Then let v be an eigenvector for this eigenvalue.
From (7.19),
)
1
1 (
f (x+tv) = f (x) + t2 vT H (x) v+ t2 vT (H (x+tv) âˆ’H (x)) v
2
2
which implies
f (x+tv) =
â‰¥

)
1
1 (
2
f (x) + t2 Î»2 |v| + t2 vT (H (x+tv) âˆ’H (x)) v
2
2
1 2 2 2
f (x) + t Î» |v|
4

whenever t is small enough. Thus in the direction v the function has a local minimum at
x. The assertion about the local maximum in some direction follows similarly. 
This theorem is an analogue of the second derivative test for higher dimensions. As in
one dimension, when there is a zero eigenvalue, it may be impossible to determine from the
Hessian matrix what the local qualitative behavior of the function is. For example, consider
f1 (x, y) = x4 + y 2 , f2 (x, y) = âˆ’x4 + y 2 .
Then Dfi (0, 0) = 0 and for both functions, the Hessian matrix evaluated at (0, 0) equals
(
)
0 0
0 2
but the behavior of the two functions is very diï¬€erent near the origin. The second has a
saddle point while the ï¬rst has a minimum there.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

186

7.8

SPECTRAL THEORY

The Estimation Of Eigenvalues

There are ways to estimate the eigenvalues for matrices. The most famous is known as
Gerschgorinâ€™s theorem. This theorem gives a rough idea where the eigenvalues are just from
looking at the matrix.
Theorem 7.8.1 Let A be an n Ã— n matrix. Consider the n Gerschgorin discs deï¬ned as
ï£±
ï£¼
ï£²
ï£½
âˆ‘
Di â‰¡ Î» âˆˆ C : |Î» âˆ’ aii | â‰¤
|aij | .
ï£³
ï£¾
jÌ¸=i

Then every eigenvalue is contained in some Gerschgorin disc.
This theorem says to add up the absolute values of the entries of the ith row which are
oï¬€ the main diagonal and form the disc centered at aii having this radius. The union of
these discs contains Ïƒ (A) .
Proof: Suppose Ax = Î»x where x Ì¸= 0. Then for A = (aij )
âˆ‘
aij xj = (Î» âˆ’ aii ) xi .
jÌ¸=i

Therefore, picking k such that |xk | â‰¥ |xj | for all xj , it follows that |xk | Ì¸= 0 since |x| Ì¸= 0
and
âˆ‘
âˆ‘
|xk |
|aij | â‰¥
|aij | |xj | â‰¥ |Î» âˆ’ aii | |xk | .
jÌ¸=i

jÌ¸=i

Now dividing by |xk |, it follows Î» is contained in the k th Gerschgorin disc. 
Example 7.8.2 Here is a matrix. Estimate its
ï£«
2 1
ï£­ 3 5
0 1

eigenvalues.
ï£¶
1
0 ï£¸
9

According to Gerschgorinâ€™s theorem the eigenvalues are contained in the disks
D1 = {Î» âˆˆ C : |Î» âˆ’ 2| â‰¤ 2} , D2 = {Î» âˆˆ C : |Î» âˆ’ 5| â‰¤ 3} ,
D3 = {Î» âˆˆ C : |Î» âˆ’ 9| â‰¤ 1}
It is important to observe that these disks are in the complex plane. In general this is the
case. If you want to ï¬nd eigenvalues they will be complex numbers.
iy

x
2

5

9

So what are the values of the eigenvalues? In this case they are real. You can compute
them by graphing the characteristic polynomial, Î»3 âˆ’ 16Î»2 + 70Î» âˆ’ 66 and then zooming in on the zeros. If you do this you ï¬nd the solution is {Î» = 1. 295 3} , {Î» = 5. 590 5} ,
{Î» = 9. 114 2} . Of course these are only approximations and so this information is useless

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.9. ADVANCED THEOREMS

187

for ï¬nding eigenvectors. However, in many applications, it is the size of the eigenvalues
which is important and so these numerical values would be helpful for such applications. In
this case, you might think there is no real reason for Gerschgorinâ€™s theorem. Why not just
compute the characteristic equation and graph and zoom? This is ï¬ne up to a point, but
what if the matrix was huge? Then it might be hard to ï¬nd the characteristic polynomial.
Remember the diï¬ƒculties in expanding a big matrix along a row or column. Also, what if
the eigenvalues were complex? You donâ€™t see these by following this procedure. However,
Gerschgorinâ€™s theorem will at least estimate them.

7.9

Advanced Theorems

More can be said but this requires some theory from complex variables1 . The following is a
fundamental theorem about counting zeros.
Theorem 7.9.1 Let U be a region and let Î³ : [a, b] â†’ U be closed, continuous, bounded
variation, and the winding number, n (Î³, z) = 0 for all z âˆˆ
/ U. Suppose also that f is
analytic on U having zeros a1 , Â· Â· Â· , am where the zeros are repeated according to multiplicity,
and suppose that none of these zeros are on Î³ ([a, b]) . Then
âˆ« â€²
m
âˆ‘
1
f (z)
dz =
n (Î³, ak ) .
2Ï€i Î³ f (z)
âˆm

k=1

Proof: It is given that f (z) = j=1 (z âˆ’ aj ) g (z) where g (z) Ì¸= 0 on U. Hence using
the product rule,
m
f â€² (z) âˆ‘ 1
g â€² (z)
=
+
f (z)
z âˆ’ aj
g (z)
j=1
where

g â€² (z)
g(z)

is analytic on U and so
1
2Ï€i

âˆ«
Î³

âˆ‘
f â€² (z)
1
dz =
n (Î³, aj ) +
f (z)
2Ï€i
j=1
m

âˆ«

âˆ‘
g â€² (z)
dz =
n (Î³, aj ) . 
g (z)
j=1
m

Î³

Now let A be an n Ã— n matrix. Recall that the eigenvalues of A are given by the zeros
of the polynomial, pA (z) = det (zI âˆ’ A) where I is the n Ã— n identity. You can argue
that small changes in A will produce small changes in pA (z) and pâ€²A (z) . Let Î³ k denote a
very small closed circle which winds around zk , one of the eigenvalues of A, in the counter
clockwise direction so that n (Î³ k , zk ) = 1. This circle is to enclose only zk and is to have no
other eigenvalue on it. Then apply Theorem 7.9.1. According to this theorem
âˆ« â€²
1
pA (z)
dz
2Ï€i Î³ pA (z)
is always an integer equal to the multiplicity of zk as a root of pA (t) . Therefore, small
changes in A result in no change to the above contour integral because it must be an integer
and small changes in A result in small changes in the integral. Therefore whenever B is close
enough to A, the two matrices have the same number of zeros inside Î³ k , the zeros being
counted according to multiplicity. By making the radius of the small circle equal to Îµ where
Îµ is less than the minimum distance between any two distinct eigenvalues of A, this shows
that if B is close enough to A, every eigenvalue of B is closer than Îµ to some eigenvalue of
A. 
1 If you havenâ€™t studied the theory of a complex variable, you should skip this section because you wonâ€™t
understand any of it.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

188

SPECTRAL THEORY

Theorem 7.9.2 If Î» is an eigenvalue of A, then if all the entries of B are close enough to
the corresponding entries of A, some eigenvalue of B will be within Îµ of Î».
Consider the situation that A (t) is an n Ã— n matrix and that t â†’ A (t) is continuous for
t âˆˆ [0, 1] .
Lemma 7.9.3 Let Î» (t) âˆˆ Ïƒ (A (t)) for t < 1 and let Î£t = âˆªsâ‰¥t Ïƒ (A (s)) . Also let Kt be the
connected component of Î» (t) in Î£t . Then there exists Î· > 0 such that Kt âˆ© Ïƒ (A (s)) Ì¸= âˆ… for
all s âˆˆ [t, t + Î·] .
Proof: Denote by D (Î» (t) , Î´) the disc centered at Î» (t) having radius Î´ > 0, with other
occurrences of this notation being deï¬ned similarly. Thus
D (Î» (t) , Î´) â‰¡ {z âˆˆ C : |Î» (t) âˆ’ z| â‰¤ Î´} .
Suppose Î´ > 0 is small enough that Î» (t) is the only element of Ïƒ (A (t)) contained in
D (Î» (t) , Î´) and that pA(t) has no zeroes on the boundary of this disc. Then by continuity, and
the above discussion and theorem, there exists Î· > 0, t + Î· < 1, such that for s âˆˆ [t, t + Î·] ,
pA(s) also has no zeroes on the boundary of this disc and A (s) has the same number
of eigenvalues, counted according to multiplicity, in the disc as A (t) . Thus Ïƒ (A (s)) âˆ©
D (Î» (t) , Î´) Ì¸= âˆ… for all s âˆˆ [t, t + Î·] . Now let
âˆª
H=
Ïƒ (A (s)) âˆ© D (Î» (t) , Î´) .
sâˆˆ[t,t+Î·]

It will be shown that H is connected. Suppose not. Then H = P âˆª Q where P, Q are
separated and Î» (t) âˆˆ P. Let s0 â‰¡ inf {s : Î» (s) âˆˆ Q for some Î» (s) âˆˆ Ïƒ (A (s))} . There exists
Î» (s0 ) âˆˆ Ïƒ (A (s0 )) âˆ© D (Î» (t) , Î´) . If Î» (s0 ) âˆˆ
/ Q, then from the above discussion there are
Î» (s) âˆˆ Ïƒ (A (s)) âˆ© Q for s > s0 arbitrarily close to Î» (s0 ) . Therefore, Î» (s0 ) âˆˆ Q which shows
that s0 > t because Î» (t) is the only element of Ïƒ (A (t)) in D (Î» (t) , Î´) and Î» (t) âˆˆ P. Now
let sn â†‘ s0 . Then Î» (sn ) âˆˆ P for any Î» (sn ) âˆˆ Ïƒ (A (sn )) âˆ© D (Î» (t) , Î´) and also it follows from
the above discussion that for some choice of sn â†’ s0 , Î» (sn ) â†’ Î» (s0 ) which contradicts P
and Q separated and nonempty. Since P is nonempty, this shows Q = âˆ…. Therefore, H is
connected as claimed. But Kt âŠ‡ H and so Kt âˆ© Ïƒ (A (s)) Ì¸= âˆ… for all s âˆˆ [t, t + Î·] . 
Theorem 7.9.4 Suppose A (t) is an n Ã— n matrix and that t â†’ A (t) is continuous for
t âˆˆ [0, 1] . Let Î» (0) âˆˆ Ïƒ (A (0)) and deï¬ne Î£ â‰¡ âˆªtâˆˆ[0,1] Ïƒ (A (t)) . Let KÎ»(0) = K0 denote the
connected component of Î» (0) in Î£. Then K0 âˆ© Ïƒ (A (t)) Ì¸= âˆ… for all t âˆˆ [0, 1] .
Proof: Let S â‰¡ {t âˆˆ [0, 1] : K0 âˆ© Ïƒ (A (s)) Ì¸= âˆ… for all s âˆˆ [0, t]} . Then 0 âˆˆ S. Let t0 =
sup (S) . Say Ïƒ (A (t0 )) = Î»1 (t0 ) , Â· Â· Â· , Î»r (t0 ) .
Claim: At least one of these is a limit point of K0 and consequently must be in K0
which shows that S has a last point. Why is this claim true? Let sn â†‘ t0 so sn âˆˆ S.
Now let the discs, D (Î»i (t0 ) , Î´) , i = 1, Â· Â· Â· , r be disjoint with pA(t0 ) having no zeroes on Î³ i
the boundary of D (Î»i (t0 ) , Î´) . Then for n large enough it follows from Theorem 7.9.1 and
the discussion following it that Ïƒ (A (sn )) is contained in âˆªri=1 D (Î»i (t0 ) , Î´). It follows that
K0 âˆ© (Ïƒ (A (t0 )) + D (0, Î´)) Ì¸= âˆ… for all Î´ small enough. This requires at least one of the
Î»i (t0 ) to be in K0 . Therefore, t0 âˆˆ S and S has a last point.
Now by Lemma 7.9.3, if t0 < 1, then K0 âˆª Kt would be a strictly larger connected set
containing Î» (0) . (The reason this would be strictly larger is that K0 âˆ© Ïƒ (A (s)) = âˆ… for
some s âˆˆ (t, t + Î·) while Kt âˆ© Ïƒ (A (s)) Ì¸= âˆ… for all s âˆˆ [t, t + Î·].) Therefore, t0 = 1. 
Corollary 7.9.5 Suppose one of the Gerschgorin discs, Di is disjoint from the union of
the others. Then Di contains an eigenvalue of A. Also, if there are n disjoint Gerschgorin
discs, then each one contains an eigenvalue of A.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.9. ADVANCED THEOREMS

189

( )
Proof: Denote by A (t) the matrix atij where if i Ì¸= j, atij = taij and atii = aii . Thus to
get A (t) multiply all non diagonal terms by t. Let t âˆˆ [0, 1] . Then A (0) = diag (a11 , Â· Â· Â· , ann )
and A (1) = A. Furthermore, the map, t â†’ A (t) is continuous. Denote by Djt the Gerschgorin disc obtained from the j th row for the matrix A (t). Then it is clear that Djt âŠ† Dj
the j th Gerschgorin disc for A. It follows aii is the eigenvalue for A (0) which is contained
in the disc, consisting of the single point aii which is contained in Di . Letting K be the
connected component in Î£ for Î£ deï¬ned in Theorem 7.9.4 which is determined by aii , Gerschgorinâ€™s theorem implies that K âˆ© Ïƒ (A (t)) âŠ† âˆªnj=1 Djt âŠ† âˆªnj=1 Dj = Di âˆª (âˆªjÌ¸=i Dj ) and
also, since K is connected, there are not points of K in both Di and (âˆªjÌ¸=i Dj ) . Since at least
one point of K is in Di ,(aii ), it follows all of K must be contained in Di . Now by Theorem
7.9.4 this shows there are points of K âˆ© Ïƒ (A) in Di . The last assertion follows immediately.

This can be improved even more. This involves the following lemma.
Lemma 7.9.6 In the situation of Theorem 7.9.4 suppose Î» (0) = K0 âˆ© Ïƒ (A (0)) and that
Î» (0) is a simple root of the characteristic equation of A (0). Then for all t âˆˆ [0, 1] ,
Ïƒ (A (t)) âˆ© K0 = Î» (t)
where Î» (t) is a simple root of the characteristic equation of A (t) .
Proof: Let S â‰¡ {t âˆˆ [0, 1] : K0 âˆ© Ïƒ (A (s)) = Î» (s) , a simple eigenvalue for all s âˆˆ [0, t]} .
Then 0 âˆˆ S so it is nonempty. Let t0 = sup (S) and suppose Î»1 Ì¸= Î»2 are two elements of
Ïƒ (A (t0 ))âˆ©K0 . Then choosing Î· > 0 small enough, and letting Di be disjoint discs containing
Î»i respectively, similar arguments to those of Lemma 7.9.3 can be used to conclude
Hi â‰¡ âˆªsâˆˆ[t0 âˆ’Î·,t0 ] Ïƒ (A (s)) âˆ© Di
is a connected and nonempty set for i = 1, 2 which would require that Hi âŠ† K0 . But
then there would be two diï¬€erent eigenvalues of A (s) contained in K0 , contrary to the
deï¬nition of t0 . Therefore, there is at most one eigenvalue Î» (t0 ) âˆˆ K0 âˆ© Ïƒ (A (t0 )) . Could
it be a repeated root of the characteristic equation? Suppose Î» (t0 ) is a repeated root of
the characteristic equation. As before, choose a small disc, D centered at Î» (t0 ) and Î· small
enough that
H â‰¡ âˆªsâˆˆ[t0 âˆ’Î·,t0 ] Ïƒ (A (s)) âˆ© D
is a nonempty connected set containing either multiple eigenvalues of A (s) or else a single
repeated root to the characteristic equation of A (s) . But since H is connected and contains
Î» (t0 ) it must be contained in K0 which contradicts the condition for s âˆˆ S for all these
s âˆˆ [t0 âˆ’ Î·, t0 ] . Therefore, t0 âˆˆ S as hoped. If t0 < 1, there exists a small disc centered
at Î» (t0 ) and Î· > 0 such that for all s âˆˆ [t0 , t0 + Î·] , A (s) has only simple eigenvalues in
D and the only eigenvalues of A (s) which could be in K0 are in D. (This last assertion
follows from noting that Î» (t0 ) is the only eigenvalue of A (t0 ) in K0 and so the others are
at a positive distance from K0 . For s close enough to t0 , the eigenvalues of A (s) are either
close to these eigenvalues of A (t0 ) at a positive distance from K0 or they are close to the
eigenvalue Î» (t0 ) in which case it can be assumed they are in D.) But this shows that t0 is
not really an upper bound to S. Therefore, t0 = 1 and the lemma is proved. 
With this lemma, the conclusion of the above corollary can be sharpened.
Corollary 7.9.7 Suppose one of the Gerschgorin discs, Di is disjoint from the union of
the others. Then Di contains exactly one eigenvalue of A and this eigenvalue is a simple
root to the characteristic polynomial of A.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

190

SPECTRAL THEORY

Proof: In the proof of Corollary 7.9.5, note that aii is a simple root of A (0) since
otherwise the ith Gerschgorin disc would not be disjoint from the others. Also, K, the
connected component determined by aii must be contained in Di because it is connected
and by Gerschgorinâ€™s theorem above, K âˆ© Ïƒ (A (t)) must be contained in the union of the
Gerschgorin discs. Since all the other eigenvalues of A (0) , the ajj , are outside Di , it follows
that K âˆ© Ïƒ (A (0)) = aii . Therefore, by Lemma 7.9.6, K âˆ© Ïƒ (A (1)) = K âˆ© Ïƒ (A) consists of
a single simple eigenvalue. 
Example 7.9.8 Consider the matrix
ï£«

5 1
ï£­ 1 1
0 1

ï£¶
0
1 ï£¸
0

The Gerschgorin discs are D (5, 1) , D (1, 2) , and D (0, 1) . Observe D (5, 1) is disjoint
from the other discs. Therefore, there should be an eigenvalue in D (5, 1) . The actual
eigenvalues are not easy to ï¬nd. They are the roots of the characteristic equation, t3 âˆ’ 6t2 +
3t + 5 = 0. The numerical values of these are âˆ’. 669 66, 1. 423 1, and 5. 246 55, verifying the
predictions of Gerschgorinâ€™s theorem.

7.10

Exercises

1. Explain why it is typically impossible to compute the upper triangular matrix whose
existence is guaranteed by Schurâ€™s theorem.
2. Now recall the QR factorization of Theorem 5.7.5 on Page 133. The QR algorithm
is a technique which does compute the upper triangular matrix in Schurâ€™s theorem.
There is much more to the QR algorithm than will be presented here. In fact, what
I am about to show you is not the way it is done in practice. One ï¬rst obtains what
is called a Hessenburg matrix for which the algorithm will work better. However,
the idea is as follows. Start with A an n Ã— n matrix having real eigenvalues. Form
A = QR where Q is orthogonal and R is upper triangular. (Right triangular.) This
can be done using the technique of Theorem 5.7.5 using Householder matrices. Next
take A1 â‰¡ RQ. Show that A = QA1 QT . In other words these two matrices, A, A1 are
similar. Explain why they have the same eigenvalues. Continue by letting A1 play the
role of A. Thus the algorithm is of the form An = QRn and An+1 = Rn+1 Q. Explain
why A = Qn An QTn for some Qn orthogonal. Thus An is a sequence of matrices each
similar to A. The remarkable thing is that often these matrices converge to an upper
triangular matrix T and A = QT QT for some orthogonal matrix, the limit of the Qn
where the limit means the entries converge. Then the process computes the upper
triangular Schur form of the matrix A. Thus the eigenvalues of A appear on the
diagonal of T. You will see approximately what these are as the process continues.
3. Try the QR algorithm on

(

âˆ’1 âˆ’2
6
6

)

which has eigenvalues 3 and 2. I suggest you use a computer algebra system to do the
computations.
4. Now try the QR algorithm on

(

Saylor URL: http://www.saylor.org/courses/ma212/

0 âˆ’1
2 0

)

The Saylor Foundation

7.10. EXERCISES

191

Show that the algorithm cannot converge for this example. Hint: Try a few iterations
of the algorithm.
(
(
)
)
0 âˆ’1
0 âˆ’2
5. Show the two matrices A â‰¡
and B â‰¡
are similar; that is
4 0
2 0
there exists a matrix S such that A = S âˆ’1 BS but there is no orthogonal matrix
Q such that QT BQ = A. Show the QR algorithm does converge for the matrix B
although it fails to do so for A.
6. Let F be an m Ã— n matrix. Show that F âˆ— F has all real eigenvalues and furthermore,
they are all nonnegative.
7. If A is a real n Ã— n matrix and Î» is a complex eigenvalue Î» = a + ib, b Ì¸= 0, of A having
eigenvector z + iw, show that w Ì¸= 0.
8. Suppose A = QT DQ where Q is an orthogonal matrix and all the matrices are real.
Also D is a diagonal matrix. Show that A must be symmetric.
9. Suppose A is an n Ã— n matrix and there exists a unitary matrix U such that
A = U âˆ— DU
where D is a diagonal matrix. Explain why A must be normal.
10. If A is Hermitian, show that det (A) must be real.
11. Show that every unitary matrix preserves distance. That is, if U is unitary,
|U x| = |x| .
12. Show that if a matrix does preserve distances, then it must be unitary.
13. â†‘Show that a complex normal matrix A is unitary if and only if its eigenvalues have
magnitude equal to 1.
14. Suppose A is an n Ã— n matrix which is diagonally dominant. Recall this means
âˆ‘
|aij | < |aii |
jÌ¸=i

show Aâˆ’1 must exist.
15. Give some disks in the complex plane whose union contains all the eigenvalues of the
matrix
ï£«
ï£¶
1 + 2i 4 2
ï£­
0
i 3 ï£¸
5
6 7
16. Show a square matrix is invertible if and only if it has no zero eigenvalues.
17. Using Schurâ€™s theorem, show the trace of an n Ã— n matrix equals the sum of the
eigenvalues and the determinant of an n Ã— n matrix is the product of the eigenvalues.
18. Using Schurâ€™s theorem, show that if A is any
nâˆ‘
Ã— n matrix having eigenvalues
âˆ‘ complex
2
n
2
{Î»i } listed according to multiplicity, then i,j |Aij | â‰¥ i=1 |Î»i | . Show that equality
holds if and only if A is normal. This inequality is called Schurâ€™s inequality. [19]

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

192

SPECTRAL THEORY

ï£«

19. Here is a matrix.

1234
ï£¬ 0
ï£¬
ï£­ 98
56

6
5
âˆ’654
9
123 10, 000
78
98

ï£¶
3
123 ï£·
ï£·
11 ï£¸
400

I know this matrix has an inverse before doing any computations. How do I know?
20. Show the critical points of the following function are
(
)
1
(0, âˆ’3, 0) , (2, âˆ’3, 0) , and 1, âˆ’3, âˆ’
3
and classify them as local minima, local maxima or saddle points.
f (x, y, z) = âˆ’ 32 x4 + 6x3 âˆ’ 6x2 + zx2 âˆ’ 2zx âˆ’ 2y 2 âˆ’ 12y âˆ’ 18 âˆ’ 32 z 2 .
21. Here is a function of three variables.
f (x, y, z) = 13x2 + 2xy + 8xz + 13y 2 + 8yz + 10z 2
change the variables so that in the new variables there are no mixed terms, terms
involving xy, yz etc. Two eigenvalues are 12 and 18.
22. Here is a function of three variables.
f (x, y, z) = 2x2 âˆ’ 4x + 2 + 9yx âˆ’ 9y âˆ’ 3zx + 3z + 5y 2 âˆ’ 9zy âˆ’ 7z 2
change the variables so that in the new variables there are no mixed terms, terms
involving xy, yz etc. The eigenvalues of the matrix which you will work with are
19
âˆ’ 17
2 , 2 , âˆ’1.
23. Here is a function of three variables.
f (x, y, z) = âˆ’x2 + 2xy + 2xz âˆ’ y 2 + 2yz âˆ’ z 2 + x
change the variables so that in the new variables there are no mixed terms, terms
involving xy, yz etc.
24. Show the critical points of the function,
f (x, y, z) = âˆ’2yx2 âˆ’ 6yx âˆ’ 4zx2 âˆ’ 12zx + y 2 + 2yz.
are points of the form,
(
)
(x, y, z) = t, 2t2 + 6t, âˆ’t2 âˆ’ 3t
for t âˆˆ R and classify them as local minima, local maxima or saddle points.
25. Show the critical points of the function
f (x, y, z) =

1
1 4
x âˆ’ 4x3 + 8x2 âˆ’ 3zx2 + 12zx + 2y 2 + 4y + 2 + z 2 .
2
2

are (0, âˆ’1, 0) , (4, âˆ’1, 0) , and (2, âˆ’1, âˆ’12) and classify them as local minima, local
maxima or saddle points.
26. Let f (x, y) = 3x4 âˆ’ 24x2 + 48 âˆ’ yx2 + 4y. Find and classify the critical points using
the second derivative test.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.10. EXERCISES

193

27. Let f (x, y) = 3x4 âˆ’ 5x2 + 2 âˆ’ y 2 x2 + y 2 . Find and classify the critical points using the
second derivative test.
28. Let f (x, y) = 5x4 âˆ’ 7x2 âˆ’ 2 âˆ’ 3y 2 x2 + 11y 2 âˆ’ 4y 4 . Find and classify the critical points
using the second derivative test.
29. Let f (x, y, z) = âˆ’2x4 âˆ’ 3yx2 + 3x2 + 5x2 z + 3y 2 âˆ’ 6y + 3 âˆ’ 3zy + 3z + z 2 . Find and
classify the critical points using the second derivative test.
30. Let f (x, y, z) = 3yx2 âˆ’ 3x2 âˆ’ x2 z âˆ’ y 2 + 2y âˆ’ 1 + 3zy âˆ’ 3z âˆ’ 3z 2 . Find and classify
the critical points using the second derivative test.
31. Let Q be orthogonal. Find the possible values of det (Q) .
32. Let U be unitary. Find the possible values of det (U ) .
33. If a matrix is nonzero can it have only zero for eigenvalues?
34. A matrix A is called nilpotent if Ak = 0 for some positive integer k. Suppose A is a
nilpotent matrix. Show it has only 0 for an eigenvalue.
35. If A is a nonzero nilpotent matrix, show it must be defective.
36. Suppose A is a nondefective n Ã— n matrix and its eigenvalues are all either 0 or 1.
Show A2 = A. Could you say anything interesting if the eigenvalues were all either
0,1,or âˆ’1? By DeMoivreâ€™s theorem, an nth root of unity is of the form
(
(
)
(
))
2kÏ€
2kÏ€
cos
+ i sin
n
n
Could you generalize the sort of thing just described to get An = A? Hint: Since A
is nondefective, there exists S such that S âˆ’1 AS = D where D is a diagonal matrix.
37. This and the following problems will present most of a diï¬€erential equations course.
Most of the explanations are given. You ï¬ll in any details needed. To begin with,
consider the scalar initial value problem
y â€² = ay, y (t0 ) = y0
When a is real, show the unique solution to this problem is y = y0 ea(tâˆ’t0 ) . Next
suppose
y â€² = (a + ib) y, y (t0 ) = y0
(7.20)
where y (t) = u (t) + iv (t) . Show there exists a unique solution and it is given by
y (t) =
y0 ea(tâˆ’t0 ) (cos b (t âˆ’ t0 ) + i sin b (t âˆ’ t0 )) â‰¡ e(a+ib)(tâˆ’t0 ) y0 .
(7.21)
Next show that for a real or complex there exists a unique solution to the initial value
problem
y â€² = ay + f, y (t0 ) = y0
and it is given by

âˆ«

t

y (t) = ea(tâˆ’t0 ) y0 + eat

eâˆ’as f (s) ds.

t0

Hint: For the ï¬rst part write as y â€² âˆ’ ay = 0 and multiply both sides by eâˆ’at . Then
explain why you get
)
d ( âˆ’at
e y (t) = 0, y (t0 ) = 0.
dt

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

194

SPECTRAL THEORY

Now you ï¬nish the argument. To show uniqueness in the second part, suppose
y â€² = (a + ib) y, y (t0 ) = 0
and verify this requires y (t) = 0. To do this, note
y â€² = (a âˆ’ ib) y, y (t0 ) = 0
2

and that |y| (t0 ) = 0 and
d
2
|y (t)| = y â€² (t) y (t) + y â€² (t) y (t)
dt
2

= (a + ib) y (t) y (t) + (a âˆ’ ib) y (t) y (t) = 2a |y (t)| .
Thus from the ï¬rst part |y (t)| = 0eâˆ’2at = 0. Finally observe by a simple computation
that (7.20) is solved by (7.21). For the last part, write the equation as
2

y â€² âˆ’ ay = f
and multiply both sides by eâˆ’at and then integrate from t0 to t using the initial
condition.
38. Now consider A an n Ã— n matrix. By Schurâ€™s theorem there exists unitary Q such that
Qâˆ’1 AQ = T
where T is upper triangular. Now consider the ï¬rst order initial value problem
xâ€² = Ax, x (t0 ) = x0 .
Show there exists a unique solution to this ï¬rst order system. Hint: Let y = Qâˆ’1 x
and so the system becomes
yâ€² = T y, y (t0 ) = Qâˆ’1 x0

(7.22)

T

Now letting y = (y1 , Â· Â· Â· , yn ) , the bottom equation becomes
(
)
ynâ€² = tnn yn , yn (t0 ) = Qâˆ’1 x0 n .
Then use the solution you get in this to get the solution to the initial value problem
which occurs one level up, namely
(
)
â€²
= t(nâˆ’1)(nâˆ’1) ynâˆ’1 + t(nâˆ’1)n yn , ynâˆ’1 (t0 ) = Qâˆ’1 x0 nâˆ’1
ynâˆ’1
Continue doing this to obtain a unique solution to (7.22).
39. Now suppose Î¦ (t) is an n Ã— n matrix of the form
(
)
Î¦ (t) = x1 (t) Â· Â· Â· xn (t)
where
Explain why

(7.23)

xâ€²k (t) = Axk (t) .
Î¦â€² (t) = AÎ¦ (t)

if and only if Î¦ (t) is given in the form of (7.23). Also explain why if c âˆˆ Fn , y (t) â‰¡
Î¦ (t) c solves the equation yâ€² (t) = Ay (t) .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.10. EXERCISES

195

40. In the above problem, consider the question whether all solutions to
xâ€² = Ax

(7.24)

are obtained in the form Î¦ (t) c for some choice of c âˆˆ Fn . In other words, is the
general solution to this equation Î¦ (t) c for c âˆˆ Fn ? Prove the following theorem using
linear algebra.
Theorem 7.10.1 Suppose Î¦ (t) is an n Ã— n matrix which satisï¬es Î¦â€² (t) = AÎ¦ (t) .
âˆ’1
Then the general solution to (7.24) is Î¦ (t) c if and only if Î¦ (t) exists for some t.
âˆ’1
âˆ’1
â€²
Furthermore, if Î¦ (t) = AÎ¦ (t) , then either Î¦ (t) exists for all t or Î¦ (t) never
exists for any t.
(det (Î¦ (t)) is called the Wronskian and this theorem is sometimes called the Wronskian
alternative.)
Hint: Suppose ï¬rst the general solution is of the form Î¦ (t) c where c is an arbitrary
âˆ’1
constant vector in Fn . You need to verify Î¦ (t)
exists for some t. In fact, show
âˆ’1
âˆ’1
Î¦ (t) exists for every t. Suppose then that Î¦ (t0 ) does not exist. Explain why
n
there exists c âˆˆ F such that there is no solution x to the equation c = Î¦ (t0 ) x. By
the existence part of Problem 38 there exists a solution to
xâ€² = Ax, x (t0 ) = c
âˆ’1

but this cannot be in the form Î¦ (t) c. Thus for every t, Î¦ (t) exists. Next suppose
âˆ’1
for some t0 , Î¦ (t0 ) exists. Let zâ€² = Az and choose c such that
z (t0 ) = Î¦ (t0 ) c
Then both z (t) , Î¦ (t) c solve
xâ€² = Ax, x (t0 ) = z (t0 )
Apply uniqueness to conclude z = Î¦ (t) c. Finally, consider that Î¦ (t) c for c âˆˆ Fn
âˆ’1
either is the general solution or it is not the general solution. If it is, then Î¦ (t)
âˆ’1
exists for all t. If it is not, then Î¦ (t) cannot exist for any t from what was just
shown.
41. Let Î¦â€² (t) = AÎ¦ (t) . Then Î¦ (t) is called a fundamental matrix if Î¦ (t)
t. Show there exists a unique solution to the equation
xâ€² = Ax + f , x (t0 ) = x0

âˆ’1

exists for all

(7.25)

and it is given by the formula
âˆ’1

x (t) = Î¦ (t) Î¦ (t0 )

âˆ«

t

x0 + Î¦ (t)

Î¦ (s)

âˆ’1

f (s) ds

t0

Now these few problems have done virtually everything of signiï¬cance in an entire undergraduate diï¬€erential equations course, illustrating the superiority of linear algebra.
The above formula is called the variation of constants formula.
Hint: Uniquenss is easy. If x1 , x2 are two solutions then let u (t) = x1 (t) âˆ’ x2 (t) and
argue uâ€² = Au, u (t0 ) = 0. Then use Problem 38. To verify there exists a solution, you

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

196

SPECTRAL THEORY

could just diï¬€erentiate the above formula using the fundamental theorem of calculus
and verify it works. Another way is to assume the solution in the form
x (t) = Î¦ (t) c (t)
and ï¬nd c (t) to make it all work out. This is called the method of variation of
parameters.
42. Show there exists a special Î¦ such that Î¦â€² (t) = AÎ¦ (t) , Î¦ (0) = I, and suppose
âˆ’1
Î¦ (t) exists for all t. Show using uniqueness that
Î¦ (âˆ’t) = Î¦ (t)

âˆ’1

and that for all t, s âˆˆ R
Î¦ (t + s) = Î¦ (t) Î¦ (s)
Explain why with this special Î¦, the solution to (7.25) can be written as
âˆ« t
x (t) = Î¦ (t âˆ’ t0 ) x0 +
Î¦ (t âˆ’ s) f (s) ds.
t0

Hint: Let Î¦ (t) be such that the j th column is xj (t) where
xâ€²j = Axj , xj (0) = ej .
Use uniqueness as required.
43. You can see more on this problem and the next one in the latest version of Horn
and Johnson, [16]. Two n Ã— n matrices A, B are said to be congruent if there is an
invertible P such that
B = P AP âˆ—
Let A be a Hermitian matrix. Thus it has all real eigenvalues. Let n+ be the number
of positive eigenvalues, nâˆ’ , the number of negative eigenvalues and n0 the number of
zero eigenvalues. For k a positive integer, let Ik denote the k Ã— k identity matrix and
Ok the k Ã— k zero matrix. Then the inertia matrix of A is the following block diagonal
n Ã— n matrix.
ï£«
ï£¶
In+
ï£­
ï£¸
Inâˆ’
On0
Show that A is congruent to its inertia matrix. Next show that congruence is an
equivalence relation. Finally, show that if two Hermitian matrices have the same
inertia matrix, then they must be congruent. Hint: First recall that there is a
unitary matrix, U such that
ï£«
ï£¶
Dn +
ï£¸
Dnâˆ’
U âˆ— AU = ï£­
On 0
where the Dn+ is a diagonal matrix having the positive eigenvalues of A, Dnâˆ’ being
deï¬ned similarly. Now let Dnâˆ’ denote the diagonal matrix which replaces each entry
of Dnâˆ’ with its absolute value. Consider the two diagonal matrices
ï£«
ï£¶
âˆ’1/2
Dn+
ï£¬
ï£·
âˆ’1/2
D = Dâˆ— = ï£­
ï£¸
Dnâˆ’
In 0
Now consider Dâˆ— U âˆ— AU D.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

7.10. EXERCISES

197

44. Show that if A, B are two congruent Hermitian matrices, then they have the same
inertia matrix. Hint: Let A = SBS âˆ— where S is invertible. Show that A, B have the
same rank and this implies that they are each unitarily similar to a diagonal matrix
which has the same number of zero entries on the main diagonal. Therefore, letting
VA be the span of the eigenvectors associated with positive eigenvalues of A and VB
being deï¬ned similarly, it suï¬ƒces to show that these have the same dimensions. Show
that (Ax, x) > 0 for all x âˆˆ VA . Next consider S âˆ— VA . For x âˆˆ VA , explain why
(
)
âˆ’1
(BS âˆ— x,S âˆ— x) =
S âˆ’1 A (S âˆ— ) S âˆ— x,S âˆ— x
)
(
) (
(
)âˆ—
= S âˆ’1 Ax,S âˆ— x = Ax, S âˆ’1 S âˆ— x = (Ax, x) > 0
Next explain why this shows that S âˆ— VA is a subspace of VB and so the dimension of VB
is at least as large as the dimension of VA . Hence there are at least as many positive
eigenvalues for B as there are for A. Switching A, B you can turn the inequality
around. Thus the two have the same inertia matrix.
45. Let A be an m Ã— n matrix. Then if you unraveled it, you could consider it as a vector
in Cnm . The Frobenius inner product on the vector space of m Ã— n matrices is deï¬ned
as
(A, B) â‰¡ trace (AB âˆ— )
Show that this really does satisfy the axioms of an inner product space and that it
also amounts to nothing more than considering m Ã— n matrices as vectors in Cnm .
46. â†‘Consider the n Ã— n unitary matrices. Show that whenever U is such a matrix, it
follows that
âˆš
|U |Cnn = n
Next explain why if {Uk } is any sequence of unitary matrices, there exists a subseâˆ
quence {Ukm }m=1 such that limmâ†’âˆ Ukm = U where U is unitary. Here the limit
takes place in the sense that the entries of Ukm converge to the corresponding entries
of U .
47. â†‘Let A, B be two n Ã— n matrices. Denote by Ïƒ (A) the set of eigenvalues of A. Deï¬ne
dist (Ïƒ (A) , Ïƒ (B)) = max min {|Î» âˆ’ Âµ| : Âµ âˆˆ Ïƒ (B)}
Î»âˆˆÏƒ(A)

Explain why dist (Ïƒ (A) , Ïƒ (B)) is small if and only if every eigenvalue of A is close
to some eigenvalue of B. Now prove the following theorem using the above problem
and Schurâ€™s theorem. This theorem says roughly that if A is close to B then the
eigenvalues of A are close to those of B in the sense that every eigenvalue of A is close
to an eigenvalue of B.
Theorem 7.10.2 Suppose limkâ†’âˆ Ak = A. Then
lim dist (Ïƒ (Ak ) , Ïƒ (A)) = 0

kâ†’âˆ

(

)
a b
48. Let A =
be a 2 Ã— 2 matrix which is not a multiple of the identity. Show
c d
that A is similar to a 2 Ã— 2 matrix which has at least one diagonal entry equal to 0.
Hint: First note that there exists a vector a such that Aa is not a multiple of a. Then
consider
(
)âˆ’1 (
)
B = a Aa
A a Aa
Show B has a zero on the main diagonal.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

198

SPECTRAL THEORY

49. â†‘ Let A be a complex n Ã— n matrix which has trace equal to 0. Show that A is similar
to a matrix which has all zeros on the main diagonal. Hint: Use Problem 30 on
Page 122 to argue that you can say that a given matrix is similar to one which has
the diagonal entries permuted in any order desired. Then use the above problem and
block multiplication to show that if the A has k nonzero entries, then it is similar to
a matrix which has k âˆ’ 1 nonzero entries. Finally, when A is similar to one which has
at most one nonzero entry, this one must also be zero because of the condition on the
trace.
50. â†‘An n Ã— n matrix X is a comutator if there are n Ã— n matrices A, B such that X =
AB âˆ’ BA. Show that the trace of any comutator is 0. Next show that if a complex
matrix X has trace equal to 0, then it is in fact a comutator. Hint: Use the above
problem to show that it suï¬ƒces to consider X having all zero entries on the main
diagonal. Then deï¬ne
ï£«
ï£¶
1
0
{
ï£¬
ï£·
Xij
2
ï£¬
ï£·
iâˆ’j if i Ì¸= j
,
B
=
A=ï£¬
ï£·
ij
..
ï£­
ï£¸
0 if i = j
.
0
n

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Vector Spaces And Fields
8.1

Vector Space Axioms

It is time to consider the idea of a Vector space.
Deï¬nition 8.1.1 A vector space is an Abelian group of â€œvectorsâ€ satisfying the axioms of
an Abelian group,
v + w = w + v,
the commutative law of addition,
(v + w) + z = v+ (w + z) ,
the associative law for addition,
v + 0 = v,
the existence of an additive identity,
v+ (âˆ’v) = 0,
the existence of an additive inverse, along with a ï¬eld of â€œscalarsâ€, F which are allowed to
multiply the vectors according to the following rules. (The Greek letters denote scalars.)
Î± (v + w) = Î±v+Î±w,

(8.1)

(Î± + Î²) v =Î±v+Î²v,

(8.2)

Î± (Î²v) = Î±Î² (v) ,

(8.3)

1v = v.

(8.4)

The ï¬eld of scalars is usually R or C and the vector space will be called real or complex
depending on whether the ï¬eld is R or C. However, other ï¬elds are also possible. For
example, one could use the ï¬eld of rational numbers or even the ï¬eld of the integers mod p
for p a prime. A vector space is also called a linear space.
For example, Rn with the usual conventions is an example of a real vector space and Cn
is an example of a complex vector space. Up to now, the discussion has been for Rn or Cn
and all that is taking place is an increase in generality and abstraction.
There are many examples of vector spaces.

199

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

200

VECTOR SPACES AND FIELDS

Example 8.1.2 Let â„¦ be a nonempty set and let V consist of all functions deï¬ned on â„¦
which have values in some ï¬eld F. The vector operations are deï¬ned as follows.
(f + g) (x)
(Î±f ) (x)

=
=

f (x) + g (x)
Î±f (x)

Then it is routine to verify that V with these operations is a vector space.
Note that Fn actually ï¬ts in to this framework. You consider the set â„¦ to be {1, 2, Â· Â· Â· , n}
and then the mappings from â„¦ to F give the elements of Fn . Thus a typical vector can be
considered as a function.
Example 8.1.3 Generalize the above example by letting V denote all functions deï¬ned on
â„¦ which have values in a vector space W which has ï¬eld of scalars F. The deï¬nitions of
scalar multiplication and vector addition are identical to those of the above example.

8.2
8.2.1

Subspaces And Bases
Basic Deï¬nitions

Deï¬nition 8.2.1 If {v1 , Â· Â· Â· , vn } âŠ† V, a vector space, then
{ n
}
âˆ‘
Î± i vi : Î± i âˆˆ F .
span (v1 , Â· Â· Â· , vn ) â‰¡
i=1

A subset, W âŠ† V is said to be a subspace if it is also a vector space with the same ï¬eld of
scalars. Thus W âŠ† V is a subspace if ax + by âˆˆ W whenever a, b âˆˆ F and x, y âˆˆ W. The
span of a set of vectors as just described is an example of a subspace.
Example 8.2.2 Consider the real valued functions deï¬ned on an interval [a, b]. A subspace
is the set of continuous real valued functions deï¬ned on the interval. Another subspace is
the set of polynomials of degree no more than 4.
Deï¬nition 8.2.3 If {v1 , Â· Â· Â· , vn } âŠ† V, the set of vectors is linearly independent if
n
âˆ‘

Î± i vi = 0

i=1

implies
Î±1 = Â· Â· Â· = Î±n = 0
and {v1 , Â· Â· Â· , vn } is called a basis for V if
span (v1 , Â· Â· Â· , vn ) = V
and {v1 , Â· Â· Â· , vn } is linearly independent. The set of vectors is linearly dependent if it is not
linearly independent.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

8.2. SUBSPACES AND BASES

8.2.2

201

A Fundamental Theorem

The next theorem is called the exchange theorem. It is very important that you understand
this theorem. It is so important that I have given several proofs of it. Some amount to the
same thing, just worded diï¬€erently.
Theorem 8.2.4 Let {x1 , Â· Â· Â· , xr } be a linearly independent set of vectors such that each xi
is in the span{y1 , Â· Â· Â· , ys } . Then r â‰¤ s.
Proof 1:
that

Deï¬ne span{y1 , Â· Â· Â· , ys } â‰¡ V, it follows there exist scalars c1 , Â· Â· Â· , cs such
x1 =

s
âˆ‘

ci yi .

(8.5)

i=1

Not all of these scalars can equal zero because if this were the case, it would follow that
x
âˆ‘1 r= 0 and so {x1 , Â· Â· Â· , xr } would not be linearly independent. Indeed, if x1 = 0, 1x1 +
i=2 0xi = x1 = 0 and so there would exist a nontrivial linear combination of the vectors
{x1 , Â· Â· Â· , xr } which equals zero.
Say ck Ì¸= 0. Then solve (8.5) for yk and obtain
ï£«
ï£¶
s-1 vectors here
z
}|
{
yk âˆˆ span ï£­x1 , y1 , Â· Â· Â· , ykâˆ’1 , yk+1 , Â· Â· Â· , ys ï£¸ .
Deï¬ne {z1 , Â· Â· Â· , zsâˆ’1 } by
{z1 , Â· Â· Â· , zsâˆ’1 } â‰¡ {y1 , Â· Â· Â· , ykâˆ’1 , yk+1 , Â· Â· Â· , ys }
Therefore, span {x1 , z1 , Â· Â· Â· , zsâˆ’1 } = V because if v âˆˆ V, there exist constants c1 , Â· Â· Â· , cs
such that
sâˆ’1
âˆ‘
v=
ci zi + cs yk .
i=1

Now replace the yk in the above with a linear combination of the vectors, {x1 , z1 , Â· Â· Â· , zsâˆ’1 }
to obtain v âˆˆ span {x1 , z1 , Â· Â· Â· , zsâˆ’1 } . The vector yk , in the list {y1 , Â· Â· Â· , ys } , has now been
replaced with the vector x1 and the resulting modiï¬ed list of vectors has the same span as
the original list of vectors, {y1 , Â· Â· Â· , ys } .
Now suppose that r > s and that span {x1 , Â· Â· Â· , xl , z1 , Â· Â· Â· , zp } = V where the vectors,
z1 , Â· Â· Â· , zp are each taken from the set, {y1 , Â· Â· Â· , ys } and l + p = s. This has now been done
for l = 1 above. Then since r > s, it follows that l â‰¤ s < r and so l + 1 â‰¤ r. Therefore, xl+1
is a vector not in the list, {x1 , Â· Â· Â· , xl } and since span {x1 , Â· Â· Â· , xl , z1 , Â· Â· Â· , zp } = V there
exist scalars ci and dj such that
xl+1 =

l
âˆ‘
i=1

ci xi +

p
âˆ‘

dj zj .

(8.6)

j=1

Now not all the dj can equal zero because if this were so, it would follow that {x1 , Â· Â· Â· , xr }
would be a linearly dependent set because one of the vectors would equal a linear combination
of the others. Therefore, ((8.6)) can be solved for one of the zi , say zk , in terms of xl+1 and
the other zi and just as in the above argument, replace that zi with xl+1 to obtain
ï£«
ï£¶
p-1 vectors here
z
}|
{
span ï£­x1 , Â· Â· Â· xl , xl+1 , z1 , Â· Â· Â· zkâˆ’1 , zk+1 , Â· Â· Â· , zp ï£¸ = V.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

202

VECTOR SPACES AND FIELDS

Continue this way, eventually obtaining
span (x1 , Â· Â· Â· , xs ) = V.
But then xr âˆˆ span {x1 , Â· Â· Â· , xs } contrary to the assumption that {x1 , Â· Â· Â· , xr } is linearly
independent. Therefore, r â‰¤ s as claimed.
Proof 2: Let
s
âˆ‘
xk =
ajk yj
j=1

If r > s, then the matrix A = (ajk ) has more columns than rows. By Corollary 4.3.9
one of these columns is a linear combination of the others. This implies there exist scalars
c1 , Â· Â· Â· , cr , not all zero such that
r
âˆ‘

ajk ck = 0, j = 1, Â· Â· Â· , r

k=1

Then

r
âˆ‘
k=1

ck xk =

r
âˆ‘
k=1

ck

s
âˆ‘

ajk yj =

j=1

( r
s
âˆ‘
âˆ‘
j=1

)
ck ajk

yj = 0

k=1

which contradicts the assumption that {x1 , Â· Â· Â· , xr } is linearly independent. Hence r â‰¤ s.
Proof 3: Suppose r > s. Let zk denote a vector of {y1 , Â· Â· Â· , ys } . Thus there exists j as
small as possible such that
span (y1 , Â· Â· Â· , ys ) = span (x1 , Â· Â· Â· , xm , z1 , Â· Â· Â· , zj )
where m + j = s. It is given that m = 0, corresponding to no vectors of {x1 , Â· Â· Â· , xm } and
j = s, corresponding to all the yk results in the above equation holding. If j > 0 then m < s
and so
j
m
âˆ‘
âˆ‘
xm+1 =
ak xk +
bi zi
k=1

i=1

Not all the bi can equal 0 and so you can solve for one of them in terms of xm+1 , xm , Â· Â· Â· , x1 ,
and the other zk . Therefore, there exists
{z1 , Â· Â· Â· , zjâˆ’1 } âŠ† {y1 , Â· Â· Â· , ys }
such that
span (y1 , Â· Â· Â· , ys ) = span (x1 , Â· Â· Â· , xm+1 , z1 , Â· Â· Â· , zjâˆ’1 )
contradicting the choice of j. Hence j = 0 and
span (y1 , Â· Â· Â· , ys ) = span (x1 , Â· Â· Â· , xs )
It follows that
xs+1 âˆˆ span (x1 , Â· Â· Â· , xs )
contrary to the assumption the xk are linearly independent. Therefore, r â‰¤ s as claimed. 
Corollary 8.2.5 If {u1 , Â· Â· Â· , um } and {v1 , Â· Â· Â· , vn } are two bases for V, then m = n.
Proof: By Theorem 8.2.4, m â‰¤ n and n â‰¤ m. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

8.2. SUBSPACES AND BASES

203

Deï¬nition 8.2.6 A vector space V is of dimension n if it has a basis consisting of n vectors.
This is well deï¬ned thanks to Corollary 8.2.5. It is always assumed here that n < âˆ and in
this case, such a vector space is said to be ï¬nite dimensional.
Example 8.2.7 Consider the polynomials deï¬ned
on R }of degree no more than 3, denoted
{
here as P3 . Then show that a basis for P3 is 1, x, x2 , x3 . Here xk symbolizes the function
x 7â†’ xk .
It is obvious that the span of the given vectors yields P3 . Why is this set of vectors
linearly independent? Suppose
c0 + c1 x + c2 x2 + c3 x3 = 0
where 0 is the zero function which maps everything to 0. Then you could diï¬€erentiate three
times and obtain the following equations
c1 + 2c2 x + 3c3 x2 =
2c2 + 6c3 x =
6c3

0
0

=

0

Now this implies c3 = 0. Then from the equations above the bottom one, you ï¬nd in
succession that c2 = 0, c1 = 0, c0 = 0.
There is a somewhat interesting theorem about linear independence of smooth functions
(those having plenty of derivatives) which I will show now. It is often used in diï¬€erential
equations.
Deï¬nition 8.2.8 Let f1 , Â· Â· Â· , fn be smooth functions deï¬ned on an interval [a, b] . The
Wronskian of these functions is deï¬ned as follows.
f1 (x)
f1â€² (x)
..
.

W (f1 , Â· Â· Â· , fn ) (x) â‰¡

(nâˆ’1)

f1

f2 (x)
f2â€² (x)
..
.
(nâˆ’1)

(x) f2

(x)

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·

fn (x)
fnâ€² (x)
..
.
(nâˆ’1)

fn

(x)

Note that to get from one row to the next, you just diï¬€erentiate everything in that row. The
notation f (k) (x) denotes the k th derivative.
With this deï¬nition, the following is the theorem. The interesting theorem involving the
Wronskian has to do with the situation where the functions are solutions of a diï¬€erential
equation. Then much more can be said and it is much more interesting than the following
theorem.
Theorem 8.2.9 Let {f1 , Â· Â· Â· , fn } be smooth functions deï¬ned on [a, b] . Then they are linearly independent if there exists some point t âˆˆ [a, b] where W (f1 , Â· Â· Â· , fn ) (t) Ì¸= 0.
Proof: Form the linear combination of these vectors (functions) and suppose it equals
0. Thus
a 1 f1 + a 2 f2 + Â· Â· Â· + a n fn = 0
The question you must answer is whether this requires each aj to equal zero. If they all
must equal 0, then this means these vectors (functions) are independent. This is what it
means to be linearly independent.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

204

VECTOR SPACES AND FIELDS

Diï¬€erentiate the above equation n âˆ’ 1 times yielding the equations
ï£«
ï£¶
a 1 f1 + a 2 f2 + Â· Â· Â· + a n fn = 0
ï£¬
ï£·
a1 f1â€² + a2 f2â€² + Â· Â· Â· + an fnâ€² = 0
ï£¬
ï£·
..
ï£¬
ï£·
ï£­
ï£¸
.
(nâˆ’1)

a1 f1

(nâˆ’1)

+ a 2 f2

Now plug in t. Then the above yields
ï£«
f1 (t)
f2 (t)
ï£¬ f1â€² (t)
f2â€² (t)
ï£¬
..
..
ï£¬
ï£­
.
.
(nâˆ’1)
(nâˆ’1)
f1
(t) f2
(t)

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·

(nâˆ’1)

+ Â· Â· Â· + a n fn

fn (t)
fnâ€² (t)
..
.
(nâˆ’1)

fn

ï£¶ï£«
ï£·ï£¬
ï£·ï£¬
ï£·ï£¬
ï£¸ï£­

(t)

=0

a1
a2
..
.

ï£¶

ï£«

ï£· ï£¬
ï£· ï£¬
ï£·=ï£¬
ï£¸ ï£­

an

0
0
..
.

ï£¶
ï£·
ï£·
ï£·
ï£¸

0

Since the determinant of the matrix on the left is assumed to be nonzero, it follows this
matrix has an inverse and so the only solution to the above system of equations is to have
each ak = 0. 
Here is a useful lemma.
Lemma 8.2.10 Suppose v âˆˆ
/ span (u1 , Â· Â· Â· , uk ) and {u1 , Â· Â· Â· , uk } is linearly independent.
Then {u1 , Â· Â· Â· , uk , v} is also linearly independent.
âˆ‘k
Proof: Suppose i=1 ci ui + dv = 0. It is required to verify that each ci = 0 and that
d = 0. But if d Ì¸= 0, then you can solve for v as a linear combination of the vectors,
{u1 , Â· Â· Â· , uk },
k ( )
âˆ‘
ci
v=âˆ’
ui
d
i=1
âˆ‘k
contrary to assumption. Therefore, d = 0. But then i=1 ci ui = 0 and the linear independence of {u1 , Â· Â· Â· , uk } implies each ci = 0 also. 
Given a spanning set, you can delete vectors till you end up with a basis. Given a linearly
independent set, you can add vectors till you get a basis. This is what the following theorem
is about, weeding and planting.
Theorem 8.2.11 If V = span (u1 , Â· Â· Â· , un ) then some subset of {u1 , Â· Â· Â· , un } is a basis for
V. Also, if {u1 , Â· Â· Â· , uk } âŠ† V is linearly independent and the vector space is ï¬nite dimensional, then the set, {u1 , Â· Â· Â· , uk }, can be enlarged to obtain a basis of V.
Proof: Let
S = {E âŠ† {u1 , Â· Â· Â· , un } such that span (E) = V }.
For E âˆˆ S, let |E| denote the number of elements of E. Let
m â‰¡ min{|E| such that E âˆˆ S}.
Thus there exist vectors
{v1 , Â· Â· Â· , vm } âŠ† {u1 , Â· Â· Â· , un }
such that
span (v1 , Â· Â· Â· , vm ) = V
and m is as small as possible for this to happen. If this set is linearly independent, it follows
it is a basis for V and the theorem is proved. On the other hand, if the set is not linearly
independent, then there exist scalars
c1 , Â· Â· Â· , cm

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

8.3. LOTS OF FIELDS

205

such that
0=

m
âˆ‘

ci vi

i=1

and not all the ci are equal to zero. Suppose ck Ì¸= 0. Then the vector, vk may be solved for
in terms of the other vectors. Consequently,
V = span (v1 , Â· Â· Â· , vkâˆ’1 , vk+1 , Â· Â· Â· , vm )
contradicting the deï¬nition of m. This proves the ï¬rst part of the theorem.
To obtain the second part, begin with {u1 , Â· Â· Â· , uk } and suppose a basis for V is
{v1 , Â· Â· Â· , vn } .
If
span (u1 , Â· Â· Â· , uk ) = V,
then k = n. If not, there exists a vector,
uk+1 âˆˆ
/ span (u1 , Â· Â· Â· , uk ) .
Then by Lemma 8.2.10, {u1 , Â· Â· Â· , uk , uk+1 } is also linearly independent. Continue adding
vectors in this way until n linearly independent vectors have been obtained. Then
span (u1 , Â· Â· Â· , un ) = V
because if it did not do so, there would exist un+1 as just described and {u1 , Â· Â· Â· , un+1 }
would be a linearly independent set of vectors having n+1 elements even though {v1 , Â· Â· Â· , vn }
is a basis. This would contradict Theorem 8.2.4. Therefore, this list is a basis. 

8.2.3

The Basis Of A Subspace

Every subspace of a ï¬nite dimensional vector space is a span of some vectors and in fact it
has a basis. This is the content of the next theorem.
Theorem 8.2.12 Let V be a nonzero subspace of a ï¬nite dimensional vector space, W of
dimension, n. Then V has a basis with no more than n vectors.
Proof: Let v1 âˆˆ V where v1 Ì¸= 0. If span {v1 } = V, stop. {v1 } is a basis for V .
Otherwise, there exists v2 âˆˆ V which is not in span {v1 } . By Lemma 8.2.10 {v1 , v2 } is a
linearly independent set of vectors. If span {v1 , v2 } = V stop, {v1 , v2 } is a basis for V. If
span {v1 , v2 } Ì¸= V, then there exists v3 âˆˆ
/ span {v1 , v2 } and {v1 , v2 , v3 } is a larger linearly
independent set of vectors. Continuing this way, the process must stop before n + 1 steps
because if not, it would be possible to obtain n + 1 linearly independent vectors contrary to
the exchange theorem, Theorem 8.2.4. 

8.3
8.3.1

Lots Of Fields
Irreducible Polynomials

I mentioned earlier that most things hold for arbitrary ï¬elds. However, I have not bothered
to give any examples of other ï¬elds. This is the point of this section. It also turns out that
showing the algebraic numbers are a ï¬eld can be understood using vector space concepts

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

206

VECTOR SPACES AND FIELDS

and it gives a very convincing application of the abstract theory presented earlier in this
chapter.
Here I will give some basic algebra relating to polynomials. This is interesting for its
own sake but also provides the basis for constructing many diï¬€erent kinds of ï¬elds. The
ï¬rst is the Euclidean algorithm for polynomials.
âˆ‘n
Deï¬nition 8.3.1 A polynomial is an expression of the form p (Î») = k=0 ak Î»k where as
usual Î»0 is deï¬ned to equal 1. Two polynomials are said to be equal if their corresponding
coeï¬ƒcients are the same. Thus, in particular, p (Î») = 0 means each of the ak = 0. An
element of the ï¬eld Î» is said to be a root of the polynomial if p (Î») = 0 in the sense that
when you plug in Î» into the formula and do the indicated operations, you get 0. The degree
of a nonzero polynomial is the highest exponent appearing on Î». The degree of the zero
polynomial p (Î») = 0 is not deï¬ned.
Example 8.3.2 Consider the polynomial p (Î») = Î»2 + Î» where the coeï¬ƒcients are in Z2 . Is
this polynomial equal to 0? Not according to the above deï¬nition, because its coeï¬ƒcients are
not all equal to 0. However, p (1) = p (0) = 0 so it sends every element of Z2 to 0. Note the
distinction between saying it sends everything in the ï¬eld to 0 with having the polynomial be
the zero polynomial.
Lemma 8.3.3 Let f (Î») and g (Î») Ì¸= 0 be polynomials. Then there exists a polynomial, q (Î»)
such that
f (Î») = q (Î») g (Î») + r (Î»)
where the degree of r (Î») is less than the degree of g (Î») or r (Î») = 0.
Proof: Consider the polynomials of the form f (Î») âˆ’ g (Î») l (Î») and out of all these
polynomials, pick one which has the smallest degree. This can be done because of the well
ordering of the natural numbers. Let this take place when l (Î») = q1 (Î») and let
r (Î») = f (Î») âˆ’ g (Î») q1 (Î») .
It is required to show degree of r (Î») < degree of g (Î») or else r (Î») = 0.
Suppose f (Î») âˆ’ g (Î») l (Î») is never equal to zero for any l (Î»). Then r (Î») Ì¸= 0. It is
required to show the degree of r (Î») is smaller than the degree of g (Î») . If this doesnâ€™t
happen, then the degree of r â‰¥ the degree of g. Let
r (Î») =
g (Î») =

bm Î»m + Â· Â· Â· + b1 Î» + b0
an Î»n + Â· Â· Â· + a1 Î» + a0

where m â‰¥ n and bm and an are nonzero. Then let r1 (Î») be given by
r1 (Î») = r (Î») âˆ’
= (bm Î»m + Â· Â· Â· + b1 Î» + b0 ) âˆ’

Î»mâˆ’n bm
g (Î»)
an

Î»mâˆ’n bm
(an Î»n + Â· Â· Â· + a1 Î» + a0 )
an

which has smaller degree than m, the degree of r (Î»). But
r(Î»)

r1 (Î») =
=

z
}|
{ Î»mâˆ’n b
m
f (Î») âˆ’ g (Î») q1 (Î») âˆ’
g (Î»)
an
(
)
Î»mâˆ’n bm
f (Î») âˆ’ g (Î») q1 (Î») +
,
an

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

8.3. LOTS OF FIELDS

207

and this is not zero by the assumption that f (Î») âˆ’ g (Î») l (Î») is never equal to zero for any
l (Î») yet has smaller degree than r (Î») which is a contradiction to the choice of r (Î»). 
Now with this lemma, here is another one which is very fundamental. First here is a
deï¬nition. A polynomial is monic means it is of the form
Î»n + cnâˆ’1 Î»nâˆ’1 + Â· Â· Â· + c1 Î» + c0 .
That is, the leading coeï¬ƒcient is 1. In what follows, the coeï¬ƒcients of polynomials are in
F, a ï¬eld of scalars which is completely arbitrary. Think R if you need an example.
Deï¬nition 8.3.4 A polynomial f is said to divide a polynomial g if g (Î») = f (Î») r (Î») for
some polynomial r (Î»). Let {Ï•i (Î»)} be a ï¬nite set of polynomials. The greatest common
divisor will be the monic polynomial q such that q (Î») divides each Ï•i (Î») and if p (Î») divides
each Ï•i (Î») , then p (Î») divides q (Î») . The ï¬nite set of polynomials {Ï•i } is said to be relatively
prime if their greatest common divisor is 1. A polynomial f (Î») is irreducible if there is no
polynomial with coeï¬ƒcients in F which divides it except nonzero scalar multiples of f (Î»)
and constants.
Proposition 8.3.5 The greatest common divisor is unique.
Proof: Suppose both q (Î») and q â€² (Î») work. Then q (Î») divides q â€² (Î») and the other way
around and so
q â€² (Î») = q (Î») l (Î») , q (Î») = lâ€² (Î») q â€² (Î»)
Therefore, the two must have the same degree. Hence lâ€² (Î») , l (Î») are both constants. However, this constant must be 1 because both q (Î») and q â€² (Î») are monic. 
Theorem 8.3.6 Let Ïˆ (Î») be the greatest common divisor of {Ï•i (Î»)} , not all of which are
zero polynomials. Then there exist polynomials ri (Î») such that
Ïˆ (Î») =

p
âˆ‘

ri (Î») Ï•i (Î») .

i=1

Furthermore, Ïˆ (Î») is the monic polynomial of smallest degree which can be written in the
above form.
Proof: Let S denote the set of monic polynomials which are of the form
p
âˆ‘

ri (Î») Ï•i (Î»)

i=1

where ri (Î») is a polynomial. Then S Ì¸=âˆ‘âˆ… because some Ï•i (Î») Ì¸= 0. Then let the ri be chosen
p
such that the degree of the expression i=1 ri (Î») Ï•i (Î») is as small as possible. Letting Ïˆ (Î»)
equal this sum, it remains to verify it is the greatest common divisor. First, does it divide
each Ï•i (Î»)? Suppose it fails to divide Ï•1 (Î») . Then by Lemma 8.3.3
Ï•1 (Î») = Ïˆ (Î») l (Î») + r (Î»)
where degree of r (Î») is less than that of Ïˆ (Î»). Then dividing r (Î») by the leading coeï¬ƒcient
if necessary and denoting the result by Ïˆ 1 (Î») , it follows the degree of Ïˆ 1 (Î») is less than
the degree of Ïˆ (Î») and Ïˆ 1 (Î») equals
Ïˆ 1 (Î») = (Ï•1 (Î») âˆ’ Ïˆ (Î») l (Î»)) a

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

208

VECTOR SPACES AND FIELDS

(
=

Ï•1 (Î») âˆ’

)
ri (Î») Ï•i (Î») l (Î») a

i=1

(
=

p
âˆ‘

(1 âˆ’ r1 (Î»)) Ï•1 (Î») +

p
âˆ‘

)
(âˆ’ri (Î») l (Î»)) Ï•i (Î») a

i=2

for a suitable a âˆˆ F. This is one of the polynomials in S. Therefore, Ïˆ (Î») does not have
the smallest degree after all because the degree of Ïˆ 1 (Î») is smaller. This is a contradiction.
Therefore, Ïˆ (Î») divides Ï•1 (Î») . Similarly it divides all the other Ï•i (Î»).
If p âˆ‘
(Î») divides all the Ï•i (Î») , then it divides Ïˆ (Î») because of the formula for Ïˆ (Î») which
p
equals i=1 ri (Î») Ï•i (Î») . 
Lemma 8.3.7 Suppose Ï• (Î») and Ïˆ (Î») are monic polynomials which are irreducible and
not equal. Then they are relatively prime.
Proof: Suppose Î· (Î») is a nonconstant polynomial. If Î· (Î») divides Ï• (Î») , then since
Ï• (Î») is irreducible, Î· (Î») equals aÏ• (Î») for some a âˆˆ F. If Î· (Î») divides Ïˆ (Î») then it must
be of the form bÏˆ (Î») for some b âˆˆ F and so it follows
a
Ïˆ (Î») = Ï• (Î»)
b
but both Ïˆ (Î») and Ï• (Î») are monic polynomials which implies a = b and so Ïˆ (Î») = Ï• (Î»).
This is assumed not to happen. It follows the only polynomials which divide both Ïˆ (Î»)
and Ï• (Î») are constants and so the two polynomials are relatively prime. Thus a polynomial
which divides them both must be a constant, and if it is monic, then it must be 1. Thus 1
is the greatest common divisor. 
Lemma 8.3.8 Let Ïˆ (Î») be an irreducible monic polynomial not equal to 1 which divides
p
âˆ

k

Ï•i (Î») i , ki a positive integer,

i=1

where each Ï•i (Î») is an irreducible monic polynomial. Then Ïˆ (Î») equals some Ï•i (Î») .
Proof : Suppose Ïˆ (Î») Ì¸= Ï•i (Î») for all i. Then by Lemma 8.3.7, there exist polynomials
mi (Î») , ni (Î») such that
1 = Ïˆ (Î») mi (Î») + Ï•i (Î») ni (Î») .
Hence
(Ï•i (Î») ni (Î»))

ki

ki

= (1 âˆ’ Ïˆ (Î») mi (Î»))

âˆp
k
Then, letting ge (Î») = i=1 ni (Î») i , and applying the binomial theorem, there exists a
polynomial h (Î») such that
ge (Î»)

p
âˆ

ki

Ï•i (Î»)

â‰¡

i=1

=

p
âˆ
i=1
p
âˆ

ni (Î»)

ki

p
âˆ

Ï•i (Î»)

ki

i=1
ki

(1 âˆ’ Ïˆ (Î») mi (Î»))

= 1 + Ïˆ (Î») h (Î»)

i=1

Thus, using the fact that Ïˆ (Î») divides

âˆp
i=1

k

Ï•i (Î») i , for a suitable polynomial g (Î») ,

g (Î») Ïˆ (Î») = 1 + Ïˆ (Î») h (Î»)
1 = Ïˆ (Î») (h (Î») âˆ’ g (Î»))
which is impossible if Ïˆ (Î») is non constant, as assumed. 
Now here is a simple lemma about canceling monic polynomials.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

8.3. LOTS OF FIELDS

209

Lemma 8.3.9 Suppose p (Î») is a monic polynomial and q (Î») is a polynomial such that
p (Î») q (Î») = 0.
Then q (Î») = 0. Also if
p (Î») q1 (Î») = p (Î») q2 (Î»)
then q1 (Î») = q2 (Î») .
Proof: Let
p (Î») =

k
âˆ‘

pj Î»j , q (Î») =

j=1

n
âˆ‘

qi Î»i , pk = 1.

i=1

Then the product equals
k âˆ‘
n
âˆ‘

pj qi Î»i+j .

j=1 i=1

Then look at those terms involving Î»k+n . This is pk qn Î»k+n and is given to be 0. Since
pk = 1, it follows qn = 0. Thus
k nâˆ’1
âˆ‘
âˆ‘
pj qi Î»i+j = 0.
j=1 i=1

Then consider the term involving Î»nâˆ’1+k and conclude that since pk = 1, it follows qnâˆ’1 = 0.
Continuing this way, each qi = 0. This proves the ï¬rst part. The second follows from
p (Î») (q1 (Î») âˆ’ q2 (Î»)) = 0. 
The following is the analog of the fundamental theorem of arithmetic for polynomials.
Theorem 8.3.10 Let f (Î») be a nonconstant
polynomial with coeï¬ƒcients in F. Then there
âˆn
is some a âˆˆ F such that f (Î») = a i=1 Ï•i (Î») where Ï•i (Î») is an irreducible nonconstant
monic polynomial and repeats are allowed. Furthermore, this factorization is unique in the
sense that any two of these factorizations have the same nonconstant factors in the product,
possibly in diï¬€erent order and the same constant a.
Proof: That such a factorization exists is obvious. If f (Î») is irreducible, you are done.
Factor out the leading coeï¬ƒcient. If not, then f (Î») = aÏ•1 (Î») Ï•2 (Î») where these are monic
polynomials. Continue doing this with the Ï•i and eventually arrive at a factorization of the
desired form.
It remains to argue the factorization is unique except for order of the factors. Suppose
a

n
âˆ

Ï•i (Î») = b

i=1

m
âˆ

Ïˆ i (Î»)

i=1

where the Ï•i (Î») and the Ïˆ i (Î») are all irreducible monic nonconstant polynomials and a, b âˆˆ
F. If n > m, then by Lemma 8.3.8, each Ïˆ i (Î») equals one of the Ï•j (Î») . By the above
cancellation lemma, Lemma 8.3.9, you can cancel all these Ïˆ i (Î») with appropriate Ï•j (Î»)
and obtain a contradiction because the resulting polynomials on either side would have
diï¬€erent degrees. Similarly, it cannot happen that n < m. It follows n = m and the two
products consist of the same polynomials. Then it follows a = b. 
The following corollary will be well used. This corollary seems rather believable but does
require a proof.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

210

VECTOR SPACES AND FIELDS

âˆp
k
Corollary 8.3.11 Let q (Î») = i=1 Ï•i (Î») i where the ki are positive integers and the Ï•i (Î»)
are irreducible monic polynomials. Suppose also that p (Î») is a monic polynomial which
divides q (Î») . Then
p
âˆ
r
p (Î») =
Ï•i (Î») i
i=1

where ri is a nonnegative integer no larger than ki .
âˆs
r
Proof: Using Theorem 8.3.10, let p (Î») = b i=1 Ïˆ i (Î») i where the Ïˆ i (Î») are each
irreducible and monic and b âˆˆ F. Since p (Î») is monic, b = 1. Then there exists a polynomial
g (Î») such that
p
s
âˆ
âˆ
r
k
p (Î») g (Î») = g (Î»)
Ïˆ i (Î») i =
Ï•i (Î») i
i=1

i=1

Hence g (Î») must be monic. Therefore,
p(Î»)

z }| {
p
s
l
âˆ
âˆ
âˆ
k
r
p (Î») g (Î») =
Ï•i (Î») i
Ïˆ i (Î») i
Î· j (Î») =
i=1

j=1

i=1

for Î· j monic and irreducible. By uniqueness, each Ïˆ i equals one of the Ï•j (Î») and the same
holding true of the Î· i (Î»). Therefore, p (Î») is of the desired form. 

8.3.2

Polynomials And Fields

When you have a polynomial like x2 âˆ’ 3 which has no rational roots, it turns out you can
enlarge the ï¬eld of rational numbers to obtain a larger ï¬eld such that this polynomial does
have roots in this larger ï¬eld. I am going to discuss a systematic way to do this. It will
turn out that for any polynomial with coeï¬ƒcients in any ï¬eld, there always exists a possibly
larger ï¬eld such that the polynomial has roots in this larger ï¬eld. This book has mainly
featured the ï¬eld of real or complex numbers but this procedure will show how to obtain
many other ï¬elds which could be used in most of what was presented earlier in the book.
Here is an important idea concerning equivalence relations which I hope is familiar.
Deï¬nition 8.3.12 Let S be a set. The symbol, âˆ¼ is called an equivalence relation on S if
it satisï¬es the following axioms.
1. x âˆ¼ x

for all x âˆˆ S. (Reï¬‚exive)

2. If x âˆ¼ y then y âˆ¼ x. (Symmetric)
3. If x âˆ¼ y and y âˆ¼ z, then x âˆ¼ z. (Transitive)
Deï¬nition 8.3.13 [x] denotes the set of all elements of S which are equivalent to x and
[x] is called the equivalence class determined by x or just the equivalence class of x.
Also recall the notion of equivalence classes.
Theorem 8.3.14 Let âˆ¼ be an equivalence class deï¬ned on a set, S and let H denote the
set of equivalence classes. Then if [x] and [y] are two of these equivalence classes, either
x âˆ¼ y and [x] = [y] or it is not true that x âˆ¼ y and [x] âˆ© [y] = âˆ….

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

8.3. LOTS OF FIELDS

211

Deï¬nition 8.3.15 Let F be a ï¬eld, for example the rational numbers, and denote by F [x]
the polynomials having coeï¬ƒcients in F. Suppose p (x) is a polynomial. Let a (x) âˆ¼ b (x)
(a (x) is similar to b (x)) when
a (x) âˆ’ b (x) = k (x) p (x)
for some polynomial k (x) .
Proposition 8.3.16 In the above deï¬nition, âˆ¼ is an equivalence relation.
Proof: First of all, note that a (x) âˆ¼ a (x) because their diï¬€erence equals 0p (x) . If
a (x) âˆ¼ b (x) , then a (x) âˆ’ b (x) = k (x) p (x) for some k (x) . But then b (x) âˆ’ a (x) =
âˆ’k (x) p (x) and so b (x) âˆ¼ a (x). Next suppose a (x) âˆ¼ b (x) and b (x) âˆ¼ c (x) . Then
a (x) âˆ’ b (x) = k (x) p (x) for some polynomial k (x) and also b (x) âˆ’ c (x) = l (x) p (x) for
some polynomial l (x) . Then
a (x) âˆ’ c (x) = a (x) âˆ’ b (x) + b (x) âˆ’ c (x)
= k (x) p (x) + l (x) p (x) = (l (x) + k (x)) p (x)
and so a (x) âˆ¼ c (x) and this shows the transitive law. 
With this proposition, here is another deï¬nition which essentially describes the elements
of the new ï¬eld. It will eventually be necessary to assume the polynomial p (x) in the above
deï¬nition is irreducible so I will begin assuming this.
Deï¬nition 8.3.17 Let F be a ï¬eld and let p (x) âˆˆ F [x] be a monic irreducible polynomial.
This means there is no polynomial having coeï¬ƒcients in F which divides p (x) except for itself
and constants. For the similarity relation deï¬ned in Deï¬nition 8.3.15, deï¬ne the following
operations on the equivalence classes. [a (x)] is an equivalence class means that it is the set
of all polynomials which are similar to a (x).
[a (x)] + [b (x)]
[a (x)] [b (x)]

â‰¡ [a (x) + b (x)]
â‰¡ [a (x) b (x)]

This collection of equivalence classes is sometimes denoted by F [x] / (p (x)).
Proposition 8.3.18 In the situation of Deï¬nition 8.3.17, p (x) and q (x) are relatively
prime for any q (x) âˆˆ F [x] which is not a multiple of p (x). Also the deï¬nitions of addition
and multiplication are well deï¬ned. In addition, if a, b âˆˆ F and [a] = [b] , then a = b.
Proof: First consider the claim about p (x) , q (x) being relatively prime. If Ïˆ (x) is the
greatest common divisor, it follows Ïˆ (x) is either equal to p (x) or 1. If it is p (x) , then
q (x) is a multiple of p (x) . If it is 1, then by deï¬nition, the two polynomials are relatively
prime.
To show the operations are well deï¬ned, suppose
[a (x)] = [aâ€² (x)] , [b (x)] = [bâ€² (x)]
It is necessary to show

[a (x) + b (x)] = [aâ€² (x) + bâ€² (x)]
[a (x) b (x)] = [aâ€² (x) bâ€² (x)]

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

212

VECTOR SPACES AND FIELDS

Consider the second of the two.

=

aâ€² (x) bâ€² (x) âˆ’ a (x) b (x)
aâ€² (x) bâ€² (x) âˆ’ a (x) bâ€² (x) + a (x) bâ€² (x) âˆ’ a (x) b (x)

=

bâ€² (x) (aâ€² (x) âˆ’ a (x)) + a (x) (bâ€² (x) âˆ’ b (x))

Now by assumption (aâ€² (x) âˆ’ a (x)) is a multiple of p (x) as is (bâ€² (x) âˆ’ b (x)) , so the above
is a multiple of p (x) and by deï¬nition this shows [a (x) b (x)] = [aâ€² (x) bâ€² (x)]. The case for
addition is similar.
Now suppose [a] = [b] . This means a âˆ’ b = k (x) p (x) for some polynomial k (x) . Then
k (x) must equal 0 since otherwise the two polynomials a âˆ’ b and k (x) p (x) could not be
equal because they would have diï¬€erent degree. 
Note that from this proposition and math induction, if each ai âˆˆ F,
[
]
an xn + anâˆ’1 xnâˆ’1 + Â· Â· Â· + a1 x + a0
n

= [an ] [x] + [anâˆ’1 ] [x]

nâˆ’1

+ Â· Â· Â· [a1 ] [x] + [a0 ]

(8.7)

With the above preparation, here is a deï¬nition of a ï¬eld in which the irreducible polynomial p (x) has a root.
Deï¬nition 8.3.19 Let p (x) âˆˆ F [x] be irreducible and let a (x) âˆ¼ b (x) when a (x) âˆ’ b (x) is
a multiple of p (x) . Let G denote the set of equivalence classes as described above with the
operations also described in Deï¬nition 8.3.17.
Also here is another useful deï¬nition and a simple proposition which comes from it.
Deï¬nition 8.3.20 Let F âŠ† K be two ï¬elds. Then clearly K is also a vector space over
F. Then also, K is called a ï¬nite ï¬eld extension of F if the dimension of this vector space,
denoted by [K : F ] is ï¬nite.
There are some easy things to observe about this.
Proposition 8.3.21 Let F âŠ† K âŠ† L be ï¬elds. Then [L : F ] = [L : K] [K : F ].
n

m

Proof: Let {li }i=1 be a basis for L over K and let {kj }j=1 be a basis of K over F . Then
if l âˆˆ L, there exist unique scalars xi in K such that
l=

n
âˆ‘

xi li

i=1

Now xi âˆˆ K so there exist fji such that
xi =

m
âˆ‘

fji kj

j=1

Then it follows that
l=

n âˆ‘
m
âˆ‘

fji kj li

i=1 j=1

It follows that {kj li } is a spanning set. If
n âˆ‘
m
âˆ‘

fji kj li = 0

i=1 j=1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

8.3. LOTS OF FIELDS

213

Then, since the li are independent, it follows that
m
âˆ‘

fji kj = 0

j=1

and since {kj } is independent, each fji = 0 for each j for a given arbitrary i. Therefore,
{kj li } is a basis. 
Theorem 8.3.22 The set of all equivalence classes G â‰¡ F/ (p (x)) described above with
the multiplicative identity given by [1] and the additive identity given by [0] along with the
operations of Deï¬nition 8.3.17, is a ï¬eld and p ([x]) = [0] . (Thus p has a root in this new
ï¬eld.) In addition to this, [G : F] = n, the degree of p (x) .
Proof: Everything is obvious except for the existence of the multiplicative inverse and
the assertion that p ([x]) = 0. Suppose then that [a (x)] Ì¸= [0] . That is, a (x) is not a multiple
âˆ’1
of p (x). Why does [a (x)] exist? By Theorem 8.3.6, a (x) , p (x) are relatively prime and
so there exist polynomials Ïˆ (x) , Ï• (x) such that
1 = Ïˆ (x) p (x) + a (x) Ï• (x)
and so
1 âˆ’ a (x) Ï• (x) = Ïˆ (x) p (x)
which, by deï¬nition implies
[1 âˆ’ a (x) Ï• (x)] = [1] âˆ’ [a (x) Ï• (x)] = [1] âˆ’ [a (x)] [Ï• (x)] = [0]
âˆ’1

and so [Ï• (x)] = [a (x)] . This shows G is a ï¬eld.
Now if p (x) = an xn + anâˆ’1 xnâˆ’1 + Â· Â· Â· + a1 x + a0 , p ([x]) = 0 by (8.7) and the deï¬nition
which says [p (x)] = [0].
[ ]
Consider the claim about the dimension.
It[ was ]just shown that [1] , [x] , x2 , Â· Â· Â· , [xn ]
[ 2]
is linearly dependent. Also [1] , [x] , x , Â· Â· Â· , xnâˆ’1 is independent because if not, there
would exist a polynomial q (x) of degree nâˆ’1 which is a multiple of p (x) which is impossible.
Now for [q (x)] âˆˆ G, you can write
q (x) = p (x) l (x) + r (x)
where the degree of r (x) is less than
0. Either way, [q (x)] = [r (x)] which
[ n] or else
[ it equals
]
is a linear combination of [1] , [x] , x2 , Â· Â· Â· , xnâˆ’1 . Thus [G : F] = n as claimed. 
Note that if p (x) were not irreducible, then you could ï¬nd a ï¬eld extension G such that
[G : F] â‰¤ n. You could do this by working with an irreducible factor of p (x).
Usually, people simply write b rather than [b] if b âˆˆ F. Then with this convention,
[bÏ• (x)] = [b] [Ï• (x)] = b [Ï• (x)] .
This shows how to enlarge a ï¬eld to get a new one in which the polynomial has a root.
By using a succession of such enlargements, called ï¬eld extensions, there will exist a ï¬eld
in which the given polynomial can be factored into a product of polynomials having degree
one. The ï¬eld you obtain in this process of enlarging in which the given polynomial factors
in terms of linear factors is called a splitting ï¬eld.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

214

VECTOR SPACES AND FIELDS

Theorem 8.3.23 Let p (x) = xn +anâˆ’1 xnâˆ’1 +Â· Â· Â·+a1 x+a0 be a polynomial with coeï¬ƒcients
in a ï¬eld of scalars F. There exists a larger ï¬eld G such that there exist {z1 , Â· Â· Â· , zn } listed
according to multiplicity such that
p (x) =

n
âˆ

(x âˆ’ zi )

i=1

This larger ï¬eld is called a splitting ï¬eld. Furthermore,
[G : F] â‰¤ n!
Proof: From Theorem 8.3.22, there exists a ï¬eld F1 such that p (x) has a root, z1 (= [x]
if p is irreducible.) Then by the Euclidean algorithm
p (x) = (x âˆ’ z1 ) q1 (x) + r
where r âˆˆ F1 . Since p (z1 ) = 0, this requires r = 0. Now do the same for q1 (x) that was
done for p (x) , enlarging the ï¬eld to F2 if necessary, such that in this new ï¬eld
q1 (x) = (x âˆ’ z2 ) q2 (x) .
and so
p (x) = (x âˆ’ z1 ) (x âˆ’ z2 ) q2 (x)
After n such extensions, you will have obtained the necessary ï¬eld G.
Finally consider the claim about dimension. By Theorem 8.3.22, there is a larger ï¬eld
G1 such that p (x) has a root a1 in G1 and [G : F] â‰¤ n. Then
p (x) = (x âˆ’ a1 ) q (x)
Continue this way until the polynomial equals the product of linear factors. Then by
Proposition 8.3.21 applied multiple times, [G : F] â‰¤ n!. 
Example 8.3.24 The polynomial x2 + 1 is irreducible in R (x) , polynomials having real
coeï¬ƒcients. To see this is the case, suppose Ïˆ (x) divides x2 + 1. Then
x2 + 1 = Ïˆ (x) q (x)
If the degree of Ïˆ (x) is less than 2, then it must be either a constant or of the form ax + b.
In the latter case, âˆ’b/a must be a zero of the right side, hence of the left but x2 + 1 has no
real zeros. Therefore, the degree of Ïˆ (x) must be two and q (x) must be a constant. Thus
the only polynomial which divides x2 + 1 are constants
of x2 + 1. Therefore,
[ 2 and multiples
]
2
this shows x( + 1 is) irreducible. Find the inverse of x + x + 1 in the space of equivalence
classes, R/ x2 + 1 .
You can solve this with partial fractions.
1
x
x+1
=âˆ’ 2
+
(x2 + 1) (x2 + x + 1)
x + 1 x2 + x + 1
and so
which implies

(
)
(
)
1 = (âˆ’x) x2 + x + 1 + (x + 1) x2 + 1
(
)
1 âˆ¼ (âˆ’x) x2 + x + 1

and so the inverse is [âˆ’x] .
The following proposition is interesting. It was essentially proved above but to emphasize
it, here it is again.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

8.3. LOTS OF FIELDS

215

Proposition 8.3.25 Suppose p (x) âˆˆ F [x] is irreducible and has degree n. Then every
element of G = F [x] / (p (x)) is of the form [0] or [r (x)] where the degree of r (x) is less
than n.
Proof: This follows right away from the Euclidean algorithm for polynomials. If k (x)
has degree larger than n âˆ’ 1, then
k (x) = q (x) p (x) + r (x)
where r (x) is either equal to 0 or has degree less than n. Hence
[k (x)] = [r (x)] . 
âˆ’1

Example 8.3.26 In the situation of the above example, ï¬nd [ax + b] assuming a2 + b2 Ì¸=
0. Note this includes all cases of interest thanks to the above proposition.
You can do it with partial fractions as above.

(x2

1
b âˆ’ ax
a2
= 2
+ 2
2
2
2
+ 1) (ax + b)
(a + b ) (x + 1) (a + b ) (ax + b)

and so
1=
Thus

a2

( 2
)
1
a2
x +1
(b âˆ’ ax) (ax + b) + 2
2
2
+b
(a + b )
1
(b âˆ’ ax) (ax + b) âˆ¼ 1
a2 + b2

and so
âˆ’1

[ax + b]

=

[(b âˆ’ ax)]
b âˆ’ a [x]
= 2
2
2
a +b
a + b2
âˆ’1

You might ï¬nd it interesting to recall that (ai + b)

8.3.3

=

bâˆ’ai
a2 +b2 .

The Algebraic Numbers

Each polynomial having coeï¬ƒcients in a ï¬eld F has a splitting ï¬eld. Consider the case of all
polynomials p (x) having coeï¬ƒcients in a ï¬eld F âŠ† G and consider all roots which are also
in G. The theory of vector spaces is very useful in the study of these algebraic numbers.
Here is a deï¬nition.
Deï¬nition 8.3.27 The algebraic numbers A are those numbers which are in G and also
roots of some polynomial p (x) having coeï¬ƒcients in F.
Theorem 8.3.28 Let a âˆˆ A. Then there exists a unique monic irreducible polynomial p (x)
having coeï¬ƒcients in F such that p (a) = 0. This is called the minimal polynomial for a.
Proof: By deï¬nition, there exists a polynomial q (x) having coeï¬ƒcients in F such that
q (a) = 0. If q (x) is irreducible, divide by the leading coeï¬ƒcient and this proves the existence.
If q (x) is not irreducible, then there exist nonconstant polynomials r (x) and k (x) such that
q (x) = r (x) k (x). Then one of r (a) , k (a) equals 0. Pick the one which equals zero and let it
play the role of q (x). Continuing this way, in ï¬nitely many steps one obtains an irreducible
polynomial p (x) such that p (a) = 0. Now divide by the leading coeï¬ƒcient and this proves
existence. Suppose pi , i = 1, 2 both work and they are not equal. Then by Lemma 8.3.7

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

216

VECTOR SPACES AND FIELDS

they must be relatively prime because they are both assumed to be irreducible and so there
exist polynomials l (x) , k (x) such that
1 = l (x) p1 (x) + k (x) p2 (x)
But now when a is substituted for x, this yields 0 = 1, a contradiction. The polynomials
are equal after all. 
Deï¬nition 8.3.29 For a an algebraic number, let deg (a) denote the degree of the minimal
polynomial of a.
Also, here is another deï¬nition.
Deï¬nition 8.3.30 Let a1 , Â· Â· Â· , am be in A. A polynomial in {a1 , Â· Â· Â· , am } will be an expression of the form
âˆ‘
ak1 Â·Â·Â·kn ak11 Â· Â· Â· aknn
k1 Â·Â·Â·kn

where the ak1 Â·Â·Â·kn are in F, each kj is a nonnegative integer, and all but ï¬nitely many of the
ak1 Â·Â·Â·kn equal zero. The collection of such polynomials will be denoted by
F [a1 , Â· Â· Â· , am ] .
Now notice that for a an algebraic number, F [a] is a vector space with ï¬eld of scalars F.
Similarly, for {a1 , Â· Â· Â· , am } algebraic numbers, F [a1 , Â· Â· Â· , am ] is a vector space with ï¬eld of
scalars F. The following fundamental proposition is important.
Proposition 8.3.31 Let {a1 , Â· Â· Â· , am } be algebraic numbers. Then
dim F [a1 , Â· Â· Â· , am ] â‰¤

m
âˆ

deg (aj )

j=1

and for an algebraic number a,
dim F [a] = deg (a)
Every element of F [a1 , Â· Â· Â· , am ] is in A and F [a1 , Â· Â· Â· , am ] is a ï¬eld.
Proof: First consider the second assertion. Let the minimal polynomial of a be
p (x) = xn + anâˆ’1 xnâˆ’1 + Â· Â· Â· + a1 x + a0 .
{
}
Since p (a) = 0, it follows 1, a, a2 , Â· Â· Â· , an is linearly dependent. However, if the degree
of q (x) is less than the degree of p (x) , then if q (x) is not a constant, the two must be
relatively prime because p (x) is irreducible and so there exist polynomials k (x) , l (x) such
that
1 = l (x) q (x) + k (x) p (x)
and this is a contradiction if q (a) = 0 because it would imply upon replacing x with a that
1 = 0. Therefore, no polynomial having degree less than n can have a as a root. It follows
{
}
1, a, a2 , Â· Â· Â· , anâˆ’1
is linearly independent. Thus dim F [a] = deg (a) = n. Here is why this is. If q (a) is any
element of F [a] ,
q (x) = p (x) k (x) + r (x)

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

8.3. LOTS OF FIELDS

217

(
)
where deg r (x) < deg p (x) and so q (a) = r (a) and r (a) âˆˆ span 1, a, a2 , Â· Â· Â· , anâˆ’1 .
Now consider{the ï¬rst claim. }
By deï¬nition, F [a1 , Â· Â· Â· , am ] is obtained from all linear
k1
k2
kn
combinations of a1 , a2 , Â· Â· Â· , an
where the ki are nonnegative integers. From the ï¬rst
part, it suï¬ƒces to consider only kj â‰¤ deg (aj ). Therefore, there exists a spanning set for
F [a1 , Â· Â· Â· , am ] which has
m
âˆ
deg (ai )
i=1

entries. By Theorem 8.2.4 this proves the ï¬rst claim.
Finally consider the last claim. Let g (a1 , Â· Â· Â· , am ) be a polynomial in {a1 , Â· Â· Â· , am } in
F [a1 , Â· Â· Â· , am ]. Since
dim F [a1 , Â· Â· Â· , am ] â‰¡ p â‰¤

m
âˆ

deg (aj ) < âˆ,

j=1

it follows
2

p

1, g (a1 , Â· Â· Â· , am ) , g (a1 , Â· Â· Â· , am ) , Â· Â· Â· , g (a1 , Â· Â· Â· , am )

are dependent. It follows g (a1 , Â· Â· Â· , am ) is the root of some polynomial having coeï¬ƒcients
in F. Thus everything in F [a1 , Â· Â· Â· , am ] is algebraic. Why is F [a1 , Â· Â· Â· , am ] a ï¬eld? Let
g (a1 , Â· Â· Â· , am ) be as just mentioned. Then it has a minimal polynomial,
p (x) = xp + apâˆ’1 xpâˆ’1 + Â· Â· Â· + a1 x + a0
where the ai âˆˆ F. Then a0 Ì¸= 0 or else the polynomial would not be minimal. Therefore,
(
)
pâˆ’1
pâˆ’2
g (a1 , Â· Â· Â· , am ) g (a1 , Â· Â· Â· , am )
+ apâˆ’1 g (a1 , Â· Â· Â· , am )
+ Â· Â· Â· + a1 = âˆ’a0
and so the multiplicative inverse for g (a1 , Â· Â· Â· , am ) is
pâˆ’1

g (a1 , Â· Â· Â· , am )

+ apâˆ’1 g (a1 , Â· Â· Â· , am )
âˆ’a0

pâˆ’2

+ Â· Â· Â· + a1

âˆˆ F [a1 , Â· Â· Â· , am ] .

The other axioms of a ï¬eld are obvious. 
Now from this proposition, it is easy to obtain the following interesting result about the
algebraic numbers.
Theorem 8.3.32 The algebraic numbers A, those roots of polynomials in F [x] which are
in G, are a ï¬eld.
Proof: Let a be an algebraic number and let p (x) be its minimal polynomial. Then
p (x) is of the form
xn + anâˆ’1 xnâˆ’1 + Â· Â· Â· + a1 x + a0
where a0 Ì¸= 0. Then plugging in a yields
( nâˆ’1
)
a
+ anâˆ’1 anâˆ’2 + Â· Â· Â· + a1 (âˆ’1)
a
= 1.
a0
(anâˆ’1 +anâˆ’1 anâˆ’2 +Â·Â·Â·+a1 )(âˆ’1)
and so aâˆ’1 =
âˆˆ F [a]. By the proposition, every element of F [a]
a0
is in A and this shows that for every element of A, its inverse is also in A. What about
products and sums of things in A? Are they still in A? Yes. If a, b âˆˆ A, then both a + b
and ab âˆˆ F [a, b] and from the proposition, each element of F [a, b] is in A. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

218

VECTOR SPACES AND FIELDS

A typical example of what is of interest here is when the ï¬eld F of scalars is Q, the
rational numbers and the ï¬eld G is R. However, you can certainly conceive of many other
examples by considering the integers mod a prime, for example (See Problem 34 on Page
222 for example.) or any of the ï¬elds which occur as ï¬eld extensions in the above.
There is a very interesting thing about F [a1 Â· Â· Â· an ] in the case where F is inï¬nite which
says that there exists a single algebraic Î³ such that F [a1 Â· Â· Â· an ] = F [Î³]. In other words,
every ï¬eld extension of this sort is a simple ï¬eld extension. I found this fact in an early
version of [5].
Proposition 8.3.33 There exists Î³ such that F [a1 Â· Â· Â· an ] = F [Î³].
Proof: To begin with, consider F [Î±, Î²]. Let Î³ = Î± + Î»Î². Then by Proposition 8.3.31 Î³
is an algebraic number and it is also clear
F [Î³] âŠ† F [Î±, Î²]
I need to show the other inclusion. This will be done for a suitable choice of Î». To do this,
it suï¬ƒces to verify that both Î± and Î² are in F [Î³].
Let the minimal polynomials of Î± and Î² be f (x) and g (x) respectively. Let the distinct
roots of f (x) and g (x) be {Î±1 , Î±2 , Â· Â· Â· , Î±n } and {Î² 1 , Î² 2 , Â· Â· Â· , Î² m } respectively. These roots
are in a ï¬eld which contains splitting ï¬elds of both f (x) and g (x). Let Î± = Î±1 and Î² = Î² 1 .
Now deï¬ne
h (x) â‰¡ f (Î± + Î»Î² âˆ’ Î»x) â‰¡ f (Î³ âˆ’ Î»x)
so that h (Î²) = f (Î±) = 0. It follows (x âˆ’ Î²) divides both h((x) and) g (x). If (x âˆ’ Î·) is a
diï¬€erent linear factor of both g (x) and h (x) then it must be x âˆ’ Î² j for some Î² j for some
j > 1 because these are the only factors of g (x) . Therefore, this would require
( )
(
)
0 = h Î² j = f Î±1 + Î»Î² 1 âˆ’ Î»Î² j
and so it would be the case that Î±1 + Î»Î² 1 âˆ’ Î»Î² j = Î±k for some k. Hence
Î»=

Î±k âˆ’ Î±1
Î²1 âˆ’ Î²j

Now there are ï¬nitely many quotients of the above form and if Î» is chosen to not be any of
them, then the above cannot happen and so in this case, the only linear factor of both g (x)
and h (x) will be (x âˆ’ Î²). Choose such a Î».
Let Ï• (x) be the minimal polynomial of Î² with respect to the ï¬eld F [Î³]. Then this
minimal polynomial must divide both h (x) and g (x) because h (Î²) = g (Î²) = 0. However,
the only factor these two have in common is x âˆ’ Î² and so Ï• (x) = x âˆ’ Î² which requires
Î² âˆˆ F [Î³] . Now also Î± = Î³ âˆ’ Î»Î² and so Î± âˆˆ F [Î³] also. Therefore, both Î±, Î² âˆˆ F [Î³] which
forces F [Î±, Î²] âŠ† F [Î³] . This proves the proposition in the case that n = 2. The general result
follows right away by observing that
F [a1 Â· Â· Â· an ] = F [a1 Â· Â· Â· anâˆ’1 ] [an ]
and using induction. 
When you have a ï¬eld F, F (a) denotes the smallest ï¬eld which contains both F and a.
When a is algebraic over F, it follows that F (a) = F [a] . The latter is easier to think about
because it just involves polynomials.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

8.4. EXERCISES

8.3.4

219

The Lindemannn Weierstrass Theorem And Vector Spaces

As another application of the abstract concept of vector spaces, there is an amazing theorem
due to Weierstrass and Lindemannn.
Theorem 8.3.34 Suppose a1 , Â· Â· Â· , an are algebraic numbers and suppose Î±1 , Â· Â· Â· , Î±n are
distinct algebraic numbers. Then
n
âˆ‘
ai eÎ±i Ì¸= 0
i=1

In other words, the {eÎ±1 , Â· Â· Â· , eÎ±n } are independent as vectors with ï¬eld of scalars equal to
the algebraic numbers.
There is a proof of this in the appendix. It is long and hard but only depends on
elementary considerations other than some algebra involving symmetric polynomials. See
Theorem F.3.5.
A number is transcendental if it is not a root of a polynomial which has integer coeï¬ƒcients. Most numbers are this way but it is hard to verify that speciï¬c numbers are
transcendental. That Ï€ is transcendental follows from
e0 + eiÏ€ = 0.
By the above theorem, this could not happen if Ï€ were algebraic because then iÏ€ would also
be algebraic. Recall these algebraic numbers form a ï¬eld and i is clearly algebraic, being
a root of x2 + 1. This fact about Ï€ was ï¬rst proved by Lindemannn in 1882 and then the
general theorem above was proved by Weierstrass in 1885. This fact that Ï€ is transcendental
solved an old problem called squaring the circle which was to construct a square with the
same area as a circle using a straight edge and compass. It can be shown that the fact Ï€ is
transcendental implies this problem is impossible.1

8.4
1.

2.
3.
4.
5.

Exercises

ï£«ï£«

ï£¶ ï£« ï£¶ ï£« ï£¶ ï£« ï£¶ï£¶
1
1
1
0
Let H denote span ï£­ï£­ 2 ï£¸ , ï£­ 4 ï£¸ , ï£­ 3 ï£¸ , ï£­ 1 ï£¸ï£¸ . Find the dimension of H
0
0
1
1
and determine a basis.
{
}
Let M = u = (u1 , u2 , u3 , u4 ) âˆˆ R4 : u3 = u1 = 0 . Is M a subspace? Explain.
{
}
Let M = u = (u1 , u2 , u3 , u4 ) âˆˆ R4 : u3 â‰¥ u1 . Is M a subspace? Explain.
{
}
Let w âˆˆ R4 and let M = u = (u1 , u2 , u3 , u4 ) âˆˆ R4 : w Â· u = 0 . Is M a subspace?
Explain.
{
}
Let M = u = (u1 , u2 , u3 , u4 ) âˆˆ R4 : ui â‰¥ 0 for each i = 1, 2, 3, 4 . Is M a subspace?
Explain.

6. Let w, w1 be given vectors in R4 and deï¬ne
{
}
M = u = (u1 , u2 , u3 , u4 ) âˆˆ R4 : w Â· u = 0 and w1 Â· u = 0 .
Is M a subspace? Explain.
1 Gilbert,

the librettist of the Savoy operas, may have heard about this great achievement. In Princess
Ida which opened in 1884 he has the following lines. â€œAs for fashion they forswear it, so the say - so they
say; and the circle - they will square it some ï¬ne day some ï¬ne day.â€ Of course it had been proved impossible
to do this a couple of years before.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

220

VECTOR SPACES AND FIELDS

{
}
7. Let M = u = (u1 , u2 , u3 , u4 ) âˆˆ R4 : |u1 | â‰¤ 4 . Is M a subspace? Explain.
{
}
8. Let M = u = (u1 , u2 , u3 , u4 ) âˆˆ R4 : sin (u1 ) = 1 . Is M a subspace? Explain.
9. Suppose {x1 , Â· Â· Â· , xk } is a set of vectors from Fn . Show that 0 is in span (x1 , Â· Â· Â· , xk ) .
10. Consider the vectors of the form
ï£±ï£«
ï£¼
ï£¶
ï£² 2t + 3s
ï£½
ï£­ s âˆ’ t ï£¸ : s, t âˆˆ R .
ï£³
ï£¾
t+s
Is this set of vectors a subspace of R3 ? If so, explain why, give a basis for the subspace
and ï¬nd its dimension.
11. Consider the vectors of the form
ï£±ï£«
2t + 3s + u
ï£´
ï£´
ï£²ï£¬
sâˆ’t
ï£¬
ï£­
t+s
ï£´
ï£´
ï£³
u

ï£¼
ï£´
ï£´
ï£½
ï£·
ï£· : s, t, u âˆˆ R .
ï£¸
ï£´
ï£´
ï£¾
ï£¶

Is this set of vectors a subspace of R4 ? If so, explain why, give a basis for the subspace
and ï¬nd its dimension.
12. Consider the vectors of the form
ï£±ï£«
2t + u + 1
ï£´
ï£´
ï£²ï£¬
ï£¬ t + 3u
ï£­ t+s+v
ï£´
ï£´
ï£³
u

ï£¼
ï£´
ï£´
ï£½
ï£·
ï£· : s, t, u, v âˆˆ R .
ï£¸
ï£´
ï£´
ï£¾
ï£¶

Is this set of vectors a subspace of R4 ? If so, explain why, give a basis for the subspace
and ï¬nd its dimension.
13. Let V denote the set of functions deï¬ned on [0, 1]. Vector addition is deï¬ned as
(f + g) (x) â‰¡ f (x) + g (x) and scalar multiplication is deï¬ned as (Î±f ) (x) â‰¡ Î± (f (x)).
Verify V is a vector space. What is its dimension, ï¬nite or inï¬nite? Justify your
answer.
14. Let V denote the set of polynomial functions deï¬ned on [0, 1]. Vector addition is
deï¬ned as (f + g) (x) â‰¡ f (x) + g (x) and scalar multiplication is deï¬ned as (Î±f ) (x) â‰¡
Î± (f (x)). Verify V is a vector space. What is its dimension, ï¬nite or inï¬nite? Justify
your answer.
15. Let V be the set of polynomials deï¬ned on R having degree no more than 4. Give a
basis for this vector space.
âˆš
16. Let the vectors be of the form a + b 2 where a, b are rational numbers and let the
ï¬eld of scalars be F = Q, the rational numbers. Show directly this is a vector space.
What is its dimension? What is a basis for this vector space?
17. Let V be a vector space with ï¬eld of scalars F and suppose {v1 , Â· Â· Â· , vn } is a basis for
V . Now let W also be a vector space with ï¬eld of scalars F. Let L : {v1 , Â· Â· Â· , vn } â†’
W be a function such that Lvj = wj . Explain how L can be extended to a linear
transformation mapping V to W in a unique way.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

8.4. EXERCISES

221

18. If you have 5 vectors in F5 and the vectors are linearly independent, can it always be
concluded they span F5 ? Explain.
19. If you have 6 vectors in F5 , is it possible they are linearly independent? Explain.
20. Suppose V, W are subspaces of Fn . Show V âˆ© W deï¬ned to be all vectors which are in
both V and W is a subspace also.
21. Suppose V and W both have dimension equal to 7 and they are subspaces of a vector
space of dimension 10. What are the possibilities for the dimension of V âˆ© W ? Hint:
Remember that a linear independent set can be extended to form a basis.
22. Suppose V has dimension p and W has dimension q and they are each contained in
a subspace, U which has dimension equal to n where n > max (p, q) . What are the
possibilities for the dimension of V âˆ© W ? Hint: Remember that a linear independent
set can be extended to form a basis.
23. If b Ì¸= 0, can the solution set of Ax = b be a plane through the origin? Explain.
24. Suppose a system of equations has fewer equations than variables and you have found
a solution to this system of equations. Is it possible that your solution is the only one?
Explain.
25. Suppose a system of linear equations has a 2Ã—4 augmented matrix and the last column
is a pivot column. Could the system of linear equations be consistent? Explain.
26. Suppose the coeï¬ƒcient matrix of a system of n equations with n variables has the
property that every column is a pivot column. Does it follow that the system of
equations must have a solution? If so, must the solution be unique? Explain.
27. Suppose there is a unique solution to a system of linear equations. What must be true
of the pivot columns in the augmented matrix.
28. State whether each of the following sets of data are possible for the matrix equation
Ax = b. If possible, describe the solution set. That is, tell whether there exists a
unique solution no solution or inï¬nitely many solutions.
(a) A is a 5 Ã— 6 matrix, rank (A) = 4 and rank (A|b) = 4. Hint: This says b is in
the span of four of the columns. Thus the columns are not independent.
(b) A is a 3 Ã— 4 matrix, rank (A) = 3 and rank (A|b) = 2.
(c) A is a 4 Ã— 2 matrix, rank (A) = 4 and rank (A|b) = 4. Hint: This says b is in
the span of the columns and the columns must be independent.
(d) A is a 5 Ã— 5 matrix, rank (A) = 4 and rank (A|b) = 5. Hint: This says b is not
in the span of the columns.
(e) A is a 4 Ã— 2 matrix, rank (A) = 2 and rank (A|b) = 2.
29. Suppose A is an m Ã— n matrix in which m â‰¤ n. Suppose also that the rank of A equals
m. Show that A maps Fn onto Fm . Hint: The vectors e1 , Â· Â· Â· , em occur as columns
in the row reduced echelon form for A.
30. Suppose A is an m Ã— n matrix in which m â‰¥ n. Suppose also that the rank of A equals
n. Show that A is one to one. Hint: If not, there exists a vector, x such that Ax = 0,
and this implies at least one column of A is a linear combination of the others. Show
this would require the column rank to be less than n.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

222

VECTOR SPACES AND FIELDS

31. Explain why an n Ã— n matrix A is both one to one and onto if and only if its rank is
n.
32. If you have not done this already, here it is again. It is a very important result.
Suppose A is an m Ã— n matrix and B is an n Ã— p matrix. Show that
dim (ker (AB)) â‰¤ dim (ker (A)) + dim (ker (B)) .
Hint: Consider the subspace, B (Fp ) âˆ© ker (A) and suppose a basis for this subspace
is {w1 , Â· Â· Â· , wk } . Now suppose {u1 , Â· Â· Â· , ur } is a basis for ker (B) . Let {z1 , Â· Â· Â· , zk }
be such that Bzi = wi and argue that
ker (AB) âŠ† span (u1 , Â· Â· Â· , ur , z1 , Â· Â· Â· , zk ) .
Here is how you do this. Suppose ABx = 0. Then Bx âˆˆ ker (A) âˆ© B (Fp ) and so
âˆ‘k
Bx = i=1 Bzi showing that
xâˆ’

k
âˆ‘

zi âˆˆ ker (B) .

i=1

33. Recall that every positive integer can be factored into a product of primes in a unique
way. Show there must be inï¬nitely many primes. Hint: Show that if you have any
ï¬nite set of primes and you multiply them and then add 1, the result cannot be
divisible by any of the primes in your ï¬nite set. This idea in the hint is due to Euclid
who lived about 300 B.C.
34. There are lots of ï¬elds. This will give an example of a ï¬nite ï¬eld. Let Z denote the set
of integers. Thus Z = {Â· Â· Â· , âˆ’3, âˆ’2, âˆ’1, 0, 1, 2, 3, Â· Â· Â· }. Also let p be a prime number.
We will say that two integers, a, b are equivalent and write a âˆ¼ b if a âˆ’ b is divisible
by p. Thus they are equivalent if a âˆ’ b = px for some integer x. First show that
a âˆ¼ a. Next show that if a âˆ¼ b then b âˆ¼ a. Finally show that if a âˆ¼ b and b âˆ¼ c
then a âˆ¼ c. For a an integer, denote by [a] the set of all integers which is equivalent
to a, the equivalence class of a. Show ï¬rst that is suï¬ƒces to consider only [a] for
a = 0, 1, 2, Â· Â· Â· , p âˆ’ 1 and that for 0 â‰¤ a < b â‰¤ p âˆ’ 1, [a] Ì¸= [b]. That is, [a] = [r] where
r âˆˆ {0, 1, 2, Â· Â· Â· , p âˆ’ 1}. Thus there are exactly p of these equivalence classes. Hint:
Recall the Euclidean algorithm. For a > 0, a = mp + r where r < p. Next deï¬ne the
following operations.
[a] + [b]
[a] [b]

â‰¡ [a + b]
â‰¡ [ab]

Show these operations are well deï¬ned. That is, if [a] = [aâ€² ] and [b] = [bâ€² ] , then
[a] + [b] = [aâ€² ] + [bâ€² ] with a similar conclusion holding for multiplication. Thus for
addition you need to verify [a + b] = [aâ€² + bâ€² ] and for multiplication you need to verify
[ab] = [aâ€² bâ€² ]. For example, if p = 5 you have [3] = [8] and [2] = [7] . Is [2 Ã— 3] = [8 Ã— 7]?
Is [2 + 3] = [8 + 7]? Clearly so in this example because when you subtract, the result
is divisible by 5. So why is this so in general? Now verify that {[0] , [1] , Â· Â· Â· , [p âˆ’ 1]}
with these operations is a Field. This is called the integers modulo a prime and is
written Zp . Since there are inï¬nitely many primes p, it follows there are inï¬nitely
many of these ï¬nite ï¬elds. Hint: Most of the axioms are easy once you have shown
the operations are well deï¬ned. The only two which are tricky are the ones which
give the existence of the additive inverse and the multiplicative inverse. Of these, the

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

8.4. EXERCISES

223

ï¬rst is not hard. âˆ’ [x] = [âˆ’x]. Since p is prime, there exist integers x, y such that
1 = px+ky and so 1âˆ’ky = px which says 1 âˆ¼ ky and so [1] = [ky] . Now you ï¬nish the
argument. What is the multiplicative identity in this collection of equivalence classes?
Of course you could now consider ï¬eld extensions based on these ï¬elds.
35. Suppose the ï¬eld of scalars is Z2 described above. Show that
(
)(
) (
)(
) (
0 1
0 0
0 0
0 1
1
âˆ’
=
0 0
1 0
1 0
0 0
0

0
1

)

Thus the identity is a comutator. Compare this with Problem 50 on Page 198.
36. Suppose V is a vector space with ï¬eld of scalars F. Let T âˆˆ L (V, W ) , the space of
linear transformations mapping V onto W where W is another vector space. Deï¬ne
an equivalence relation on V as follows. v âˆ¼ w means v âˆ’ w âˆˆ ker (T ) . Recall that
ker (T ) â‰¡ {v : T v = 0}. Show this is an equivalence relation. Now for [v] an equivalence class deï¬ne T â€² [v] â‰¡ T v. Show this is well deï¬ned. Also show that with the
operations
[v] + [w] â‰¡ [v + w]
Î± [v] â‰¡ [Î±v]
this set of equivalence classes, denoted by V / ker (T ) is a vector space. Show next that
T â€² : V / ker (T ) â†’ W is one to one, linear, and onto. This new vector space, V / ker (T )
is called a quotient space. Show its dimension equals the diï¬€erence between the
dimension of V and the dimension of ker (T ).
37. Let V be an n dimensional vector space and let W be a subspace. Generalize the
above problem to deï¬ne and give properties of V /W . What is its dimension? What
is a basis?
38. If F and G are two ï¬elds and F âŠ† G, can you consider G as a vector space with ï¬eld
of scalars F? Explain.
39. Let A denote the algebraic numbers, those numbers which are roots of polynomials
having rational coeï¬ƒcients which are in R. Show A can be considered a vector space
with ï¬eld of scalars Q. What is the dimension of this vector space, ï¬nite or inï¬nite?
n

40. As mentioned, for distinct algebraic numbers Î±i , the complex numbers {eÎ±i }i=1 are
linearly independent over the ï¬eld of scalars A where A denotes the algebraic numbers,
those which are roots of a polynomial having integer (rational) coeï¬ƒcients. What is
the dimension of the vector space C with ï¬eld of scalars A, ï¬nite or inï¬nite? If the
ï¬eld of scalars were C instead of A, would this change? What if the ï¬eld of scalars
were R?
41. Suppose F is a countable ï¬eld and let A be the algebraic numbers, those numbers
which are roots of a polynomial having coeï¬ƒcients in F which are in G, some other
ï¬eld containing F. Show A is also countable.
42. This problem is on partial fractions. Suppose you have
R (x) =

p (x)
, degree of p (x) < degree of denominator.
q1 (x) Â· Â· Â· qm (x)

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

224

VECTOR SPACES AND FIELDS

where the polynomials qi (x) are relatively prime and all the polynomials p (x) and
qi (x) have coeï¬ƒcients in a ï¬eld of scalars F. Thus there exist polynomials ai (x)
having coeï¬ƒcients in F such that
1=

m
âˆ‘

ai (x) qi (x)

i=1

Explain why
R (x) =

âˆ‘m
m
p (x) i=1 ai (x) qi (x) âˆ‘ ai (x) p (x)
âˆ
=
q1 (x) Â· Â· Â· qm (x)
jÌ¸=i qj (x)
i=1

Now continue doing this on each term in the above sum till ï¬nally you obtain an
expression of the form
m
âˆ‘
bi (x)
i=1

qi (x)

Using the Euclidean algorithm for polynomials, explain why the above is of the form
M (x) +

m
âˆ‘
ri (x)
i=1

qi (x)

where the degree of each ri (x) is less than the degree of qi (x) and M (x) is a polynomial. Now argue that M (x) = 0. From this explain why the usual partial fractions
expansion of calculus must be true. You can use the fact that every polynomial having
real coeï¬ƒcients factors into a product of irreducible quadratic polynomials and linear
polynomials having real coeï¬ƒcients. This follows from the fundamental theorem of
algebra in the appendix.
43. Suppose {f1 , Â· Â· Â· , fn } is an independent set of smooth functions deï¬ned on some interval (a, b). Now let A be an invertible n Ã— n matrix. Deï¬ne new functions {g1 , Â· Â· Â· , gn }
as follows.
ï£«
ï£¶
ï£«
ï£¶
g1
f1
ï£¬ .. ï£·
ï£¬ . ï£·
ï£­ . ï£¸ = A ï£­ .. ï£¸
gn

fn

Is it the case that {g1 , Â· Â· Â· , gn } is also independent? Explain why.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Linear Transformations
9.1

Matrix Multiplication As A Linear Transformation

Deï¬nition 9.1.1 Let V and W be two ï¬nite dimensional vector spaces. A function, L
which maps V to W is called a linear transformation and written L âˆˆ L (V, W ) if for all
scalars Î± and Î², and vectors v,w,
L (Î±v+Î²w) = Î±L (v) + Î²L (w) .
An example of a linear transformation is familiar matrix multiplication. Let A = (aij )
be an m Ã— n matrix. Then an example of a linear transformation L : Fn â†’ Fm is given by
(Lv)i â‰¡

n
âˆ‘

aij vj .

j=1

Here

9.2

ï£«

ï£¶
v1
ï£¬
ï£·
v â‰¡ ï£­ ... ï£¸ âˆˆ Fn .
vn

L (V, W ) As A Vector Space

Deï¬nition 9.2.1 Given L, M âˆˆ L (V, W ) deï¬ne a new element of L (V, W ) , denoted by
L + M according to the rule1
(L + M ) v â‰¡ Lv + M v.
For Î± a scalar and L âˆˆ L (V, W ) , deï¬ne Î±L âˆˆ L (V, W ) by
Î±L (v) â‰¡ Î± (Lv) .
You should verify that all the axioms of a vector space hold for L (V, W ) with the
above deï¬nitions of vector addition and scalar multiplication. What about the dimension
of L (V, W )?
Before answering this question, here is a useful lemma. It gives a way to deï¬ne linear
transformations and a way to tell when two of them are equal.
1 Note

that this is the standard way of deï¬ning the sum of two functions.

225

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

226

LINEAR TRANSFORMATIONS

Lemma 9.2.2 Let V and W be vector spaces and suppose {v1 , Â· Â· Â· , vn } is a basis for V.
Then if L : V â†’ W is given by Lvk = wk âˆˆ W and
( n
)
n
n
âˆ‘
âˆ‘
âˆ‘
L
a k vk â‰¡
ak Lvk =
ak wk
k=1

k=1

k=1

then L is well deï¬ned and is in L (V, W ) . Also, if L, M are two linear transformations such
that Lvk = M vk for all k, then M = L.
Proof: L is well deï¬ned on V because, since {v1 , Â· Â· Â· , vn } is a basis, there is exactly one
way to write a given vector of V as a linear combination. Next, observe
that L is obviously
âˆ‘n
linear from the deï¬nition. If L, M are equal on the basis, then if k=1 ak vk is an arbitrary
vector of V,
( n
)
( n
)
n
n
âˆ‘
âˆ‘
âˆ‘
âˆ‘
L
ak vk =
ak Lvk =
ak M v k = M
ak vk
k=1

k=1

k=1

k=1

and so L = M because they give the same result for every vector in V . 
The message is that when you deï¬ne a linear transformation, it suï¬ƒces to tell what it
does to a basis.
Theorem 9.2.3 Let V and W be ï¬nite dimensional linear spaces of dimension n and m
respectively Then dim (L (V, W )) = mn.
Proof: Let two sets of bases be
{v1 , Â· Â· Â· , vn } and {w1 , Â· Â· Â· , wm }
for V and W respectively. Using Lemma 9.2.2, let wi vj âˆˆ L (V, W ) be the linear transformation deï¬ned on the basis, {v1 , Â· Â· Â· , vn }, by
wi vk (vj ) â‰¡ wi Î´ jk
where Î´ ik = 1 if i = k and 0 if i Ì¸= k. I will show that L âˆˆ L (V, W ) is a linear combination
of these special linear transformations called dyadics.
Then let L âˆˆ L (V, W ). Since {w1 , Â· Â· Â· , wm } is a basis, there exist constants, djk such
that
m
âˆ‘
Lvr =
djr wj
j=1

Now consider the following sum of dyadics.
m âˆ‘
n
âˆ‘

dji wj vi

j=1 i=1

Apply this to vr . This yields
m âˆ‘
n
âˆ‘
j=1 i=1

dji wj vi (vr ) =

m âˆ‘
n
âˆ‘
j=1 i=1

dji wj Î´ ir =

m
âˆ‘

djr wi = Lvr

j=1

âˆ‘m âˆ‘n
Therefore, L = j=1 i=1 dji wj vi showing the span of the dyadics is all of L (V, W ) .
Now consider whether these dyadics form a linearly independent set. Suppose
âˆ‘
dik wi vk = 0.
i,k

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

9.3. THE MATRIX OF A LINEAR TRANSFORMATION

227

Are all the scalars dik equal to 0?
0=

âˆ‘

dik wi vk (vl ) =

m
âˆ‘

dil wi

i=1

i,k

and so, since {w1 , Â· Â· Â· , wm } is a basis, dil = 0 for each i = 1, Â· Â· Â· , m. Since l is arbitrary,
this shows dil = 0 for all i and l. Thus these linear transformations form a basis and this
shows that the dimension of L (V, W ) is mn as claimed because there are m choices for the
wi and n choices for the vj . 

9.3

The Matrix Of A Linear Transformation

Deï¬nition 9.3.1 In Theorem 9.2.3, the matrix of the linear transformation L âˆˆ L (V, W )
with respect to the ordered bases Î² â‰¡ {v1 , Â· Â· Â· , vn } for V and Î³ â‰¡ {wâˆ‘
1 , Â· Â· Â· , wm } for W is
deï¬ned to be [L] where [L]ij = dij . Thus this matrix is deï¬ned by L = i,j [L]ij wi vi . When
it is desired to feature the bases Î², Î³, this matrix will be denoted as [L]Î³Î² . When there is
only one basis Î², this is denoted as [L]Î² .
If V is an n dimensional vector space and Î² = {v1 , Â· Â· Â· , vn } is a basis for V, there exists
a linear map
q Î² : Fn â†’ V
deï¬ned as
qÎ² (a) â‰¡

n
âˆ‘

ai vi

i=1

ï£«

ï£¶
a1
n
ï£¬
ï£· âˆ‘
a = ï£­ ... ï£¸ =
ai ei ,
i=1
an
(
)T
for ei the standard basis vectors for Fn consisting of 0 Â· Â· Â· 1 Â· Â· Â· 0
. Thus the 1
is in the ith position and the other entries are 0.
It is clear that q deï¬ned in this way, is one to one, onto, and linear. For v âˆˆ V, qÎ²âˆ’1 (v)
is a vector in Fn called the component vector of v with respect to the basis {v1 , Â· Â· Â· , vn }.
where

Proposition 9.3.2 The matrix of a linear transformation with respect to ordered bases Î², Î³
as described above is characterized by the requirement that multiplication of the components
of v by [L]Î³Î² gives the components of Lv.
âˆ‘
Proof: This happens because by deï¬nition, if v = i xi vi , then
âˆ‘
âˆ‘âˆ‘
âˆ‘âˆ‘
Lv =
xi Lvi â‰¡
[L]ji xi wj =
[L]ji xi wj
i

i

j

j

i

âˆ‘

and so the j th component of Lv is i [L]ji xi , the j th component of the matrix times the
component vector of v. Could there be some other matrix which will do this? No, because if
such a matrix is M, then for any x , it follows from what was just shown that [L] x = M x.
Hence [L] = M . 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

228

LINEAR TRANSFORMATIONS

The above proposition shows that the following diagram determines the matrix of a
linear transformation. Here qÎ² and qÎ³ are the maps deï¬ned above with reference to the
ordered bases, {v1 , Â· Â· Â· , vn } and {w1 , Â· Â· Â· , wm } respectively.
Î² = {v1 , Â· Â· Â· , vn }

L
â†’
â—¦
â†’
[L]Î³Î²

V
qÎ² â†‘
Fn

{w1 , Â· Â· Â· , wm } = Î³

W
â†‘ qÎ³
Fm

(9.1)

In terms of this diagram, the matrix [L]Î³Î² is the matrix chosen to make the diagram
â€œcommuteâ€ It may help to write the description of [L]Î³Î² in the form
(

Lv1

Â·Â·Â·

Lvn

)

=

(

Â·Â·Â·

w1

wm

)

[L]Î³Î²

(9.2)

with the understanding that you do the multiplications in a formal manner just as you
would if everything were numbers. If this helps, use it. If it does not help, ignore it.
Example 9.3.3 Let
V â‰¡ { polynomials of degree 3 or less},
W â‰¡ { polynomials of degree 2 or less},

{
}
and L â‰¡ D where D is the diï¬€erentiation operator. A basis for V is Î² = 1, x, x2 , x3 and
a basis for W is Î³ = {1, x, x2 }.
What is the matrix of this linear transformation with respect to this basis? Using (9.2),
(
) (
)
0 1 2x 3x2 = 1 x x2 [D]Î³Î² .
It follows from this that the ï¬rst column of [D]Î³Î² is
ï£«

ï£¶
0
ï£­ 0 ï£¸
0
The next three columns of [D]Î³Î² are
ï£¶ ï£« ï£¶ ï£« ï£¶
0
0
1
ï£­ 0 ï£¸,ï£­ 2 ï£¸,ï£­ 0 ï£¸
3
0
0
ï£«

ï£«

and so
[D]Î³Î²

0
=ï£­ 0
0

1
0
0

0
2
0

ï£¶
0
0 ï£¸.
3

Now consider the important case where V = Fn , W = Fm , and the basis chosen is the
standard basis of vectors ei described above.
Î² = {e1 , Â· Â· Â· , en } , Î³ = {e1 , Â· Â· Â· , em }
Let L be a linear transformation from Fn to Fm and let A be the matrix of the transformation
with respect to these bases. In this case the coordinate maps qÎ² and qÎ³ are simply the
identity maps on Fn and Fm respectively, and can be accomplished by simply multiplying

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

9.3. THE MATRIX OF A LINEAR TRANSFORMATION

229

by the appropriate sized identity matrix. The requirement that A is the matrix of the
transformation amounts to
Lb = Ab
What about the situation where diï¬€erent pairs of bases are chosen for V and W ? How
are the two matrices with respect to these choices related? Consider the following diagram
which illustrates the situation.
Fn A2
Fm
âˆ’
â†’
qÎ² 2 â†“
â—¦ qÎ³ 2 â†“
V âˆ’
L
W
â†’
qÎ² 1 â†‘
â—¦ qÎ³ 1 â†‘
Fn A1
Fm
âˆ’
â†’
In this diagram qÎ² i and qÎ³ i are coordinate maps as described above. From the diagram,
qÎ³âˆ’1
qÎ³ 2 A2 qÎ²âˆ’1
qÎ² 1 = A1 ,
1
2
where qÎ²âˆ’1
qÎ² 1 and qÎ³âˆ’1
qÎ³ 2 are one to one, onto, and linear maps which may be accomplished
1
2
by multiplication by a square matrix. Thus there exist matrices P, Q such that P : Fn â†’ Fn
and Q : Fm â†’ Fm are invertible and
P A2 Q = A1 .
Example 9.3.4 Let Î² â‰¡ {v1 , Â· Â· Â· , vn } and Î³ â‰¡ {w1 , Â· Â· Â· , wn } be two bases for V . Let L
be the linear transformation which maps vi to wi . Find [L]Î³Î² . In case V = Fn and letting
Î´ = {e1 , Â· Â· Â· , en } , the usual basis for Fn , ï¬nd [L]Î´ .
âˆ‘ Letting Î´ ij be the symbol which equals 1 if i = j and 0 if i Ì¸= j, it follows that L =
i,j Î´ ij wi vj and so [L]Î³Î² = I the identity matrix. For the second part, you must have
(
and so
where

(

w1

Â·Â·Â·

w1

Â·Â·Â·

wn

)

=

(

v1

Â·Â·Â·

vn

)

[L]Î´

(
)âˆ’1 (
)
w1 Â· Â· Â· wn
[L]Î´ = v1 Â· Â· Â· vn
)
wn is the n Ã— n matrix having ith column equal to wi .

Deï¬nition 9.3.5 In the special case where V = W and only one basis is used for V = W,
this becomes
qÎ²âˆ’1
qÎ² 2 A2 qÎ²âˆ’1
qÎ² 1 = A1 .
1
2
Letting S be the matrix of the linear transformation qÎ²âˆ’1
qÎ² 1 with respect to the standard basis
2
vectors in Fn ,
S âˆ’1 A2 S = A1 .
(9.3)
When this occurs, A1 is said to be similar to A2 and A â†’ S âˆ’1 AS is called a similarity
transformation.
Recall the following.
Deï¬nition 9.3.6 Let S be a set. The symbol âˆ¼ is called an equivalence relation on S if it
satisï¬es the following axioms.
1. x âˆ¼ x

for all x âˆˆ S. (Reï¬‚exive)

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

230

LINEAR TRANSFORMATIONS

2. If x âˆ¼ y then y âˆ¼ x. (Symmetric)
3. If x âˆ¼ y and y âˆ¼ z, then x âˆ¼ z. (Transitive)
Deï¬nition 9.3.7 [x] denotes the set of all elements of S which are equivalent to x and [x]
is called the equivalence class determined by x or just the equivalence class of x.
Also recall the notion of equivalence classes.
Theorem 9.3.8 Let âˆ¼ be an equivalence class deï¬ned on a set S and let H denote the set
of equivalence classes. Then if [x] and [y] are two of these equivalence classes, either x âˆ¼ y
and [x] = [y] or it is not true that x âˆ¼ y and [x] âˆ© [y] = âˆ….
Theorem 9.3.9 In the vector space of n Ã— n matrices, deï¬ne
Aâˆ¼B
if there exists an invertible matrix S such that
A = S âˆ’1 BS.
Then âˆ¼ is an equivalence relation and A âˆ¼ B if and only if whenever V is an n dimensional
vector space, there exists L âˆˆ L (V, V ) and bases {v1 , Â· Â· Â· , vn } and {w1 , Â· Â· Â· , wn } such that
A is the matrix of L with respect to {v1 , Â· Â· Â· , vn } and B is the matrix of L with respect to
{w1 , Â· Â· Â· , wn }.
Proof: A âˆ¼ A because S = I works in the deï¬nition. If A âˆ¼ B , then B âˆ¼ A, because
A = S âˆ’1 BS
implies B = SAS âˆ’1 . If A âˆ¼ B and B âˆ¼ C, then
A = S âˆ’1 BS, B = T âˆ’1 CT
and so

âˆ’1

A = S âˆ’1 T âˆ’1 CT S = (T S)

CT S

which implies A âˆ¼ C. This veriï¬es the ï¬rst part of the conclusion.
Now let V be an n dimensional vector space, A âˆ¼ B so A = S âˆ’1 BS and pick a basis for
V,
Î² â‰¡ {v1 , Â· Â· Â· , vn }.
Deï¬ne L âˆˆ L (V, V ) by
Lvi â‰¡

âˆ‘

aji vj

j

where A = (aij ) . Thus A is the matrix of the linear transformation L. Consider the diagram
Fn
qÎ³ â†“
V
qÎ² â†‘
Fn

B
Fn
âˆ’
â†’
â—¦ qÎ³ â†“
L
V
âˆ’
â†’
â—¦ qÎ² â†‘
A
Fn
âˆ’
â†’

where qÎ³ is chosen to make the diagram commute. Thus we need S = qÎ³âˆ’1 qÎ² which requires
qÎ³ = qÎ² S âˆ’1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

9.3. THE MATRIX OF A LINEAR TRANSFORMATION

231

Then it follows that B is the matrix of L with respect to the basis
{qÎ³ e1 , Â· Â· Â· , qÎ³ en } â‰¡ {w1 , Â· Â· Â· , wn }.
That is, A and B are matrices of the same linear transformation L. Conversely, if A âˆ¼ B,
let L be as just described. Thus L = qÎ² AqÎ²âˆ’1 = qÎ² SBS âˆ’1 qÎ²âˆ’1 . Let qÎ³ â‰¡ qÎ² S and it follows
that B is the matrix of L with respect to {qÎ² Se1 , Â· Â· Â· , qÎ² Sen }. 
What if the linear transformation consists of multiplication by a matrix A and you want
to ï¬nd the matrix of this linear transformation with respect to another basis? Is there an
easy way to do it? The next proposition considers this.
Proposition 9.3.10 Let A be an mÃ—n matrix and let L be the linear transformation which
is deï¬ned by
( n
)
n
m âˆ‘
n
âˆ‘
âˆ‘
âˆ‘
L
xk ek â‰¡
(Aek ) xk â‰¡
Aik xk ei
k=1

i=1 k=1

k=1

In simple language, to ï¬nd Lx, you multiply on the left of x by A. (A is the matrix of L
with respect to the standard basis.) Then the matrix M of this linear transformation with
respect to the bases Î² = {u1 , Â· Â· Â· , un } for Fn and Î³ = {w1 , Â· Â· Â· , wm } for Fm is given by

where

(

w1

Â·Â·Â·

(
)âˆ’1 (
)
M = w1 Â· Â· Â· wm
A u1 Â· Â· Â· un
)
wm is the m Ã— m matrix which has wj as its j th column.

Proof: Consider the following diagram.
{u1 , Â· Â· Â· , un }

Fn
qÎ² â†‘
Fn

L
â†’
â—¦
â†’
M

Fm
â†‘ qÎ³
Fm

{w1 , Â· Â· Â· , wm }

Here the coordinate maps are deï¬ned in the usual way. Thus
qÎ²

(

x1

Â·Â·Â·

xn

)T

â‰¡

n
âˆ‘

xi ui .

i=1

Therefore, q(Î² can be considered
the same as multiplication of a vector in Fn on the left by
)
the matrix u1 Â· Â· Â· un . Similar considerations apply to qÎ³ . Thus it is desired to have
the following for an arbitrary x âˆˆ Fn .
)
)
(
(
A u1 Â· Â· Â· un x = w1 Â· Â· Â· wn M x
Therefore, the conclusion of the proposition follows. 
In the special case where m = n and F = C or R and {u1 , Â· Â· Â· , un } is an orthonormal
basis and you want M , the matrix of L with respect to this new orthonormal basis, it follows
from the above that
(
)âˆ— (
)
M = u1 Â· Â· Â· um
A u1 Â· Â· Â· un = U âˆ— AU
where U is a unitary matrix. Thus matrices with respect to two orthonormal bases are
unitarily similar.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

232

LINEAR TRANSFORMATIONS

Deï¬nition 9.3.11 An n Ã— n matrix A, is diagonalizable if there exists an invertible n Ã— n
matrix S such that S âˆ’1 AS = D, where D is a diagonal matrix. Thus D has zero entries
everywhere except on the main diagonal. Write diag (Î»1 Â· Â· Â· , Î»n ) to denote the diagonal
matrix having the Î»i down the main diagonal.
The following theorem is of great signiï¬cance.
Theorem 9.3.12 Let A be an n Ã— n matrix. Then A is diagonalizable if and only if Fn has
a basis of eigenvectors of A. In this case, S of Deï¬nition 9.3.11 consists of the n Ã— n matrix
whose columns are the eigenvectors of A and D = diag (Î»1 , Â· Â· Â· , Î»n ) .
Proof: Suppose ï¬rst that Fn has a basis of eigenvectors, {v1 ï£«
, Â· Â· Â· , vnï£¶} where Avi = Î»i vi .
uT1
(
)
ï£¬
ï£·
Then let S denote the matrix v1 Â· Â· Â· vn and let S âˆ’1 â‰¡ ï£­ ... ï£¸ where
{
uTi vj = Î´ ij â‰¡

uTn
1 if i = j
.
0 if i Ì¸= j

S âˆ’1 exists because S has rank n. Then from block multiplication,
ï£« T ï£¶
ï£« T ï£¶
u1
u1
ï£¬ .. ï£·
ï£¬ .. ï£·
âˆ’1
S AS = ï£­ . ï£¸ (Av1 Â· Â· Â· Avn ) = ï£­ . ï£¸ (Î»1 v1 Â· Â· Â· Î»n vn )
uTn

ï£«

ï£¬
ï£¬
=ï£¬
ï£­

Î»1
0
..
.

0
Î»2
..
.

Â·Â·Â·
0
..
.

0

Â·Â·Â·

0

0
Â·Â·Â·
..
.
Î»n

uTn
ï£¶
ï£·
ï£·
ï£· = D.
ï£¸

âˆ’1

Next suppose A is diagonalizable so S AS = D â‰¡ diag (Î»1 , Â· Â· Â· , Î»n ) . Then the columns
of S form a basis because S âˆ’1 is given to exist. (It only remains) to verify that these
v1 Â· Â· Â· vn , AS = SD and so
columns
of S are eigenvectors.
But letting) S =
(
) (
Av1 Â· Â· Â· Avn = Î»1 v1 Â· Â· Â· Î»n vn which shows that Avi = Î»i vi . 
It makes sense to speak of the determinant of a linear transformation as described in the
following corollary.
Corollary 9.3.13 Let L âˆˆ L (V, V ) where V is an n dimensional vector space and let A be
the matrix of this linear transformation with respect to a basis on V. Then it is possible to
deï¬ne
det (L) â‰¡ det (A) .
Proof: Each choice of basis for V determines a matrix for L with respect to the basis.
If A and B are two such matrices, it follows from Theorem 9.3.9 that
A = S âˆ’1 BS
and so
But

(
)
det (A) = det S âˆ’1 det (B) det (S) .
(
)
(
)
1 = det (I) = det S âˆ’1 S = det (S) det S âˆ’1

and so
det (A) = det (B) 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

9.3. THE MATRIX OF A LINEAR TRANSFORMATION

233

Deï¬nition 9.3.14 Let A âˆˆ L (X, Y ) where X and Y are ï¬nite dimensional vector spaces.
Deï¬ne rank (A) to equal the dimension of A (X) .
The following theorem explains how the rank of A is related to the rank of the matrix
of A.
Theorem 9.3.15 Let A âˆˆ L (X, Y ). Then rank (A) = rank (M ) where M is the matrix of
A taken with respect to a pair of bases for the vector spaces X, and Y.
Proof: Recall the diagram which describes what is meant by the matrix of A. Here the
two bases are as indicated.
Î² = {v1 , Â· Â· Â· , vn }

X
A
Y
âˆ’
â†’
qÎ² â†‘ â—¦ â†‘ qÎ³
m
Fn M
âˆ’
â†’ F

{w1 , Â· Â· Â· , wm } = Î³

Let {Ax1 , Â· Â· Â· , Axr } be a basis for AX. Thus
{
}
qÎ³ M qÎ²âˆ’1 x1 , Â· Â· Â· , qÎ³ M qÎ²âˆ’1 xr
is a basis for AX. It follows that
{
}
âˆ’1
âˆ’1
M qX
x1 , Â· Â· Â· , M qX
xr
is linearly independent and so rank (A) â‰¤ rank (M ) . However, one could interchange the
roles of M and A in the above argument and thereby turn the inequality around. 
The following result is a summary of many concepts.
Theorem 9.3.16 Let L âˆˆ L (V, V ) where V is a ï¬nite dimensional vector space. Then the
following are equivalent.
1. L is one to one.
2. L maps a basis to a basis.
3. L is onto.
4. det (L) Ì¸= 0
5. If Lv = 0 then v = 0.
âˆ‘n
n
Proof: Suppose
âˆ‘n ï¬rst L is one to one and let Î² = {vi }i=1 be a basis. Then if i=1 ci Lvi =
0 it follows L ( i=1
âˆ‘nci vi ) = 0 which means that since L (0) = 0, and L is one to one, it must
be the case that i=1 ci vi = 0. Since {vi } is a basis, each ci = 0 which shows {Lvi } is a
linearly independent set. Since there are n of these, it must be that this is a basis.
Now suppose 2.). Then letting {vi } âˆ‘
be a basis, and yâˆ‘
âˆˆ V, it follows from part 2.) that
n
n
there are constants, {ci } such that y = i=1 ci Lvi = L ( i=1 ci vi ) . Thus L is onto. It has
been shown that 2.) implies 3.).
Now suppose 3.). Then the operation consisting of multiplication by the matrix of L, [L],
must be onto. However, the vectors in Fn so obtained, consist of linear combinations of the
columns of [L] . Therefore, the column rank of [L] is n. By Theorem 3.3.23 this equals the
determinant rank and so det ([L]) â‰¡ det (L) Ì¸= 0.
Now assume 4.) If Lv = 0 for some v Ì¸= 0, it follows that [L] x = 0 for some x Ì¸= 0.
Therefore, the columns of [L] are linearly dependent and so by Theorem 3.3.23, det ([L]) =
det (L) = 0 contrary to 4.). Therefore, 4.) implies 5.).

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

234

LINEAR TRANSFORMATIONS

Now suppose 5.) and suppose Lv = Lw. Then L (v âˆ’ w) = 0 and so by 5.), v âˆ’ w = 0
showing that L is one to one. 
Also it is important to note that composition of linear transformations corresponds to
multiplication of the matrices. Consider the following diagram in which [A]Î³Î² denotes the
matrix of A relative to the bases Î³ on Y and Î² on X, [B]Î´Î³ deï¬ned similarly.
X
qÎ² â†‘
Fn

A
Y
âˆ’
â†’
â—¦
â†‘ qÎ³
[A]Î³Î² Fm
âˆ’âˆ’âˆ’â†’

B
Z
âˆ’
â†’
â—¦
â†‘ qÎ´
[B]Î´Î³ Fp
âˆ’âˆ’âˆ’â†’

where A and B are two linear transformations, A âˆˆ L (X, Y ) and B âˆˆ L (Y, Z) . Then
B â—¦ A âˆˆ L (X, Z) and so it has a matrix with respect to bases given on X and Z, the
coordinate maps for these bases being qÎ² and qÎ´ respectively. Then
B â—¦ A = qÎ´ [B]Î´Î³ qÎ³ qÎ³âˆ’1 [A]Î³Î² qÎ²âˆ’1 = qÎ´ [B]Î´Î³ [A]Î³Î² qÎ²âˆ’1 .
But this shows that [B]Î´Î³ [A]Î³Î² plays the role of [B â—¦ A]Î´Î² , the matrix of B â—¦ A. Hence the
matrix of B â—¦ A equals the product of the two matrices [A]Î³Î² and [B]Î´Î³ . Of course it is
interesting to note that although [B â—¦ A]Î´Î² must be unique, the matrices, [A]Î³Î² and [B]Î´Î³
are not unique because they depend on Î³, the basis chosen for Y .
Theorem 9.3.17 The matrix of the composition of linear transformations equals the product of the matrices of these linear transformations.

9.3.1

Some Geometrically Deï¬ned Linear Transformations

If T is any linear transformation which maps Fn to Fm , there is always an m Ã— n matrix
A â‰¡ [T ] with the property that
Ax = T x
(9.4)
for all x âˆˆ Fn . You simply take the matrix of the linear transformation with respect to the
standard basis. What is the form of A? Suppose T : Fn â†’ Fm is a linear transformation
and you want to ï¬nd the matrix deï¬ned by this linear transformation as described in (9.4).
Then if x âˆˆ Fn it follows
n
âˆ‘
x=
xi ei
i=1

where ei is the vector which has zeros in every slot but the ith and a 1 in this slot. Then
since T is linear,
n
âˆ‘
Tx =
xi T (ei )
i=1

ï£«

|
= ï£­ T (e1 )
|

Â·Â·Â·

ï£«
ï£¶ï£« x ï£¶
1
|
ï£¬
ï£·
ï£¬
T (en ) ï£¸ ï£­ ... ï£¸ â‰¡ A ï£­
|
xn

ï£¶
x1
.. ï£·
. ï£¸
xn

and so you see that the matrix desired is obtained from letting the ith column equal T (ei ) .
This proves the following theorem.
Theorem 9.3.18 Let T be a linear transformation from Fn to Fm . Then the matrix A
satisfying (9.4) is given by
ï£«
ï£¶
|
|
ï£­ T (e1 ) Â· Â· Â· T (en ) ï£¸
|
|

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

9.3. THE MATRIX OF A LINEAR TRANSFORMATION

235

where T ei is the ith column of A.
Example 9.3.19 Determine the matrix for the transformation mapping R2 to R2 which
consists of rotating every vector counter clockwise through an angle of Î¸.
(
)
(
)
1
0
Let e1 â‰¡
and e2 â‰¡
. These identify the geometric vectors which point
0
1
along the positive x axis and positive y axis as shown.
e2

6
-

e1

From Theorem 9.3.18, you only need to ï¬nd T e1 and T e2 , the ï¬rst being the ï¬rst column
of the desired matrix A and the second being the second column. From drawing a picture
and doing a little geometry, you see that
(
)
(
)
cos Î¸
âˆ’ sin Î¸
T e1 =
, T e2 =
.
sin Î¸
cos Î¸
Therefore, from Theorem 9.3.18,
(
A=

cos Î¸
sin Î¸

âˆ’ sin Î¸
cos Î¸

)

Example 9.3.20 Find the matrix of the linear transformation which is obtained by ï¬rst
rotating all vectors through an angle of Ï• and then through an angle Î¸. Thus you want the
linear transformation which rotates all angles through an angle of Î¸ + Ï•.
Let TÎ¸+Ï• denote the linear transformation which rotates every vector through an angle
of Î¸ + Ï•. Then to get TÎ¸+Ï• , you could ï¬rst do TÏ• and then do TÎ¸ where TÏ• is the linear
transformation which rotates through an angle of Ï• and TÎ¸ is the linear transformation
which rotates through an angle of Î¸. Denoting the corresponding matrices by AÎ¸+Ï• , AÏ• ,
and AÎ¸ , you must have for every x
AÎ¸+Ï• x = TÎ¸+Ï• x = TÎ¸ TÏ• x = AÎ¸ AÏ• x.
Consequently, you must have
(

AÎ¸+Ï•

)
cos (Î¸ + Ï•) âˆ’ sin (Î¸ + Ï•)
= AÎ¸ AÏ•
sin (Î¸ + Ï•) cos (Î¸ + Ï•)
(
)(
)
cos Î¸ âˆ’ sin Î¸
cos Ï• âˆ’ sin Ï•
=
.
sin Î¸ cos Î¸
sin Ï• cos Ï•

=

Therefore,
(
) (
cos (Î¸ + Ï•) âˆ’ sin (Î¸ + Ï•)
cos Î¸ cos Ï• âˆ’ sin Î¸ sin Ï•
=
sin (Î¸ + Ï•) cos (Î¸ + Ï•)
sin Î¸ cos Ï• + cos Î¸ sin Ï•

âˆ’ cos Î¸ sin Ï• âˆ’ sin Î¸ cos Ï•
cos Î¸ cos Ï• âˆ’ sin Î¸ sin Ï•

)
.

Donâ€™t these look familiar? They are the usual trig. identities for the sum of two angles
derived here using linear algebra concepts.
Example 9.3.21 Find the matrix of the linear transformation which rotates vectors in
R3 counterclockwise about the positive z axis.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

236

LINEAR TRANSFORMATIONS

Let T be the name of this linear transformation. In this case, T e3 = e3 , T e1 =
T
T
(cos Î¸, sin Î¸, 0) , and T e2 = (âˆ’ sin Î¸, cos Î¸, 0) . Therefore, the matrix of this transformation
is just
ï£«
ï£¶
cos Î¸ âˆ’ sin Î¸ 0
ï£­ sin Î¸ cos Î¸ 0 ï£¸
(9.5)
0
0
1
In Physics it is important to consider the work done by a force ï¬eld on an object. This
involves the concept of projection onto a vector. Suppose you want to ï¬nd the projection
of a vector, v onto the given vector, u, denoted by proju (v) This is done using the dot
product as follows.
(v Â· u)
proju (v) =
u
uÂ·u
Because of properties of the dot product, the map v â†’ proju (v) is linear,
(
)
(v Â· u)
(w Â· u)
Î±v+Î²w Â· u
proju (Î±v+Î²w) =
u=Î±
u+Î²
u
uÂ·u
uÂ·u
uÂ·u
= Î± proju (v) + Î² proju (w) .
T

Example 9.3.22 Let the projection map be deï¬ned above and let u = (1, 2, 3) . Find the
matrix of this linear transformation with respect to the usual basis.
You can ï¬nd this matrix in the same way as in earlier examples. proju (ei ) gives the ith
column of the desired matrix. Therefore, it is only necessary to ï¬nd
( e Â·u )
i
proju (ei ) â‰¡
u
uÂ·u
For the given vector in the example, this implies the columns of the desired matrix are
ï£¶
ï£¶
ï£« ï£¶
ï£«
ï£«
1
1
1
3
1 ï£­ ï£¸ 2 ï£­
ï£­ 2 ï£¸.
2
2 ï£¸,
,
14
14
14
3
3
3
Hence the matrix is

ï£¶
ï£«
1 2 3
1 ï£­
2 4 6 ï£¸.
14
3 6 9

Example 9.3.23 Find the matrix of the linear transformation which reï¬‚ects all vectors in
R3 through the xz plane.
As illustrated above, you just need to ï¬nd T ei where T is the name of the transformation.
But T e1 = e1 , T e3 = e3 , and T e2 = âˆ’e2 so the matrix is
ï£«
ï£¶
1 0 0
ï£­ 0 âˆ’1 0 ï£¸ .
0 0 1
Example 9.3.24 Find the matrix of the linear transformation which ï¬rst rotates counter
clockwise about the positive z axis and then reï¬‚ects through the xz plane.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

9.3. THE MATRIX OF A LINEAR TRANSFORMATION
This linear transformation is just the composition of two
matrices
ï£«
ï£¶ ï£«
cos Î¸ âˆ’ sin Î¸ 0
1 0
ï£­ sin Î¸ cos Î¸ 0 ï£¸ , ï£­ 0 âˆ’1
0
0
1
0 0
respectively. Thus the matrix desired is
ï£«
ï£¶ï£«
1 0 0
cos Î¸ âˆ’ sin Î¸
ï£­ 0 âˆ’1 0 ï£¸ ï£­ sin Î¸ cos Î¸
0 0 1
0
0

9.3.2

237
linear transformations having
ï£¶
0
0 ï£¸
1

ï£¶ ï£«
0
cos Î¸
0 ï£¸ = ï£­ âˆ’ sin Î¸
1
0

âˆ’ sin Î¸
âˆ’ cos Î¸
0

ï£¶
0
0 ï£¸.
1

Rotations About A Given Vector

As an application, I will consider the problem of rotating counter clockwise about a given
unit vector which is possibly not one of the unit vectors in coordinate directions. First
consider a pair of perpendicular unit vectors, u1 and u2 and the problem of rotating in the
counterclockwise direction about u3 where u3 = u1 Ã— u2 so that u1 , u2 , u3 forms a right
handed orthogonal coordinate system. Thus the vector u3 is coming out of the page.
Î¸
Î¸
u1
u2R
?
Let T denote the desired rotation. Then
T (au1 + bu2 + cu3 ) = aT u1 + bT u2 + cT u3
= (a cos Î¸ âˆ’ b sin Î¸) u1 + (a sin Î¸ + b cos Î¸) u2 + cu3 .
Thus in terms of the basis Î³ â‰¡ {u1 , u2 , u3 } , the matrix of this transformation is
ï£«
ï£¶
cos Î¸ âˆ’ sin Î¸ 0
[T ]Î³ â‰¡ ï£­ sin Î¸ cos Î¸ 0 ï£¸ .
0
0
1
I want to obtain the matrix of the transformation in terms of the usual basis Î² â‰¡ {e1 , e2 , e3 }
because it is in terms of this basis that we usually deal with vectors. From Proposition 9.3.10,
if [T ]Î² is this matrix,
ï£«
ï£¶
cos Î¸ âˆ’ sin Î¸ 0
ï£­ sin Î¸ cos Î¸ 0 ï£¸
0
0
1
(
)âˆ’1
(
)
u1 u2 u3
=
[T ]Î² u1 u2 u3
and so you can solve for [T ]Î² if you know the ui .
Recall why this is so.
R3 [T ]Î³
âˆ’âˆ’â†’
qÎ³ â†“
â—¦
R3 âˆ’âˆ’
T
â†’
Iâ†‘
â—¦
R3 [T ]Î²
âˆ’âˆ’â†’

Saylor URL: http://www.saylor.org/courses/ma212/

R3
qÎ³ â†“
R3
Iâ†‘
R3

The Saylor Foundation

238

LINEAR TRANSFORMATIONS

The map qÎ³ is accomplished by a multiplication on the left by
[T ]Î² = qÎ³ [T ]Î³ qÎ³âˆ’1 =

(

u1

u2

u3

)

[T ]Î³

(

u1

(

u1

u2

u2
u3

u3

)âˆ’1

)

. Thus

.

Suppose the unit vector u3 about which the counterclockwise rotation takes place is
(a, b, c). Then I obtain vectors, u1 and u2 such that {u1 , u2 , u3 } is a right handed orthonormal system with u3 = (a, b, c) and then use the above result. It is of course somewhat
arbitrary how this is accomplished. I will assume however, that |c| Ì¸= 1 since otherwise you
are looking at either clockwise or counter clockwise rotation about the positive z axis and
this is a problem which has been dealt with earlier. (If c = âˆ’1, it amounts to clockwise
rotation about the positive z axis while if c = 1, it is counter clockwise rotation about the
positive z axis.)
Then let u3 = (a, b, c) and u2 â‰¡ âˆša21+b2 (b, âˆ’a, 0) . This one is perpendicular to u3 . If
{u1 , u2 , u3 } is to be a right hand system it is necessary to have
u1 = u2 Ã— u3 = âˆš

(

1
(a2

+

b2 ) (a2

+

b2

+

c2 )

âˆ’ac, âˆ’bc, a2 + b2

)

Now recall that u3 is a unit vector and so the above equals
(
)
1
âˆš
âˆ’ac, âˆ’bc, a2 + b2
2
2
(a + b )
Then from the above, A is
ï£«
âˆš b
âˆš âˆ’ac
2 +b2 )
a2 +b2
ï£¬ (aâˆ’bc
ï£¬ âˆš
âˆš âˆ’a
ï£­ (a2 +b2 )
a2 +b2
âˆš
2
2
a +b
0

given by
ï£¶
ï£«
a
cos Î¸
ï£·
ï£­ sin Î¸
b ï£·
ï£¸
0
c

âˆ’ sin Î¸
cos Î¸
0

ï£«
ï£¶
âˆš âˆ’ac
2 +b2 )
0
ï£¬ (aâˆ’bc
âˆš 2 2
0 ï£¸ï£¬
ï£­ (a +b )
âˆš
1
a2 + b2

âˆš b
a2 +b2
âˆš âˆ’a
a2 +b2

0

a

ï£¶âˆ’1

ï£·
b ï£·
ï£¸
c

Of course the matrix is an orthogonal matrix so it is easy to take the inverse by simply
taking the transpose. Then doing the computation and then some simpliï¬cation yields
(
)
a2 + 1 âˆ’ a2 cos Î¸
= ï£­ ab (1 âˆ’ cos Î¸) + c sin Î¸
ac (1 âˆ’ cos Î¸) âˆ’ b sin Î¸
ï£«

ab (1 âˆ’(cos Î¸) âˆ’
) c sin Î¸
b2 + 1 âˆ’ b2 cos Î¸
bc (1 âˆ’ cos Î¸) + a sin Î¸

ï£¶
ac (1 âˆ’ cos Î¸) + b sin Î¸
ï£¸.
bc (1 âˆ’(cos Î¸) âˆ’
) a sin Î¸
2
2
c + 1 âˆ’ c cos Î¸

(9.6)

With this, it is clear how to rotate clockwise about the unit vector, (a, b, c) . Just rotate
counter clockwise through an angle of âˆ’Î¸. Thus the matrix for this clockwise rotation is just
(
)
ï£¶
ï£«
a2 + 1 âˆ’ a2 cos Î¸
ab (1 âˆ’(cos Î¸) +
c sin Î¸ ac (1 âˆ’ cos Î¸) âˆ’ b sin Î¸
)
ï£¸.
b2 + 1 âˆ’ b2 cos Î¸
bc (1 âˆ’(cos Î¸) +
= ï£­ ab (1 âˆ’ cos Î¸) âˆ’ c sin Î¸
) a sin Î¸
ac (1 âˆ’ cos Î¸) + b sin Î¸ bc (1 âˆ’ cos Î¸) âˆ’ a sin Î¸
c2 + 1 âˆ’ c2 cos Î¸
In deriving (9.6) it was assumed that c Ì¸= Â±1 but even in this case, it gives the correct
answer. Suppose for example that c = 1 so you are rotating in the counter clockwise
direction about the positive z axis. Then a, b are both equal to zero and (9.6) reduces to
(9.5).

9.3.3

The Euler Angles

An important application of the above theory is to the Euler angles, important in the
mechanics of rotating bodies. Lagrange studied these things back in the 1700â€™s. To describe

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

9.3. THE MATRIX OF A LINEAR TRANSFORMATION

239

the Euler angles consider the following picture in which x1 , x2 and x3 are the usual coordinate
axes ï¬xed in space and the axes labeled with a superscript denote other coordinate axes.
Here is the picture.
x13
x23
x3 = x13
x23 = x33
Î¸
Î¸
Ï•
Ï•
x1

x12
x2

x22
x12
Ïˆ

x11 = x21

x11

We obtain Ï• by rotating counter clockwise
has the matrix
ï£«
cos Ï• âˆ’ sin Ï•
ï£­ sin Ï• cos Ï•
0
0

x21

x32
x22

Ïˆ
x31

about the ï¬xed x3 axis. Thus this rotation
ï£¶
0
0 ï£¸ â‰¡ M1 (Ï•)
1

Next rotate counter clockwise about the x11 axis which results from the ï¬rst rotation through
an angle of Î¸. Thus it is desired to rotate counter clockwise through an angle Î¸ about the
unit vector
ï£¶
ï£¶ï£« ï£¶ ï£«
ï£«
cos Ï•
1
cos Ï• âˆ’ sin Ï• 0
ï£­ sin Ï• cos Ï• 0 ï£¸ ï£­ 0 ï£¸ = ï£­ sin Ï• ï£¸ .
0
0
0
0
1
Therefore, in (9.6), a = cos Ï•, b = sin Ï•, and c = 0. It follows the matrix of this transformation with respect to the usual basis is
ï£¶
ï£«
cos2 Ï• + sin2 Ï• cos Î¸ cos Ï• sin Ï• (1 âˆ’ cos Î¸)
sin Ï• sin Î¸
ï£­ cos Ï• sin Ï• (1 âˆ’ cos Î¸) sin2 Ï• + cos2 Ï• cos Î¸ âˆ’ cos Ï• sin Î¸ ï£¸ â‰¡ M2 (Ï•, Î¸)
âˆ’ sin Ï• sin Î¸
cos Ï• sin Î¸
cos Î¸
Finally, we rotate counter clockwise about the positive x23 axis by Ïˆ. The vector in the
positive x13 axis is the same as the vector in the ï¬xed x3 axis. Thus the unit vector in the
positive direction of the x23 axis is
ï£«
ï£¶ï£« ï£¶
cos2 Ï• + sin2 Ï• cos Î¸ cos Ï• sin Ï• (1 âˆ’ cos Î¸)
sin Ï• sin Î¸
1
ï£­ cos Ï• sin Ï• (1 âˆ’ cos Î¸) sin2 Ï• + cos2 Ï• cos Î¸ âˆ’ cos Ï• sin Î¸ ï£¸ ï£­ 0 ï£¸
0
âˆ’ sin Ï• sin Î¸
cos Ï• sin Î¸
cos Î¸
ï£«
ï£¶
ï£«
ï£¶
cos2 Ï• + sin2 Ï• cos Î¸
cos2 Ï• + sin2 Ï• cos Î¸
ï£­
ï£¸
ï£­
=
=
cos Ï• sin Ï• (1 âˆ’ cos Î¸)
cos Ï• sin Ï• (1 âˆ’ cos Î¸) ï£¸
âˆ’ sin Ï• sin Î¸
âˆ’ sin Ï• sin Î¸
and it is desired to rotate counter clockwise through an angle of Ïˆ about this vector. Thus,
in this case,
a = cos2 Ï• + sin2 Ï• cos Î¸, b = cos Ï• sin Ï• (1 âˆ’ cos Î¸) , c = âˆ’ sin Ï• sin Î¸.
and you could substitute in to the formula of Theorem (9.6) and obtain a matrix which represents the linear transformation obtained by rotating counter clockwise about the positive
x23 axis, M3 (Ï•, Î¸, Ïˆ) . Then what would be the matrix with respect to the usual basis for the

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

240

LINEAR TRANSFORMATIONS

linear transformation which is obtained as a composition of the three just described? By
Theorem 9.3.17, this matrix equals the product of these three,
M3 (Ï•, Î¸, Ïˆ) M2 (Ï•, Î¸) M1 (Ï•) .
I leave the details to you. There are procedures due to Lagrange which will allow you to
write diï¬€erential equations for the Euler angles in a rotating body. To give an idea how
these angles apply, consider the following picture.
x3
x (t)
3

R

Ïˆ

Î¸

x2

Ï•
x1
line of nodes
This is as far as I will go on this topic. The point is, it is possible to give a systematic
description in terms of matrix multiplication of a very elaborate geometrical description of
a composition of linear transformations. You see from the picture it is possible to describe
the motion of the spinning top shown in terms of these Euler angles.

9.4

Eigenvalues And Eigenvectors Of Linear Transformations

Let V be a ï¬nite dimensional vector space. For example, it could be a subspace of Cn or Rn .
Also suppose A âˆˆ L (V, V ) .
Deï¬nition 9.4.1 The characteristic polynomial of A is deï¬ned as q (Î») â‰¡ det (Î»I âˆ’ A) .
The zeros of q (Î») in C are called the eigenvalues of A.
Lemma 9.4.2 When Î» is an eigenvalue of A which is also in F, the ï¬eld of scalars, then
there exists v Ì¸= 0 such that Av = Î»v.
Proof: This follows from Theorem 9.3.16. Since Î» âˆˆ F,
Î»I âˆ’ A âˆˆ L (V, V )
and since it has zero determinant, it is not one to one. 
The following lemma gives the existence of something called the minimal polynomial.
Lemma 9.4.3 Let A âˆˆ L (V, V ) where V is a ï¬nite dimensional vector space of dimension
n with arbitrary ï¬eld of scalars. Then there exists a unique polynomial of the form
p (Î») = Î»m + cmâˆ’1 Î»mâˆ’1 + Â· Â· Â· + c1 Î» + c0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

9.4. EIGENVALUES AND EIGENVECTORS OF LINEAR TRANSFORMATIONS

241

such that p (A) = 0 and m is as small as possible for this to occur.
2

Proof: Consider the linear transformations, I, A, A2 , Â· Â· Â· , An . There are n2 + 1 of these
transformations and so by Theorem 9.2.3 the set is linearly dependent. Thus there exist
constants, ci âˆˆ F such that
n2
âˆ‘
c0 I +
ck Ak = 0.
k=1

This implies there exists a polynomial, q (Î») which has the property that q (A) = 0. In fact,
âˆ‘n2
one example is q (Î») â‰¡ c0 + k=1 ck Î»k . Dividing by the leading term, it can be assumed
this polynomial is of the form Î»m + cmâˆ’1 Î»mâˆ’1 + Â· Â· Â· + c1 Î» + c0 , a monic polynomial. Now
consider all such monic polynomials, q such that q (A) = 0 and pick the one which has the
smallest degree m. This is called the minimal polynomial and will be denoted here by p (Î») .
If there were two minimal polynomials, the one just found and another,
Î»m + dmâˆ’1 Î»mâˆ’1 + Â· Â· Â· + d1 Î» + d0 .
Then subtracting these would give the following polynomial,
q â€² (Î») = (dmâˆ’1 âˆ’ cmâˆ’1 ) Î»mâˆ’1 + Â· Â· Â· + (d1 âˆ’ c1 ) Î» + d0 âˆ’ c0
Since q â€² (A) = 0, this requires each dk = ck since otherwise you could divide by dk âˆ’ ck
where k is the largest one which is nonzero. Thus the choice of m would be contradicted. 
Theorem 9.4.4 Let V be a nonzero ï¬nite dimensional vector space of dimension n with
the ï¬eld of scalars equal to F. Suppose A âˆˆ L (V, V ) and for p (Î») the minimal polynomial
deï¬ned above, let Âµ âˆˆ F be a zero of this polynomial. Then there exists v Ì¸= 0,v âˆˆ V such
that
Av = Âµv.
If F = C, then A always has an eigenvector and eigenvalue. Furthermore, if {Î»1 , Â· Â· Â· , Î»m }
are the zeros of p (Î») in F, these are exactly the eigenvalues of A for which there exists an
eigenvector in V.
Proof: Suppose ï¬rst Âµ is a zero of p (Î») . Since p (Âµ) = 0, it follows
p (Î») = (Î» âˆ’ Âµ) k (Î»)
where k (Î») is a polynomial having coeï¬ƒcients in F. Since p has minimal degree, k (A) Ì¸= 0
and so there exists a vector, u Ì¸= 0 such that k (A) u â‰¡ v Ì¸= 0. But then
(A âˆ’ ÂµI) v = (A âˆ’ ÂµI) k (A) (u) = 0.
The next claim about the existence of an eigenvalue follows from the fundamental theorem of algebra and what was just shown.
It has been shown that every zero of p (Î») is an eigenvalue which has an eigenvector in
V . Now suppose Âµ is an eigenvalue which has an eigenvector in V so that Av = Âµv for some
v âˆˆ V, v Ì¸= 0. Does it follow Âµ is a zero of p (Î»)?
0 = p (A) v = p (Âµ) v
and so Âµ is indeed a zero of p (Î»). 
In summary, the theorem says that the eigenvalues which have eigenvectors in V are
exactly the zeros of the minimal polynomial which are in the ï¬eld of scalars F.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

242

9.5

LINEAR TRANSFORMATIONS

Exercises

1. If A, B, and C are each n Ã— n matrices and ABC is invertible, why are each of A, B,
and C invertible?
2. Give an example of a 3 Ã— 2 matrix with the property that the linear transformation
determined by this matrix is one to one but not onto.
3. Explain why Ax = 0 always has a solution whenever A is a linear transformation.
4. Review problem: Suppose det (A âˆ’ Î»I) = 0. Show using Theorem 3.1.15 there exists
x Ì¸= 0 such that (A âˆ’ Î»I) x = 0.
5. How does the minimal polynomial of an algebraic number relate to the minimal polynomial of a linear transformation? Can an algebraic number be thought of as a linear
transformation? How?
6. Recall the fact from algebra that if p (Î») and q (Î») are polynomials, then there exists
l (Î») , a polynomial such that
q (Î») = p (Î») l (Î») + r (Î»)
where the degree of r (Î») is less than the degree of p (Î») or else r (Î») = 0. With this in
mind, why must the minimal polynomial always divide the characteristic polynomial?
That is, why does there always exist a polynomial l (Î») such that p (Î») l (Î») = q (Î»)?
Can you give conditions which imply the minimal polynomial equals the characteristic
polynomial? Go ahead and use the Cayley Hamilton theorem.
7. In the following examples, a linear transformation, T is given by specifying its action
on a basis Î². Find its matrix with respect to this basis.
(
)
( )
(
)
(
) (
)
1
1
âˆ’1
âˆ’1
âˆ’1
(a) T
=2
+1
,T
=
2
2
1
1
1
(
)
( )
(
)
(
) (
)
0
0
âˆ’1
âˆ’1
0
(b) T
=2
+1
,T
=
1
1
1
1
1
(
)
( )
( )
(
)
(
) ( )
1
1
1
1
1
1
(c) T
=2
+1
,T
=1
âˆ’
0
2
0
2
0
2
8. Let Î² = {u1 , Â· Â· Â· , un } be a basis for Fn and let T : Fn â†’ Fn be deï¬ned as follows.
( n
)
n
âˆ‘
âˆ‘
T
ak uk =
ak bk uk
k=1

k=1

First show that T is a linear transformation. Next show that the matrix of T with
respect to this basis, [T ]Î² is
ï£«
ï£¶
b1
ï£¬
ï£·
..
ï£­
ï£¸
.
bn
Show that the above deï¬nition is equivalent to simply specifying T on the basis vectors
of Î² by
T (uk ) = bk uk .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

9.5. EXERCISES

243

9. â†‘In the situation of the above problem, let Î³ = {e1 , Â· Â· Â· , en } be the standard basis for
Fn where ek is the vector which has 1 in the k th entry and zeros elsewhere. Show that
[T ]Î³ =
)âˆ’1
(
)
(
u1 Â· Â· Â· un [T ]Î² u1 Â· Â· Â· un
(9.7)
10. â†‘Generalize the above problem to the situation where T is given by specifying its
action on the vectors of a basis Î² = {u1 , Â· Â· Â· , un } as follows.
T uk =

n
âˆ‘

ajk uj .

j=1

Letting A = (aij ) , verify that for Î³ = {e1 , Â· Â· Â· , en } , (9.7) still holds and that [T ]Î² = A.
11. Let P3 denote the set of real polynomials of degree no more than 3, deï¬ned on an
interval [a, b]. Show that P3 is a subspace of the{ vector space
} of all functions deï¬ned
on this interval. Show that a basis for P3 is 1, x, x2 , x3 . Now let D denote the
diï¬€erentiation operator which sends a function to its derivative. Show D is a linear
transformation which sends P3 to P3 . Find the matrix of this linear transformation
with respect to the given basis.
12. Generalize the above problem to Pn , the space of polynomials of degree no more than
n with basis {1, x, Â· Â· Â· , xn } .
13. In the situation of the above problem, let the linear transformation be T = D2 + 1,
deï¬ned as T f = f â€²â€² + f. Find the matrix of this linear transformation with respect to
the given basis {1, x, Â· Â· Â· , xn }. Write it down for n = 4.
14. In calculus, the following situation is encountered. There exists a vector valued function f :U â†’ Rm where U is an open subset of Rn . Such a function is said to have
a derivative or to be diï¬€erentiable at x âˆˆ U if there exists a linear transformation
T : Rn â†’ Rm such that
lim

vâ†’0

|f (x + v) âˆ’ f (x) âˆ’ T v|
= 0.
|v|

First show that this linear transformation, if it exists, must be unique. Next show
that for Î² = {e1 , Â· Â· Â· , en } , , the standard basis, the k th column of [T ]Î² is
âˆ‚f
(x) .
âˆ‚xk
Actually, the result of this problem is a well kept secret. People typically donâ€™t see
this in calculus. It is seen for the ï¬rst time in advanced calculus if then.
15. Recall that A is similar to B if there exists a matrix P such that A = P âˆ’1 BP. Show
that if A and B are similar, then they have the same determinant. Give an example
of two matrices which are not similar but have the same determinant.
16. Suppose A âˆˆ L (V, W ) where dim (V ) > dim (W ) . Show ker (A) Ì¸= {0}. That is, show
there exist nonzero vectors v âˆˆ V such that Av = 0.
17. A vector v is in the convex hull of a nonempty set S if there are ï¬nitely many vectors
of S, {v1 , Â· Â· Â· , vm } and nonnegative scalars {t1 , Â· Â· Â· , tm } such that
v=

m
âˆ‘
k=1

Saylor URL: http://www.saylor.org/courses/ma212/

tk vk ,

m
âˆ‘

tk = 1.

k=1

The Saylor Foundation

244

LINEAR TRANSFORMATIONS

Such a linear combination is called a convex combination.
Suppose now that S âŠ† V,
âˆ‘m
a vector space of dimension n. Show that if v = k=1 tk vk is a vector in the convex
hull for m > n + 1, then there exist other scalars {tâ€²k } such that
v=

mâˆ’1
âˆ‘

tâ€²k vk .

k=1

Thus every vector in the convex hull of S can be obtained as a convex combination
of at most n + 1 points of S. This incredible result is in Rudin [23]. Hint: Consider
L : Rm â†’ V Ã— R deï¬ned by
(m
)
m
âˆ‘
âˆ‘
L (a) â‰¡
ak vk ,
ak
k=1

k=1

Explain why ker (L) Ì¸= {0} . Next, letting a âˆˆ ker (L) \ {0} and Î» âˆˆ R, note that
Î»a âˆˆ ker (L) . Thus for all Î» âˆˆ R,
v=

m
âˆ‘

(tk + Î»ak ) vk .

k=1

Now vary Î» till some tk + Î»ak = 0 for some ak Ì¸= 0.
18. For those who know about compactness, use Problem 17 to show that if S âŠ† Rn and
S is compact, then so is its convex hull.
19. Suppose Ax = b has a solution. Explain why the solution is unique precisely when
Ax = 0 has only the trivial (zero) solution.
20. Let A be an n Ã— n matrix of elements of F. There are two cases. In the ï¬rst case,
F contains a splitting ï¬eld of pA (Î») so that p (Î») factors into a product of linear
polynomials having coeï¬ƒcients in F. It is the second case which is of interest here
where pA (Î») does not factor into linear factors having coeï¬ƒcients in F. Let G be a
splitting ï¬eld of pA (Î») and let qA (Î») be the minimal polynomial of A with respect
to the ï¬eld G. Explain why qA (Î») must divide pA (Î»). Now why must qA (Î») factor
completely into linear factors?
21. In Lemma 9.2.2 verify that L is linear.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Linear Transformations
Canonical Forms
10.1

A Theorem Of Sylvester, Direct Sums

The notation is deï¬ned as follows.
Deï¬nition 10.1.1 Let L âˆˆ L (V, W ) . Then ker (L) â‰¡ {v âˆˆ V : Lv = 0} .
Lemma 10.1.2 Whenever L âˆˆ L (V, W ) , ker (L) is a subspace.
Proof: If a, b are scalars and v,w are in ker (L) , then
L (av + bw) = aL (v) + bL (w) = 0 + 0 = 0 
Suppose now that A âˆˆ L (V, W ) and B âˆˆ L (W, U ) where V, W, U are all ï¬nite dimensional vector spaces. Then it is interesting to consider ker (BA). The following theorem of
Sylvester is a very useful and important result.
Theorem 10.1.3 Let A âˆˆ L (V, W ) and B âˆˆ L (W, U ) where V, W, U are all vector spaces
over a ï¬eld F. Suppose also that ker (A) and A (ker (BA)) are ï¬nite dimensional subspaces.
Then
dim (ker (BA)) â‰¤ dim (ker (B)) + dim (ker (A)) .
Proof: If x âˆˆ ker (BA) , then Ax âˆˆ ker (B) and so A (ker (BA)) âŠ† ker (B) . The following
picture may help.
ker(BA)

ker(B)
A

ker(A)

-

A(ker(BA))

Now let {x1 , Â· Â· Â· , xn } be a basis of ker (A) and
âˆ‘m let {Ay1 , Â· Â· Â· , Aym } be a basis for
A (ker (BA)) . Take any z âˆˆ ker (BA) . Then Az = i=1 ai Ayi and so
(
)
m
âˆ‘
A zâˆ’
ai yi = 0
which means z âˆ’

âˆ‘m
i=1

i=1

ai yi âˆˆ ker (A) and so there are scalars bi such that
zâˆ’

m
âˆ‘

ai yi =

i=1

n
âˆ‘

bi xi .

j=1

245

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

246

LINEAR TRANSFORMATIONS CANONICAL FORMS

It follows span (x1 , Â· Â· Â· , xn , y1 , Â· Â· Â· , ym ) âŠ‡ ker (BA) and so by the ï¬rst part, (See the picture.)
dim (ker (BA)) â‰¤ n + m â‰¤ dim (ker (A)) + dim (ker (B)) 
Of course this result holds for any ï¬nite product of linear transformations by induction. One way this
âˆl is quite useful is in the case where you have a ï¬nite product of linear
transformations i=1 Li all in L (V, V ) . Then
(
)
l
l
âˆ
âˆ‘
dim ker
Li â‰¤
dim (ker Li )
i=1

i=1

and so if you can ï¬nd a linearly independent set of vectors in ker
l
âˆ‘

then it must be a basis for ker

(âˆ

(âˆ

l
i=1

)
Li of size

dim (ker Li ) ,

i=1
l
i=1

)
Li . This is discussed below.

r

Deï¬nition 10.1.4 Let {Vi }i=1 be subspaces of V. Then
r
âˆ‘

denotes all sums of the form

âˆ‘r
i=1

Vi

i=1

vi where vi âˆˆ Vi . If whenever
r
âˆ‘

vi = 0, vi âˆˆ Vi ,

i=1

it follows that vi = 0 for each i, then a special notation is used to denote
notation is
V1 âŠ• Â· Â· Â· âŠ• Vr ,

(10.1)
âˆ‘r
i=1

Vi . This

and it is called a direct sum of subspaces.
}
{
i
is a basis for Vi , then a
Lemma 10.1.5 If V = V1 âŠ• Â· Â· Â· âŠ• Vr and if Î² i = v1i , Â· Â· Â· , vm
i
basis for V is {Î² 1 , Â· Â· Â· , Î² r }.
âˆ‘ r âˆ‘ mi
Proof: Suppose i=1 j=1
cij vji = 0. then since it is a direct sum, it follows for each i,
mi
âˆ‘

cij vji = 0

j=1

{
}
i
and now since v1i , Â· Â· Â· , vm
is a basis, each cij = 0. 
i
Here is a useful lemma.
Lemma 10.1.6 Let Li be in L (V, V ) and suppose for i Ì¸= j, Li Lj = Lj Li and also Li is
one to one on ker (Lj ) whenever i Ì¸= j. Then
( p
)
âˆ
ker
Li = ker (L1 ) âŠ• + Â· Â· Â· + âŠ• ker (Lp )
âˆp

i=1

Here i=1 Li is the product of all the linear transformations. A symbol like
product of all of them but Li .

Saylor URL: http://www.saylor.org/courses/ma212/

âˆ
jÌ¸=i

Lj is the

The Saylor Foundation

10.1. A THEOREM OF SYLVESTER, DIRECT SUMS

247

Proof: Note that since the operators commute, Lj : ker (Li ) â†’ ker (Li ). Here is why. If
Li y = 0 so that y âˆˆ ker (Li ) , then
Li Lj y = Lj Li y = Lj 0 = 0
and so Lj : ker (Li ) â†’ ker (Li ). Suppose
p
âˆ‘

vi = 0, vi âˆˆ ker (Li ) ,

i=1

but some vi Ì¸= 0. Then do
this results in

âˆ
jÌ¸=i

Lj to both sides. Since the linear transformations commute,
âˆ
Lj (vi ) = 0
jÌ¸=i

which contradicts the assumption that these Lj are one to one and the observation that
they map ker (Li ) to ker (Li ). Thus if
âˆ‘
vi = 0, vi âˆˆ ker (Li )
i

then each vi = 0. {
}
i
i
Suppose Î² i =
{ v1 , Â· Â· Â· , v}mi is a basis for ker (Li ). Then from what was just shown and
Lemma 10.1.5, Î² 1 , Â· Â· Â· , Î² p must be linearly independent and a basis for
ker (L1 ) âŠ• + Â· Â· Â· + âŠ• ker (Lp ) .
It is also clear that since these operators commute,
(
ker (L1 ) âŠ• + Â· Â· Â· + âŠ• ker (Lp ) âŠ† ker

p
âˆ

)
Li

i=1

Therefore, by Sylvesterâ€™s theorem and the above,
(
( p
))
p
âˆ
âˆ‘
dim ker
Li
dim (ker (Lj ))
â‰¤
i=1

j=1

(
= dim (ker (L1 ) âŠ• + Â· Â· Â· + âŠ• ker (Lp )) â‰¤ dim ker

(

p
âˆ

))
Li

.

i=1

Now in general, if W is a subspace of V, a ï¬nite dimensional vector space and the two have
the same dimension, then W = V . This is because W has a basis and if v is not in the span
of this basis, then v adjoined to the basis of W would be a linearly independent set, yielding
a linearly independent set which has more vectors in it than a basis, a contradiction.
It follows that
( p
)
âˆ
ker (L1 ) âŠ• + Â· Â· Â· + âŠ• ker (Lp ) = ker
Li

i=1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

248

10.2

LINEAR TRANSFORMATIONS CANONICAL FORMS

Direct Sums, Block Diagonal Matrices

Let V be a ï¬nite dimensional vector space with ï¬eld of scalars F. Here I will make no
assumption on F. Also suppose A âˆˆ L (V, V ) .
Recall Lemma 9.4.3 which gives the existence of the minimal polynomial for a linear
transformation A. This is the monic polynomial p which has smallest possible degree such
that p(A) = 0. It is stated again for convenience.
Lemma 10.2.1 Let A âˆˆ L (V, V ) where V is a ï¬nite dimensional vector space of dimension
n with ï¬eld of scalars F. Then there exists a unique monic polynomial of the form
p (Î») = Î»m + cmâˆ’1 Î»mâˆ’1 + Â· Â· Â· + c1 Î» + c0
such that p (A) = 0 and m is as small as possible for this to occur.
Now here is a useful lemma which will be used below.
Lemma 10.2.2 Let L âˆˆ L (V, V ) where V is an n dimensional vector space. Then if L
is one to one, it follows that L is also onto. In fact, if {v1 , Â· Â· Â· , vn } is a basis, then so is
{Lv1 , Â· Â· Â· , Lvn }.
Proof: Let {v1 , Â· Â· Â· , vn } be a basis for V . Then I claim that {Lv1 , Â· Â· Â· , Lvn } is also a
basis for V . First of all, I show {Lv1 , Â· Â· Â· , Lvn } is linearly independent. Suppose
n
âˆ‘

ck Lvk = 0.

k=1

(

Then
L

n
âˆ‘

)
ck vk

=0

k=1

and since L is one to one, it follows
n
âˆ‘

ck vk = 0

k=1

which implies each ck = 0. Therefore, {Lv1 , Â· Â· Â· , Lvn } is linearly independent. If there
exists w not in the span of these vectors, then by Lemma 8.2.10, {Lv1 , Â· Â· Â· , Lvn , w} would
be independent and this contradicts the exchange theorem, Theorem 8.2.4 because it would
be a linearly independent set having more vectors than the spanning set {v1 , Â· Â· Â· , vn } . 
Now it is time to consider the notion of a direct sum of subspaces. Recall you can
always assert the existence of a factorization of the minimal polynomial into a product of
irreducible polynomials. This fact will now be used to show how to obtain such a direct
sum of subspaces.
Deï¬nition 10.2.3 For A âˆˆ L (V, V ) where dim (V ) = n, suppose the minimal polynomial
is
q
âˆ
p (Î») =
(Ï•k (Î»))rk
k=1

where the polynomials Ï•k have coeï¬ƒcients in F and are irreducible. Now deï¬ne the generalized eigenspaces
r
Vk â‰¡ ker ((Ï•k (A)) k )
Note that if one of these polynomials (Ï•k (Î»))rk is a monic linear polynomial, then the generalized eigenspace would be an eigenspace.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

10.2. DIRECT SUMS, BLOCK DIAGONAL MATRICES

249

Theorem 10.2.4 In the context of Deï¬nition 10.2.3,
V = V1 âŠ• Â· Â· Â· âŠ• Vq

(10.2)

and each{ Vk is A invariant,
meaning A (Vk ) âŠ†{ Vk . Ï•l (A) is one
}
} to one on each Vk for k Ì¸= l.
i
If Î² i = v1i , Â· Â· Â· , vm
is
a
basis
for
V
,
then
Î²
,
Î²
,
Â·
Â·
Â·
,
Î²
i
1
2
q is a basis for V.
i
Proof: It is clear Vk is a subspace which is A invariant because A commutes with
m
r
Ï•k (A) k . It is clear the operators Ï•k (A) k commute. Thus if v âˆˆ Vk ,
Ï•k (A)

rk

r

r

Ï•l (A) l v = Ï•l (A) l Ï•k (A)

rk

r

v = Ï•l (A) l 0 = 0

r

and so Ï•l (A) l : Vk â†’ Vk .
I claim Ï•l (A) is one to one on Vk whenever k Ì¸= l. The two polynomials Ï•l (Î») and
r
Ï•k (Î») k are relatively prime so there exist polynomials m (Î») , n (Î») such that
rk

m (Î») Ï•l (Î») + n (Î») Ï•k (Î»)

=1

It follows that the sum of all coeï¬ƒcients of Î» raised to a positive power are zero and the
constant term on the left is 1. Therefore, using the convention A0 = I it follows
rk

=I

rk

v=v

m (A) Ï•l (A) + n (A) Ï•k (A)
If v âˆˆ Vk , then from the above,
m (A) Ï•l (A) v + n (A) Ï•k (A)
Since v is in Vk , it follows by deï¬nition,
m (A) Ï•l (A) v = v

r

and so Ï•l (A) v Ì¸= 0 unless v = 0. Thus Ï•l (A) and âˆ
hence Ï•l (A) l is one to one on Vk for
q
r
every k Ì¸= l. By Lemma 10.1.6 and the fact that ker ( k=1 Ï•k (Î») k ) = V, (10.2) is obtained.
The claim about the bases follows from Lemma 10.1.5. 
You could consider the restriction of A to Vk . It turns out that this restriction has
m
minimal polynomial equal to Ï•k (Î») k .
âˆq
m
Corollary 10.2.5 Let the minimal polynomial of A be p (Î») = k=1 Ï•k (Î») k where each
mk
Ï•k is irreducible. Let Vk = ker (Ï• (A) ) . Then
V1 âŠ• Â· Â· Â· âŠ• Vq = V
and letting Ak denote the restriction of A to Vk , it follows the minimal polynomial of Ak is
m
Ï•k (Î») k .
m

the direct sum, V1 âŠ• Â· Â· Â· âŠ• Vq = V where Vk = ker (Ï•k (A) k ) for p (Î») =
âˆq Proof: Recall
mk
the minimal polynomial for A where the Ï•k (Î») are all irreducible. Thus each
k=1 Ï•k (Î»)
Vk is invariant with respect to A. What is the minimal polynomial of Ak , the restriction of
m
A to Vk ? First note that Ï•k (Ak ) k (Vk ) = {0} by deï¬nition. Thus if Î· (Î») is the minimal
m
r
polynomial for Ak then it must divide Ï•k (Î») k and so by Corollary 8.3.11 Î· (Î») = Ï•k (Î») k
where rk â‰¤ mk . Could rk < mk ? No, this is not possible because then p (Î») would fail
m
to be the minimal polynomial for A. You could substitute for the term Ï•k (Î») k in the
rk
â€²
factorization of p (Î») with Ï•k (Î») and the resulting polynomial p would satisfy pâ€² (A) = 0.
Here is why. From Theorem 10.2.4, a typical x âˆˆ V is of the form
q
âˆ‘

vi , vi âˆˆ Vi

i=1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

250

LINEAR TRANSFORMATIONS CANONICAL FORMS

Then since all the factors commute,
( q
)
( q
)
q
âˆ
âˆ‘
âˆ‘
mi
rk
â€²
p (A)
vi =
Ï•i (A) Ï•k (A)
vi
i=1

i=1

iÌ¸=k

For j Ì¸= k
q
âˆ

Ï•i (A)

mi

rk

Ï•k (A)

vj =

iÌ¸=k

If j = k,

q
âˆ

mi

Ï•i (A)

Ï•k (A)

rk

mj

Ï•j (A)

vj = 0

iÌ¸=k,j
q
âˆ

mi

Ï•i (A)

rk

Ï•k (A)

vk = 0

iÌ¸=k

which shows pâ€² (Î») is a monic polynomial having smaller degree than p (Î») such that pâ€² (A) =
m
0. Thus the minimal polynomial for Ak is Ï•k (Î») k as claimed. 
How does Theorem 10.2.4 relate to matrices?
Theorem 10.2.6 Suppose V is a vector space with ï¬eld of scalars F and A âˆˆ L (V, V ).
Suppose also
V = V1 âŠ• Â· Â· Â· âŠ• Vq
where each Vk is A invariant. (AVk âŠ† Vk ) Also let Î² k be a basis for Vk and let Ak denote
the restriction of A to Vk . Letting M k denote the{ matrix of }
Ak with respect to this basis, it
follows the matrix of A with respect to the basis Î² 1 , Â· Â· Â· , Î² q is
ï£«
ï£¶
M1
0
ï£¬
ï£·
..
ï£­
ï£¸
.
Mq

0

Proof: Recall the matrix M of a linear transformation A is deï¬ned such that the
following diagram commutes.
{v1 , Â· Â· Â· , vn }

V
qâ†‘
Fn

A
â†’
â—¦
â†’
M

where
q (x) â‰¡

n
âˆ‘

V
â†‘q
Fn

{v1 , Â· Â· Â· , vn }

xi vi

i=1

and {v1 , Â· Â· Â· , vn } is a basis for V. Now when V = V1 âŠ• Â· Â· Â· âŠ• Vq each {
Vk being invariant
with
}
k
respect to the linear transformation A, and Î² k a basis for Vk , Î² k = v1k , Â· Â· Â· , vm
,
one
can
k
consider the matrix M k of Ak taken with respect to the basis Î² k where Ak is the restriction
of A to Vk . Then the claim of the theorem is true because if M is given as described it
causes the diagram to commute. To see this, let x âˆˆ Fmk .
ï£«
ï£¶ï£«
ï£¶
ï£«
ï£¶
M1
0
0
0
ï£¬
ï£· ï£¬ .. ï£·
ï£¬ .. ï£·
..
ï£¬
ï£·ï£¬ . ï£·
ï£¬ . ï£· âˆ‘
.
ï£¬
ï£·
ï£¬
ï£·
ï£¬
ï£·
k
k
k
ï£¬
ï£·
ï£¬
ï£·
ï£¬
ï£·
M
qï£¬
Mij
xj vik
ï£·ï£¬ x ï£· = qï£¬ M x ï£· â‰¡
ï£¬
ï£·
ï£¬
ï£·
ï£¬
ï£·
.
.
..
ij
ï£­
ï£¸ ï£­ .. ï£¸
ï£­ .. ï£¸
.
q
0
0
M
0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

10.3. CYCLIC SETS

251

ï£«

ï£¶
0
ï£¬ .. ï£·
ï£¬ . ï£·
âˆ‘
âˆ‘
âˆ‘ âˆ‘
ï£¬
ï£·
k k
ï£·
Aq ï£¬
xj vjk =
xj Ak vjk =
xj
Mij
vi
ï£¬ x ï£·â‰¡A
ï£¬ . ï£·
j
j
j
i
ï£­ .. ï£¸
0
âˆ‘
k k
vi because M k is the matrix of Ak with respect
because, as discussed earlier, Avjk = i Mij
to the basis Î² k . 
An examination of the proof of the above theorem yields the following corollary.
while

Corollary 10.2.7 If any Î² k in the above consists of eigenvectors, then M k is a diagonal
matrix having the corresponding eigenvalues down the diagonal.
It follows that it would be interesting to consider special bases for the vector spaces in
the direct sum. This leads to the Jordan form or more generally other canonical forms such
as the rational canonical form.

10.3

Cyclic Sets

It was shown above that for A âˆˆ L (V, V ) for V a ï¬nite dimensional vector space over the
ï¬eld of scalars F, there exists a direct sum decomposition
V = V1 âŠ• Â· Â· Â· âŠ• Vq
where
mk

Vk = ker (Ï•k (A)

)

and Ï•k (Î») is an irreducible polynomial. Here the minimal polynomial of A was
q
âˆ

mk

Ï•k (Î»)

k=1

Next I will consider the problem of ï¬nding a basis for Vk such that the matrix of A
restricted to Vk assumes various forms.
{
}
Deï¬nition 10.3.1 Letting x Ì¸= 0 denote by
Î² x the vectors) x, Ax, A2 x, Â· Â· Â· , Amâˆ’1 x where
(
m is the smallest such that Am x âˆˆ span x, Â· Â· Â· , Amâˆ’1 x . This is called an A cyclic set.
The vectors which result are also called a Krylov sequence.
The following is the main idea. To help organize the ideas in this lemma, here is a
diagram.

ker(Ï•(A)m )
U âŠ† ker(Ï•(A))

W
v1 , ..., vs

x1 , x2 , ..., xp

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

252

LINEAR TRANSFORMATIONS CANONICAL FORMS
m

Lemma 10.3.2 Let W be an A invariant (AW âŠ† W ) subspace of ker (Ï• (A) ) for m a
positive integer where Ï• (Î») is an irreducible monic polynomial of degree d. Then if Î· (Î») is
a monic polynomial of smallest degree such that for
m

x âˆˆ ker (Ï• (A) ) \ {0} ,
Î· (A) x = 0,
then
k

Î· (Î») = Ï• (Î»)

for some positive integer k. Thus if r is the degree of Î·, then r = kd. Also, for a cyclic set,
{
}
Î² x â‰¡ x, Ax, Â· Â· Â· , Arâˆ’1 x
r
is linearly
independent.
{
} Recall that r is the smallest such that A x is a linear combination
râˆ’1
of x, Ax, Â· Â· Â· , A x .
Now let U be an A invariant subspace of ker (Ï• (A)) .
If {v1 , Â· Â· Â· , vs } is a basis for W then if x âˆˆ U \ W,

{v1 , Â· Â· Â· , vs , Î² x }
is linearly independent.
There exist vectors x1 , Â· Â· Â· , xp each in U such that
{
}
v1 , Â· Â· Â· , vs , Î² x1 , Â· Â· Â· , Î² xp
is a basis for
U + W.
Proof: Consider the ï¬rst claim. If Î· (A) x = 0, then writing
m

Ï• (Î»)

= Î· (Î») g (Î») + r (Î»)

where either r (Î») = 0 or the degree of r (Î») is less than that of Î· (Î») , the latter possibility
cannot occur because if it did, r (A) x = 0 and this would contradict the deï¬nition of Î· (Î»).
m
Therefore r (Î») = 0 and so Î· (Î») divides Ï• (Î») . From Corollary 8.3.11,
k

Î· (Î») = Ï• (Î»)

for some integer, k â‰¤ m. Since x Ì¸= 0, it follows k > 0. In particular, the degree of Î· (Î»)
equals kd.
m
Now consider x Ì¸= 0, x âˆˆ ker (Ï• (A) ){ and the vectors Î² x . Do
} these vectors yield a
linearly independent set? The vectors are x, Ax, A2 x, Â· Â· Â· , Arâˆ’1 x where Ar x is in
(
)
span x, Ax, A2 x, Â· Â· Â· , Arâˆ’1 x
and r is as small as possible for this to happen. Suppose then that there are scalars dj , not
all zero such that
râˆ’1
âˆ‘
dj Aj x = 0, x Ì¸= 0.
(10.3)
j=0

Suppose m is the largest nonzero scalar in the above linear combination. dm Ì¸= 0, m â‰¤ r âˆ’ 1.
Then Am x is a linear combination of the preceeding vectors in the list, which contradicts
the deï¬nition of r. Thus from the ï¬rst part, r = kd for some positive integer k.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

10.3. CYCLIC SETS

253

Since Î² x is independent for each x Ì¸= 0, it follows that whenever x Ì¸= 0,
{
}
x, Ax, A2 x, Â· Â· Â· , Adâˆ’1 x
is linearly independent because, the length of Î² x is a multiple of d and is therefore, at least
as long as the above list. However, if x âˆˆ ker (Ï• (A)) , then Î² x is equal to the above list.
This is because Ï• (Î») is of degree d so Î² x is no longer than the above list. However, from
the ï¬rst part Î² x has length equal to kd for some integer k so it is at least as long.
Suppose now x âˆˆ U \ W where U âŠ† ker (Ï• (A)). Consider
{
}
v1 , Â· Â· Â· , vs , x, Ax, A2 x, Â· Â· Â· , Adâˆ’1 x .
Is this set of vectors independent? First note that
(
)
span x, Ax, A2 x, Â· Â· Â· , Adâˆ’1 x
{
}
is A invariant because, from what was just shown, x, Ax, A2 x, Â· Â· Â· , Adâˆ’1 x = Î² x and so
Ad x is a linear combination of the other vectors in the above list. Suppose now that
s
âˆ‘

ai vi +

i=1

If z â‰¡

âˆ‘d
j=1

d
âˆ‘

dj Ajâˆ’1 x = 0.

j=1

(
)
dj Ajâˆ’1 x, then z âˆˆ W âˆ© span x, Ax, Â· Â· Â· , Adâˆ’1 x . Then also for each m â‰¤ d âˆ’ 1,

(
)
Am z âˆˆ W âˆ© span x, Ax, Â· Â· Â· , Adâˆ’1 x
(
)
because W, span x, Ax, Â· Â· Â· , Adâˆ’1 x are A invariant, and so
(
)
span z, Az, Â· Â· Â· , Adâˆ’1 z

(
)
âŠ† W âˆ© span x, Ax, Â· Â· Â· , Adâˆ’1 x
(
)
âŠ† span x, Ax, Â· Â· Â· , Adâˆ’1 x
(10.4)
{
}
Suppose z =
Ì¸ 0. Then from the above, z, Az, Â· Â· Â· , Adâˆ’1 z must be linearly independent.
Therefore,
(
(
))
(
(
))
d = dim span z, Az, Â· Â· Â· , Adâˆ’1 z â‰¤ dim W âˆ© span x, Ax, Â· Â· Â· , Adâˆ’1 x
(
(
))
â‰¤ dim span x, Ax, Â· Â· Â· , Adâˆ’1 x = d
Thus

(
)
(
)
span z, Az, Â· Â· Â· , Adâˆ’1 z âŠ† span x, Ax, Â· Â· Â· , Adâˆ’1 x

and both have the same dimension and so the two sets are equal. It follows from (10.4)
(
)
(
)
W âˆ© span x, Ax, Â· Â· Â· , Adâˆ’1 x = span x, Ax, Â· Â· Â· , Adâˆ’1 x
which would require x âˆˆ W but this is assumed not to take place. Hence z = 0 and so the
linearly
independence}of the {v1 , Â· Â· Â· , vs } implies each ai = 0. Then the linear independence
{
of x,{Ax, Â· Â· Â· , Adâˆ’1 x which follows
} from the ï¬rst part of the argument shows each dj = 0.
Thus v1 , Â· Â· Â· , vs , x, Ax, Â· Â· Â· , Adâˆ’1 x is linearly independent as claimed.
Let x âˆˆ U \ W âŠ† ker (Ï• (A))
{ . Then it was just shown
} that {v1 , Â· Â· Â· , vs , Î² x } is linearly
independent. Recall that Î² x = x, Ax, A2 x, Â· Â· Â· , Adâˆ’1 x because x âˆˆ ker (Ï• (A)). Also, if
(
)
y âˆˆ span v1 , Â· Â· Â· , vs , x, Ax, A2 x, Â· Â· Â· , Adâˆ’1 x â‰¡ W1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

254

LINEAR TRANSFORMATIONS CANONICAL FORMS

then Ay âˆˆ W1 also. This is because W is A invariant, and if you take A
must remain in
(
)
span x, Ax, A2 x, Â· Â· Â· , Adâˆ’1 x

âˆ‘dâˆ’1
i=0

ai Ai x, It

because Ad x is in the above span, due to the assumption that Ï• (A) x = 0. If W1 equals
U + W, then you are done. If not, let W(1 play the role of W and
) pick x1 âˆˆ U \ W1 and
repeat the argument. Continue till span v1 , Â· Â· Â· , vs , Î² x1 , Â· Â· Â· , Î² xn = U + W . The process
m
stops because ker (Ï• (A) ) is ï¬nite dimensional. 
Now here is a simple lemma.
Lemma 10.3.3 Let V be a vector space and let B âˆˆ L (V, V ) . Then
V = B (V ) âŠ• ker (B)
Proof: Let {Bv1 , Â· Â· Â· , Bvr } be a basis for B (V ). Now let {w1 , Â· Â· Â· , ws } be a basis for
ker (B) . Then if v âˆˆ V, there exist unique scalars ci such that
Bv =

r
âˆ‘

ci Bvi

i=1

and so B (v âˆ’

âˆ‘r

i=1 ci vi )

= 0 and so there exist unique scalars di such that
vâˆ’

r
âˆ‘

ci vi =

s
âˆ‘

i=1

dj w j .

j=1

It remains to verify that {v1 , Â· Â· Â· , vr , w1 , Â· Â· Â· , ws } is linearly independent. Suppose then that
âˆ‘
âˆ‘
a i vi +
bj w j = 0
i

j

âˆ‘

Do B to both sides. This yields i ai Bvi = 0 and by assumption, this requires each ai = 0.
Then independence of the wi yields each bj = 0. 
m
With this preparation, here is the main result about a basis for ker (Ï• (A) ) for Ï• (Î»)
irreducible. For more on this theorem, including extra details, see [14]. See also Exercise 27
on Page 266..
m

Theorem 10.3.4 Let V = ker (Ï• (A) ) for m a positive integer and A âˆˆ L (Z, Z) where
Z is some vector space containing V, and Ï• (Î») is an irreducible monic polynomial over
the ï¬eld of scalars. Then there exist vectors {v1 , Â· Â· Â· , vs } and A cyclic sets Î² vj such that
{
}
Î² v1 , Â· Â· Â· , Î² vs is a basis for V .
Proof: First suppose m = 1. Then in Lemma 10.3.2 you can let W {= {0} and U} =
V = ker (Ï• (A)). Then by this lemma, there exist v1 , v2 , Â· Â· Â· , vs such that
Â· , Î² vs is
( Î² v1 , Â· Â· )
mâˆ’1
a basis for V . Suppose then that the theorem is true whenever V = ker Ï• (A)
, m â‰¥ 2.
m

mâˆ’1

Suppose V = ker (Ï• (A) ). Then Ï• (A)
maps V to V and so by Lemma 10.3.3,
(
)
mâˆ’1
mâˆ’1
V = ker Ï• (A)
+ Ï• (A)
(V )
mâˆ’1

mâˆ’1

Clearly Ï• (A)
(V ) âŠ† ker (Ï• (A)) . Is Ï• (A)
(V ) also A invariant? Yes, this is the case
m
mâˆ’1
mâˆ’1
because if y âˆˆ V = ker (Ï• (A) ) , then Ï• (A)
y is a typical thing in Ï• (A)
(V ) . But
mâˆ’1

AÏ• (A)

mâˆ’1

(y) = Ï• (A)

Saylor URL: http://www.saylor.org/courses/ma212/

mâˆ’1

(Ay) âˆˆ Ï• (A)

(V )

The Saylor Foundation

10.4. NILPOTENT TRANSFORMATIONS

255

(
)
mâˆ’1
By induction, there exists a basis for ker Ï• (A)
which is of the form
{
}
Î² v1 , Â· Â· Â· , Î² vr
and now, by Lemma 10.3.2, there exists a basis
{
}
Î² x1 , Â· Â· Â· , Î² xl , Î² v1 , Â· Â· Â· , Î² vr
(
)
mâˆ’1
mâˆ’1
for V = ker Ï• (A)
+ Ï• (A)
(V ). 

10.4

Nilpotent Transformations

Deï¬nition 10.4.1 Let V be a vector space over the ï¬eld of scalars F. Then N âˆˆ L (V, V )
is called nilpotent if for some m, it follows that N m = 0.
The following lemma contains some signiï¬cant observations about nilpotent transformations.
{
}
Lemma 10.4.2 Suppose N k x Ì¸= 0. Then x, N x, Â· Â· Â· , N k x is linearly independent. Also,
the minimal polynomial of N is Î»m where m is the ï¬rst such that N m = 0.
âˆ‘k
Proof: Suppose i=0 ci N i x = 0. There exists l such that k â‰¤ l < m and N l+1 x = 0
but N l x Ì¸= 0. Then multiply both sides by N l to conclude that c0 = 0. Next multiply both
sides by N lâˆ’1 to conclude that c1 = 0 and continue this way to obtain that all the ci = 0.
Next consider the claim that Î»m is the minimal polynomial. If p (Î») is the minimal
polynomial, then
p (Î») = Î»m l (Î») + r (Î»)
where the degree of r (Î») is less than m or else r (Î») = 0. Suppose the degree of r (Î») is less
than m. Then you would have
0 = 0 + r (N ) .
If r (Î») = a0 + a1 Î» + Â· Â· Â· + as Î»s for s â‰¤ m âˆ’ 1, as Ì¸= 0, then for any x âˆˆ V,
0 = a0 x + a1 N x + Â· Â· Â· + as N s x
If for some x, N s x Ì¸= 0, then from the ï¬rst part of the argument, the above equation could
not hold. Hence N s x = 0 for all x and so N s = 0 for some s < m, a contradiction to the
choice of m. It follows that r (Î») = 0 and so p (Î») cannot be the minimal polynomial unless
l (Î») = 1. Hence p (Î») = Î»m as claimed.  {
}
For such a nilpotent transformation, let Î² x1 , Â· Â· Â· , Î² xq be a basis for ker (N m ) = V
where these Î² xi are cyclic. This basis exists thanks to Theorem 10.3.4. Thus
(
)
(
)
V = span Î² x1 âŠ• Â· Â· Â· âŠ• span Î² xq ,
each of these subspaces in the above direct sum being N invariant. For x one of the xk ,
consider Î² x given by
x, N x, N 2 x, Â· Â· Â· , N râˆ’1 x
where N r x is in the span of the above vectors. Then by the above lemma, N r x = 0.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

256

LINEAR TRANSFORMATIONS CANONICAL FORMS

By Theorem 10.2.6, the matrix of N with respect to the above basis is the block diagonal
matrix
ï£«
ï£¶
M1
0
ï£¬
ï£·
..
ï£­
ï£¸
.
Mq

0

(
)
where M k denotes the matrix of N restricted to span Î² xk . In computing this matrix, I
will order Î² xk as follows:
( r âˆ’1
)
N k xk , Â· Â· Â· , xk
Also the cyclic sets Î² x1 , Î² x2 , Â· Â· Â· , Î² xq will be ordered according to length, the length of
Î² xi being at least as large as the length of Î² xi+1 . Then since N rk xk = 0, it is now easy
to ï¬nd M k . Using the procedure mentioned above for determining the matrix of a linear
transformation,
(
)
0 N rk âˆ’1 xk Â· Â· Â· N xk =
ï£«
ï£¶
0 1
0
ï£¬
ï£·
ï£·
( r âˆ’1
) ï£¬ 0 0 ...
ï£·
N k xk N rk âˆ’2 xk Â· Â· Â· xk ï£¬
ï£¬ . . .
ï£·
.. 1 ï£¸
ï£­ .. ..
0 0 Â·Â·Â· 0
Thus the matrix Mk is the rk Ã— rk matrix which has ones down the super diagonal and zeros
elsewhere. The following convenient notation will be used.
Deï¬nition 10.4.3 Jk (Î±) is a Jordan block
ï£«
Î±
ï£¬
ï£¬ 0
Jk (Î±) = ï£¬
ï£¬ .
ï£­ ..
0

if it is a k Ã— k matrix of the form
ï£¶
1
0
ï£·
..
..
ï£·
.
.
ï£·
ï£·
..
..
.
. 1 ï£¸
Â·Â·Â·
0 Î±

In words, there is an unbroken string of ones down the super diagonal and the number Î±
ï¬lling every space on the main diagonal with zeros everywhere else.
Then with this deï¬nition and the above discussion, the following proposition has been
proved.
Proposition 10.4.4 Let N âˆˆ L (W, W ) be nilpotent,
Nm = 0
for some m âˆˆ N. Here W is a p dimensional vector space with ï¬eld of scalars F. Then there
exists a basis for W such that the matrix of N with respect to this basis is of the form
ï£«
ï£¶
Jr1 (0)
0
ï£¬
ï£·
Jr2 (0)
ï£¬
ï£·
J =ï£¬
ï£·
.
.
ï£­
ï£¸
.
0

Jrs (0)

âˆ‘s
where r1 â‰¥ r2 â‰¥ Â· Â· Â· â‰¥ rs â‰¥ 1 and i=1 ri = p. In the above, the Jrj (0) is a Jordan block of
size rj Ã— rj with 0 down the main diagonal.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

10.5. THE JORDAN CANONICAL FORM

257

In fact, the matrix of the above proposition is unique.
Corollary 10.4.5 Let J, J â€² both be matrices of the nilpotent linear transformation N âˆˆ
L (W, W ) which are of the form described in Proposition 10.4.4. Then J = J â€² . In fact, if
the rank of J k equals the rank of J â€²k for all nonnegative integers k, then J = J â€² .
Proof: Since J and J â€² are similar, it follows that for each k an integer, J k and J â€²k are
similar. Hence, for each k, these matrices have the same rank. Now suppose J Ì¸= J â€² . Note
ï¬rst that
r
râˆ’1
Jr (0) = 0, Jr (0)
Ì¸= 0.
Denote the blocks of J as Jrk (0) and the blocks of J â€² as Jrkâ€² (0). Let k be the ï¬rst such that
Jrk (0) Ì¸= Jrkâ€² (0). Suppose that rk > rkâ€² . By block multiplication and the above observation,
it follows that the two matrices J rk âˆ’1 and J â€²rk âˆ’1 are respectively of the forms
ï£«
ï£¶
M r1
0
ï£¬
ï£·
..
ï£¬
ï£·
.
ï£·
ï£¬
ï£·
ï£¬
M
r
k
ï£¬
ï£·
ï£¬
ï£·
0
ï£·
ï£¬
ï£¬
ï£·
.
..
ï£­
ï£¸
0
0
and

ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­

Mr1â€²

0
..

.
Mrkâ€²
0
..

.

0

ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

0

where Mrj = Mrjâ€² for j â‰¤ k âˆ’ 1 but Mrkâ€² is a zero rkâ€² Ã— rkâ€² matrix while Mrk is a larger matrix
which is not equal to 0. For example,
ï£«
ï£¶
0 Â·Â·Â· 1
ï£¬
. ï£·
..
Mrk = ï£­
. .. ï£¸
0

0
rk âˆ’1

Thus there are more pivot columns in J rk âˆ’1 than in (J â€² )
that J k and J â€²k have the same rank. 

10.5

, contradicting the requirement

The Jordan Canonical Form

The Jordan canonical form has to do with the case where the minimal polynomial of A âˆˆ
L (V, V ) splits. Thus there exist Î»k in the ï¬eld of scalars such that the minimal polynomial
of A is of the form
r
âˆ
m
p (Î») =
(Î» âˆ’ Î»k ) k
k=1

Recall the following which follows from Theorem 9.4.4.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

258

LINEAR TRANSFORMATIONS CANONICAL FORMS

Proposition 10.5.1 Let the minimal polynomial of A âˆˆ L (V, V ) be given by
p (Î») =

r
âˆ

(Î» âˆ’ Î»k )

mk

k=1

Then the eigenvalues of A are {Î»1 , Â· Â· Â· , Î»r }.
It follows from Corollary 10.2.4 that
V

m1

= ker (A âˆ’ Î»1 I)
â‰¡ V1 âŠ• Â· Â· Â· âŠ• Vr

âŠ• Â· Â· Â· âŠ• ker (A âˆ’ Î»r I)

mr

where I denotes the identity linear transformation. Without loss of generality, let the
dimensions of the Vk be decreasing from left to right. These Vk are called the generalized
eigenspaces.
It follows from the deï¬nition of Vk that (A âˆ’ Î»k I) is nilpotent on Vk and clearly each
Vk is A invariant. Therefore from Proposition 10.4.4, and letting Ak denote the restriction
of A to Vk , there exists an ordered basis for Vk , Î² k such that with respect to this basis, the
matrix of (Ak âˆ’ Î»k I) is of the form given in that proposition, denoted here by J k . What is
the matrix of Ak with respect to Î² k ? Letting {b1 , Â· Â· Â· , br } = Î² k ,
âˆ‘
âˆ‘
âˆ‘(
)
k
k
Ak bj = (Ak âˆ’ Î»k I) bj + Î»k Ibj â‰¡
bs +
Jsj
Î»k Î´ sj bs =
+ Î»k Î´ sj bs
Jsj
s

s

s

and so the matrix of Ak with respect to this basis is
J k + Î»k I
where I is the identity matrix. Therefore, with respect to the ordered basis {Î² 1 , Â· Â· Â· , Î² r }
the matrix of A is in Jordan canonical form. This means the matrix is of the form
ï£«
ï£¶
J (Î»1 )
0
ï£¬
ï£·
..
(10.5)
ï£­
ï£¸
.
0

J (Î»r )

where J (Î»k ) is an mk Ã— mk matrix of the form
ï£«
ï£¶
Jk1 (Î»k )
0
ï£¬
ï£·
Jk2 (Î»k )
ï£¬
ï£·
(10.6)
ï£¬
ï£·
..
ï£­
ï£¸
.
0
Jkr (Î»k )
âˆ‘r
where k1 â‰¥ k2 â‰¥ Â· Â· Â· â‰¥ kr â‰¥ 1 and i=1 ki = mk . Here Jk (Î») is a k Ã— k Jordan block of the
form
ï£«
ï£¶
Î» 1
0
ï£¬
ï£·
ï£¬ 0 Î» ...
ï£·
ï£¬
ï£·
(10.7)
ï£¬
ï£·
.. ..
ï£­
.
. 1 ï£¸
0
0 Î»
This proves the existence part of the following fundamental theorem.
Note that if any of the Î² k consists of eigenvectors, then the corresponding Jordan block
will consist of a diagonal matrix having Î»k down the main diagonal. This corresponds to

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

10.5. THE JORDAN CANONICAL FORM

259
mk

mk = 1. The vectors which are in ker (A âˆ’ Î»k I)
generalized eigenvectors.
To illustrate the main idea used in proving
following two matrices.
ï£«
ï£¶ ï£«
0 1 0 0
ï£¬ 0 0 1 0 ï£· ï£¬
ï£¬
ï£· ï£¬
ï£­ 0 0 0 0 ï£¸,ï£­
0 0 0 0

which are not in ker (A âˆ’ Î»k I) are called

uniqueness in this theorem, consider the
0
0
0
0

1
0
0
0

ï£¶
0
0 ï£·
ï£·
1 ï£¸
0

0
0
0
0

The ï¬rst has one 3Ã—3 block and the second has two 2 Ã— 2 blocks. Initially both matrices
have rank 2. Now lets raise them to a power 2.
ï£«

0
ï£¬ 0
ï£¬
ï£­ 0
0

1
0
0
0

0
1
0
0

ï£¶2 ï£«
0 0
0
ï£¬ 0 0
0 ï£·
ï£· =ï£¬
ï£­ 0 0
0 ï£¸
0 0
0

1
0
0
0

ï£¶
0
0 ï£·
ï£·
0 ï£¸
0

1
0
0
0

0
0
0
0

ï£¶2 ï£«
0
0 0
ï£¬
0 ï£·
ï£· =ï£¬ 0 0
ï£­ 0 0
1 ï£¸
0
0 0

0
0
0
0

ï£¶
0
0 ï£·
ï£·
0 ï£¸
0

which has rank 1 and
ï£«

0
ï£¬ 0
ï£¬
ï£­ 0
0

which has no rank. You see, discrepancies occur in the rank upon raising to higher powers
if the blocks are not the same. Now with this preparation, here is the main theorem.
Theorem 10.5.2 Let V be an n dimensional vector space with ï¬eld of scalars C or some
other ï¬eld such that the minimal polynomial of A âˆˆ L (V, V ) completely factors into powers
of linear factors. Then there exists a unique Jordan canonical form for A as described in
(10.5) - (10.7), where uniqueness is in the sense that any two have the same number and
size of Jordan blocks.
Proof: It only remains to verify uniqueness. Suppose there are two, J and J â€² . Then these
are matrices of A with respect to possibly diï¬€erent bases and so they are similar. Therefore,
they have the same minimal polynomials and the generalized eigenspaces have the same
dimension. Thus the size of the matrices J (Î»k ) and J â€² (Î»k ) deï¬ned by the dimension of
these generalized eigenspaces, also corresponding to the algebraic multiplicity of Î»k , must
be the same. Therefore, they comprise the same set of positive integers. Thus listing the
eigenvalues in the same order, corresponding blocks J (Î»k ) , J â€² (Î»k ) are the same size.
It remains to show that J (Î»k ) and J â€² (Î»k ) are not just the same size but also are the
same up to order of the Jordan blocks running down their respective diagonals. It is only
necessary to worry about the number and size of the Jordan blocks making up J (Î»k ) and
J â€² (Î»k ) . Since J, J â€² are similar, so are J âˆ’ Î»k I and J â€² âˆ’ Î»k I. Thus the following two matrices
are similar
ï£«
ï£¶
J (Î»1 ) âˆ’ Î»k I
0
ï£¬
ï£·
..
ï£¬
ï£·
.
ï£¬
ï£·
ï£·
J
(Î»
)
âˆ’
Î»
I
Aâ‰¡ï£¬
k
k
ï£¬
ï£·
ï£¬
ï£·
.
..
ï£­
ï£¸
0
J (Î»r ) âˆ’ Î»k I

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

260

LINEAR TRANSFORMATIONS CANONICAL FORMS

ï£«
ï£¬
ï£¬
ï£¬
Bâ‰¡ï£¬
ï£¬
ï£¬
ï£­

J â€² (Î»1 ) âˆ’ Î»k I

0
..

.
J â€² (Î»k ) âˆ’ Î»k I
..

ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

.
0
J â€² (Î»r ) âˆ’ Î»k I
( )
( )
and consequently, rank Ak = rank B k for all k âˆˆ N. Also, both J (Î»j ) âˆ’ Î»k I and
J â€² (Î»j ) âˆ’ Î»k I are one to one for every Î»j Ì¸= Î»k . Since all the blocks in both of these matrices
are one to one except the blocks J â€² (Î»k ) âˆ’ Î»k I, J (Î»k ) âˆ’ Î»k I, it
the
{ follows
( that this requires
m )}âˆ
m âˆ
two sequences of numbers {rank ((J (Î»k ) âˆ’ Î»k I) )}m=1 and rank (J â€² (Î»k ) âˆ’ Î»k I)
m=1
must be the same.
Then
ï£«
ï£¶
Jk1 (0)
0
ï£¬
ï£·
Jk2 (0)
ï£¬
ï£·
J (Î»k ) âˆ’ Î»k I â‰¡ ï£¬
ï£·
.
..
ï£­
ï£¸
0
Jkr (0)
and a similar formula holds for J â€² (Î»k )
ï£«
ï£¬
ï£¬
J â€² (Î»k ) âˆ’ Î»k I â‰¡ ï£¬
ï£­

Jl1 (0)

0
Jl2 (0)
..

0

.

ï£¶
ï£·
ï£·
ï£·
ï£¸

Jlp (0)

and it is required to verify that p = r and that the same blocks occur in both. Without
loss of generality, let the blocks be arranged according to size with the largest on upper left
corner falling to smallest in lower right. Now the desired conclusion follows from Corollary
10.4.5. 
m
Note that if any of the generalized eigenspaces ker (A âˆ’ Î»k I) k has a basis of eigenvectors, then it would be possible to use this basis and obtain a diagonal matrix in the
block corresponding to Î»k . By uniqueness, this is the block corresponding to the eigenvalue
Î»k . Thus when this happens, the block in the Jordan canonical form corresponding to Î»k
is just the diagonal matrix having Î»k down the diagonal and there are no generalized
eigenvectors.
The Jordan canonical form is very signiï¬cant when you try to understand powers of a
matrix. There exists an n Ã— n matrix S 1 such that
A = S âˆ’1 JS.
Therefore, A2 = S âˆ’1 JSS âˆ’1 JS = S âˆ’1 J 2 S and continuing this way, it follows
Ak = S âˆ’1 J k S.
where J is given in the above corollary. Consider J k . By block multiplication,
ï£« k
ï£¶
J1
0
ï£¬
ï£·
..
Jk = ï£­
ï£¸.
.
k
0
Jr
1 The

S here is written as S âˆ’1 in the corollary.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

10.5. THE JORDAN CANONICAL FORM

261

The matrix Js is an ms Ã— ms matrix which is of the form
ï£«
ï£¶
Î± Â·Â·Â· âˆ—
ï£¬
ï£·
Js = ï£­ ... . . . ... ï£¸
0 Â·Â·Â· Î±

(10.8)

which can be written in the form
Js = D + N
for D a multiple of the identity and N an upper triangular matrix with zeros down the main
diagonal. Therefore, by the Cayley Hamilton theorem, N ms = 0 because the characteristic
equation for N is just Î»ms = 0. (You could also verify this directly.) Now since D is just a
multiple of the identity, it follows that DN = N D. Therefore, the usual binomial theorem
may be applied and this yields the following equations for k â‰¥ ms .
Jsk

=

=

k ( )
âˆ‘
k
(D + N ) =
Dkâˆ’j N j
j
j=0
ms ( )
âˆ‘
k
Dkâˆ’j N j ,
j
j=0
k

(10.9)

the third equation holding because N ms = 0. Thus Jsk is of the form
ï£« k
ï£¶
Î± Â·Â·Â· âˆ—
ï£¬
.. ï£· .
..
Jsk = ï£­ ...
.
. ï£¸
0 Â· Â· Â· Î±k
Lemma 10.5.3 Suppose J is of the form Js described above in (10.8) where the constant
Î±, on the main diagonal is less than one in absolute value. Then
( )
lim J k ij = 0.
kâ†’âˆ

Proof: From (10.9), it follows that for large k, and j â‰¤ ms ,
( )
k
k (k âˆ’ 1) Â· Â· Â· (k âˆ’ ms + 1)
.
â‰¤
ms !
j
Therefore, letting C be the largest value of
( k)
J pq â‰¤ ms C

(

(

Nj

)
pq

for 0 â‰¤ j â‰¤ ms ,

k (k âˆ’ 1) Â· Â· Â· (k âˆ’ ms + 1)
ms !

)
|Î±|

kâˆ’ms

which converges to zero as k â†’ âˆ. This is most easily seen by applying the ratio test to
the series
)
âˆ (
âˆ‘
k (k âˆ’ 1) Â· Â· Â· (k âˆ’ ms + 1)
kâˆ’ms
|Î±|
ms !
k=ms

and then noting that if a series converges, then the k th term converges to zero. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

262

LINEAR TRANSFORMATIONS CANONICAL FORMS

10.6

Exercises

1. In the discussion of Nilpotent transformations, it was asserted that if two nÃ—n matrices
A, B are similar, then Ak is also similar to B k . Why is this so? If two matrices are
similar, why must they have the same rank?
2. If A, B are both invertible, then they are both row equivalent to the identity matrix.
Are they necessarily similar? Explain.
3. Suppose you have two nilpotent matrices A, B and Ak and B k both have the same
rank for all k â‰¥ 1. Does it follow that A, B are similar? What if it is not known that
A, B are nilpotent? Does it follow then?
4. When we say a polynomial equals zero, we mean that all the coeï¬ƒcients equal 0. If
we assign a diï¬€erent meaning to it which says that a polynomial
p (Î») =

n
âˆ‘

ak Î»k = 0,

k=0

when the value of the polynomial equals zero whenever a particular value of Î» âˆˆ F
is placed in the formula for p (Î») , can the same conclusion be drawn? Is there any
diï¬€erence in the two deï¬nitions for ordinary ï¬elds like Q? Hint: Consider Z2 , the
integers mod 2.
5. Let A âˆˆ L (V, V ) where V is a ï¬nite dimensional vector space with ï¬eld of scalars F.
Let p (Î») be the minimal polynomial and suppose Ï• (Î») is any nonzero polynomial such
that Ï• (A) is not one to one and Ï• (Î») has smallest possible degree such that Ï• (A) is
nonzero and not one to one. Show Ï• (Î») must divide p (Î»).
6. Let A âˆˆ L (V, V ) where V is a ï¬nite dimensional vector space with ï¬eld of scalars F.
Let p (Î») be the minimal polynomial and suppose Ï• (Î») is an irreducible polynomial
with the property that Ï• (A) x = 0 for some speciï¬c x Ì¸= 0. Show that Ï• (Î») must
divide p (Î») . Hint: First write
p (Î») = Ï• (Î») g (Î») + r (Î»)
where r (Î») is either 0 or has degree smaller than the degree of Ï• (Î»). If r (Î») = 0 you
are done. Suppose it is not 0. Let Î· (Î») be the monic polynomial of smallest degree
with the property that Î· (A) x = 0. Now use the Euclidean algorithm to divide Ï• (Î»)
by Î· (Î») . Contradict the irreducibility of Ï• (Î») .
7. Suppose A is a linear transformation and let the characteristic polynomial be
det (Î»I âˆ’ A) =

q
âˆ

Ï•j (Î»)

nj

j=1

where the Ï•j (Î») are irreducible. Explain using Corollary 8.3.11 why the irreducible
factors of the minimal polynomial are Ï•j (Î») and why the minimal polynomial is of
the form
q
âˆ
r
Ï•j (Î») j
j=1

where rj â‰¤ nj . You can use the Cayley Hamilton theorem if you like.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

10.6. EXERCISES

263
ï£«

8. Let

1
A=ï£­ 0
0

0
0
1

ï£¶
0
âˆ’1 ï£¸
0

Find the minimal polynomial for A.
9. Suppose{A is an n Ã— n matrix
} and let v be a vector. Consider the A cyclic set of
vectors v, Av, Â· Â· Â· , Amâˆ’1 v where this is an independent set of vectors but Am v is
a linear combination of the preceding vectors in the list. Show how to obtain a monic
polynomial of smallest degree, m, Ï•v (Î») such that
Ï•v (A) v = 0
Now let {w1 , Â· Â· Â· , wn } be a basis and let Ï• (Î») be the least common multiple of the
Ï•wk (Î») . Explain why this must be the minimal polynomial of A. Give a reasonably
easy algorithm for computing Ï•v (Î»).
10. Here is a matrix.

ï£«

âˆ’7
ï£­ âˆ’21
70

ï£¶
âˆ’1 âˆ’1
âˆ’3 âˆ’3 ï£¸
10 10

Using the process of Problem 9 ï¬nd the minimal polynomial of this matrix. It turns
out the characteristic polynomial is Î»3 .
11. Find the minimal polynomial for
ï£¶
1 2 3
A=ï£­ 2 1 4 ï£¸
âˆ’3 2 1
ï£«

by the above technique. Is what you found also the characteristic polynomial?
12. Let A be an n Ã— n matrix with ï¬eld of scalars C. Letting Î» be an eigenvalue, show
the dimension of the eigenspace equals the number of Jordan blocks in the Jordan
canonical form which are associated with Î». Recall the eigenspace is ker (Î»I âˆ’ A) .
13. For any n Ã— n matrix, why is the dimension of the eigenspace always less than or
equal to the algebraic multiplicity of the eigenvalue as a root of the characteristic
equation? Hint: Note the algebraic multiplicity is the size of the appropriate block
in the Jordan form.
14. Give an example of two nilpotent matrices which are not similar but have the same
minimal polynomial if possible.
15. Use the existence of the Jordan canonical form for a linear transformation whose
minimal polynomial factors completely to give a proof of the Cayley Hamilton theorem
which is valid for any ï¬eld of scalars. Hint: First assume the minimal polynomial
factors completely into linear factors. If this does not happen, consider a splitting ï¬eld
of the minimal polynomial. Then consider the minimal polynomial with respect to
this larger ï¬eld. How will the two minimal polynomials be related? Show the minimal
polynomial always divides the characteristic polynomial.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

264

LINEAR TRANSFORMATIONS CANONICAL FORMS

16. Here is a matrix. Find its Jordan canonical form by directly ï¬nding the eigenvectors
and generalized eigenvectors based on these to ï¬nd a basis which will yield the Jordan
form. The eigenvalues are 1 and 2.
ï£«
ï£¶
âˆ’3 âˆ’2 5 3
ï£¬ âˆ’1 0 1 2 ï£·
ï£¬
ï£·
ï£­ âˆ’4 âˆ’3 6 4 ï£¸
âˆ’1 âˆ’1 1 3
Why is it typically impossible to ï¬nd the Jordan canonical form?
17. People like to consider the solutions of ï¬rst order linear systems of equations which
are of the form
xâ€² (t) = Ax (t)
where here A is an n Ã— n matrix. From the theorem on the Jordan canonical form,
there exist S and S âˆ’1 such that A = SJS âˆ’1 where J is a Jordan form. Deï¬ne
y (t) â‰¡ S âˆ’1 x (t) . Show yâ€² = Jy. Now suppose Î¨ (t) is an n Ã— n matrix whose columns
are solutions of the above diï¬€erential equation. Thus
Î¨â€² = AÎ¨
Now let Î¦ be deï¬ned by SÎ¦S âˆ’1 = Î¨. Show
Î¦â€² = JÎ¦.
18. In the above Problem show that
â€²

det (Î¨) = trace (A) det (Î¨)
and so
det (Î¨ (t)) = Cetrace(A)t
This is called Abelâ€™s formula and det (Î¨ (t)) is called the Wronskian. Hint: Show it
suï¬ƒces to consider
Î¦â€² = JÎ¦
and establish the formula for Î¦. Next let
ï£«

ï£¶
Ï•1
ï£¬
ï£·
Î¦ = ï£­ ... ï£¸
Ï•n

where the Ï•j are the rows of Î¦. Then explain why
â€²

det (Î¦) =

n
âˆ‘

det (Î¦i )

(10.10)

i=1

where Î¦i is the same as Î¦ except the ith row is replaced with Ï•â€²i instead of the row
Ï•i . Now from the form of J,
Î¦â€² = DÎ¦ + N Î¦
where N has all nonzero entries above the main diagonal. Explain why
Ï•â€²i (t) = Î»i Ï•i (t) + ai Ï•i+1 (t)

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

10.6. EXERCISES

265

Now use this in the formula for the derivative of the Wronskian given in (10.10) and
use properties of determinants to obtain
â€²

det (Î¦) =

n
âˆ‘

Î»i det (Î¦) .

i=1

Obtain Abelâ€™s formula
det (Î¦) = Cetrace(A)t
and so the Wronskian det Î¦ either vanishes identically or never.
19. Let A be an n Ã— n matrix and let J be its Jordan canonical form. Recall J is a block
diagonal matrix having blocks Jk (Î») down the diagonal. Each of these blocks is of
the form
ï£«
ï£¶
Î» 1
0
ï£¬
ï£·
.
ï£¬
ï£·
Î» ..
ï£·
Jk (Î») = ï£¬
ï£·
ï£¬
..
ï£­
. 1 ï£¸
0
Î»
Now for Îµ > 0 given, let the diagonal matrix DÎµ be given by
ï£«
ï£¶
1
0
ï£¬
ï£·
Îµ
ï£¬
ï£·
DÎµ = ï£¬
ï£·
..
ï£­
ï£¸
.
kâˆ’1
0
Îµ
Show that DÎµâˆ’1 Jk (Î») DÎµ has the same form as Jk (Î») but instead of ones down the
super diagonal, there is Îµ down the super diagonal. That is Jk (Î») is replaced with
ï£«
ï£¶
Î» Îµ
0
ï£¬
ï£·
.
ï£¬
ï£·
Î» ..
ï£¬
ï£·
ï£¬
ï£·
.
.. Îµ ï£¸
ï£­
0
Î»
Now show that for A an n Ã— n matrix, it is similar to one which is just like the Jordan
canonical form except instead of the blocks having 1 down the super diagonal, it has
Îµ.
20. Let A be in L (V, V ) and suppose that Ap x Ì¸= 0 for some x Ì¸= 0. Show that Ap ek Ì¸= 0
for some ek âˆˆ {e1 , Â· Â· Â· , en } , a basis for V . If you have a matrix which is nilpotent,
(Am = 0 for some m) will it always be possible to ï¬nd its Jordan form? Describe how
to do it if this is the case. Hint: First explain why all the eigenvalues are 0. Then
consider the way the Jordan form for nilpotent transformations was constructed in the
above.
21. Suppose A is an n Ã— n matrix and that it has n distinct eigenvalues. How do the minimal polynomial and characteristic polynomials compare? Determine other conditions
based on the Jordan Canonical form which will cause the minimal and characteristic
polynomials to be diï¬€erent.
22. Suppose A is a 3 Ã— 3 matrix and it has at least two distinct eigenvalues. Is it possible
that the minimal polynomial is diï¬€erent than the characteristic polynomial?

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

266

LINEAR TRANSFORMATIONS CANONICAL FORMS

23. If A is an n Ã— n matrix of entries from a ï¬eld of scalars and if the minimal polynomial
of A splits over this ï¬eld of scalars, does it follow that the characteristic polynomial
of A also splits? Explain why or why not.
24. In proving the uniqueness of the Jordan canonical form, it was asserted that if two
n Ã— n matrices A, B are similar, then they have the same minimal polynomial and also
that if this minimal polynomial is of the form
p (Î») =

s
âˆ

ri

Ï•i (Î»)

i=1
r

r

where the Ï•i (Î») are irreducible, then ker (Ï•i (A) i ) and ker (Ï•i (B) i ) have the same
dimension. Why is this so? This was what was responsible for the blocks corresponding
to an eigenvalue being of the same size.
25. Show that a given complex n Ã— n matrix is non defective (diagonalizable) if and only
if the minimal polynomial has no repeated roots.
26. Describe a straight forward way to determine the minimal polynomial of an n Ã— n
matrix using row operations. Next show that if p (Î») and pâ€² (Î») are relatively prime,
then p (Î») has no repeated roots. With the above problem, explain how this gives a
way to determine whether a matrix is non defective.
27. In Theorem 10.3.4 show that the cyclic sets can be arranged in such a way that the
length of Î² vi+1 divides the length of Î² vi .
28. Show that if A is a complex n Ã— n matrix, then A and AT are similar. Hint: Consider
a Jordan block. Note that
ï£«
ï£¶ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
0 0 1
Î» 1 0
0 0 1
Î» 0 0
ï£­ 0 1 0 ï£¸ï£­ 0 Î» 1 ï£¸ï£­ 0 1 0 ï£¸ = ï£­ 1 Î» 0 ï£¸
1 0 0
0 0 Î»
1 0 0
0 1 Î»
29. Let A be a linear transformation deï¬ned on a ï¬nite dimensional vector space V . Let
the minimal polynomial be
q
âˆ
m
Ï•i (Î») i
(

i=1

{
}
be the cyclic sets such that Î² ivi , Â· Â· Â· , Î² vi ri
is a basis for
1
1
i âˆ‘ âˆ‘
i
mi
i
ker (Ï•i (A) ). Let v = i j vj . Now let q (Î») be any polynomial and suppose that
and let

Î² ivi , Â· Â· Â· , Î² ivri

)

q (A) v = 0
Show{that it follows q (A)
} = 0. Hint: First consider the special case where a basis for
V is x, Ax, Â· Â· Â· , Anâˆ’1 x and q (A) x = 0.

10.7

The Rational Canonical Form

âˆq
m
Here one has the minimal polynomial in the form k=1 Ï• (Î») k where Ï• (Î») is an irreducible
monic polynomial. It is not necessarily the case that Ï• (Î») is a linear factor. Thus this case
is completely general and includes the situation where the ï¬eld is arbitrary. In particular, it
includes the case where the ï¬eld of scalars is, for example, the rational numbers. This may

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

10.7. THE RATIONAL CANONICAL FORM

267

be partly why it is called the rational canonical form. As you know, the rational numbers
are notorious for not having roots to polynomial equations which have integer or rational
coeï¬ƒcients.
This canonical form is due to Frobenius. I am following the presentation given in [9] and
there are more details given in this reference. Another good source which has many of the
same ideas is [14].
Here is a deï¬nition of the concept of a companion matrix.
Deï¬nition 10.7.1 Let
q (Î») = a0 + a1 Î» + Â· Â· Â· + anâˆ’1 Î»nâˆ’1 + Î»n
be a monic polynomial. The companion matrix of q (Î») ,
ï£«
0 Â·Â·Â·
0
âˆ’a0
ï£¬ 1 0
âˆ’a1
ï£¬
ï£¬
..
.
.
..
..
ï£­
.
0
1 âˆ’anâˆ’1

denoted as C (q (Î»)) is the matrix
ï£¶
ï£·
ï£·
ï£·
ï£¸

Proposition 10.7.2 Let q (Î») be a polynomial and let C (q (Î»)) be its companion matrix.
Then q (C (q (Î»))) = 0.
Proof: Write C instead of C (q (Î»)) for short. Note that
Ce1 = e2 , Ce2 = e3 , Â· Â· Â· , Cenâˆ’1 = en
Thus
and so it follows

ek = C kâˆ’1 e1 , k = 1, Â· Â· Â· , n

(10.11)

{
}
e1 , Ce1 , C 2 e1 , Â· Â· Â· , C nâˆ’1 e1

(10.12)

are linearly independent. Hence these form a basis for F . Now note that Cen is given by
n

Cen = âˆ’a0 e1 âˆ’ a1 e2 âˆ’ Â· Â· Â· âˆ’ anâˆ’1 en
and from (10.11) this implies
C n e1 = âˆ’a0 e1 âˆ’ a1 Ce1 âˆ’ Â· Â· Â· âˆ’ anâˆ’1 C nâˆ’1 e1
and so
q (C) e1 = 0.
Now since (10.12) is a basis, every vector of Fn is of the form k (C) e1 for some polynomial
k (Î»). Therefore, if v âˆˆ Fn ,
q (C) v = q (C) k (C) e1 = k (C) q (C) e1 = 0
which shows q (C) = 0. 
The following theorem is on the existence of the rational canonical form.
Theorem 10.7.3 Let A âˆˆ L (V, V ) where V is a vector space with ï¬eld of scalars F and
minimal polynomial
q
âˆ
m
Ï•i (Î») i
i=1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

268

LINEAR TRANSFORMATIONS CANONICAL FORMS
mk

where each Ï•i (Î») is irreducible. Letting Vk â‰¡ ker (Ï•k (Î»)

) , it follows

V = V1 âŠ• Â· Â· Â· âŠ• Vq
where each Vk is A invariant. Letting Bk denote a basis for Vk and M k the matrix of the
restriction of A to Vk , it follows that the matrix of A with respect to the basis {B1 , Â· Â· Â· , Bq }
is the block diagonal matrix of the form
ï£«
ï£¶
M1
0
ï£¬
ï£·
..
(10.13)
ï£­
ï£¸
.
0

Mq

{
}
If Bk is given as Î² v1 , Â· Â· Â· , Î² vs as described in Theorem 10.3.4 where each Î² vj is an A
cyclic set of vectors, then the matrix M k is of the form
ï£«
ï£¶
r
C (Ï•k (Î») 1 )
0
ï£¬
ï£·
..
Mk = ï£­
(10.14)
ï£¸
.
rs
0
C (Ï•k (Î») )
where the A cyclic sets of vectors may be arranged in order such that the positive integers rj
r
r
satisfy r1 â‰¥ Â· Â· Â· â‰¥ rs and C (Ï•k (Î») j ) is the companion matrix of the polynomial Ï•k (Î») j .
Proof: By Theorem 10.2.6 the matrix of A with respect to {B1 , Â· Â· Â· , Bq } is of the
form
given in}(10.13). Now by Theorem 10.3.4 the basis Bk may be chosen in the form
{
Î² v1 , Â· Â· Â· , Î² vs where each Î² vk is an A cyclic set of vectors and also it can be assumed the
lengths of these Î² vk are decreasing. Thus
( )
( )
Vk = span Î² v1 âŠ• Â· Â· Â· âŠ• span Î² vs
(
)
and it only remains to consider the matrix of A restricted to span Î² vk . Then you can
apply Theorem 10.2.6 to get the result in (10.14). Say
Î² vk = vk , Avk , Â· Â· Â· , Adâˆ’1 vk
where Î· (A) vk = 0 and the degree of Î· (Î») is d, the smallest degree such that this is so, Î·
r
being a monic polynomial. Then by Corollary 8.3.11, Î· (Î») = Ï•k (Î») k where rk â‰¤ mk . Now
(
(
))
(
)
A span Î² vk âŠ† span Î² vk
(
)
dâˆ’1
because Ad vk is in
( span
) vk , Avk , Â· Â· Â· , A vk . It remains to consider the matrix of A
restricted to span Î² vk . Say
Î· (Î») = Ï•k (Î»)

rk

= a0 + a1 Î» + Â· Â· Â· + adâˆ’1 Î»dâˆ’1 + Î»d

Thus
Ad vk = âˆ’a0 vk âˆ’ a1 Avk âˆ’ Â· Â· Â· âˆ’ adâˆ’1 Adâˆ’1 vk
Recall the formalism for ï¬nding the matrix of A restricted to this invariant subspace.
(

(

Avk

vk

A2 vk

Avk

A3 vk

A2 vk

Â·Â·Â·

Â·Â·Â·

âˆ’a0 vk âˆ’ a1 Avk âˆ’ Â· Â· Â· âˆ’ adâˆ’1 Adâˆ’1 vk
ï£«
0 0
0 Â·Â·Â·
âˆ’a0
ï£¬ 1 0
âˆ’a1
ï£¬
ï£¬
)
..
.
..
.
Adâˆ’1 vk ï£¬
ï£¬ 0 1
ï£¬
.
.
.
.
ï£­
.
. 0 âˆ’adâˆ’2
0
0
1 âˆ’adâˆ’1

Saylor URL: http://www.saylor.org/courses/ma212/

)

=
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

The Saylor Foundation

10.8. UNIQUENESS

269

Thus the matrix of the transformation is the above. The is the companion matrix of
r
r
Ï•k (Î») k = Î· (Î»). In other words, C = C (Ï•k (Î») k ) and so M k has the form claimed in
the theorem. 

10.8

Uniqueness

Given A âˆˆ L (V, V ) where V is a vector space having ï¬eld of scalars F, the above shows
there exists a rational canonical form for A. Could A have more than one rational canonical
form? Recall the deï¬nition of an A cyclic set. For convenience, here it is again.
{
}
Deï¬nition 10.8.1 Letting x Ì¸= 0 denote by
Î² x the vectors) x, Ax, A2 x, Â· Â· Â· , Amâˆ’1 x where
(
m is the smallest such that Am x âˆˆ span x, Â· Â· Â· , Amâˆ’1 x . This is called an A cyclic set,
denoted by Î² x .
The following proposition ties these A cyclic sets to polynomials. It is just a review of
ideas used above to prove existence.
{
}
Proposition 10.8.2 Let x Ì¸= 0 and consider x, Ax, A2 x, Â· Â· Â· , Amâˆ’1 x . Then this is an
A cyclic set if and only if there exists a monic polynomial Î· (Î») such that Î· (A) x = 0 and
among all such polynomials Ïˆ (Î») satisfying Ïˆ (A) x = 0, Î· (Î») has the smallest degree. If
m
V = ker (Ï• (Î») ) where Ï• (Î») is irreducible, then for some positive integer p â‰¤ m, Î· (Î») =
p
Ï• (Î») .
m

Lemma 10.8.3 Let V be a vector space and A âˆˆ L (V, V ) has minimal polynomial
Ï• (Î») }
{
where Ï• (Î») is irreducible and has degree d. Let the basis for V consist of Î² v1 , Â· Â· Â· , Î² vs
where Î² vk is A cyclic as described above and the rational canonical form for A is the matrix
taken with respect to this basis. Then letting Î² vk denote the number of vectors in Î² vk , it
follows there is only one possible set of numbers Î² vk .
p

Proof: Say Î² vj is associated with the polynomial Ï• (Î») j . Thus, as described above
Î² vj equals pj d. Consider the following table which comes from the A cyclic set
{
}
vj , Avj , Â· Â· Â· , Adâˆ’1 vj , Â· Â· Â· , Apj dâˆ’1 vj
Î±j0
vj
Ï• (A) vj
..
.
p âˆ’1
Ï• (A) j vj

Î±j1
Avj
Ï• (A) Avj
..
.
p âˆ’1
Ï• (A) j Avj

Î±j2
A2 vj
Ï• (A) A2 vj
..
.
p âˆ’1
Ï• (A) j A2 vj

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·

Î±jdâˆ’1
Adâˆ’1 vj
Ï• (A) Adâˆ’1 vj
..
.
p âˆ’1
Ï• (A) j Adâˆ’1 vj

In the above, Î±jk signiï¬es the vectors below it in the k th column. None of these vectors
p âˆ’1
below the top row are equal to 0 because the degree of Ï• (Î») j Î»dâˆ’1 is dpj âˆ’ 1, which is
less than pj d and the smallest degree of a nonzero polynomial sending vj to 0 is pj d. Also,
each of these vectors is in the span of Î² vj and there are dpj of them, just as there are dpj
vectors in Î² vj .
{
}
Claim: The vectors Î±j0 , Â· Â· Â· , Î±jdâˆ’1 are linearly independent.
Proof of claim: Suppose
j âˆ’1
dâˆ’1 pâˆ‘
âˆ‘

k

cik Ï• (A) Ai vj = 0

i=0 k=0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

270

LINEAR TRANSFORMATIONS CANONICAL FORMS

Then multiplying both sides by Ï• (A)
dâˆ’1
âˆ‘

pj âˆ’1

this yields
pj âˆ’1

ci0 Ï• (A)

Ai vj = 0

i=0

Now if any of the ci0 is nonzero this would imply there exists a polynomial having degree
smaller than pj d which sends vj to 0. Since this does not happen, it follows each ci0 = 0.
Thus
j âˆ’1
dâˆ’1 pâˆ‘
âˆ‘
k
cik Ï• (A) Ai vj = 0
i=0 k=1
p âˆ’2

Now multiply both sides by Ï• (A) j and do a similar argument to assert that ci1 = 0 for
each i. Continuing this
{ way, all the c}ik = 0 and this proves the claim.
Thus the vectors Î±j0 , Â· Â· Â· , Î±jdâˆ’1 are linearly independent and there are pj d = Î² vj
( )
of them. Therefore, they form a basis for span Î² vj . Also note that if you list the
columns
in reverse
order starting from the bottom and going toward the top, the vectors
{
}
j
j
Î±0 , Â· Â· Â· , Î±dâˆ’1 yield Jordan blocks in the matrix of Ï• (A). Hence, considering all these
}s
{
listed in the reverse order, the matrix of Ï• (A) with respect to
vectors Î±j0 , Â· Â· Â· , Î±jdâˆ’1
j=1

this basis of V is in Jordan canonical form. See Proposition 10.4.4 and Theorem 10.5.2 on
existence and uniqueness for
{ the Jordan}form. This Jordan form is unique up to order of
the blocks. For a given j Î±j0 , Â· Â· Â· , Î±jdâˆ’1 yields d Jordan blocks of size pj for Ï• (A). The
size and number of Jordan blocks of Ï• (A) depends only on Ï• (A) , hence only on A. Once
A is determined, Ï• (A) is determined and hence the number and size of Jordan blocks is
determined so the exponents pj are determined and this shows the lengths of the Î² vj , pj d
are also determined. 
Note that if the pj are known, then so is the rational canonical form because it comes
p
from blocks which are companion matrices of the polynomials Ï• (Î») j . Now here is the main
result.
Theorem 10.8.4 Let V be a vector space having ï¬eld of scalars F and let A âˆˆ L (V, V ).
Then the rational canonical form of A is unique up to order of the blocks.
âˆq
m
Proof: Let the minimal polynomial of A be k=1 Ï•k (Î») k . Then recall from Corollary
10.2.4
V = V1 âŠ• Â· Â· Â· âŠ• Vq
m

where Vk = ker (Ï•k (A) k ) . Also recall from Corollary 10.2.5 that the minimal polynomial
m
of the restriction of A to Vk is Ï•k (Î») k . Now apply Lemma 10.8.3 to A restricted to Vk . 
In the case where two n Ã— n matrices M, N are similar, recall this is equivalent to the
two being matrices of the same linear transformation taken with respect to two diï¬€erent
bases. Hence each are similar to the same rational canonical form.
Example 10.8.5 Here is a matrix.
ï£«

5
A=ï£­ 2
9

ï£¶
âˆ’2 1
10 âˆ’2 ï£¸
0
9

Find a similarity transformation which will produce the rational canonical form for A.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

10.8. UNIQUENESS

271

The characteristic polynomial is Î»3 âˆ’ 24Î»2 + 180Î» âˆ’ 432. This factors as
2

(Î» âˆ’ 6) (Î» âˆ’ 12)
It turns out this is also the minimal polynomial. You can see this by plugging in A where
you see Î» and observing things donâ€™t work if you delete one of the Î» âˆ’ 6 factors. There is
more on this in the exercises. It turns out
the minimal polynomial pretty
( you can compute
)
2
3
easily. Thus Q is the direct sum of ker (A âˆ’ 6I) and ker (A âˆ’ 12I) . Consider the ï¬rst
of these. You see easily that this is
ï£« ï£¶
ï£«
ï£¶
1
âˆ’1
y ï£­ 1 ï£¸ + z ï£­ 0 ï£¸ , y, z âˆˆ Q.
0
1
What about the length of A cyclic sets? It turns out it doesnâ€™t matter much. You can start
with either of these and get a cycle of length 2. Lets pick the second one. This leads to the
cycle
ï£«
ï£¶ ï£«
ï£¶
ï£«
ï£¶ ï£«
ï£¶
ï£«
ï£¶
âˆ’1
âˆ’4
âˆ’1
âˆ’12
âˆ’1
ï£­ 0 ï£¸ , ï£­ âˆ’4 ï£¸ = A ï£­ 0 ï£¸ , ï£­ âˆ’48 ï£¸ = A2 ï£­ 0 ï£¸
1
0
1
âˆ’36
1
where the last of the three is a linear combination of the ï¬rst two. Take the ï¬rst two as
the ï¬rst two columns of S. To get the third, you need a cycle of length 1 corresponding to
(
)T
ker (A âˆ’ 12I) . This yields the eigenvector 1 âˆ’2 3
. Thus
ï£«
ï£¶
âˆ’1 âˆ’4 1
S = ï£­ 0 âˆ’4 âˆ’2 ï£¸
1
0
3
Now using Proposition 9.3.10, the Rational canonical form for A should be
ï£«
ï£¶âˆ’1 ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
âˆ’1 âˆ’4 1
5 âˆ’2 1
âˆ’1 âˆ’4 1
0 âˆ’36 0
ï£­ 0 âˆ’4 âˆ’2 ï£¸ ï£­ 2 10 âˆ’2 ï£¸ ï£­ 0 âˆ’4 âˆ’2 ï£¸ = ï£­ 1 12
0 ï£¸
1
0
3
9 0
9
1
0
3
0
0
12
Example 10.8.6 Here is a matrix.
ï£«
12
ï£¬ âˆ’4
ï£¬
A=ï£¬
ï£¬ 4
ï£­ 0
âˆ’4

âˆ’3 âˆ’19
1
1
5
5
âˆ’5 âˆ’5
3
11

âˆ’14
6
âˆ’2
2
6

8
âˆ’4
4
0
0

ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸

Find a basis such that if S is the matrix which has these vectors as columns S âˆ’1 AS is in
rational canonical form assuming the ï¬eld of scalars is Q.
First it is necessary to ï¬nd the minimal polynomial. Of course you can ï¬nd the characteristic polynomial and then take away factors till you ï¬nd the minimal polynomial. However,
there is a much better way which is described in the exercises. Leaving out this detail, the
minimal polynomial is
Î»3 âˆ’ 12Î»2 + 64Î» âˆ’ 128
This polynomial factors as
(
)
(Î» âˆ’ 4) Î»2 âˆ’ 8Î» + 32 â‰¡ Ï•1 (Î») Ï•2 (Î»)

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

272

LINEAR TRANSFORMATIONS CANONICAL FORMS

where the second factor is irreducible over Q. Consider Ï•2 (Î») ï¬rst. Messy computations
yield
ï£«
ï£¶
âˆ’16 âˆ’16 âˆ’16 âˆ’16 âˆ’32
ï£¬ 0
0
0
0
0 ï£·
ï£¬
ï£·
0
0
0
0
0 ï£·
Ï•2 (A) = ï£¬
ï£¬
ï£·
ï£­ 0
0
0
0
0 ï£¸
16
16
16
16
32
and so

ï£«
ï£¬
ï£¬
ker (Ï•2 (A)) = a ï£¬
ï£¬
ï£­

âˆ’1
1
0
0
0

ï£¶

ï£«

ï£·
ï£¬
ï£·
ï£¬
ï£· + bï£¬
ï£·
ï£¬
ï£­
ï£¸

âˆ’1
0
1
0
0

ï£¶

ï£«

ï£¬
ï£·
ï£·
ï£¬
ï£· + cï£¬
ï£·
ï£¬
ï£¸
ï£­

âˆ’1
0
0
1
0

ï£¶

ï£«

ï£·
ï£¬
ï£·
ï£¬
ï£· + dï£¬
ï£¬
ï£·
ï£¸
ï£­

âˆ’2
0
0
0
1

ï£¶
ï£·
ï£·
ï£·.
ï£·
ï£¸

Now start with one of these basis vectors and look for an A cycle. Picking the ï¬rst one, you
obtain the cycle
ï£¶
ï£¶ ï£«
ï£«
âˆ’15
âˆ’1
ï£¬ 1 ï£· ï£¬ 5 ï£·
ï£·
ï£· ï£¬
ï£¬
ï£¬ 0 ï£·,ï£¬ 1 ï£·
ï£·
ï£· ï£¬
ï£¬
ï£­ 0 ï£¸ ï£­ âˆ’5 ï£¸
7
0
because the next vector involving A2 yields a vector which is in the span of the above two.
You check this by making the vectors the columns of a matrix and ï¬nding the row reduced
echelon form. Clearly this cycle does not span ker (Ï•2 (A)) , so look for another cycle. Begin
with a vector which is not in the span of these two. The last one works well. Thus another
A cycle is
ï£«
ï£¶ ï£«
ï£¶
âˆ’2
âˆ’16
ï£¬ 0 ï£· ï£¬ 4 ï£·
ï£¬
ï£· ï£¬
ï£·
ï£¬ 0 ï£· , ï£¬ âˆ’4 ï£·
ï£¬
ï£· ï£¬
ï£·
ï£­ 0 ï£¸ ï£­ 0 ï£¸
1
8
It follows a basis for ker (Ï•2 (A)) is
ï£±ï£«
ï£¶ ï£«
ï£¶ ï£«
ï£¶ ï£«
ï£¶ï£¼
âˆ’2
âˆ’16
âˆ’1
âˆ’15 ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£· ï£¬
ï£· ï£¬
ï£· ï£¬
ï£·ï£´
ï£´
ï£²ï£¬
ï£¬ 0 ï£· ï£¬ 4 ï£· ï£¬ 1 ï£· ï£¬ 5 ï£·ï£½
ï£¬ 0 ï£· , ï£¬ âˆ’4 ï£· , ï£¬ 0 ï£· , ï£¬ 1 ï£·
ï£¬
ï£· ï£¬
ï£· ï£¬
ï£· ï£¬
ï£·
ï£´
ï£´
ï£´
ï£­ 0 ï£¸ ï£­ 0 ï£¸ ï£­ 0 ï£¸ ï£­ âˆ’5 ï£¸ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
ï£¾
1
8
0
7
From the above theory, these vectors are linearly independent. Finally consider a cycle
coming from ker (Ï•1 (A)). This amounts to nothing more than ï¬nding an eigenvector for A
(
)T
corresponding to the eigenvalue 4. An eigenvector is âˆ’1 0 0 0 1
.Now the desired
matrix for the similarity transformation is
ï£«
ï£¶
âˆ’2 âˆ’16 âˆ’1 âˆ’15 âˆ’1
ï£¬ 0
4
1
5
0 ï£·
ï£¬
ï£·
ï£¬
âˆ’4
0
1
0 ï£·
Sâ‰¡ï£¬ 0
ï£·
ï£­ 0
0
0
âˆ’5
0 ï£¸
1
8
0
7
1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

10.9. EXERCISES

273

Then doing the computations, you get
ï£«
S

âˆ’1

ï£¬
ï£¬
AS = ï£¬
ï£¬
ï£­

0 âˆ’32 0
1
8
0
0
0
0
0
0
1
0
0
0

ï£¶
0
0
0
0 ï£·
ï£·
âˆ’32 0 ï£·
ï£·
8
0 ï£¸
0
4

and you see this is in rational canonical form, the two 2Ã—2 blocks being companion matrices
for the polynomial Î»2 âˆ’8Î»+32 and the 1Ã—1 block being a companion matrix for Î»âˆ’4. Note
that you could have written this without ï¬nding a similarity transformation to produce it.
This follows from the above theory which gave the existence of the rational canonical form.
Obviously there is a lot more which could be considered about rational canonical forms.
Just begin with a strange ï¬eld and start investigating what can be said. It is as far as I
feel like going on this subject at this time. One can also derive more systematic methods
for ï¬nding the rational canonical form. The advantage of this is you donâ€™t need to ï¬nd the
eigenvalues in order to compute the rational canonical form and it can often be computed
for this reason, unlike the Jordan form. The uniqueness of this rational canonical form can
be used to determine whether two matrices consisting of entries in some ï¬eld are similar.

10.9

Exercises

1. Letting A be a complex n Ã— n matrix, in obtaining the rational canonical form, one
obtains the Cn as a direct sum of the form
(
)
(
)
span Î² x1 âŠ• Â· Â· Â· âŠ• span Î² xr
where Î² x is an ordered cyclic set of vectors, x, Ax, Â· Â· Â· , Amâˆ’1 x such that Am x is in
the span
( of the previous)vectors. Now apply the Gram Schmidt process to the ordered
basis Î² x1 , Î² x2 , Â· Â· Â· , Î² xr , the vectors in each Î² xi listed according to increasing power
of A, thus obtaining an ordered basis (q1 , Â· Â· Â· , qn ) . Letting Q be the unitary matrix
which has these vectors as columns, show that Qâˆ— AQ equals a matrix B which satisï¬es
Bij = 0 if i âˆ’ j â‰¥ 2. Such a matrix is called an upper Hessenberg matrix and this
shows that every n Ã— n matrix is orthogonally similar to an upper Hessenberg matrix.
These are zero below the main sub diagonal, like companion matrices discussed above.
2. In the argument for Theorem 10.2.4 it was shown that m (A) Ï•l (A) v = v whenever
r
r
v âˆˆ ker (Ï•k (A) k ) . Show that m (A) restricted to ker (Ï•k (A) k ) is the inverse of the
rk
linear transformation Ï•l (A) on ker (Ï•k (A) ) .
3. Suppose A is a linear transformation and let the characteristic polynomial be
det (Î»I âˆ’ A) =

q
âˆ

Ï•j (Î»)

nj

j=1

where the Ï•j (Î») are irreducible. Explain using Corollary 8.3.11 why the irreducible
factors of the minimal polynomial are Ï•j (Î») and why the minimal polynomial is of
the form
q
âˆ
r
Ï•j (Î») j
j=1

where rj â‰¤ nj . You can use the Cayley Hamilton theorem if you like.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

274

LINEAR TRANSFORMATIONS CANONICAL FORMS

4. Find the minimal polynomial for
ï£«

ï£¶
1 2 3
A=ï£­ 2 1 4 ï£¸
âˆ’3 2 1
by the above technique assuming the ï¬eld of scalars is the rational numbers. Is what
you found also the characteristic polynomial?
5. Show, using the rational root theorem, the minimal polynomial for A in the above
problem is irreducible with respect to Q. Letting the ï¬eld of scalars be Q ï¬nd the
rational canonical form and a similarity transformation which will produce it.
6. Find the rational canonical form for the matrix
ï£«
1 2 1 âˆ’1
ï£¬ 2 3 0 2
ï£¬
ï£­ 1 3 2 4
1 2 1 2

ï£¶
ï£·
ï£·
ï£¸

(
)
7. Let A : Q3 â†’ Q3 be linear. Suppose the minimal polynomial is (Î» âˆ’ 2) Î»2 + 2Î» + 7 .
Find the rational canonical form. Can you give generalizations of this rather simple
problem to other situations?
8. Find the rational canonical form with respect to the ï¬eld of scalars equal to Q for the
matrix
ï£¶
ï£«
0 0 1
A = ï£­ 1 0 âˆ’1 ï£¸
0 1 1
Observe that this particular matrix is already a companion matrix of Î»3 âˆ’ Î»2 + Î» âˆ’ 1.
Then ï¬nd the rational canonical form if the ï¬eld of scalars equals C or Q + iQ.
9. Let q (Î») be a polynomial and C its companion matrix. Show the characteristic and
minimal polynomial of C are the same and both equal q (Î»).
10. â†‘Use the existence of the rational canonical form to give a proof of the Cayley Hamilton
theorem valid for any ï¬eld, even ï¬elds like the integers mod p for p a prime. The earlier
proof based on determinants was ï¬ne for ï¬elds like Q or R where you could let Î» â†’ âˆ
but it is not clear the same result holds in general.
11. Suppose you have two nÃ—n matrices A, B whose entries are in a ï¬eld F and suppose G
is an extension of F. For example, you could have F = Q and G = C. Suppose A and
B are similar with respect to the ï¬eld G. Can it be concluded that they are similar
with respect to the ï¬eld F? Hint: First show that the two have the same minimal
polynomial over F. Next consider the proof of Lemma 10.8.3 and show that they have
the same rational canonical form with respect to F.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Markov Chains And Migration
Processes
11.1

Regular Markov Matrices

The existence of the Jordan form is the basis for the proof of limit theorems for certain
kinds of matrices called Markov matrices.
Deï¬nition 11.1.1 An n Ã— n matrix A = (aij ) , is a Markov matrix if aij â‰¥ 0 for all i, j
and
âˆ‘
aij = 1.
i

It may also be called a stochastic matrix. A matrix which has nonnegative entries such that
âˆ‘
aij = 1
j

will also be called a stochastic matrix. A Markov or stochastic matrix is called regular if
some power of A has all entries strictly positive. A vector, v âˆˆ Rn , is a steady state if
Av = v.
Lemma 11.1.2 The property of being a stochastic matrix is preserved by taking products.
Proof: Suppose the sum over a row equals 1 for A and B. Then letting the entries be
denoted by (aij ) and (bij ) respectively,
(
)
âˆ‘âˆ‘
âˆ‘ âˆ‘
âˆ‘
aik bkj =
aik bkj =
bkj = 1.
i

k

k

i

k

A similar argument yields the same result in the case where it is the sum over a column
which is equal to 1. It is obvious that when the product is taken, if each aij , bij â‰¥ 0, then
the same will be true of sums of products of these numbers.
The following theorem is convenient for showing the existence of limits.
Theorem 11.1.3 Let A be a real p Ã— p matrix having the properties
1. aij â‰¥ 0
2. Either

âˆ‘p
i=1

aij = 1 or

âˆ‘p
j=1

aij = 1.

3. The distinct eigenvalues of A are {1, Î»2 , . . . , Î»m } where each |Î»j | < 1.
275

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

276

MARKOV CHAINS AND MIGRATION PROCESSES

th
Then limnâ†’âˆ An = Aâˆ exists in the sense that limnâ†’âˆ anij = aâˆ
entry Aâˆ .
ij , the ij
n
th
n
Here aij denotes the ij entry of A . Also, if Î» = 1 has algebraic multiplicity r, then
the Jordan block corresponding to Î» = 1 is just the r Ã— r identity.

Proof. By the existence of the Jordan form for A, it follows that there exists an invertible
matrix P such that
ï£¶
ï£«
I +N
ï£¬
ï£·
Jr2 (Î»2 )
ï£·
ï£¬
P âˆ’1 AP = ï£¬
ï£·=J
.
..
ï£¸
ï£­
Jrm (Î»m )
where I is r Ã— r for r the multiplicity of the eigenvalue 1 and N is a nilpotent matrix for
which N r = 0. I will show that because of Condition 2, N = 0.
First of all,
Jri (Î»i ) = Î»i I + Ni
where Ni satisï¬es Niri = 0 for some ri > 0. It is clear that Ni (Î»i I) = (Î»i I) N and so
n

(Jri (Î»i )) =

n ( )
âˆ‘
n
k=0

k

N k Î»nâˆ’k
=
i

r ( )
âˆ‘
n
k=0

k

N k Î»nâˆ’k
i

which converges to 0 due to the assumption that |Î»i | < 1. There are ï¬nitely many terms
and a typical one is a matrix whose entries are no larger than an expression of the form
nâˆ’k

|Î»i |

nâˆ’k

nk

|Î»i |

nâˆ’k

Ck n (n âˆ’ 1) Â· Â· Â· (n âˆ’ k + 1) â‰¤ Ck |Î»i |

which converges to 0 because, by the root test, the series
for each i = 2, . . . , p,
n
lim (Jri (Î»i )) = 0.

âˆ‘âˆ
n=1

nk converges. Thus

nâ†’âˆ

By Condition 2, if

anij

denotes the ij th entry of An , then either
p
âˆ‘

anij = 1 or

p
âˆ‘

i=1

anij = 1, anij â‰¥ 0.

j=1

This follows from Lemma 11.1.2. It is obvious each anij â‰¥ 0, and so the entries of An must
be bounded independent of n.
It follows easily from
n times

z
}|
{
P âˆ’1 AP P âˆ’1 AP P âˆ’1 AP Â· Â· Â· P âˆ’1 AP = P âˆ’1 An P
that

P âˆ’1 An P = J n

(11.1)

Hence J n must also have bounded entries as n â†’ âˆ. However, this requirement is incompatible with an assumption that N Ì¸= 0.
If N Ì¸= 0, then N s Ì¸= 0 but N s+1 = 0 for some 1 â‰¤ s â‰¤ r. Then
n

(I + N ) = I +

s ( )
âˆ‘
n
Nk
k

k=1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

11.1. REGULAR MARKOV MATRICES

277

One of the entries of N s is nonzero by the deï¬nition of s. Let this entry be nsij . Then this
( )
n
implies that one of the entries of (I + N ) is of the form ns nsij . This entry dominates the
(
)
ij th entries of nk N k for all k < s because
( ) ( )
n
n
lim
/
=âˆ
nâ†’âˆ s
k
n

Therefore, the entries of (I + N ) cannot all be bounded. From block multiplication,
ï£«
ï£¶
n
(I + N )
n
ï£¬
ï£·
(Jr2 (Î»2 ))
ï£¬
ï£·
âˆ’1 n
P A P =ï£¬
ï£·
.
..
ï£­
ï£¸
n
(Jrm (Î»m ))
and this is a contradiction because entries are bounded on the left and unbounded on the
right.
Since N = 0, the above equation implies limnâ†’âˆ An exists and equals
ï£«
ï£¶
I
ï£¬
ï£·
0
ï£¬
ï£· âˆ’1
Pï£¬

ï£·P
.
..
ï£­
ï£¸
0
Are there examples which will cause the eigenvalue condition of this theorem to hold?
The following lemma gives such a condition. It turns out that if aij > 0, not just â‰¥ 0, then
the eigenvalue condition of the above theorem is valid.
Lemma 11.1.4 Suppose A = (aij ) is a stochastic matrix. Then Î» = 1 is an eigenvalue. If
aij > 0 for all i, j, then if Âµ is an eigenvalue of A, either |Âµ| < 1 or Âµ = 1. In addition to
this, if Av = v for a nonzero vector, v âˆˆ Rn , then vj vi â‰¥ 0 for all i, j so the components of
v have the same sign.
Proof: Suppose the matrix satisï¬es

âˆ‘

aij = 1.

j

(
)T
Then if v = 1 Â· Â· Â· 1
, it is obvious that Av = v. Therefore, this matrix has Î» = 1
as an eigenvalue. Suppose then that Âµ is an eigenvalue. Is |Âµ| < 1 or Âµ = 1? Let v be an
eigenvector and let |vi | be the largest of the |vj | .
âˆ‘
Âµvi =
aij vj
j

and now multiply both sides by Âµvi to obtain
âˆ‘
âˆ‘
2
2
|Âµ| |vi | =
aij Re (vj vi Âµ)
aij vj vi Âµ =
j

j

â‰¤

âˆ‘

2

2

aij |Âµ| |vi | = |Âµ| |vi |

j

Therefore, |Âµ| â‰¤ 1. If |Âµ| = 1, then equality must hold in the above, and so vj vi Âµ must be
real and nonnegative for each j. In particular, this holds for j = 1 which shows Âµ and hence
Âµ are real. Thus,
âˆ‘ in this case, Âµ = 1. The only other case is where |Âµ| < 1.
If instead, i aij = 1, consider AT . Both A and AT have the same characteristic polynomial and so their eigenvalues are exactly the same. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

278

MARKOV CHAINS AND MIGRATION PROCESSES

Lemma 11.1.5 Let
âˆ‘ A be any Markov matrix and let v be a vector having all its components
âˆ‘
non negative with i vi = c. Then if w = Av, it follows that wi â‰¥ 0 for all i and i wi = c.
Proof: From the deï¬nition of w,
wi â‰¡

âˆ‘

aij vj â‰¥ 0.

j

Also

âˆ‘

wi =

âˆ‘âˆ‘

i

i

aij vj =

j

âˆ‘âˆ‘
j

aij vj =

i

âˆ‘

vj = c.

j

The following theorem about limits is now easy to obtain.
Theorem 11.1.6 Suppose A is a Markov matrix (The sum over a column equals 1) in
which aij > 0 for all i, j and suppose w is a vector. Then for each i,
(
)
lim Ak w i = vi
kâ†’âˆ

where Av = v. In words, Ak w always converges
to a steady state. In addition to this, if
âˆ‘
w
=
c, then the vector v will also satisfy the
the vector, w satisï¬es
w
â‰¥
0
for
all
i
and
i i
âˆ‘ i
conditions, vi â‰¥ 0, i vi = c.
Proof: By Lemma 11.1.4, since each aij > 0, the eigenvalues are either 1 or have absolute
value less than 1. Therefore, the claimed limit exists by Theorem 11.1.3. The assertion that
the components are nonnegative and sum to c follows from Lemma 11.1.5. That Av = v
follows from
v = lim An w = lim An+1 w = A lim An w = Av. 
nâ†’âˆ

nâ†’âˆ

nâ†’âˆ

It is not hard to generalize the conclusion of this theorem to regular Markov processes.
Corollary 11.1.7 Suppose A is a regular Markov matrix, on for which the entries of Ak
are all positive for some k, and suppose w is a vector. Then for each i,
lim (An w)i = vi

nâ†’âˆ

where Av = v. In words, An w always converges
to a steady state. In addition to this, if
âˆ‘
the vector w satisï¬es
w
â‰¥
0
for
all
i
and
w
=
c, Then the vector v will also satisfy the
i i
âˆ‘ i
conditions vi â‰¥ 0, i vi = c.
Proof: Let the entries of Ak be all positive. Now suppose that aij â‰¥ 0 for all i, j and
A = (aij ) is a transition matrix. Then if B = (bij ) is a transition matrix with bij > 0 for
all ij, it follows that BA is a transition matrix which has strictly positive entries. The ij th
entry of BA is
âˆ‘
bik akj > 0,
k
k

Thus, from Lemma 11.1.4, A has an eigenvalue equal to 1 for all k suï¬ƒciently large, and
all the other eigenvalues have absolute value strictly less than 1. The same must be true of
A, for if Î» is an eigenvalue of A with |Î»| = 1, then Î»k is an eigenvalue for Ak and so, for
all k large enough, Î»k = 1 which is absurd unless Î» = 1. By Theorem 11.1.3, limnâ†’âˆ An w
exists. The rest follows as in Theorem 11.1.6. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

11.2. MIGRATION MATRICES

11.2

279

Migration Matrices

Deï¬nition 11.2.1 Let n locations be denoted by the numbers 1, 2, Â· Â· Â· , n. Also suppose it is
the case that each year aij denotes the proportion of residents in location j which move to
location i. Also suppose
âˆ‘ no one escapes or emigrates from without these n locations. This last
assumption requires i aij = 1. Thus (aij ) is a Markov matrix referred to as a migration
matrix.
T

If v = (x1 , Â· Â· Â· , xn ) where xi is the population of location
âˆ‘ i at a given instant, you obtain
the population of location i one year later by computing j aij xj = (Av)i . Therefore, the
(
)
population of location i after k years is Ak v i . Furthermore, Corollary 11.1.7 can be used
to predict in the case where A is regular what the long time population will be for the given
locations.
As an example of the above, consider the case where n = 3 and the migration matrix is
of the form
ï£«
ï£¶
.6 0 .1
ï£­ .2 .8 0 ï£¸ .
.2 .2 .9
Now

ï£¶
ï£¶2 ï£«
. 38 .0 2 . 15
.6 0 .1
ï£­ .2 .8 0 ï£¸ = ï£­ . 28 . 64 .0 2 ï£¸
. 34 . 34 . 83
.2 .2 .9
(
)
and so the Markov matrix is regular. Therefore, Ak v i will converge to the ith component
of a steady state. It follows the steady state can be obtained from solving the system
ï£«

. 6x + . 1z = x
. 2x + . 8y = y
. 2x + . 2y + . 9z = z
along with the stipulation that the sum of x, y, and z must equal the constant value present
at the beginning of the process. The solution to this system is
{y = x, z = 4x, x = x} .
If the total population at the beginning is 150,000, then you solve the following system
y=x
z = 4x
x + y + z = 150000
whose solution is easily seen to be {x = 25 000, z = 100 000, y = 25 000} . Thus, after a long
time there would be about four times as many people in the third location as in either of
the other two.

11.3

Markov Chains

A random variable is just a function which can have certain values which have probabilities
associated with them. Thus it makes sense to consider the probability that the random
variable has a certain value or is in some set. The idea of a Markov chain is a sequence of
random variables, {Xn } which can be in any of a collection of states which can be labeled
with nonnegative integers. Thus you can speak of the probability the random variable, Xn

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

280

MARKOV CHAINS AND MIGRATION PROCESSES

is in state i. The probability that Xn+1 is in state j given that Xn is in state i is called
a one step transition probability. When this probability does not depend on n it is called
stationary and this is the case of interest here. Since this probability does not depend on n
it can be denoted by pij . Here is a simple example called a random walk.
Example 11.3.1 Let there be n points, xi , and consider a process of something moving
randomly from one point to another. Suppose Xn is a sequence of random variables which
has values {1, 2, Â· Â· Â· , n} where Xn = i indicates the process has arrived at the ith point. Let
pij be the probability that Xn+1 has the value jâˆ‘given that Xn has the value i. Since Xn+1
must have some value, it must be the case that j aij = 1. Note this says that the sum over
a row equals 1 and so the situation is a little diï¬€erent than the above in which the sum was
over a column.
As an example, let x1 , x2 , x3 , x4 be four points taken in order on R and suppose x1
and x4 are absorbing. This means that p4k = 0 for all k Ì¸= 4 and p1k = 0 for all k Ì¸= 1.
Otherwise, you can move either to the left or to the right with probability 21 . The Markov
matrix associated with this situation is
ï£«
ï£¶
1 0 0 0
ï£¬ .5 0 .5 0 ï£·
ï£¬
ï£·
ï£­ 0 .5 0 .5 ï£¸ .
0 0 0 1
Deï¬nition 11.3.2 Let the stationary transition probabilities, pij be deï¬ned above. The
resulting matrix having pij as its ij th entry is called the matrix of transition probabilities.
The sequence of random variables for which these pij are the transition probabilities is called
a Markov chain. The matrix of transition probabilities is called a stochastic matrix.
The next proposition is fundamental and shows the signiï¬cance of the powers of the
matrix of transition probabilities.
Proposition 11.3.3 Let pnij denote the probability that Xn is in state j given that X0 was
in state i. Then pnij is the ij th entry of the matrix P n where P = (pij ) .
Proof: This is clearly true if n = 1 and follows from the deï¬nition of the pij .âˆ‘Suppose
true for n. Then the probability that Xn+1 is at j given that X0 was at i equals k pnik pkj
because Xn must have some value, k, and so this represents all possible ways to go from i
to j. You can go from i to 1 in n steps with probability pi1 and then from 1 to j in one step
with probability p1j and so the probability of this is pni1 p1j but you can also go from i to 2
and then from 2 to j and from i to 3 and then from 3 to j etc. Thus the sum of these is
just what is given and represents the probability of Xn+1 having the value j given X0 has
the value i. 
In the above random walk example, lets take a power of the transition probability matrix
to determine what happens. Rounding oï¬€ to two decimal places,
ï£«
ï£¶20 ï£«
ï£¶
1 0 0 0
1
0
0
0
ï£¬ .5 0 .5 0 ï£·
ï£¬ . 67 9. 5 Ã— 10âˆ’7
0
. 33 ï£·
ï£¬
ï£¬
ï£·
ï£·.
âˆ’7
ï£­ 0 .5 0 .5 ï£¸ = ï£­ . 33
0
9. 5 Ã— 10
. 67 ï£¸
0 0 0 1
0
0
0
1
Thus p21 is about 2/3 while p32 is about 1/3
seems to be converging to the matrix
ï£«
1 0
ï£¬ 2 0
ï£¬ 13
ï£­
0
3
0 0

Saylor URL: http://www.saylor.org/courses/ma212/

and terms like p22 are very small. You see this
0
0
0
0

0
1
3
2
3

ï£¶
ï£·
ï£·.
ï£¸

1

The Saylor Foundation

11.3. MARKOV CHAINS

281

After many iterations of the process, if you start at 2 you will end up at 1 with probability
2/3 and at 4 with probability 1/3. This makes good intuitive sense because it is twice as far
from 2 to 4 as it is from 2 to 1.
Theorem 11.3.4 The eigenvalues of
ï£«
0
ï£¬ q
ï£¬
ï£¬
ï£¬ 0
ï£¬
ï£¬ ..
ï£¬ .
ï£­

p
0

0
p

q

0
..

0
..
.

0

0

Â·Â·Â·
Â·Â·Â·
..
.
.

..

.

q

0
0
..
.

ï£¶

ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
p ï£·
ï£¸
0

have absolute value less than 1. Here p + q = 1 and both p, q > 0.
Proof: By Gerschgorinâ€™s theorem, if Î» is an eigenvalue, then |Î»| â‰¤ 1. Now suppose v
is an eigenvector for Î». Then
ï£«
ï£¶
ï£«
ï£¶
pv2
v1
ï£¬ qv1 + pv3 ï£·
ï£¬ v2 ï£·
ï£¬
ï£·
ï£¬
ï£·
ï£¬
ï£·
ï£¬ . ï£·
.
..
Av = ï£¬
ï£· = Î» ï£¬ .. ï£·
ï£¬
ï£·
ï£¬
ï£·
ï£­ qvnâˆ’2 + pvn ï£¸
ï£­ vnâˆ’1 ï£¸
qvnâˆ’1
vn
Suppose |Î»| = 1. Then the top row shows p |v2 | = |v1 | so |v1 | < |v2 | . Suppose |v1 | < |v2 | <
Â· Â· Â· < |vk | for some k < n. Then
|Î»vk | = |vk | â‰¤ q |vkâˆ’1 | + p |vk+1 | < q |vk | + p |vk+1 |
and so subtracting q |vk | from both sides,
p |vk | < p |vk+1 |
n

showing {|vk |}k=1 is an increasing sequence. Now a contradiction results on the last line
which requires |vnâˆ’1 | > |vn |. Therefore, |Î»| < 1 for any eigenvalue of the above matrix. 
Corollary 11.3.5 Let p, q be positive
ï£«
a
ï£¬ q
ï£¬
ï£¬
ï£¬ 0
ï£¬
ï£¬ ..
ï£¬ .
ï£­

numbers and let p + q = 1. The eigenvalues of
ï£¶
p 0 Â·Â·Â· 0
a p Â·Â·Â· 0 ï£·
ï£·
.. ï£·
..
. . ï£·
q a
ï£·
ï£·
.
..
. p ï£·
0 ..
ï£¸
..
0 . 0
q a

are all strictly closer than 1 to a. That is, whenever Î» is an eigenvalue,
|Î» âˆ’ a| < 1
have absolute value less than 1.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

282

MARKOV CHAINS AND MIGRATION PROCESSES

Proof: Let A be the above matrix
ï£«
0
ï£¬ q
ï£¬
ï£¬
ï£¬ 0
ï£¬
ï£¬ ..
ï£¬ .
ï£­
0
it follows

and suppose Ax =Î»x. Then letting Aâ€² denote
ï£¶
p 0 Â·Â·Â· 0
0 p Â·Â·Â· 0 ï£·
ï£·
. ï£·
..
. .. ï£·
q 0
ï£·,
ï£·
..
..
.
. p ï£·
0
ï£¸
..
. 0
q 0

Aâ€² x = (Î» âˆ’ a) x

and so from the above theorem,
|Î» âˆ’ a| < 1. 
Example 11.3.6 In the gamblerâ€™s ruin problem a gambler plays a game with someone, say
a casino, until he either wins all the otherâ€™s money or loses all of his own. A simple version
of this is as follows. Let Xk denote the amount of money the gambler has. Each time the
game is played he wins with probability p âˆˆ (0, 1) or loses with probability (1 âˆ’ p) â‰¡ q. In
case he wins, his money increases to Xk + 1 and if he loses, his money decreases to Xk âˆ’ 1.
The transition probability matrix
ï£«
1
ï£¬ q
ï£¬
ï£¬
ï£¬ 0
ï£¬
ï£¬
P =ï£¬
ï£¬ 0
ï£¬ .
ï£¬ ..
ï£¬
ï£¬
ï£­ 0
0

P, describing this situation is as follows.
ï£¶
0 0 0 Â·Â·Â· 0 0
0 p 0 Â·Â·Â· 0 0 ï£·
ï£·
. ï£·
q 0 p Â· Â· Â· 0 .. ï£·
ï£·
ï£·
..
..
. . 0 ï£·
0 q 0
ï£·
ï£·
..
..
..
.
.
. 0
p 0 ï£·
ï£·
ï£·
..
0 . 0
q 0 p ï£¸
0

0

0

0

0

(11.2)

1

Here the matrix is b + 1 Ã— b + 1 because the possible values of Xk are all integers from 0 up
to b. The 1 in the upper left corner corresponds to the gamblerâ€™s ruin. It involves Xk = 0
so he has no money left. Once this state has been reached, it is not possible to ever leave
it. This is indicated by the row of zeros to the right of this entry the k th of which gives the
probability of going from state 1 corresponding to no money to state k 1 .
In this case 1 is a repeated root of the characteristic equation of multiplicity 2 and all
the other eigenvalues have absolute value less than 1. To see that this is the case, note that
the characteristic polynomial is of the form
ï£«
ï£¶
âˆ’Î» p
0 Â·Â·Â·
0
ï£¬ q âˆ’Î» p Â· Â· Â·
0 ï£·
ï£¬
ï£·
ï£¬
.. ï£·
.
.
ï£¬ 0
2
.
q âˆ’Î»
. ï£·
(1 âˆ’ Î») det ï£¬
ï£·
ï£¬ ..
ï£·
.
.
..
..
ï£¬ .
ï£·
0
p
ï£­
ï£¸
..
0
.
0
q âˆ’Î»
1 No one will give the gambler money. This is why the only reasonable number for entries in this row to
the right of 1 is 0.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

11.3. MARKOV CHAINS

283
2

and the factor after (1 âˆ’ Î») has zeros which
the eigenvalues of the matrix
ï£«
0 p
ï£¬ q 0
ï£¬
ï£¬
ï£¬
Aâ‰¡ï£¬ 0 q
ï£¬ ..
ï£¬ . 0
ï£­
.
0 ..

are in absolute value less than 1. Its zeros are
0
p
0
..
.

Â·Â·Â·
Â·Â·Â·
..
.
..

0

.

q

0
0
..
.

ï£¶

ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
p ï£·
ï£¸
0

and by Corollary 11.3.5 these all have absolute value less than 1.
Therefore, by Theorem 11.1.3 limnâ†’âˆ P n exists. The case of limnâ†’âˆ pnj0 is particularly
interesting because it gives the probability that, starting with an amount j, the gambler
eventually ends up at 0 and is ruined. From the matrix, it follows
pnj0

nâˆ’1
= qpnâˆ’1
(jâˆ’1)0 + pp(j+1)0 for j âˆˆ [1, b âˆ’ 1] ,

pn00

=

1, and pnb0 = 0.

To simplify the notation, deï¬ne Pj â‰¡ limnâ†’âˆ pnj0 as the probability of ruin given the initial
fortune of the gambler equals j. Then the above simpliï¬es to
Pj
P0

= qPjâˆ’1 + pPj+1 for j âˆˆ [1, b âˆ’ 1] ,
= 1, and Pb = 0.

(11.3)

Now, knowing that Pj exists, it is not too hard to ï¬nd it from (11.3). This equation is
called a diï¬€erence equation and there is a standard procedure for ï¬nding solutions of these.
You try a solution of the form Pj = xj and then try to ï¬nd x such that things work out.
Therefore, substitute this in to the ï¬rst equation of (11.3) and obtain
xj = qxjâˆ’1 + pxj+1 .
Therefore,
px2 âˆ’ x + q = 0
and so in case p Ì¸= q, you can use the fact that p + q = 1 to obtain
x =
=
=

)
)
âˆš
âˆš
1 (
1 (
1 + (1 âˆ’ 4pq) or
1 âˆ’ (1 âˆ’ 4pq)
2p
2p
)
)
âˆš
âˆš
1 (
1 (
1 + (1 âˆ’ 4p (1 âˆ’ p)) or
1 âˆ’ (1 âˆ’ 4p (1 âˆ’ p))
2p
2p
q
1 or .
p
( )j

Now it follows that both Pj = 1 and Pj =
Therefore, anything of the form

q
p

( )j
q
Î±+Î²
p

satisfy the diï¬€erence equation (11.3).

(11.4)

will satisfy this equation. Find a, b such that this also satisï¬es the second equation of (11.3).
Thus it is required that
( )b
q
Î± + Î² = 1, Î± + Î²
=0
p

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

284

MARKOV CHAINS AND MIGRATION PROCESSES

and so
Î±+Î² =1
( )b
Î± + Î² pq = 0
{
}
b
( pq )
1
,
Î±
=
. Substituting this in to (11.4) and simplifying,
Solution is : Î² = âˆ’
q b
q b
âˆ’1+( p
âˆ’1+( p
)
)
yields the following in the case that p Ì¸= q.
Pj =
Note that
lim

pâ†’q

pbâˆ’j q j âˆ’ q b
pb âˆ’ q b

(11.5)

bâˆ’j
pbâˆ’j q j âˆ’ q b
=
.
pb âˆ’ q b
b

Thus as the game becomes more fair in the sense the probabilities of winning become closer
to 1/2, the probability of ruin given an initial amount j is bâˆ’j
b .
Alternatively, you could consider the diï¬€erence equation directly in the case where p =
q = 1/2. In this case, you can see that two solutions to the diï¬€erence equation
Pj
P0

1
1
Pjâˆ’1 + Pj+1 for j âˆˆ [1, b âˆ’ 1] ,
2
2
= 1, and Pb = 0.

=

(11.6)

are Pj = 1 and Pj = j. This leads to a solution to the above of
Pj =

bâˆ’j
.
b

(11.7)

This last case is pretty interesting because it shows, for example that if the gambler
starts with a fortune of 1 so that he starts at state j = 1, then his probability of losing all
is bâˆ’1
b which might be quite large, especially if the other player has a lot of money to begin
with. As the gambler starts with more and more money, his probability of losing everything
does decrease.

11.4

Exercises

1. Suppose the migration matrix for three
ï£«
.5
ï£­ .3
.2

locations is
ï£¶
0 .3
.8 0 ï£¸ .
.2 .7

Find a comparison for the populations in the three locations after a long time.
âˆ‘
2. Show that if i aij = 1, then if A = (aij ) , then the sum of the entries of Av equals
the sum of the entries of v. Thus it does not matter whether aij â‰¥ 0 for this to be so.
3. If A satisï¬es the conditions of the above problem, can it be concluded that limnâ†’âˆ An
exists?
4. Give an example of a non regular Markov matrix which has an eigenvalue equal to
âˆ’1.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

11.4. EXERCISES

285

5. Show that when a Markov matrix is non defective, all of the above theory can be proved
very easily. In particular, prove the theorem about the existence of limnâ†’âˆ An if the
eigenvalues are either 1 or have absolute value less than 1.
6. Find a formula for An where
ï£«

5
2

ï£¬ 5
A=ï£¬
ï£­ 7
2
7
2

âˆ’ 12
0
âˆ’ 12
âˆ’ 12

ï£¶
âˆ’1
âˆ’4 ï£·
ï£·
âˆ’ 52 ï£¸
âˆ’2

0
0
1
2

0

Does limnâ†’âˆ An exist? Note that all the rows sum to 1. Hint: This matrix is similar
to a diagonal matrix. The eigenvalues are 1, âˆ’1, 21 , 12 .
7. Find a formula for An where
ï£«

2
ï£¬ 4
A=ï£¬
ï£­ 5
2
3

âˆ’ 12
0
âˆ’ 12
âˆ’ 12

ï£¶
âˆ’1
âˆ’4 ï£·
ï£·
âˆ’2 ï£¸
âˆ’2

1
2

1
1
1
2

Note that the rows sum to 1 in this matrix also. Hint: This matrix is not similar
to a diagonal matrix but you can ï¬nd the Jordan form and consider this in order to
obtain a formula for this product. The eigenvalues are 1, âˆ’1, 12 , 21 .
8. Find limnâ†’âˆ An if it exists for the matrix
ï£« 1
âˆ’ 12
2
1
ï£¬ âˆ’1
2
2
A=ï£¬
1
ï£­ 1
2
3
2

2
3
2

âˆ’ 12
âˆ’ 12
3
2
3
2

ï£¶
0
0 ï£·
ï£·
0 ï£¸
1

The eigenvalues are 21 , 1, 1, 1.
9. Give an example of a matrix A which has eigenvalues which are either equal to 1,âˆ’1,
or have absolute value strictly less than 1 but which has the property that limnâ†’âˆ An
does not exist.
10. If A is an n Ã— n matrix such that all the eigenvalues have absolute value less than 1,
show limnâ†’âˆ An = 0.
11. Find an example of a 3 Ã— 3 matrix A such that limnâ†’âˆ An does not exist but
limrâ†’âˆ A5r does exist.
12. If A is a Markov matrix and B is similar to A, does it follow that B is also a Markov
matrix?
13. âˆ‘
In Theorem 11.1.3
âˆ‘ suppose everything is unchanged except that you assume either
j aij â‰¤ 1 or
i aij â‰¤ 1. Would the same conclusion be valid? What if you donâ€™t
insist that each aij â‰¥ 0? Would the conclusion hold in this case?
14. Let V be an n dimensional vector space and let x âˆˆ V and x Ì¸= 0. Consider Î² x â‰¡
x,Ax, Â· Â· Â· ,Amâˆ’1 x where
(
)
Am x âˆˆ span x,Ax, Â· Â· Â· ,Amâˆ’1 x

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

286

MARKOV CHAINS AND MIGRATION PROCESSES

and m
{ is the smallest such
} that the above inclusion in the span takes place. Show
that x,Ax, Â· Â· Â· ,Amâˆ’1 x must be linearly independent. Next suppose {v1 , Â· Â· Â· , vn }
is a basis for V . Consider Î² vi as just discussed, having length mi . Thus Ami vi is a
linearly combination of vi ,Avi , Â· Â· Â· ,Amâˆ’1 vi for m as small as possible. Let pvi (Î») be
the monic polynomial which expresses this linear combination. Thus pvi (A) vi = 0
and the degree of pvi (Î») is as small as possible for this to take place. Show that the
minimal polynomial for A must be the monic polynomial which is the least common
multiple of these polynomials pvi (Î»).
15. If A is a complex Hermitian n Ã— n matrix which has all eigenvalues nonnegative, show
that there exists a complex Hermitian matrix B such that BB = A.
16. â†‘Suppose A, B are n Ã— n real Hermitian matrices and they both have all nonnegative
eigenvalues. Show that det (A + B) â‰¥ det (A)+det (B). Hint: Use the above problem
and the Cauchy Binet theorem. Let P 2 = A, Q2 = B where P, Q are Hermitian and
nonnegative. Then
(
)
(
) P
A+B = P Q
.
Q
(

)
Î± câˆ—
is an (n + 1) Ã— (n + 1) Hermitian nonnegative matrix where
b A
Î± is a scalar and A is n Ã— n. Show that Î± must be real, c = b, and A = Aâˆ— , A is
nonnegative, and that if Î± = 0, then b = 0. Otherwise, Î± > 0.

17. Suppose B =

18. â†‘If A is an n Ã— n complex Hermitian and nonnegative matrix, show that there exists
an upper triangular matrix B such that B âˆ— B = A. Hint: Prove this by induction. It
is obviously true if n = 1. Now if you have an (n + 1) Ã— (n
( + 21) Hermitian
) nonnegative
Î±
Î±bâˆ—
matrix, then from the above problem, it is of the form
, Î± real.
Î±b A
19. â†‘ Suppose A is a nonnegative Hermitian matrix which is partitioned as
(
)
A11 A12
A=
A21 A22
where A11 , A22 are square matrices. Show that det (A) â‰¤ det (A11 ) det (A22 ). Hint:
Use the above problem to factor A getting
( âˆ—
)(
)
B11 0âˆ—
B11 B12
A=
âˆ—
âˆ—
B22
0
B22
B12
âˆ—
âˆ—
âˆ—
B22 . Use the Cauchy Binet theoNext argue that A11 = B11
B11 , A22 = B12
B12 + B22
âˆ—
âˆ—
âˆ—
rem to argue that det (A22 ) = det (B12 B12 + B22 B22 ) â‰¥ det (B22
B22 ) . Then explain
why

det (A)

=

âˆ—
âˆ—
det (B11
) det (B22
) det (B11 ) det (B22 )

=

âˆ—
âˆ—
det (B11
B11 ) det (B22
B22 )

20. â†‘ Prove the inequality of Hadamard. If A is a Hermitian matrix which is nonnegative,
then
âˆ
det (A) â‰¤
Aii
i

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Inner Product Spaces
12.1

General Theory

It is assumed here that the ï¬eld of scalars is either R or C. The usual example of an inner
product space is Cn or Rn as described earlier. However, there are many other inner product
spaces and the topic is of such importance that it seems appropriate to discuss the general
theory of these spaces.
Deï¬nition 12.1.1 A vector space X is said to be a normed linear space if there exists a
function, denoted by |Â·| : X â†’ [0, âˆ) which satisï¬es the following axioms.
1. |x| â‰¥ 0 for all x âˆˆ X, and |x| = 0 if and only if x = 0.
2. |ax| = |a| |x| for all a âˆˆ F.
3. |x + y| â‰¤ |x| + |y| .
This function |Â·| is called a norm.
The notation ||x|| is also often used. Not all norms are created equal. There are many
geometric properties which they may or may not possess. There is also a concept called an
inner product which is discussed next. It turns out that the best norms come from an inner
product.
Deï¬nition 12.1.2 A mapping (Â·, Â·) : V Ã— V â†’ F is called an inner product if it satisï¬es
the following axioms.
1. (x, y) = (y, x).
2. (x, x) â‰¥ 0 for all x âˆˆ V and equals zero if and only if x = 0.
3. (ax + by, z) = a (x, z) + b (y, z) whenever a, b âˆˆ F.
Note that 2 and 3 imply (x, ay + bz) = a(x, y) + b(x, z).
Then a norm is given by
1/2
(x, x)
â‰¡ |x| .
It remains to verify this really is a norm.
Deï¬nition 12.1.3 A normed linear space in which the norm comes from an inner product
as just described is called an inner product space.

287

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

288

INNER PRODUCT SPACES

Example 12.1.4 Let V = Cn with the inner product given by
(x, y) â‰¡

n
âˆ‘

xk y k .

k=1

This is an example of a complex inner product space already discussed.
Example 12.1.5 Let V = Rn,
(x, y) = x Â· y â‰¡

n
âˆ‘

xj yj .

j=1

This is an example of a real inner product space.
Example 12.1.6 Let V be any ï¬nite dimensional vector space and let {v1 , Â· Â· Â· , vn } be a
basis. Decree that
{
1 if i = j
(vi , vj ) â‰¡ Î´ ij â‰¡
0 if i Ì¸= j
and deï¬ne the inner product by
(x, y) â‰¡

n
âˆ‘

xi y i

i=1

where
x=

n
âˆ‘

x i vi , y =

i=1

n
âˆ‘

y i vi .

i=1

The above is well deï¬ned because {v1 , Â· Â· Â· , vn } is a basis. Thus the components xi
associated with any given x âˆˆ V are uniquely determined.
This example shows there is no loss of generality when studying ï¬nite dimensional vector
spaces with ï¬eld of scalars R or C in assuming the vector space is actually an inner product
space. The following theorem was presented earlier with slightly diï¬€erent notation.
Theorem 12.1.7 (Cauchy Schwarz) In any inner product space
|(x, y)| â‰¤ |x||y|.
1/2

where |x| â‰¡ (x, x)

.

Proof: Let Ï‰ âˆˆ C, |Ï‰| = 1, and Ï‰(x, y) = |(x, y)| = Re(x, yÏ‰). Let
F (t) = (x + tyÏ‰, x + tÏ‰y).
Then from the axioms of the inner product,
F (t) = |x|2 + 2t Re(x, Ï‰y) + t2 |y|2 â‰¥ 0.
This yields
|x|2 + 2t|(x, y)| + t2 |y|2 â‰¥ 0.
If |y| = 0, then the inequality requires that |(x, y)| = 0 since otherwise, you could pick large
negative t and contradict the inequality. If |y| > 0, it follows from the quadratic formula
that
4|(x, y)|2 âˆ’ 4|x|2 |y|2 â‰¤ 0. 
Earlier it was claimed that the inner product deï¬nes a norm. In this next proposition
this claim is proved.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

12.2. THE GRAM SCHMIDT PROCESS

289

Proposition 12.1.8 For an inner product space, |x| â‰¡ (x, x)

1/2

does specify a norm.

Proof: All the axioms are obvious except the triangle inequality. To verify this,
2

|x + y|

2

2

â‰¡

(x + y, x + y) â‰¡ |x| + |y| + 2 Re (x, y)

â‰¤

|x| + |y| + 2 |(x, y)|

â‰¤

|x| + |y| + 2 |x| |y| = (|x| + |y|) . 

2

2

2

2

2

The best norms of all are those which come from an inner product because of the following
identity which is known as the parallelogram identity.
1/2

Proposition 12.1.9 If (V, (Â·, Â·)) is an inner product space then for |x| â‰¡ (x, x)
following identity holds.
2

2

2

, the

2

|x + y| + |x âˆ’ y| = 2 |x| + 2 |y| .
It turns out that the validity of this identity is equivalent to the existence of an inner
product which determines the norm as described above. These sorts of considerations are
topics for more advanced courses on functional analysis.
Deï¬nition 12.1.10 A basis for an inner product space, {u1 , Â· Â· Â· , un } is an orthonormal
basis if
{
1 if k = j
(uk , uj ) = Î´ kj â‰¡
.
0 if k Ì¸= j
Note that if a list of vectors satisï¬es the above condition for being an orthonormal set,
then the list of vectors is automatically linearly independent. To see this, suppose
n
âˆ‘

cj uj = 0

j=1

Then taking the inner product of both sides with uk ,
0=

n
âˆ‘

cj (uj , uk ) =

j=1

12.2

n
âˆ‘

cj Î´ jk = ck .

j=1

The Gram Schmidt Process

Lemma 12.2.1 Let X be a ï¬nite dimensional inner product space of dimension n whose
basis is {x1 , Â· Â· Â· , xn } . Then there exists an orthonormal basis for X, {u1 , Â· Â· Â· , un } which has
the property that for each k â‰¤ n, span(x1 , Â· Â· Â· , xk ) = span (u1 , Â· Â· Â· , uk ) .
Proof: Let {x1 , Â· Â· Â· , xn } be a basis for X. Let u1 â‰¡ x1 / |x1 | . Thus for k = 1, span (u1 ) =
span (x1 ) and {u1 } is an orthonormal set. Now suppose for some k < n, u1 , Â· Â· Â· , uk have
been chosen such that (uj , ul ) = Î´ jl and span (x1 , Â· Â· Â· , xk ) = span (u1 , Â· Â· Â· , uk ). Then deï¬ne
uk+1 â‰¡

xk+1 âˆ’
xk+1 âˆ’

âˆ‘k
j=1

âˆ‘k

(xk+1 , uj ) uj

,

(12.1)

j=1 (xk+1 , uj ) uj

where the denominator is not equal to zero because the xj form a basis and so
xk+1 âˆˆ
/ span (x1 , Â· Â· Â· , xk ) = span (u1 , Â· Â· Â· , uk )

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

290

INNER PRODUCT SPACES

Thus by induction,
uk+1 âˆˆ span (u1 , Â· Â· Â· , uk , xk+1 ) = span (x1 , Â· Â· Â· , xk , xk+1 ) .
Also, xk+1 âˆˆ span (u1 , Â· Â· Â· , uk , uk+1 ) which is seen easily by solving (12.1) for xk+1 and it
follows
span (x1 , Â· Â· Â· , xk , xk+1 ) = span (u1 , Â· Â· Â· , uk , uk+1 ) .
If l â‰¤ k,

ï£«
C ï£­(xk+1 , ul ) âˆ’

(uk+1 , ul ) =

k
âˆ‘

ï£¶
(xk+1 , uj ) (uj , ul )ï£¸

j=1

ï£«
= C ï£­(xk+1 , ul ) âˆ’

k
âˆ‘

ï£¶
(xk+1 , uj ) Î´ lj ï£¸

j=1

= C ((xk+1 , ul ) âˆ’ (xk+1 , ul )) = 0.
n
{uj }j=1

The vectors,
, generated in this way are therefore an orthonormal basis because
each vector has unit length. 
The process by which these vectors were generated is called the Gram Schmidt process.
The following corollary is obtained from the above process.
Corollary 12.2.2 Let X be a ï¬nite dimensional inner product space of dimension n whose
basis is {u1 , Â· Â· Â· , uk , xk+1 , Â· Â· Â· , xn } . Then if {u1 , Â· Â· Â· , uk } is orthonormal, then the Gram
Schmidt process applied to the given list of vectors in order leaves {u1 , Â· Â· Â· , uk } unchanged.
n

Lemma 12.2.3 Suppose {uj }j=1 is an orthonormal basis for an inner product space X.
Then for all x âˆˆ X,
n
âˆ‘
x=
(x, uj ) uj .
j=1

Proof: By assumption that this is an orthonormal basis,
n
âˆ‘

Î´ jl

z }| {
(x, uj ) (uj , ul ) = (x, ul ) .

j=1

Letting y =

âˆ‘n
k=1

(x, uk ) uk , it follows
(x âˆ’ y, uj )

=

(x, uj ) âˆ’

n
âˆ‘

(x, uk ) (uk , uj )

k=1

=

(x, uj ) âˆ’ (x, uj ) = 0

for all j. Hence, for any choice of scalars c1 , Â· Â· Â· , cn ,
ï£«
ï£¶
n
âˆ‘
ï£­x âˆ’ y,
cj uj ï£¸ = 0
j=1

and so (x âˆ’ y, z) = 0 for all z âˆˆ X. Thus this holds in particular for z = x âˆ’ y. Therefore, x
= y. 
The following theorem is of fundamental importance. First note that a subspace of an
inner product space is also an inner product space because you can use the same inner
product.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

12.2. THE GRAM SCHMIDT PROCESS

291

Theorem 12.2.4 Let M be a subspace of X, a ï¬nite dimensional inner product space and
m
let {xi }i=1 be an orthonormal basis for M . Then if y âˆˆ X and w âˆˆ M,
{
}
2
2
|y âˆ’ w| = inf |y âˆ’ z| : z âˆˆ M
(12.2)
if and only if
(y âˆ’ w, z) = 0
for all z âˆˆ M. Furthermore,
w=

m
âˆ‘

(12.3)

(y, xi ) xi

(12.4)

i=1

is the unique element of M which has this property. It is called the orthogonal projection.
Proof: Let t âˆˆ R. Then from the properties of the inner product,
2

2

2

|y âˆ’ (w + t (z âˆ’ w))| = |y âˆ’ w| + 2t Re (y âˆ’ w, w âˆ’ z) + t2 |z âˆ’ w| .

(12.5)

If (y âˆ’ w, z) = 0 for all z âˆˆ M, then letting t = 1, the middle term in the above expression
2
vanishes and so |y âˆ’ z| is minimized when z = w.
Conversely, if (12.2) holds, then the middle term of (12.5) must also vanish since otherwise, you could choose small real t such that
2

2

|y âˆ’ w| > |y âˆ’ (w + t (z âˆ’ w))| .
Here is why. If Re (y âˆ’ w, w âˆ’ z) < 0, then let t be very small and positive. The middle
term in (12.5) will then be more negative than the last term is positive and the right side
2
of this formula will then be less than |y âˆ’ w| . If Re (y âˆ’ w, w âˆ’ z) > 0 then choose t small
and negative to achieve the same result.
It follows, letting z1 = w âˆ’ z that
Re (y âˆ’ w, z1 ) = 0
for all z1 âˆˆ M. Now letting Ï‰ âˆˆ C be such that Ï‰ (y âˆ’ w, z1 ) = |(y âˆ’ w, z1 )| ,
|(y âˆ’ w, z1 )| = (y âˆ’ w, Ï‰z1 ) = Re (y âˆ’ w, Ï‰z1 ) = 0,
which proves the ï¬rst part of the theorem since z1 is arbitrary.
It only remains to verify that w given in (12.4) satisï¬es (12.3) and is the only point of
M which does so. To do this, note that if ci , di are scalars, then the properties of the inner
product and the fact the {xi } are orthonormal implies
ï£«
ï£¶
m
m
âˆ‘
âˆ‘
âˆ‘
ï£­
ci xi ,
dj x j ï£¸ =
ci di .
i=1

j=1

By Lemma 12.2.3,
z=

âˆ‘

i

(z, xi ) xi

i

and so

(
yâˆ’

m
âˆ‘

)
(y, xi ) xi , z

(
=

i=1

Saylor URL: http://www.saylor.org/courses/ma212/

yâˆ’

m
âˆ‘
i=1

(y, xi ) xi ,

m
âˆ‘

)
(z, xi ) xi

i=1

The Saylor Foundation

292

INNER PRODUCT SPACES
m
âˆ‘

=

ï£«
ï£¶
m
m
âˆ‘
âˆ‘
(z, xi ) (y, xi ) âˆ’ ï£­
(y, xi ) xi ,
(z, xj ) xj ï£¸

i=1

=

i=1
m
âˆ‘

(z, xi ) (y, xi ) âˆ’

i=1

j=1

m
âˆ‘

(y, xi ) (z, xi ) = 0.

i=1
2

This shows w given in (12.4) does minimize the function, z â†’ |y âˆ’ z| for z âˆˆ M. It only
remains to verify uniqueness. Suppose than that wi , i = 1, 2 minimizes this function of z
for z âˆˆ M. Then from what was shown above,
2

|y âˆ’ w1 |

= |y âˆ’ w2 + w2 âˆ’ w1 |

2

2

= |y âˆ’ w2 | + 2 Re (y âˆ’ w2 , w2 âˆ’ w1 ) + |w2 âˆ’ w1 |
2

2

2

2

= |y âˆ’ w2 | + |w2 âˆ’ w1 | â‰¤ |y âˆ’ w2 | ,
the last equal sign holding because w2 is a minimizer and the last inequality holding because
w1 minimizes. 

12.3

Riesz Representation Theorem

The next theorem is one of the most important results in the theory of inner product spaces.
It is called the Riesz representation theorem.
Theorem 12.3.1 Let f âˆˆ L (X, F) where X is an inner product space of dimension n.
Then there exists a unique z âˆˆ X such that for all x âˆˆ X,
f (x) = (x, z) .
Proof: First I will verify uniqueness. Suppose zj works for j = 1, 2. Then for all x âˆˆ X,
0 = f (x) âˆ’ f (x) = (x, z1 âˆ’ z2 )
and so z1 = z2 .
It remains to verify existence. By Lemma 12.2.1, there exists an orthonormal basis,
n
{uj }j=1 . Deï¬ne
n
âˆ‘
zâ‰¡
f (uj )uj .
j=1

Then using Lemma 12.2.3,
(x, z)

ï£«
=

ï£­x,
ï£«

= fï£­

n
âˆ‘

ï£¶
f (uj )uj ï£¸ =

j=1
n
âˆ‘

ï£¶

n
âˆ‘

f (uj ) (x, uj )

j=1

(x, uj ) uj ï£¸ = f (x) . 

j=1

Corollary 12.3.2 Let A âˆˆ L (X, Y ) where X and Y are two inner product spaces of ï¬nite
dimension. Then there exists a unique Aâˆ— âˆˆ L (Y, X) such that
(Ax, y)Y = (x, Aâˆ— y)X

(12.6)

for all x âˆˆ X and y âˆˆ Y. The following formula holds
âˆ—

(Î±A + Î²B) = Î±Aâˆ— + Î²B âˆ—

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

12.3. RIESZ REPRESENTATION THEOREM

293

Proof: Let fy âˆˆ L (X, F) be deï¬ned as
fy (x) â‰¡ (Ax, y)Y .
Then by the Riesz representation theorem, there exists a unique element of X, Aâˆ— (y) such
that
(Ax, y)Y = (x, Aâˆ— (y))X .
It only remains to verify that Aâˆ— is linear. Let a and b be scalars. Then for all x âˆˆ X,
(x, Aâˆ— (ay1 + by2 ))X â‰¡ (Ax, (ay1 + by2 ))Y
â‰¡ a (Ax, y1 ) + b (Ax, y2 ) â‰¡
a (x, A (y1 )) + b (x, Aâˆ— (y2 )) = (x, aAâˆ— (y1 ) + bAâˆ— (y2 )) .
âˆ—

Since this holds for every x, it follows
Aâˆ— (ay1 + by2 ) = aAâˆ— (y1 ) + bAâˆ— (y2 )
which shows Aâˆ— is linear as claimed.
Consider the last assertion that âˆ— is conjugate linear.
(
âˆ— )
x, (Î±A + Î²B) y â‰¡ ((Î±A + Î²B) x, y)
= Î± (Ax, y) + Î² (Bx, y) = Î± (x, Aâˆ— y) + Î² (x, B âˆ— y)
(
) ( (
) )
= (x, Î±Aâˆ— y) + x, Î²Aâˆ— y = x, Î±Aâˆ— + Î²Aâˆ— y .
Since x is arbitrary,

(
)
âˆ—
(Î±A + Î²B) y = Î±Aâˆ— + Î²Aâˆ— y

and since this is true for all y,
âˆ—

(Î±A + Î²B) = Î±Aâˆ— + Î²Aâˆ— . 
Deï¬nition 12.3.3 The linear map, Aâˆ— is called the adjoint of A. In the case when A : X â†’
X and A = Aâˆ— , A is called a self adjoint map. Such a map is also called Hermitian.
( )T
Theorem 12.3.4 Let M be an m Ã— n matrix. Then M âˆ— = M
in words, the transpose
of the conjugate of M is equal to the adjoint.
Proof: Using the deï¬nition of the inner product in Cn ,
âˆ‘ âˆ‘
âˆ‘
(M x, y) = (x,M âˆ— y) â‰¡
xi
(M âˆ— )ij yj =
(M âˆ— )ij yj xi .
i

Also
(M x, y) =

j

i,j

âˆ‘âˆ‘
j

Mji yj xi .

i

Since x, y are arbitrary vectors, it follows that Mji = (M âˆ— )ij and so, taking conjugates of
both sides,
âˆ—
Mij
= Mji
which gives the conclusion of the theorem.
The next theorem is interesting. You have a p dimensional subspace of Fn where F = R
or C. Of course this might be â€œslantedâ€. However, there is a linear transformation Q which
preserves distances which maps this subspace to Fp .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

294

INNER PRODUCT SPACES

Theorem 12.3.5 Suppose V is a subspace of Fn having dimension p â‰¤ n. Then there exists
a Q âˆˆ L (Fn , Fn ) such that
QV âŠ† span (e1 , Â· Â· Â· , ep )
and |Qx| = |x| for all x. Also

Qâˆ— Q = QQâˆ— = I.
p

Proof: By Lemma 12.2.1 there exists an orthonormal basis for V, {vi }i=1 . By using the
Gram Schmidt process this may be extended to an orthonormal basis of the whole space,
Fn ,
{v1 , Â· Â· Â· , vp , vp+1 , Â· Â· Â· , vn } .
âˆ‘n
Now deï¬ne Q âˆˆ L (Fn , Fn ) by Q (vi ) â‰¡ ei and extend linearly. If i=1 xi vi is an arbitrary
element of Fn ,
(
Q

n
âˆ‘

)
x i vi

2

=

n
âˆ‘

2

xi ei

=

i=1

i=1

n
âˆ‘

2

|xi | =

n
âˆ‘

2

x i vi .

i=1

i=1

It remains to verify that Qâˆ— Q = QQâˆ— = I. To do so, let x, y âˆˆ Fn . Then
(Q (x + y) , Q (x + y)) = (x + y, x + y) .
Thus
2

2

2

2

|Qx| + |Qy| + 2 Re (Qx,Qy) = |x| + |y| + 2 Re (x, y)
and since Q preserves norms, it follows that for all x, y âˆˆ Fn ,
Re (Qx,Qy) = Re (x,Qâˆ— Qy) = Re (x, y) .
Thus

Re (x,Qâˆ— Qy âˆ’ y) = 0

(12.7)

for all x, y. Let Ï‰ be a complex number such that |Ï‰| = 1 and
Ï‰ (x,Qâˆ— Qy âˆ’ y) = |(x,Qâˆ— Qy âˆ’ y)| .
Then from (12.7),
0 = Re (Ï‰x, Qâˆ— Qy âˆ’ y) = Re Ï‰ (x,Qâˆ— Qy âˆ’ y)
= |(x,Qâˆ— Qy âˆ’ y)|
and since x is arbitrary, it follows that for all y,
Qâˆ— Qy âˆ’ y = 0
Thus

I = Qâˆ— Q.

Similarly QQâˆ— = I. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

12.4. THE TENSOR PRODUCT OF TWO VECTORS

12.4

295

The Tensor Product Of Two Vectors

Deï¬nition 12.4.1 Let X and Y be inner product spaces and let x âˆˆ X and y âˆˆ Y. Deï¬ne
the tensor product of these two vectors, y âŠ— x, an element of L (X, Y ) by
y âŠ— x (u) â‰¡ y (u, x)X .
This is also called a rank one transformation because the image of this transformation is
contained in the span of the vector, y.
The veriï¬cation that this is a linear map is left to you. Be sure to verify this! The
following lemma has some of the most important properties of this linear transformation.
Lemma 12.4.2 Let X, Y, Z be inner product spaces. Then for Î± a scalar,
âˆ—

(Î± (y âŠ— x)) = Î±x âŠ— y

(12.8)

(z âŠ— y1 ) (y2 âŠ— x) = (y2 , y1 ) z âŠ— x

(12.9)

Proof: Let u âˆˆ X and v âˆˆ Y. Then
(Î± (y âŠ— x) u, v) = (Î± (u, x) y, v) = Î± (u, x) (y, v)
and
(u, Î±x âŠ— y (v)) = (u, Î± (v, y) x) = Î± (y, v) (u, x) .
Therefore, this veriï¬es (12.8).
To verify (12.9), let u âˆˆ X.
(z âŠ— y1 ) (y2 âŠ— x) (u) = (u, x) (z âŠ— y1 ) (y2 ) = (u, x) (y2 , y1 ) z
and
(y2 , y1 ) z âŠ— x (u) = (y2 , y1 ) (u, x) z.
Since the two linear transformations on both sides of (12.9) give the same answer for every
u âˆˆ X, it follows the two transformations are the same. 
Deï¬nition 12.4.3 Let X, Y be two vector spaces. Then deï¬ne for A, B âˆˆ L (X, Y ) and
Î± âˆˆ F, new elements of L (X, Y ) denoted by A + B and Î±A as follows.
(A + B) (x) â‰¡ Ax + Bx, (Î±A) x â‰¡ Î± (Ax) .
Theorem 12.4.4 Let X and Y be ï¬nite dimensional inner product spaces. Then L (X, Y )
is a vector space with the above deï¬nition of what it means to multiply by a scalar and add.
Let {v1 , Â· Â· Â· , vn } be an orthonormal basis for X and {w1 , Â· Â· Â· , wm } be an orthonormal basis
for Y. Then a basis for L (X, Y ) is
{wj âŠ— vi : i = 1, Â· Â· Â· , n, j = 1, Â· Â· Â· , m} .
Proof: It is obvious that L (X, Y ) is a vector space. It remains to verify the given set
is a basis. Consider the following:
ï£¶
ï£¶
ï£«ï£«
âˆ‘
ï£­ï£­A âˆ’
(Avk , wl ) wl âŠ— vk ï£¸ vp , wr ï£¸ = (Avp , wr ) âˆ’
k,l

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

296

INNER PRODUCT SPACES

âˆ‘

(Avk , wl ) (vp , vk ) (wl , wr )

k,l

= (Avp , wr ) âˆ’

âˆ‘

(Avk , wl ) Î´ pk Î´ rl

k,l

= (Avp , wr ) âˆ’ (Avp , wr ) = 0.

âˆ‘

Letting A âˆ’ k,l (Avk , wl ) wl âŠ— vk = B, this shows that Bvp = 0 since wr is an arbitrary
element of the basis for Y. Since vp is an arbitrary element of the basis for X, it follows
B = 0 as hoped. This has shown {wj âŠ— vi : i = 1, Â· Â· Â· , n, j = 1, Â· Â· Â· , m} spans L (X, Y ) .
It only remains to verify the wj âŠ— vi are linearly independent. Suppose then that
âˆ‘
cij wj âŠ— vi = 0
i,j

Then do both sides to vs . By deï¬nition this gives
âˆ‘
âˆ‘
âˆ‘
0=
cij wj (vs , vi ) =
cij wj Î´ si =
csj wj
i,j

i,j

j

Now the vectors {w1 , Â· Â· Â· , wm } are independent because it is an orthonormal set and so the
above requires csj = 0 for each j. Since s was arbitrary, this shows the linear transformations,
{wj âŠ— vi } form a linearly independent set. 
Note this shows the dimension of L (X, Y ) = nm. The theorem is also of enormous
importance because it shows you can always consider an arbitrary linear transformation as
a sum of rank one transformations whose properties are easily understood. The following
theorem is also of great interest.
âˆ‘
Theorem 12.4.5 Let A = i,j cij wi âŠ— vj âˆˆ L (X, Y ) where as before, the vectors, {wi } are
an orthonormal basis for Y and the vectors, {vj } are an orthonormal basis for X. Then if
the matrix of A has entries Mij , it follows that Mij = cij .
Proof: Recall
Avi â‰¡

âˆ‘

Mki wk

k

Also
Avi

=

âˆ‘

ckj wk âŠ— vj (vi ) =

k,j

=

âˆ‘

ckj wk Î´ ij =

âˆ‘

k,j

Therefore,

âˆ‘

ckj wk (vi , vj )

k,j

cki wk

k

âˆ‘

Mki wk =

k

âˆ‘

cki wk

k

and so Mki = cki for all k. This happens for each i. 

12.5

Least Squares

A common problem in experimental work is to ï¬nd a straight line which approximates as
p
well as possible a collection of points in the plane {(xi , yi )}i=1 . The usual way of dealing
with these problems is by the method of least squares and it turns out that all these sorts
of approximation problems can be reduced to Ax = b where the problem is to ï¬nd the best
x for solving this equation even when there is no solution.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

12.5. LEAST SQUARES

297

Lemma 12.5.1 Let V and W be ï¬nite dimensional inner product spaces and let A : V â†’ W
be linear. For each y âˆˆ W there exists x âˆˆ V such that
|Ax âˆ’ y| â‰¤ |Ax1 âˆ’ y|
for all x1 âˆˆ V. Also, x âˆˆ V is a solution to this minimization problem if and only if x is a
solution to the equation, Aâˆ— Ax = Aâˆ— y.
Proof: By Theorem 12.2.4 on Page 291 there exists a point, Ax0 , in the ï¬nite dimen2
2
sional subspace, A (V ) , of W such that for all x âˆˆ V, |Ax âˆ’ y| â‰¥ |Ax0 âˆ’ y| . Also, from
this theorem, this happens if and only if Ax0 âˆ’ y is perpendicular to every Ax âˆˆ A (V ) .
Therefore, the solution is characterized by (Ax0 âˆ’ y, Ax) = 0 for all x âˆˆ V which is the
same as saying (Aâˆ— Ax0 âˆ’ Aâˆ— y, x) = 0 for all x âˆˆ V. In other words the solution is obtained
by solving Aâˆ— Ax0 = Aâˆ— y for x0 . 
Consider the problem of ï¬nding the least squares regression line in statistics. Suppose
n
you have given points in the plane, {(xi , yi )}i=1 and you would like to ï¬nd constants m
and b such that the line y = mx + b goes through all these points. Of course this will be
impossible in general. Therefore, try to ï¬nd m, b such that you do the best you can to solve
the system
ï£«
ï£¶ ï£«
ï£¶
y1
x1 1
(
)
m
ï£¬ .. ï£· ï£¬ ..
ï£·
.
.. ï£¸
ï£­ . ï£¸=ï£­ .
b
yn
xn 1
ï£«
ï£¶2
y1
(
)
m
ï£¬
ï£·
which is of the form y = Ax. In other words try to make A
âˆ’ ï£­ ... ï£¸ as small
b
yn
as possible. According to what was just shown, it is desired to solve the following for m and
b.
ï£«
ï£¶
y1
(
)
m
ï£¬
ï£·
Aâˆ— A
= Aâˆ— ï£­ ... ï£¸ .
b
yn
Since Aâˆ— = AT in this case,
( âˆ‘n
x2i
âˆ‘i=1
n
i=1 xi

âˆ‘n
i=1

xi

)(

n

m
b

)

)
( âˆ‘n
xi yi
i=1
âˆ‘
=
n
i=1 yi

Solving this system of equations for m and b,
âˆ‘n
âˆ‘n
âˆ‘n
âˆ’ ( i=1 xi ) ( i=1 yi ) + ( i=1 xi yi ) n
m=
âˆ‘n
âˆ‘n
2
( i=1 x2i ) n âˆ’ ( i=1 xi )
and
b=

âˆ‘n
âˆ‘n
âˆ‘n
âˆ‘n
âˆ’ ( i=1 xi ) i=1 xi yi + ( i=1 yi ) i=1 x2i
.
âˆ‘n
âˆ‘n
2
( i=1 x2i ) n âˆ’ ( i=1 xi )

One could clearly do a least squares ï¬t for curves of the form y = ax2 + bx + c in the
same way. In this case you solve as well as possible for a, b, and c the system
ï£« 2
ï£¶ï£«
ï£¶ ï£« y ï£¶
x1 x1 1
1
a
ï£¬ ..
..
.. ï£· ï£­ b ï£¸ = ï£¬ .. ï£·
ï£­ .
ï£­ . ï£¸
.
. ï£¸
c
x2 xn 1
yn
n

using the same techniques.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

298

INNER PRODUCT SPACES

12.6

Fredholm Alternative Again

The best context in which to study the Fredholm alternative is in inner product spaces.
This is done here.
Deï¬nition 12.6.1 Let S be a subset of an inner product space, X. Deï¬ne
S âŠ¥ â‰¡ {x âˆˆ X : (x, s) = 0 for all s âˆˆ S} .
The following theorem also follows from the above lemma. It is sometimes called the
Fredholm alternative.
Theorem 12.6.2 Let A : V â†’ W where A is linear and V and W are inner product spaces.
âŠ¥
Then A (V ) = ker (Aâˆ— ) .
Proof: Let y = Ax so y âˆˆ A (V ) . Then if Aâˆ— z = 0,
(y, z) = (Ax, z) = (x, Aâˆ— z) = 0
âŠ¥

âŠ¥

showing that y âˆˆ ker (Aâˆ— ) . Thus A (V ) âŠ† ker (Aâˆ— ) .
âŠ¥
Now suppose y âˆˆ ker (Aâˆ— ) . Does there exists x such that Ax = y? Since this might
not be immediately clear, take the least squares solution to the problem. Thus let x be a
solution to Aâˆ— Ax = Aâˆ— y. It follows Aâˆ— (y âˆ’ Ax) = 0 and so y âˆ’ Ax âˆˆ ker (Aâˆ— ) which implies
from the assumption about y that (y âˆ’ Ax, y) = 0. Also, since Ax is the closest point to
y in A (V ) , Theorem 12.2.4 on Page 291 implies that (y âˆ’ Ax, Ax1 ) = 0 for all x1 âˆˆ V.
=0

z
}|
{
2
In particular this is true for x1 = x and so 0 = (y âˆ’ Ax, y) âˆ’ (y âˆ’ Ax, Ax) = |y âˆ’ Ax| ,
âŠ¥
showing that y = Ax. Thus A (V ) âŠ‡ ker (Aâˆ— ) . 
Corollary 12.6.3 Let A, V, and W be as described above. If the only solution to Aâˆ— y = 0
is y = 0, then A is onto W.
Proof: If the only solution to Aâˆ— y = 0 is y = 0, then ker (Aâˆ— ) = {0} and so every vector
âŠ¥
from W is contained in ker (Aâˆ— ) and by the above theorem, this shows A (V ) = W . 

12.7

Exercises

1. Find the best solution to the system
x + 2y = 6
2x âˆ’ y = 5
3x + 2y = 0
2. Find an orthonormal basis for R3 , {w1 , w2 , w3 } given that w1 is a multiple of the
vector (1, 1, 2).
3. Suppose A = AT is a symmetric real n Ã— n matrix which has all positive eigenvalues.
Deï¬ne
(x, y) â‰¡ (Ax, y) .
Show this is an inner product on Rn . What does the Cauchy Schwarz inequality say
in this case?

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

12.7. EXERCISES

299

4. Let
||x||âˆ â‰¡ max {|xj | : j = 1, 2, Â· Â· Â· , n} .
(
)T
Show this is a norm on Cn . Here x = x1 Â· Â· Â· xn
. Show
1/2

||x||âˆ â‰¤ |x| â‰¡ (x, x)
where the above is the usual inner product on Cn .
5. Let

n
âˆ‘

||x||1 â‰¡

|xj | .

j=1

Show this is a norm on Cn . Here x =

(

Â·Â·Â·

x1

xn

)T

. Show

1/2

||x||1 â‰¥ |x| â‰¡ (x, x)

where the above is the usual inner product on Cn . Show there cannot exist an inner
product such that this norm comes from the inner product as described above for
inner product spaces.
6. Show that if ||Â·|| is any norm on any vector space, then
|||x|| âˆ’ ||y||| â‰¤ ||x âˆ’ y|| .
7. Relax the assumptions in the axioms for the inner product. Change the axiom about
(x, x) â‰¥ 0 and equals 0 if and only if x = 0 to simply read (x, x) â‰¥ 0. Show the Cauchy
Schwarz inequality still holds in the following form.
1/2

|(x, y)| â‰¤ (x, x)

(y, y)

1/2

.

n

8. Let H be an inner product space and let {uk }k=1 be an orthonormal basis for H.
Show
n
âˆ‘
(x, y) =
(x, uk ) (y, uk ).
k=1

9. Let the vector space V consist of real polynomials of degree no larger than 3. Thus a
typical vector is a polynomial of the form
a + bx + cx2 + dx3 .
For p, q âˆˆ V deï¬ne the inner product,
âˆ«

1

(p, q) â‰¡

p (x) q (x) dx.
0

Show this is indeed an inner product.
{ Then state
} the Cauchy Schwarz inequality in
terms of this inner product. Show 1, x, x2 , x3 is a basis for V . Finally, ï¬nd an
orthonormal basis for V. This is an example of some orthonormal polynomials.
10. Let Pn denote the polynomials of degree no larger than n âˆ’ 1 which are deï¬ned on an
interval [a, b] . Let {x1 , Â· Â· Â· , xn } be n distinct points in [a, b] . Now deï¬ne for p, q âˆˆ Pn ,
(p, q) â‰¡

n
âˆ‘

p (xj ) q (xj )

j=1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

300

INNER PRODUCT SPACES

Show this yields an inner product on Pn . Hint: Most of the axioms are obvious. The
one which says (p, p) = 0 if and only if p = 0 is the only interesting one. To verify this
one, note that a nonzero polynomial of degree no more than n âˆ’ 1 has at most n âˆ’ 1
zeros.
11. Let C ([0, 1]) denote the vector space of continuous real valued functions deï¬ned on
[0, 1]. Let the inner product be given as
âˆ« 1
(f, g) â‰¡
f (x) g (x) dx
0

Show this is an inner product. Also let V be the subspace described in Problem 9.
Using the result of this problem, ï¬nd the vector in V which is closest to x4 .
12. A regular Sturm Liouville problem involves the diï¬€erential equation, for an unknown function of x which is denoted here by y,
â€²

(p (x) y â€² ) + (Î»q (x) + r (x)) y = 0, x âˆˆ [a, b]
and it is assumed that p (t) , q (t) > 0 for any t âˆˆ [a, b] and also there are boundary
conditions,
C1 y (a) + C2 y â€² (a) = 0
C3 y (b) + C4 y â€² (b) = 0
where
C12 + C22 > 0, and C32 + C42 > 0.
There is an immense theory connected to these important problems. The constant, Î»
is called an eigenvalue. Show that if y is a solution to the above problem corresponding
to Î» = Î»1 and if z is a solution corresponding to Î» = Î»2 Ì¸= Î»1 , then
âˆ« b
q (x) y (x) z (x) dx = 0.
(12.10)
a

and this deï¬nes an inner product. Hint: Do something like this:
â€²

(p (x) y â€² ) z + (Î»1 q (x) + r (x)) yz

= 0,

(p (x) z ) y + (Î»2 q (x) + r (x)) zy

= 0.

â€² â€²

Now subtract and either use integration by parts or show
â€²

â€²

â€²

(p (x) y â€² ) z âˆ’ (p (x) z â€² ) y = ((p (x) y â€² ) z âˆ’ (p (x) z â€² ) y)

and then integrate. Use the boundary conditions to show that y â€² (a) z (a)âˆ’z â€² (a) y (a) =
0 and y â€² (b) z (b) âˆ’ z â€² (b) y (b) = 0. The formula, (12.10) is called an orthogonality relation. It turns out there are typically inï¬nitely many eigenvalues and it is interesting
to write given functions as an inï¬nite series of these â€œeigenfunctionsâ€.
13. Consider the continuous functions deï¬ned on [0, Ï€] , C ([0, Ï€]) . Show
âˆ« Ï€
(f, g) â‰¡
f gdx
0

}âˆ
{âˆš
2
is an inner product on this vector space. Show the functions
are
Ï€ sin (nx)
n=1
an orthonormal set. What does this mean about the dimension of the vector space

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

12.7. EXERCISES

301

âˆš
(âˆš
)
2
2
C ([0, Ï€])? Now let VN = span
,
Â·
Â·
Â·
,
x)
. For f âˆˆ C ([0, Ï€]) ï¬nd
sin
(x)
sin
(N
Ï€
Ï€
a formula for the vector in VN which is closest to f with respect to the norm determined
from the above inner product. This is called the N th partial sum of the Fourier series
of f . An important problem is to determine whether and in what way this Fourier
series converges to the function f . The norm which comes from this inner product is
sometimes called the mean square norm.
14. Consider the subspace V â‰¡ ker (A) where
ï£«
1 4
ï£¬ 2 1
A=ï£¬
ï£­ 4 9
5 6

âˆ’1
2
0
3

ï£¶
âˆ’1
3 ï£·
ï£·
1 ï£¸
4

Find an orthonormal basis for V. Hint: You might ï¬rst ï¬nd a basis and then use the
Gram Schmidt procedure.
15. The Gram Schmidt process starts with a basis for a subspace {v1 , Â· Â· Â· , vn } and produces an orthonormal basis for the same subspace {u1 , Â· Â· Â· , un } such that
span (v1 , Â· Â· Â· , vk ) = span (u1 , Â· Â· Â· , uk )
for each k. Show that in the case of Rm the QR factorization does the same thing.
More speciï¬cally, if
(
)
A = v1 Â· Â· Â· vn
and if
A = QR â‰¡

(

q1

Â·Â·Â·

qn

)

R

then the vectors {q1 , Â· Â· Â· , qn } is an orthonormal set of vectors and for each k,
span (q1 , Â· Â· Â· , qk ) = span (v1 , Â· Â· Â· , vk )
16. Verify the parallelogram identify for any inner product space,
2

2

2

2

|x + y| + |x âˆ’ y| = 2 |x| + 2 |y| .
Why is it called the parallelogram identity?
17. Let H be an inner product space and let K âŠ† H be a nonempty convex subset. This
means that if k1 , k2 âˆˆ K, then the line segment consisting of points of the form
tk1 + (1 âˆ’ t) k2 for t âˆˆ [0, 1]
is also contained in K. Suppose for each x âˆˆ H, there exists P x deï¬ned to be a point
of K closest to x. Show that P x is unique so that P actually is a map. Hint: Suppose
z1 and z2 both work as closest points. Consider the midpoint, (z1 + z2 ) /2 and use the
parallelogram identity of Problem 16 in an auspicious manner.
18. In the situation of Problem 17 suppose K is a closed convex subset and that H
is complete. This means every Cauchy sequence converges. Recall from calculus a
sequence {kn } is a Cauchy sequence if for every Îµ > 0 there exists NÎµ such that
whenever m, n > NÎµ , it follows |km âˆ’ kn | < Îµ. Let {kn } be a sequence of points of K
such that
lim |x âˆ’ kn | = inf {|x âˆ’ k| : k âˆˆ K}
nâ†’âˆ

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

302

INNER PRODUCT SPACES

This is called a minimizing sequence. Show there exists a unique k âˆˆ K such that
limnâ†’âˆ |kn âˆ’ k| and that k = P x. That is, there exists a well deï¬ned projection map
onto the convex subset of H. Hint: Use the parallelogram identity in an auspicious
manner to show {kn } is a Cauchy sequence which must therefore converge. Since K
is closed it follows this will converge to something in K which is the desired vector.
19. Let H be an inner product space which is also complete and let P denote the projection
map onto a convex closed subset, K. Show this projection map is characterized by
the inequality
Re (k âˆ’ P x, x âˆ’ P x) â‰¤ 0
for all k âˆˆ K. That is, a point z âˆˆ K equals P x if and only if the above variational
inequality holds. This is what that inequality is called. This is because k is allowed
to vary and the inequality continues to hold for all k âˆˆ K.
20. Using Problem 19 and Problems 17 - 18 show the projection map, P onto a closed
convex subset is Lipschitz continuous with Lipschitz constant 1. That is
|P x âˆ’ P y| â‰¤ |x âˆ’ y|
21. Give an example of two vectors in R4 x, y and a subspace V such that x Â· y = 0 but
P xÂ·P y Ì¸= 0 where P denotes the projection map which sends x to its closest point on
V.
22. Suppose you are given the data, (1, 2) , (2, 4) , (3, 8) , (0, 0) . Find the linear regression
line using the formulas derived above. Then graph the given data along with your
regression line.
23. Generalize the least squares procedure to the situation in which data is given and you
desire to ï¬t it with an expression of the form y = af (x) + bg (x) + c where the problem
would be to ï¬nd a, b and c in order to minimize the error. Could this be generalized
to higher dimensions? How about more functions?
24. Let A âˆˆ L (X, Y ) where X and Y are ï¬nite dimensional vector spaces with the dimension of X equal to n. Deï¬ne rank (A) â‰¡ dim (A (X)) and nullity(A) â‰¡ dim (ker (A)) .
r
Show that nullity(A) + rank (A) = dim (X) . Hint: Let {xi }i=1 be a basis for ker (A)
r
nâˆ’r
nâˆ’r
and let {xi }i=1 âˆª {yi }i=1 be a basis for X. Then show that {Ayi }i=1 is linearly
independent and spans AX.
25. Let A be an mÃ—n matrix. Show the column rank of A equals the column rank of Aâˆ— A.
Next verify column rank of Aâˆ— A is no larger than column rank of Aâˆ— . Next justify the
following inequality to conclude the column rank of A equals the column rank of Aâˆ— .
rank (A) = rank (Aâˆ— A) â‰¤ rank (Aâˆ— ) â‰¤
= rank (AAâˆ— ) â‰¤ rank (A) .
Hint: Start with an orthonormal basis, {Axj }j=1 of A (Fn ) and verify {Aâˆ— Axj }j=1
is a basis for Aâˆ— A (Fn ) .
r

r

26. Let A be a real m Ã— n matrix and let A = QR be the QR factorization with Q
orthogonal and R upper triangular. Show that there exists a solution x to the equation
RT Rx = RT QT b
and that this solution is also a least squares solution deï¬ned above such that AT Ax =
AT b.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

12.8. THE DETERMINANT AND VOLUME

12.8

303

The Determinant And Volume

The determinant is the essential algebraic tool which provides a way to give a uniï¬ed treatment of the concept of p dimensional volume of a parallelepiped in RM . Here is the deï¬nition
of what is meant by such a thing.
Deï¬nition 12.8.1 Let u1 , Â· Â· Â· , up be vectors in RM , M â‰¥ p. The parallelepiped determined
by these vectors will be denoted by P (u1 , Â· Â· Â· , up ) and it is deï¬ned as
ï£±
ï£¼
p
ï£²âˆ‘
ï£½
P (u1 , Â· Â· Â· , up ) â‰¡
sj uj : sj âˆˆ [0, 1] .
ï£³
ï£¾
j=1

The volume of this parallelepiped is deï¬ned as
volume of P (u1 , Â· Â· Â· , up ) â‰¡ v (P (u1 , Â· Â· Â· , up )) â‰¡ (det (ui Â· uj ))

1/2

.

If the vectors are dependent, this deï¬nition will give the volume to be 0.
âˆ‘
First lets observe the last assertion is true. Say ui = jÌ¸=i Î±j uj . Then the ith row is
a linear combination of the other rows and so from the properties of the determinant, the
determinant of this matrix is indeed zero as it should be.
A parallelepiped is a sort of a squashed box. Here is a picture which shows the relationship between P (u1 , Â· Â· Â· , upâˆ’1 ) and P (u1 , Â· Â· Â· , up ).
6
N

up
3
Î¸

P (u1 , Â· Â· Â· , upâˆ’1 )
-

In a sense, we can deï¬ne the volume any way we want but if it is to be reasonable, the
following relationship must hold. The appropriate deï¬nition of the volume of P (u1 , Â· Â· Â· , up )
in terms of P (u1 , Â· Â· Â· , upâˆ’1 ) is
v (P (u1 , Â· Â· Â· , up )) = |up | |cos (Î¸)| v (P (u1 , Â· Â· Â· , upâˆ’1 ))

(12.11)

In the case where p = 1, the parallelepiped P (v) consists of the single vector and the one
(
)1/2
. Now having made this deï¬nition, I will show
dimensional volume should be |v| = vT v
that this is the appropriate deï¬nition of p dimensional volume for every p.
Deï¬nition 12.8.2 Let {u1 , Â· Â· Â· , up } be vectors. Then
v (P (u1 , Â· Â· Â· , up )) â‰¡
ï£«ï£«
ï£¬ï£¬
ï£¬ï£¬
â‰¡ det ï£¬ï£¬
ï£­ï£­

uT1
uT2
..
.

ï£¶
ï£·(
ï£·
ï£· u1
ï£¸

ï£¶1/2
u2

Â·Â·Â·

up

)ï£·
ï£·
ï£·
ï£¸

uTp

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

304

INNER PRODUCT SPACES

As just pointed out, this is the only reasonable deï¬nition of volume in the case of one
vector. The next theorem shows that it is the only reasonable deï¬nition of volume of a
parallelepiped in the case of p vectors because (12.11) holds.
Theorem 12.8.3 With the above deï¬nition of volume, (12.11) holds.
Proof: To check whether this is so, it is necessary to ï¬nd |cos (Î¸)| . This involves ï¬nding
the vector perpendicular to P (u1 , Â· Â· Â· , upâˆ’1 ) . Let {w1 , Â· Â· Â· , wp } be an orthonormal basis
for span (u1 , Â· Â· Â· , up ) such that span (w1 , Â· Â· Â· , wk ) = span (u1 , Â· Â· Â· , uk ) for each k â‰¤ p. Such
an orthonormal basis exists because of the Gram Schmidt procedure. First note that since
{wk } is an orthonormal basis for span (u1 , Â· Â· Â· , up ) ,
uj =

p
âˆ‘

(uj Â· wk ) wk

k=1

and if i, j â‰¤ k
uj Â· ui =

k
âˆ‘

(uj Â· wk ) (ui Â· wk )

k=1

Therefore, for each k â‰¤ p

ï£«ï£«
ï£¬ï£¬
ï£¬ï£¬
det ï£¬ï£¬
ï£­ï£­

uT1
uT2
..
.

ï£¶

ï£¶

ï£·(
ï£·
ï£· u1
ï£¸

u2

Â·Â·Â·

uk

)ï£·
ï£·
ï£·
ï£¸

uTk
is the determinant of a matrix whose ij th entry is
uTi uj = ui Â· uj =

k
âˆ‘

(ui Â· wr ) (wr Â· uj )

r=1

Thus this matrix is the product of the two k Ã— k matrices, one which is the transpose of the
other.
ï£«
ï£¶
(u1 Â· w1 ) (u1 Â· w2 ) Â· Â· Â· (u1 Â· wk )
ï£¬ (u2 Â· w1 ) (u2 Â· w2 ) Â· Â· Â· (u2 Â· wk ) ï£·
ï£¬
ï£·
ï£¬
ï£·Â·
..
..
..
ï£­
ï£¸
.
.
.
(uk Â· w1 ) (uk Â· w2 ) Â· Â· Â· (uk Â· wk )
ï£«
ï£¶
(u1 Â· w1 ) (u2 Â· w1 ) Â· Â· Â· (uk Â· w1 )
ï£¬ (u1 Â· w2 ) (u2 Â· w2 ) Â· Â· Â· (uk Â· w2 ) ï£·
ï£¬
ï£·
ï£¬
ï£·
..
..
..
ï£­
ï£¸
.
.
.
(u1 Â· wk )
ï£«ï£«

It follows

ï£¬ï£¬
ï£¬ï£¬
det ï£¬ï£¬
ï£­ï£­
ï£«

ï£«

ï£¬
ï£¬
ï£¬
ï£¬
= ï£¬det ï£¬
ï£­
ï£­

uT1
uT2
..
.

(u2 Â· wk )

Â·Â·Â·

(uk Â· wk )

ï£¶

ï£¶

ï£·(
ï£·
ï£· u1
ï£¸

u2

Â·Â·Â·

uk

)ï£·
ï£·
ï£·
ï£¸

uTk
(u1 Â· w1 )
(u2 Â· w1 )
..
.

(u1 Â· w2 )
(u2 Â· w2 )
..
.

Â·Â·Â·
Â·Â·Â·

(u1 Â· wk )
(u2 Â· wk )
..
.

(uk Â· w1 ) (uk Â· w2 ) Â· Â· Â·

(uk Â· wk )

Saylor URL: http://www.saylor.org/courses/ma212/

ï£¶ï£¶2
ï£·ï£·
ï£·ï£·
ï£·ï£·
ï£¸ï£¸

The Saylor Foundation

12.8. THE DETERMINANT AND VOLUME

305

and so from the deï¬nition,
ï£«
ï£¬
ï£¬
det ï£¬
ï£­
Now consider the vector
ï£«
ï£¬
ï£¬
N â‰¡ det ï£¬
ï£­

v (P (u1 , Â· Â· Â· , uk )) =
(u1 Â· w1 )
(u2 Â· w1 )
..
.

(u1 Â· w2 )
(u2 Â· w2 )
..
.

Â·Â·Â·
Â·Â·Â·

(u1 Â· wk )
(u2 Â· wk )
..
.

(uk Â· w1 ) (uk Â· w2 )

Â·Â·Â·

(uk Â· wk )

w1
(u1 Â· w1 )
..
.

w2
(u1 Â· w2 )
..
.

Â·Â·Â·
Â·Â·Â·

(upâˆ’1 Â· w1 ) (upâˆ’1 Â· w2 ) Â· Â· Â·

ï£¶
ï£·
ï£·
ï£·
ï£¸

wp
(u1 Â· wp )
..
.

ï£¶
ï£·
ï£·
ï£·
ï£¸

(upâˆ’1 Â· wp )

which results from formally expanding along the top row. Note that from what was just
discussed,
v (P (u1 , Â· Â· Â· , upâˆ’1 )) = Â±A1p
Now it follows from the formula for expansion of a determinant along the top row that for
each j â‰¤ p âˆ’ 1
p
p
âˆ‘
âˆ‘
(uj Â· wk ) A1k
(uj Â· wk ) (N Â· wk ) =
N Â· uj =
k=1

k=1

cofactor of the above matrix. Thus if j â‰¤ p âˆ’ 1
ï£«
(uj Â· w1 )
(uj Â· w2 )
Â·Â·Â·
(uj Â· wp )
ï£¬ (u1 Â· w1 )
(u
Â·
w
)
Â·
Â·
Â·
(u
1
2
1 Â· wp )
ï£¬
N Â· uj = det ï£¬
..
..
..
ï£­
.
.
.
(upâˆ’1 Â· w1 ) (upâˆ’1 Â· w2 ) Â· Â· Â· (upâˆ’1 Â· wp )

where A1k is the 1k

th

ï£¶
ï£·
ï£·
ï£·=0
ï£¸

because the matrix has two equal rows while if j = p, the above discussion shows N Â· up
equals Â±v (P (u1 , Â· Â· Â· , up )). Therefore, N points in the direction of the normal vector in the
above picture or else it points in the opposite direction to this vector. From the geometric
description of the dot product,
|N Â· up |
|cos (Î¸)| =
|up | |N|
and it follows
|up | |cos (Î¸)| v (P (u1 , Â· Â· Â· , upâˆ’1 )) = |up |
=

|N Â· up |
v (P (u1 , Â· Â· Â· , upâˆ’1 ))
|up | |N|

v (P (u1 , Â· Â· Â· , up ))
v (P (u1 , Â· Â· Â· , upâˆ’1 ))
|N|

Now at this point, note that from the construction, wp Â· uk = 0 whenever k â‰¤ p âˆ’ 1 because
uk âˆˆ span (w1 , Â· Â· Â· , wpâˆ’1 ). Therefore, |N| = |A1p | = v (P (u1 , Â· Â· Â· , upâˆ’1 )) and so the above
reduces to
|up | |cos (Î¸)| v (P (u1 , Â· Â· Â· , upâˆ’1 )) = v (P (u1 , Â· Â· Â· , up )) . 
The theorem shows that the only reasonable deï¬nition of p dimensional volume of a
parallelepiped is the one given in the above deï¬nition.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

306

INNER PRODUCT SPACES

12.9

Exercises
T

T

T

1. Here are three vectors in R4 : (1, 2, 0, 3) , (2, 1, âˆ’3, 2) , (0, 0, 1, 2) . Find the three
dimensional volume of the parallelepiped determined by these three vectors.
T

T

2. Here are two vectors in R4 : (1, 2, 0, 3) , (2, 1, âˆ’3, 2) . Find the volume of the parallelepiped determined by these two vectors.
T

T

T

3. Here are three vectors in R2 : (1, 2) , (2, 1) , (0, 1) . Find the three dimensional
volume of the parallelepiped determined by these three vectors. Recall that from the
above theorem, this should equal 0.
4. Find the equation of the plane through the three points (1, 2, 3) , (2, âˆ’3, 1) , (1, 1, 7) .
5. Let T map a vector space V to itself. Explain why T is one to one if and only if T is
onto. It is in the text, but do it again in your own words.
6. â†‘Let all matrices be complex with complex ï¬eld of scalars and let A be an nÃ—n matrix
and B a m Ã— m matrix while X will be an n Ã— m matrix. The problem is to consider
solutions to Sylvesterâ€™s equation. Solve the following equation for X
AX âˆ’ XB = C
where C is an arbitrary n Ã— m matrix. Show there exists a unique solution if and only
if Ïƒ (A) âˆ© Ïƒ (B) = âˆ…. Hint: If q (Î») is a polynomial, show ï¬rst that if AX âˆ’ XB = 0,
then q (A) X âˆ’ Xq (B) = 0. Next deï¬ne the linear map T which maps the n Ã— m
matrices to the n Ã— m matrices as follows.
T X â‰¡ AX âˆ’ XB
Show that the only solution to T X = 0 is X = 0 so that T is one to one if and only if
Ïƒ (A)âˆ©Ïƒ (B) = âˆ…. Do this by using the ï¬rst part for q (Î») the characteristic polynomial
âˆ’1
for B and then use the Cayley Hamilton theorem. Explain why q (A) exists if and
only if the condition Ïƒ (A) âˆ© Ïƒ (B) = âˆ….
7. Compare Deï¬nition 12.8.2 with the Binet Cauchy theorem, Theorem 3.3.14. What is
the geometric meaning of the Binet Cauchy theorem in this context?

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Self Adjoint Operators
13.1

Simultaneous Diagonalization

Recall the following deï¬nition of what it means for a matrix to be diagonalizable.
Deï¬nition 13.1.1 Let A be an n Ã— n matrix. It is said to be diagonalizable if there exists
an invertible matrix S such that
S âˆ’1 AS = D
where D is a diagonal matrix.
Also, here is a useful observation.
Observation 13.1.2 If A is an n Ã— n matrix and AS = SD for D a diagonal matrix, then
each column of S is an eigenvector or else it is the zero vector. This follows from observing
that for sk the k th column of S and from the way we multiply matrices,
Ask = Î»k sk
It is sometimes interesting to consider the problem of ï¬nding a single similarity transformation which will diagonalize all the matrices in some set.
Lemma 13.1.3 Let A be an n Ã— n matrix and let B be an m Ã— m matrix. Denote by C the
matrix
(
)
A 0
Câ‰¡
.
0 B
Then C is diagonalizable if and only if both A and B are diagonalizable.
âˆ’1
âˆ’1
Proof: Suppose SA
ASA = DA and SB
BSB = DB where (
DA and DB)are diagonal
SA 0
matrices. You should use block multiplication to verify that S â‰¡
is such that
0 SB
S âˆ’1 CS = DC , a diagonal matrix.
Conversely, suppose C is diagonalized by S = (s1 , Â· Â· Â· , sn+m ) . Thus S has columns si .
For each of these columns, write in the form
(
)
xi
si =
yi

where xi âˆˆ Fn and where yi âˆˆ Fm . The result is
(
)
S11 S12
S=
S21 S22
307

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

308

SELF ADJOINT OPERATORS

where S11 is an n Ã— n matrix and S22 is an m Ã— m matrix. Then there is a diagonal matrix
(
)
D1 0
D = diag (Î»1 , Â· Â· Â· , Î»n+m ) =
0 D2
such that

(
(
=

A
0
S11
S21

0
B

)(

S12
S22

)
S11 S12
S21 S22
)(
)
D1 0
0 D2

Hence by block multiplication
AS11 = S11 D1 , BS22 = S22 D2
BS21 = S21 D1 , AS12 = S12 D2
It follows each of the xi is an eigenvector of A or else is the zero vector and that each of the
yi is an eigenvector of B or is the zero vector. If there are n linearly independent xi , then
A is diagonalizable by Theorem 9.3.12 on Page 9.3.12.
The row rank of the matrix (x1 , Â· Â· Â· , xn+m ) must be n because if this is not so, the rank
of S would be less than n + m which would mean S âˆ’1 does not exist. Therefore, since the
column rank equals the row rank, this matrix has column rank equal to n and this means
there are n linearly independent eigenvectors of A implying that A is diagonalizable. Similar
reasoning applies to B. 
The following corollary follows from the same type of argument as the above.
Corollary 13.1.4 Let Ak be an nk Ã— nk matrix and let C denote the block diagonal
) ( r
)
( r
âˆ‘
âˆ‘
nk
nk Ã—
k=1

k=1

matrix given below.

ï£«
ï£¬
Câ‰¡ï£­

A1

0
..

0

.

ï£¶
ï£·
ï£¸.

Ar

Then C is diagonalizable if and only if each Ak is diagonalizable.
Deï¬nition 13.1.5 A set, F of n Ã— n matrices is said to be simultaneously diagonalizable if
and only if there exists a single invertible matrix S such that for every A âˆˆ F , S âˆ’1 AS = DA
where DA is a diagonal matrix.
Lemma 13.1.6 If F is a set of n Ã— n matrices which is simultaneously diagonalizable, then
F is a commuting family of matrices.
Proof: Let A, B âˆˆ F and let S be a matrix which has the property that S âˆ’1 AS is a
diagonal matrix for all A âˆˆ F. Then S âˆ’1 AS = DA and S âˆ’1 BS = DB where DA and DB
are diagonal matrices. Since diagonal matrices commute,
AB

= SDA S âˆ’1 SDB S âˆ’1 = SDA DB S âˆ’1
= SDB DA S âˆ’1 = SDB S âˆ’1 SDA S âˆ’1 = BA.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

13.1. SIMULTANEOUS DIAGONALIZATION
Lemma 13.1.7 Let D be a diagonal matrix of the form
ï£«
Î»1 In1
0
Â·Â·Â·
0
ï£¬
..
.
.
ï£¬ 0
.
Î»2 In2
.
ï£¬
Dâ‰¡ï£¬
..
.
.
..
..
ï£­
.
0
0
Â·Â·Â·
0 Î»r Inr

309

ï£¶
ï£·
ï£·
ï£·,
ï£·
ï£¸

(13.1)

where Ini denotes the ni Ã— ni identity matrix and Î»i Ì¸= Î»j for i Ì¸= j and suppose B is a
matrix which commutes with D. Then B is a block diagonal matrix of the form
ï£«
ï£¶
B1 0 Â· Â· Â·
0
ï£¬
.. ï£·
ï£¬ 0 B2 . . .
. ï£·
ï£·
B=ï£¬
(13.2)
ï£¬ .
ï£·
..
..
ï£­ ..
.
. 0 ï£¸
0 Â·Â·Â·
0 Br
where Bi is an ni Ã— ni matrix.
Proof: Let B = (Bij ) where Bii = Bi a block matrix as above in (13.2).
ï£«
ï£¶
B11 B12 Â· Â· Â· B1r
ï£¬
ï£·
ï£¬ B21 B22 . . . B2r ï£·
ï£¬
ï£·
ï£¬ .
.. ï£·
..
..
ï£­ ..
.
.
. ï£¸
Br1 Br2 Â· Â· Â· Brr
Then by block multiplication, since B is given to commute with D,
Î»j Bij = Î»i Bij
Therefore, if i Ì¸= j, Bij = 0. 
Lemma 13.1.8 Let F denote a commuting family of n Ã— n matrices such that each A âˆˆ F
is diagonalizable. Then F is simultaneously diagonalizable.
Proof: First note that if every matrix in F has only one eigenvalue, there is nothing to
prove. This is because for A such a matrix,
S âˆ’1 AS = Î»I
and so
A = Î»I
Thus all the matrices in F are diagonal matrices and you could pick any S to diagonalize
them all. Therefore, without loss of generality, assume some matrix in F has more than one
eigenvalue.
The signiï¬cant part of the lemma is proved by induction on n. If n = 1, there is nothing
to prove because all the 1 Ã— 1 matrices are already diagonal matrices. Suppose then that
the theorem is true for all k â‰¤ n âˆ’ 1 where n â‰¥ 2 and let F be a commuting family of
diagonalizable n Ã— n matrices. Pick A âˆˆ F which has more than one eigenvalue and let
S be an invertible matrix such that S âˆ’1 AS = D where D is of the form given in (13.1).
By permuting the columns of S there is no loss
{ of generality in
} assuming D has this form.
Now denote by Fe the collection of matrices, S âˆ’1 CS : C âˆˆ F . Note Fe features the single
matrix S.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

310

SELF ADJOINT OPERATORS

It follows easily that Fe is also a commuting family of diagonalizable matrices. By
Lemma 13.1.7 every B âˆˆ Fe is of the form given in (13.2) because each of these commutes
with D described above as S âˆ’1 AS and so by block multiplication, the diagonal blocks Bi
corresponding to diï¬€erent B âˆˆ Fe commute.
By Corollary 13.1.4 each of these blocks is diagonalizable. This is because B is known to
be so. Therefore, by induction, since all the blocks are no larger than n âˆ’ 1 Ã— n âˆ’ 1 thanks to
the assumption that A has more than one eigenvalue, there exist invertible ni Ã— ni matrices,
Ti such that Tiâˆ’1 Bi Ti is a diagonal matrix whenever Bi is one of the matrices making up
the block diagonal of any B âˆˆ F . It follows that for T deï¬ned by
ï£«
ï£¶
T1 0 Â· Â· Â· 0
ï£¬
.. ï£·
ï£¬ 0 T2 . . .
. ï£·
ï£·,
T â‰¡ï£¬
ï£¬ .
ï£·
..
..
ï£­ ..
.
. 0 ï£¸
0 Â·Â·Â·
0 Tr
then T âˆ’1 BT = a diagonal matrix for every B âˆˆ Fe including D. Consider ST. It follows
that for all C âˆˆ F ,
e
something in F

T âˆ’1

z }| {
S âˆ’1 CS

T = (ST )

âˆ’1

C (ST ) = a diagonal matrix. 

Theorem 13.1.9 Let F denote a family of matrices which are diagonalizable. Then F is
simultaneously diagonalizable if and only if F is a commuting family.
Proof: If F is a commuting family, it follows from Lemma 13.1.8 that it is simultaneously
diagonalizable. If it is simultaneously diagonalizable, then it follows from Lemma 13.1.6 that
it is a commuting family. 

13.2

Schurâ€™s Theorem

Recall that for a linear transformation, L âˆˆ L (V, V ) for V a ï¬nite dimensional inner product
space, it could be represented in the form
âˆ‘
lij vi âŠ— vj
L=
ij

where {v1 , Â· Â· Â· , vn } is an orthonormal basis. Of course diï¬€erent bases will yield diï¬€erent
matrices, (lij ) . Schurâ€™s theorem gives the existence of a basis in an inner product space such
that (lij ) is particularly simple.
Deï¬nition 13.2.1 Let L âˆˆ L (V, V ) where V is vector space. Then a subspace U of V is L
invariant if L (U ) âŠ† U.
In what follows, F will be the ï¬eld of scalars, usually C but maybe something else.
Theorem 13.2.2 Let L âˆˆ L (H, H) for H a ï¬nite dimensional inner product space such
that the restriction of Lâˆ— to every L invariant subspace has its eigenvalues in F. Then there
n
exist constants, cij for i â‰¤ j and an orthonormal basis, {wi }i=1 such that
L=

j
n âˆ‘
âˆ‘

cij wi âŠ— wj

j=1 i=1

The constants, cii are the eigenvalues of L.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

13.2. SCHURâ€™S THEOREM

311

Proof: If dim (H) = 1, let H = span (w) where |w| = 1. Then Lw = kw for some k.
Then
L = kw âŠ— w
because by deï¬nition, w âŠ— w (w) = w. Therefore, the theorem holds if H is 1 dimensional.
Now suppose the theorem holds for n âˆ’ 1 = dim (H) . Let wn be an eigenvector for Lâˆ— .
Dividing by its length, it can be assumed |wn | = 1. Say Lâˆ— wn = Âµwn . Using the Gram
Schmidt process, there exists an orthonormal basis for H of the form {v1 , Â· Â· Â· , vnâˆ’1 , wn } .
Then
(Lvk , wn ) = (vk , Lâˆ— wn ) = (vk , Âµwn ) = 0,
which shows
L : H1 â‰¡ span (v1 , Â· Â· Â· , vnâˆ’1 ) â†’ span (v1 , Â· Â· Â· , vnâˆ’1 ) .
Denote by L1 the restriction of L to H1 . Since H1 has dimension n âˆ’ 1, the induction
hypothesis yields an orthonormal basis, {w1 , Â· Â· Â· , wnâˆ’1 } for H1 such that
j
nâˆ’1
âˆ‘âˆ‘

L1 =

cij wi âŠ—wj .

(13.3)

j=1 i=1

Then {w1 , Â· Â· Â· , wn } is an orthonormal basis for H because every vector in
span (v1 , Â· Â· Â· , vnâˆ’1 )
has the property that its inner product with wn is 0 so in particular, this is true for the
vectors {w1 , Â· Â· Â· , wnâˆ’1 }. Now deï¬ne cin to be the scalars satisfying
Lwn â‰¡

n
âˆ‘

cin wi

(13.4)

i=1

and let

j
n âˆ‘
âˆ‘

Bâ‰¡

cij wi âŠ—wj .

j=1 i=1

Then by (13.4),
Bwn =

j
n âˆ‘
âˆ‘

cij wi Î´ nj =

j=1 i=1

If 1 â‰¤ k â‰¤ n âˆ’ 1,
Bwk =

n
âˆ‘

cin wi = Lwn .

j=1

j
n âˆ‘
âˆ‘

cij wi Î´ kj =

j=1 i=1

k
âˆ‘

cik wi

i=1

while from (13.3),
Lwk = L1 wk =

j
nâˆ’1
âˆ‘âˆ‘

cij wi Î´ jk =

j=1 i=1

k
âˆ‘

cik wi .

i=1

Since L = B on the basis {w1 , Â· Â· Â· , wn } , it follows L = B.
It remains to verify the constants, ckk are the eigenvalues of L, solutions of the equation,
det (Î»I âˆ’ L) = 0. However, the deï¬nition of det (Î»I âˆ’ L) is the same as
det (Î»I âˆ’ C)
where C is the upper triangular matrix which has cij for i â‰¤ j and zeros elsewhere. This
equals 0 if and only if Î» is one of the diagonal entries, one of the ckk . 
Now with the above Schurâ€™s theorem, the following diagonalization theorem comes very
easily. Recall the following deï¬nition.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

312

SELF ADJOINT OPERATORS

Deï¬nition 13.2.3 Let L âˆˆ L (H, H) where H is a ï¬nite dimensional inner product space.
Then L is Hermitian if Lâˆ— = L.
Theorem 13.2.4 Let L âˆˆ L (H, H) where H is an n dimensional inner product space. If
L is Hermitian, then all of its eigenvalues Î»k are real and there exists an orthonormal basis
of eigenvectors {wk } such that
âˆ‘
L=
Î»k wk âŠ—wk .
k

Proof: By Schurâ€™s theorem, Theorem 13.2.2, there exist lij âˆˆ F such that
L=

j
n âˆ‘
âˆ‘

lij wi âŠ—wj

j=1 i=1

Then by Lemma 12.4.2,
j
n âˆ‘
âˆ‘

lij wi âŠ—wj

= L = Lâˆ— =

j=1 i=1

j
n âˆ‘
âˆ‘

âˆ—

(lij wi âŠ—wj )

j=1 i=1

=

j
n âˆ‘
âˆ‘

lij wj âŠ—wi =

j=1 i=1

n âˆ‘
i
âˆ‘

lji wi âŠ—wj

i=1 j=1

By independence, if i = j,
lii = lii
and so these are all real. If i < j, it follows from independence again that
lij = 0
because the coeï¬ƒcients corresponding to i < j are all 0 on the right side. Similarly if i > j,
it follows lij = 0. Letting Î»k = lkk , this shows
âˆ‘
L=
Î»k wk âŠ— wk
k

That each of these wk is an eigenvector corresponding to Î»k is obvious from the deï¬nition
of the tensor product. 

13.3

Spectral Theory Of Self Adjoint Operators

The following theorem is about the eigenvectors and eigenvalues of a self adjoint operator.
Such operators are also called Hermitian as in the case of matrices. The proof given generalizes to the situation of a compact self adjoint operator on a Hilbert space and leads to
many very useful results. It is also a very elementary proof because it does not use the
fundamental theorem of algebra and it contains a way, very important in applications, of
ï¬nding the eigenvalues. This proof depends more directly on the methods of analysis than
the preceding material. The ï¬eld of scalars will be R or C. The following is useful notation.
Deï¬nition 13.3.1 Let X be an inner product space and let S âŠ† X. Then
S âŠ¥ â‰¡ {x âˆˆ X : (x, s) = 0 for all s âˆˆ S} .
Note that even if S is not a subspace, S âŠ¥ is.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

13.3. SPECTRAL THEORY OF SELF ADJOINT OPERATORS

313

Deï¬nition 13.3.2 A Hilbert space is a complete inner product space. Recall this means
that every Cauchy sequence,{xn } , one which satisï¬es
lim |xn âˆ’ xm | = 0,

n,mâ†’âˆ

converges. It can be shown, although I will not do so here, that for the ï¬eld of scalars either
R or C, any ï¬nite dimensional inner product space is automatically complete.
Theorem 13.3.3 Let A âˆˆ L (X, X) be self adjoint (Hermitian) where X is a ï¬nite dimensional Hilbert space. Thus A = Aâˆ— . Then there exists an orthonormal basis of eigenvectors,
n
{uj }j=1 .
Proof: Consider (Ax, x) . This quantity is always a real number because
(Ax, x) = (x, Ax) = (x, Aâˆ— x) = (Ax, x)
thanks to the assumption that A is self adjoint. Now deï¬ne
Î»1 â‰¡ inf {(Ax, x) : |x| = 1, x âˆˆ X1 â‰¡ X} .
Claim: Î»1 is ï¬nite and there exists v1 âˆˆ X with |v1 | = 1 such that (Av1 , v1 ) = Î»1 .
n
Proof of claim: Let {uj }j=1 be an orthonormal basis for X and for x âˆˆ X, let (x1 , Â· Â· Â· ,
xn ) be deï¬ned as the components of the vector x. Thus,
x=

n
âˆ‘

xj uj .

j=1

Since this is an orthonormal basis, it follows from the axioms of the inner product that
2

|x| =

n
âˆ‘

2

|xj | .

j=1

Thus

ï£«
(Ax, x) = ï£­

n
âˆ‘
k=1

xk Auk ,

âˆ‘

ï£¶
xj uj ï£¸ =

âˆ‘

j=1

xk xj (Auk , uj ) ,

k,j

a real valued continuous function of (x1 , Â· Â· Â· , xn ) which is deï¬ned on the compact set
K â‰¡ {(x1 , Â· Â· Â· , xn ) âˆˆ Fn :

n
âˆ‘

2

|xj | = 1}.

j=1

Therefore, it achieves its minimum from the extreme value theorem. Then deï¬ne
v1 â‰¡

n
âˆ‘

xj uj

j=1

where (x1 , Â· Â· Â· , xn ) is the point of K at which the above function achieves its minimum.
This proves the claim.
âŠ¥
Continuing with the proof of the theorem, let X2 â‰¡ {v1 } . This is a closed subspace of
X. Let
Î»2 â‰¡ inf {(Ax, x) : |x| = 1, x âˆˆ X2 }

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

314

SELF ADJOINT OPERATORS
âŠ¥

As before, there exists v2 âˆˆ X2 such that (Av2 , v2 ) = Î»2 , Î»1 â‰¤ Î»2 . Now let X3 â‰¡ {v1 , v2 }
n
and continue in this way. This leads to an increasing sequence of real numbers, {Î»k }k=1 and
an orthonormal set of vectors, {v1 , Â· Â· Â· , vn }. It only remains to show these are eigenvectors
and that the Î»j are eigenvalues.
Consider the ï¬rst of these vectors. Letting w âˆˆ X1 â‰¡ X, the function of the real variable,
t, given by
(A (v1 + tw) , v1 + tw)
f (t) â‰¡
2
|v1 + tw|
=

(Av1 , v1 ) + 2t Re (Av1 , w) + t2 (Aw, w)
2

2

|v1 | + 2t Re (v1 , w) + t2 |w|
achieves its minimum when t = 0. Therefore, the derivative of this function evaluated at
t = 0 must equal zero. Using the quotient rule, this implies, since |v1 | = 1 that
2

2 Re (Av1 , w) |v1 | âˆ’ 2 Re (v1 , w) (Av1 , v1 )
= 2 (Re (Av1 , w) âˆ’ Re (v1 , w) Î»1 ) = 0.
Thus Re (Av1 âˆ’ Î»1 v1 , w) = 0 for all w âˆˆ X. This implies Av1 = Î»1 v1 . To see this, let w âˆˆ X
be arbitrary and let Î¸ be a complex number with |Î¸| = 1 and
|(Av1 âˆ’ Î»1 v1 , w)| = Î¸ (Av1 âˆ’ Î»1 v1 , w) .
Then

(
)
|(Av1 âˆ’ Î»1 v1 , w)| = Re Av1 âˆ’ Î»1 v1 , Î¸w = 0.

Since this holds for all w, Av1 = Î»1 v1 .
Now suppose Avk = Î»k vk for all k < m. Observe that A : Xm â†’ Xm because if y âˆˆ Xm
and k < m,
(Ay, vk ) = (y, Avk ) = (y, Î»k vk ) = 0,
âŠ¥

showing that Ay âˆˆ {v1 , Â· Â· Â· , vmâˆ’1 } â‰¡ Xm . Thus the same argument just given shows that
for all w âˆˆ Xm ,
(Avm âˆ’ Î»m vm , w) = 0.
(13.5)
Since Avm âˆˆ Xm , I can let w = Avm âˆ’ Î»m vm in the above and thereby conclude Avm =
Î»m vm . 
Contained in the proof of this theorem is the following important corollary.
Corollary 13.3.4 Let A âˆˆ L (X, X) be self adjoint where X is a ï¬nite dimensional Hilbert
space. Then all the eigenvalues are real and for Î»1 â‰¤ Î»2 â‰¤ Â· Â· Â· â‰¤ Î»n the eigenvalues of A,
there exists an orthonormal set of vectors {u1 , Â· Â· Â· , un } for which
Auk = Î»k uk .
Furthermore,
Î»k â‰¡ inf {(Ax, x) : |x| = 1, x âˆˆ Xk }
where

âŠ¥

Xk â‰¡ {u1 , Â· Â· Â· , ukâˆ’1 } , X1 â‰¡ X.
Corollary 13.3.5 Let A âˆˆ L (X, X) be self adjoint (Hermitian) where X is a ï¬nite dimensional Hilbert space. Then the largest eigenvalue of A is given by
max {(Ax, x) : |x| = 1}

(13.6)

and the minimum eigenvalue of A is given by
min {(Ax, x) : |x| = 1} .

Saylor URL: http://www.saylor.org/courses/ma212/

(13.7)

The Saylor Foundation

13.3. SPECTRAL THEORY OF SELF ADJOINT OPERATORS

315

Proof: The proof of this is just like the proof of Theorem 13.3.3. Simply replace inf
with sup and obtain a decreasing list of eigenvalues. This establishes (13.6). The claim
(13.7) follows from Theorem 13.3.3.
Another important observation is found in the following corollary.
âˆ‘
Corollary 13.3.6 Let A âˆˆ L (X, X) where A is self adjoint. Then A = i Î»i vi âŠ— vi where
n
Avi = Î»i vi and {vi }i=1 is an orthonormal basis.
Proof : If vk is one of the orthonormal basis vectors, Avk = Î»k vk . Also,
âˆ‘
âˆ‘
Î»i vi âŠ— vi (vk ) =
Î»i vi (vk , vi )
i

i

âˆ‘

=

Î»i Î´ ik vi = Î»k vk .

i

Since the two linear transformations agree on a basis, it follows they must coincide. 
n
By Theorem 12.4.5 this says the matrix of A with respect to this basis {vi }i=1 is the
diagonal matrix having the eigenvalues Î»1 , Â· Â· Â· , Î»n down the main diagonal.
The result of Courant and Fischer which follows resembles Corollary 13.3.4 but is more
useful because it does not depend on a knowledge of the eigenvectors.
Theorem 13.3.7 Let A âˆˆ L (X, X) be self adjoint where X is a ï¬nite dimensional Hilbert
space. Then for Î»1 â‰¤ Î»2 â‰¤ Â· Â· Â· â‰¤ Î»n the eigenvalues of A, there exist orthonormal vectors
{u1 , Â· Â· Â· , un } for which
Auk = Î»k uk .
Furthermore,
Î»k â‰¡

max

w1 ,Â·Â·Â· ,wkâˆ’1

{
{
}}
âŠ¥
min (Ax, x) : |x| = 1, x âˆˆ {w1 , Â· Â· Â· , wkâˆ’1 }

(13.8)

âŠ¥

where if k = 1, {w1 , Â· Â· Â· , wkâˆ’1 } â‰¡ X.
Proof: From Theorem 13.3.3, there exist eigenvalues and eigenvectors with {u1 , Â· Â· Â· , un }
orthonormal and Î»i â‰¤ Î»i+1 . Therefore, by Corollary 13.3.6
A=

n
âˆ‘

Î»j uj âŠ— uj

j=1

Fix {w1 , Â· Â· Â· , wkâˆ’1 }.
(Ax, x) =

n
âˆ‘

Î»j (x, uj ) (uj , x) =

j=1

n
âˆ‘

2

Î»j |(x, uj )|

j=1

âŠ¥

Then let Y = {w1 , Â· Â· Â· , wkâˆ’1 }

inf {(Ax, x) : |x| = 1, x âˆˆ Y } = inf

â‰¤ inf

ï£±
k
ï£²âˆ‘
ï£³

2

ï£±
n
ï£²âˆ‘
ï£³

2

Î»j |(x, uj )| : |x| = 1, x âˆˆ Y

j=1

Î»j |(x, uj )| : |x| = 1, (x, uj ) = 0 for j > k, and x âˆˆ Y

j=1

Saylor URL: http://www.saylor.org/courses/ma212/

ï£¼
ï£½
ï£¾

ï£¼
ï£½
ï£¾

.

(13.9)

The Saylor Foundation

316

SELF ADJOINT OPERATORS

The reason this is so is that the inï¬mum is taken over a smaller set. Therefore, the inï¬mum
gets larger. Now (13.9) is no larger than
ï£±
ï£¼
k
ï£² âˆ‘
ï£½
2
inf Î»k
|(x, uj )| : |x| = 1, (x, uj ) = 0 for j > k, and x âˆˆ Y = Î»k
ï£³
ï£¾
j=1

âˆ‘n
2
2
because since {u1 , Â· Â· Â· , un } is an orthonormal basis, |x| = j=1 |(x, uj )| . It follows since
{w1 , Â· Â· Â· , wkâˆ’1 } is arbitrary,
{ {
}}
âŠ¥
sup
inf (Ax, x) : |x| = 1, x âˆˆ {w1 , Â· Â· Â· , wkâˆ’1 }
â‰¤ Î»k .
(13.10)
w1 ,Â·Â·Â· ,wkâˆ’1

However, for each w1 , Â· Â· Â· , wkâˆ’1 , the inï¬mum is achieved so you can replace the inf in the
above with min. In addition to this, it follows from Corollary 13.3.4 that there exists a set,
{w1 , Â· Â· Â· , wkâˆ’1 } for which
{
}
âŠ¥
inf (Ax, x) : |x| = 1, x âˆˆ {w1 , Â· Â· Â· , wkâˆ’1 }
= Î»k .
Pick {w1 , Â· Â· Â· , wkâˆ’1 } = {u1 , Â· Â· Â· , ukâˆ’1 } . Therefore, the sup in (13.10) is achieved and equals
Î»k and (13.8) follows. 
The following corollary is immediate.
Corollary 13.3.8 Let A âˆˆ L (X, X) be self adjoint where X is a ï¬nite dimensional Hilbert
space. Then for Î»1 â‰¤ Î»2 â‰¤ Â· Â· Â· â‰¤ Î»n the eigenvalues of A, there exist orthonormal vectors
{u1 , Â· Â· Â· , un } for which
Auk = Î»k uk .
Furthermore,
{
Î»k â‰¡

max

{
min

w1 ,Â·Â·Â· ,wkâˆ’1

(Ax, x)
2

|x|

}}
âŠ¥

: x Ì¸= 0, x âˆˆ {w1 , Â· Â· Â· , wkâˆ’1 }

(13.11)

âŠ¥

where if k = 1, {w1 , Â· Â· Â· , wkâˆ’1 } â‰¡ X.
Here is a version of this for which the roles of max and min are reversed.
Corollary 13.3.9 Let A âˆˆ L (X, X) be self adjoint where X is a ï¬nite dimensional Hilbert
space. Then for Î»1 â‰¤ Î»2 â‰¤ Â· Â· Â· â‰¤ Î»n the eigenvalues of A, there exist orthonormal vectors
{u1 , Â· Â· Â· , un } for which
Auk = Î»k uk .
Furthermore,
{
Î»k â‰¡

min

w1 ,Â·Â·Â· ,wnâˆ’k

{

max

(Ax, x)
2

|x|

}}
âŠ¥

: x Ì¸= 0, x âˆˆ {w1 , Â· Â· Â· , wnâˆ’k }

(13.12)

âŠ¥

where if k = n, {w1 , Â· Â· Â· , wnâˆ’k } â‰¡ X.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

13.4. POSITIVE AND NEGATIVE LINEAR TRANSFORMATIONS

13.4

317

Positive And Negative Linear Transformations

The notion of a positive deï¬nite or negative deï¬nite linear transformation is very important
in many applications. In particular it is used in versions of the second derivative test for
functions of many variables. Here the main interest is the case of a linear transformation
which is an nÃ—n matrix but the theorem is stated and proved using a more general notation
because all these issues discussed here have interesting generalizations to functional analysis.
Lemma 13.4.1 Let X be a ï¬nite dimensional Hilbert space and let A âˆˆ L (X, X) . Then
if {v1 , Â· Â· Â· , vn } is an orthonormal basis for X and M (A) denotes the matrix of the linear
âˆ—
transformation A then M (Aâˆ— ) = (M (A)) . In particular, A is self adjoint, if and only if
M (A) is.
Proof: Consider the following picture
A
â†’
X
â—¦
â†‘q
â†’
Fn
M (A)
âˆ‘
where q is the coordinate map which satisï¬es q (x) â‰¡ i xi vi . Therefore, since {v1 , Â· Â· Â· , vn }
is orthonormal, it is clear that |x| = |q (x)| . Therefore,
X
qâ†‘
Fn

2

2

|x| + |y| + 2 Re (x, y)

=

2

2

|x + y| = |q (x + y)|
2

2

= |q (x)| + |q (y)| + 2 Re (q (x) , q (y))

(13.13)

Now in any inner product space,
(x, iy) = Re (x, iy) + i Im (x, iy) .
Also
(x, iy) = (âˆ’i) (x, y) = (âˆ’i) Re (x, y) + Im (x, y) .
Therefore, equating the real parts, Im (x, y) = Re (x, iy) and so
(x, y) = Re (x, y) + i Re (x, iy)

(13.14)

Now from (13.13), since q preserves distances, . Re (q (x) , q (y)) = Re (x, y) which implies
from (13.14) that
(x, y) = (q (x) , q (y)) .
(13.15)
Now consulting the diagram which gives the meaning for the matrix of a linear transformation, observe that q â—¦ M (A) = A â—¦ q and q â—¦ M (Aâˆ— ) = Aâˆ— â—¦ q. Therefore, from (13.15)
(A (q (x)) , q (y)) = (q (x) , Aâˆ— q (y)) = (q (x) , q (M (Aâˆ— ) (y))) = (x, M (Aâˆ— ) (y))
but also
(
)
âˆ—
(A (q (x)) , q (y)) = (q (M (A) (x)) , q (y)) = (M (A) (x) , y) = x, M (A) (y) .
âˆ—

Since x, y are arbitrary, this shows that M (Aâˆ— ) = M (A) as claimed. Therefore, if A is self
âˆ—
âˆ—
adjoint, M (A) = M (Aâˆ— ) = M (A) and so M (A) is also self adjoint. If M (A) = M (A)
âˆ—
âˆ—
then M (A) = M (A ) and so A = A . 
The following corollary is one of the items in the above proof.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

318

SELF ADJOINT OPERATORS

Corollary 13.4.2 Let X be a ï¬nite dimensional Hilbert space and let {v1 , Â· Â· Â· , vn } be an
orthonormal basis
âˆ‘ for X. Also, let q be the coordinate map associated with this basis satisfying q (x) â‰¡ i xi vi . Then (x, y)Fn = (q (x) , q (y))X . Also, if A âˆˆ L (X, X) , and M (A)
is the matrix of A with respect to this basis,
(Aq (x) , q (y))X = (M (A) x, y)Fn .
Deï¬nition 13.4.3 A self adjoint A âˆˆ L (X, X) , is positive deï¬nite if whenever x Ì¸= 0,
(Ax, x) > 0 and A is negative deï¬nite if for all x Ì¸= 0, (Ax, x) < 0. A is positive semidefinite or just nonnegative for short if for all x, (Ax, x) â‰¥ 0. A is negative semideï¬nite or
nonpositive for short if for all x, (Ax, x) â‰¤ 0.
The following lemma is of fundamental importance in determining which linear transformations are positive or negative deï¬nite.
Lemma 13.4.4 Let X be a ï¬nite dimensional Hilbert space. A self adjoint A âˆˆ L (X, X)
is positive deï¬nite if and only if all its eigenvalues are positive and negative deï¬nite if and
only if all its eigenvalues are negative. It is positive semideï¬nite if all the eigenvalues are
nonnegative and it is negative semideï¬nite if all the eigenvalues are nonpositive.
Proof: Suppose ï¬rst that A is positive deï¬nite and let Î» be an eigenvalue. Then for x
an eigenvector corresponding to Î», Î» (x, x) = (Î»x, x) = (Ax, x) > 0. Therefore, Î» > 0 as
claimed.
Now suppose
âˆ‘n all the eigenvalues of A are positive. From Theorem 13.3.3 and Corollary
13.3.6, A =
i=1 Î»i ui âŠ— ui where the Î»i are the positive eigenvalues and {ui } are an
orthonormal set of eigenvectors. Therefore, letting x Ì¸= 0,
)
(( n
)
) ( n
âˆ‘
âˆ‘
Î»i ui âŠ— ui x, x =
Î»i ui (x, ui ) , x
(Ax, x) =
(
=

i=1
n
âˆ‘

i=1

)

Î»i (x, ui ) (ui , x)

i=1

=

n
âˆ‘

2

Î»i |(ui , x)| > 0

i=1
2

âˆ‘n

2

because, since {ui } is an orthonormal basis, |x| = i=1 |(ui , x)| .
To establish the claim about negative deï¬nite, it suï¬ƒces to note that A is negative
deï¬nite if and only if âˆ’A is positive deï¬nite and the eigenvalues of A are (âˆ’1) times the
eigenvalues of âˆ’A. The claims about positive semideï¬nite and negative semideï¬nite are
obtained similarly. 
The next theorem is about a way to recognize whether a self adjoint A âˆˆ L (X, X) is
positive or negative deï¬nite without having to ï¬nd the eigenvalues. In order to state this
theorem, here is some notation.
Deï¬nition 13.4.5 Let A be an n Ã— n matrix. Denote by Ak the k Ã— k matrix obtained by
deleting the k + 1, Â· Â· Â· , n columns and the k + 1, Â· Â· Â· , n rows from A. Thus An = A and Ak
is the k Ã— k submatrix of A which occupies the upper left corner of A. The determinants of
these submatrices are called the principle minors.
The following theorem is proved in [8]
Theorem 13.4.6 Let X be a ï¬nite dimensional Hilbert space and let A âˆˆ L (X, X) be self
adjoint. Then A is positive deï¬nite if and only if det (M (A)k ) > 0 for every k = 1, Â· Â· Â· , n.
Here M (A) denotes the matrix of A with respect to some ï¬xed orthonormal basis of X.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

13.5. FRACTIONAL POWERS

319

Proof: This theorem is proved by induction on n. It is clearly true if n = 1. Suppose then
that it is true for nâˆ’1 where n â‰¥ 2. Since det (M (A)) > 0, it follows that all the eigenvalues
are nonzero. Are they all positive? Suppose not. Then there is some even number of them
which are negative, even because the product of all the eigenvalues is known to be positive,
equaling det (M (A)). Pick two, Î»1 and Î»2 and let M (A) ui = Î»i ui where ui Ì¸= 0 for i = 1, 2
and (u1 , u2 ) = 0. Now if y â‰¡ Î±1 u1 + Î±2 u2 is an element of span (u1 , u2 ) , then since these
are eigenvalues and (u1 , u2 ) = 0, a short computation shows
(M (A) (Î±1 u1 + Î±2 u2 ) , Î±1 u1 + Î±2 u2 )
2

2

2

2

= |Î±1 | Î»1 |u1 | + |Î±2 | Î»2 |u2 | < 0.
Now letting x âˆˆ Cnâˆ’1 , the induction hypothesis implies
(
)
x
âˆ—
(x , 0) M (A)
= xâˆ— M (A)nâˆ’1 x = (M (A) x, x) > 0.
0
Now the dimension of {z âˆˆ Cn : zn = 0} is n âˆ’ 1 and the dimension of span (u1 , u2 ) = 2 and
so there must be some nonzero x âˆˆ Cn which is in both of these subspaces of Cn . However,
the ï¬rst computation would require that (M (A) x, x) < 0 while the second would require
that (M (A) x, x) > 0. This contradiction shows that all the eigenvalues must be positive.
This proves the if part of the theorem. The only if part is left to the reader.
Corollary 13.4.7 Let X be a ï¬nite dimensional Hilbert space and let A âˆˆ L (X, X) be
k
self adjoint. Then A is negative deï¬nite if and only if det (M (A)k ) (âˆ’1) > 0 for every
k = 1, Â· Â· Â· , n. Here M (A) denotes the matrix of A with respect to some ï¬xed orthonormal
basis of X.
Proof: This is immediate from the above theorem by noting that, as in the proof of
Lemma 13.4.4, A is negative deï¬nite if and only if âˆ’A is positive deï¬nite. Therefore, if
det (âˆ’M (A)k ) > 0 for all k = 1, Â· Â· Â· , n, it follows that A is negative deï¬nite. However,
k
det (âˆ’M (A)k ) = (âˆ’1) det (M (A)k ) . 

13.5

Fractional Powers

With the above theory, it is possible to take fractional powers of certain elements of L (X, X)
where X is a ï¬nite dimensional Hilbert space. To begin with, consider the square root of a
nonnegative self adjoint operator. This is easier than the general theory and it is the square
root which is of most importance.
Theorem 13.5.1 Let A âˆˆ L (X, X) be self adjoint and nonnegative. Then there exists a
unique self adjoint nonnegative B âˆˆ L (X, X) such that B 2 = A and B commutes with every
element of L (X, X) which commutes with A.
Proof: By Theorem 13.3.3, there exists an orthonormal basis of
âˆ‘eigenvectors of A, say
n
{vi }i=1 such that Avi = Î»i vi . Therefore, by Theorem 13.2.4, A = i Î»i vi âŠ— vi where each
Î»i â‰¥ 0.
Now by Lemma 13.4.4, each Î»i â‰¥ 0. Therefore, it makes sense to deï¬ne
âˆ‘ 1/2
Bâ‰¡
Î»i vi âŠ— vi .
i

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

320

SELF ADJOINT OPERATORS

It is easy to verify that
{

0 if i Ì¸= j
.
vi âŠ— vi if i = j
âˆ‘
Therefore, a short computation veriï¬es that B 2 = i Î»i vi âŠ— vi = A. If C commutes with
A, then for some cij ,
âˆ‘
C=
cij vi âŠ— vj
(vi âŠ— vi ) (vj âŠ— vj ) =

ij

and so since they commute,
âˆ‘
âˆ‘
âˆ‘
cij vi âŠ— vj Î»k vk âŠ— vk =
cij Î»k Î´ jk vi âŠ— vk =
cik Î»k vi âŠ— vk
i,j,k

=

i,j,k

âˆ‘

cij Î»k vk âŠ— vk vi âŠ— vj =

i,j,k

=

âˆ‘

âˆ‘

i,k

cij Î»k Î´ ki vk âŠ— vj =

i,j,k

âˆ‘

ckj Î»k vk âŠ— vj

j,k

cik Î»i vi âŠ— vk

k,i

Then by independence,
cik Î»i = cik Î»k
1/2
cik Î»i

1/2
cik Î»k

Therefore,
=
which amounts to saying that B also commutes with C. It is
clear that this operator is self adjoint. This proves existence.
Suppose B1 is another square root which is self adjoint, nonnegative and commutes with
every matrix which commutes with A. Since both B, B1 are nonnegative,
(B (B âˆ’ B1 ) x, (B âˆ’ B1 ) x) â‰¥ 0,
(B1 (B âˆ’ B1 ) x, (B âˆ’ B1 ) x) â‰¥ 0

(13.16)

Now, adding these together, and using the fact that the two commute,
(( 2
)
)
B âˆ’ B12 x, (B âˆ’ B1 ) x = ((A âˆ’ A) x, (B âˆ’ B1 ) x) = 0.
It follows that both inner products in (13.16) equal 0. Next
âˆš use
âˆš the existence part of this
to take the square root of B and B1 which is denoted by B, B1 respectively. Then
(âˆš
)
âˆš
0 =
B (B âˆ’ B1 ) x, B (B âˆ’ B1 ) x
(âˆš
)
âˆš
0 =
B1 (B âˆ’ B1 ) x, B1 (B âˆ’ B1 ) x
which implies

âˆš
âˆš
B (B âˆ’ B1 ) x = B1 (B âˆ’ B1 ) x = 0. Thus also,
B (B âˆ’ B1 ) x = B1 (B âˆ’ B1 ) x = 0

Hence
0 = (B (B âˆ’ B1 ) x âˆ’ B1 (B âˆ’ B1 ) x, x) = ((B âˆ’ B1 ) x, (B âˆ’ B1 ) x)
and so, since x is arbitrary, B1 = B. 
The main result is the following theorem.
Theorem 13.5.2 Let A âˆˆ L (X, X) be self adjoint and nonnegative and let k be a positive
integer. Then there exists a unique self adjoint nonnegative B âˆˆ L (X, X) such that B k = A.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

13.5. FRACTIONAL POWERS

321

Proof: By Theorem 13.3.3, there exists an orthonormal basis of eigenvectors of A, say
n
{v
}
âˆ‘i i=1 such that Avi = Î»i vi . Therefore, by Corollary 13.3.6 or Theorem 13.2.4, A =
i Î»i vi âŠ— vi where each Î»i â‰¥ 0.
Now by Lemma 13.4.4, each Î»i â‰¥ 0. Therefore, it makes sense to deï¬ne
âˆ‘ 1/k
Bâ‰¡
Î» i vi âŠ— vi .
i

It is easy to verify that

{

0 if i Ì¸= j
.
vi âŠ— vi if i = j
âˆ‘
Therefore, a short computation veriï¬es that B k = i Î»i vi âŠ— vi = A. This proves existence.
In order to prove uniqueness, let p (t) be a polynomial which has (the property
that
)
1/k
1/k
p (Î»i ) = Î»i for each i. In other words, goes through the ordered pairs Î»i , Î»i
. Then a
similar short computation shows
âˆ‘
âˆ‘ 1/k
p (A) =
p (Î»i ) vi âŠ— vi =
Î»i vi âŠ— vi = B.
(vi âŠ— vi ) (vj âŠ— vj ) =

i

i

Now suppose C = A where C âˆˆ L (X, X) is self adjoint and nonnegative. Then
( )
( )
CB = Cp (A) = Cp C k = p C k C = p (A) C = BC.
k

Therefore, {B, C} is a commuting family of linear transformations which are both self
adjoint. Letting M (B) and M (C) denote matrices of these linear transformations taken
with respect to some ï¬xed orthonormal basis, {v1 , Â· Â· Â· , vn }, it follows that M (B) and M (C)
commute and that both can be diagonalized (Lemma 13.4.1). See the diagram for a short
veriï¬cation of the claim the two matrices commute..
B
C
X
â†’
X
â†’
X
qâ†‘
â—¦
â†‘q
â—¦
â†‘q
Fn
â†’
Fn
â†’
Fn
M (B)
M (C)
Therefore, by Theorem 13.1.9, these two matrices can be simultaneously diagonalized. Thus
U âˆ’1 M (B) U = D1 , U âˆ’1 M (C) U = D2

(13.17)

where the Di is a diagonal matrix consisting of the eigenvalues of B or C. Also it is clear
that
k
M (C) = M (A)
k

because M (C) is given by
k times

z
}|
{
q âˆ’1 Cqq âˆ’1 Cq Â· Â· Â· q âˆ’1 Cq = q âˆ’1 C k q = q âˆ’1 Aq = M (A)
and similarly
k

M (B) = M (A) .
Then raising these to powers,
U âˆ’1 M (A) U = U âˆ’1 M (B) U = D1k
k

and

U âˆ’1 M (A) U = U âˆ’1 M (C) U = D2k .
k

Therefore, D1k = D2k and since the diagonal entries of Di are nonnegative, this requires that
D1 = D2 . Therefore, from (13.17), M (B) = M (C) and so B = C. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

322

13.6

SELF ADJOINT OPERATORS

Polar Decompositions

An application of Theorem 13.3.3, is the following fundamental result, important in geometric measure theory and continuum mechanics. It is sometimes called the right polar
decomposition. The notation used is that which is seen in continuum mechanics, see for
example Gurtin [11]. Donâ€™t confuse the U in this theorem with a unitary transformation.
It is not so. When the following theorem is applied in continuum mechanics, F is normally
the deformation gradient, the derivative of a nonlinear map from some subset of three dimensional space to three dimensional space. In this context, U is called the right Cauchy
Green strain tensor. It is a measure of how a body is stretched independent of rigid motions.
First, here is a simple lemma.
Lemma 13.6.1 Suppose R âˆˆ L (X, Y ) where X, Y are Hilbert spaces and R preserves distances. Then Râˆ— R = I.
Proof: Since R preserves distances, |Rx| = |x| for every x. Therefore from the axioms
of the inner product,
2

2

2

|x| + |y| + (x, y) + (y, x) = |x + y| = (R (x + y) , R (x + y))
= (Rx,Rx) + (Ry,Ry) + (Rx, Ry) + (Ry, Rx)
= |x| + |y| + (Râˆ— Rx, y) + (y, Râˆ— Rx)
2

and so for all x, y,

2

(Râˆ— Rx âˆ’ x, y) + (y,Râˆ— Rx âˆ’ x) = 0

Hence for all x, y,

Re (Râˆ— Rx âˆ’ x, y) = 0

Now for x, y given, choose Î± âˆˆ C such that
Î± (Râˆ— Rx âˆ’ x, y) = |(Râˆ— Rx âˆ’ x, y)|
Then
0

=

Re (Râˆ— Rx âˆ’ x,Î±y) = Re Î± (Râˆ— Rx âˆ’ x, y)

= |(Râˆ— Rx âˆ’ x, y)|
Thus |(Râˆ— Rx âˆ’ x, y)| = 0 for all x, y because the given x, y were arbitrary. Let y =
Râˆ— Rx âˆ’ x to conclude that for all x,
Râˆ— Rx âˆ’ x = 0
which says Râˆ— R = I since x is arbitrary. 
The decomposition in the following is called the right polar decomposition.
Theorem 13.6.2 Let X be a Hilbert space of dimension n and let Y be a Hilbert space of
dimension m â‰¥ n and let F âˆˆ L (X, Y ). Then there exists R âˆˆ L (X, Y ) and U âˆˆ L (X, X)
such that
F = RU, U = U âˆ— , (U is Hermitian),
all eigenvalues of U are non negative,
U 2 = F âˆ— F, Râˆ— R = I,
and |Rx| = |x| .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

13.6. POLAR DECOMPOSITIONS

323

âˆ—

Proof: (F âˆ— F ) = F âˆ— F and so by Theorem 13.3.3, there is an orthonormal basis of
eigenvectors, {v1 , Â· Â· Â· , vn } such that
F âˆ— F vi = Î»i vi , F âˆ— F =

n
âˆ‘

Î»i vi âŠ— vi .

i=1

It is also clear that Î»i â‰¥ 0 because
Î»i (vi , vi ) = (F âˆ— F vi , vi ) = (F vi , F vi ) â‰¥ 0.
Let
Uâ‰¡

n
âˆ‘

1/2

Î»i vi âŠ— vi .

i=1

{
}n
1/2
Then U 2 = F âˆ— F, U = U âˆ— , and the eigenvalues of U, Î»i

are all non negative.

i=1

Let {U x1 , Â· Â· Â· , U xr } be an orthonormal basis for U (X) . By the Gram Schmidt procedure
there exists an extension to an orthonormal basis for X,
{U x1 , Â· Â· Â· , U xr , yr+1 , Â· Â· Â· , yn } .
Next note that {F x1 , Â· Â· Â· , F xr } is also an orthonormal set of vectors in Y because
(
)
(F xk , F xj ) = (F âˆ— F xk , xj ) = U 2 xk , xj = (U xk , U xj ) = Î´ jk .
By the Gram Schmidt procedure, there exists an extension of {F x1 , Â· Â· Â· , F xr } to an orthonormal basis for Y,
{F x1 , Â· Â· Â· , F xr , zr+1 , Â· Â· Â· , zm } .
Since m â‰¥ n, there are at least as many zk as there are yk . Now for x âˆˆ X, since
{U x1 , Â· Â· Â· , U xr , yr+1 , Â· Â· Â· , yn }
is an orthonormal basis for X, there exist unique scalars
c1 , Â· Â· Â· , cr , dr+1 , Â· Â· Â· , dn
such that
x=

r
âˆ‘
k=1

Deï¬ne
Rx â‰¡

r
âˆ‘

2

|Rx| =

r
âˆ‘

dk yk

k=r+1
n
âˆ‘

ck F xk +

k=1

Thus

n
âˆ‘

ck U xk +

dk zk

(13.18)

k=r+1
n
âˆ‘

2

|ck | +

k=1

2

2

|dk | = |x| .

k=r+1

âˆ—

Therefore, by Lemma 13.6.1 R R = I.
Then also there exist scalars bk such that
Ux =

r
âˆ‘

bk U xk

(13.19)

k=1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

324

SELF ADJOINT OPERATORS

and so from (13.18),
RU x =

r
âˆ‘

(
bk F xk = F

k=1

r
âˆ‘

)
bk xk

k=1

âˆ‘r
Is F ( k=1 bk xk ) = F (x)?
( ( r
)
( r
)
)
âˆ‘
âˆ‘
F
bk xk âˆ’ F (x) , F
bk xk âˆ’ F (x)
k=1

k=1

(
=

(
âˆ—

(F F )
(

=

(

U
(

=

U
(

=

2

r
âˆ‘

) (
bk xk âˆ’ x ,

k=1
r
âˆ‘

) (

bk xk âˆ’ x ,

k=1
( r
âˆ‘

)

bk xk âˆ’ x , U

k=1
r
âˆ‘

r
âˆ‘

))
bk xk âˆ’ x

k=1
r
âˆ‘

bk xk âˆ’ x

k=1
( r
âˆ‘

))
bk xk âˆ’ x

k=1

bk U xk âˆ’ U x,

k=1

r
âˆ‘

))

)

bk U xk âˆ’ U x

=0

k=1

âˆ‘r
âˆ‘r
Because from (13.19), U x = k=1 bk U xk . Therefore, RU x = F ( k=1 bk xk ) = F (x). 
The following corollary follows as a simple consequence of this theorem. It is called the
left polar decomposition.
Corollary 13.6.3 Let F âˆˆ L (X, Y ) and suppose n â‰¥ m where X is a Hilbert space of
dimension n and Y is a Hilbert space of dimension m. Then there exists a Hermitian U âˆˆ
L (X, X) , and an element of L (X, Y ) , R, such that
F = U R, RRâˆ— = I.
âˆ—

Proof: Recall that Lâˆ—âˆ— = L and (M L) = Lâˆ— M âˆ— . Now apply Theorem 13.6.2 to
âˆ—
F âˆˆ L (Y, X). Thus,
F âˆ— = Râˆ— U
where Râˆ— and U satisfy the conditions of that theorem. Then
F = UR
and RRâˆ— = Râˆ—âˆ— Râˆ— = I. 
The following existence theorem for the polar decomposition of an element of L (X, X)
is a corollary.
Corollary 13.6.4 Let F âˆˆ L (X, X). Then there exists a Hermitian W âˆˆ L (X, X) , and
a unitary matrix Q such that F = W Q, and there exists a Hermitian U âˆˆ L (X, X) and a
unitary R, such that F = RU.
This corollary has a fascinating relation to the question whether a given linear transformation is normal. Recall that an n Ã— n matrix A, is normal if AAâˆ— = Aâˆ— A. Retain the same
deï¬nition for an element of L (X, X) .
Theorem 13.6.5 Let F âˆˆ L (X, X) . Then F is normal if and only if in Corollary 13.6.4
RU = U R and QW = W Q.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

13.7. AN APPLICATION TO STATISTICS

325

Proof: I will prove the statement about RU = U R and leave the other part as an
exercise. First suppose that RU = U R and show F is normal. To begin with,
âˆ—

âˆ—

U Râˆ— = (RU ) = (U R) = Râˆ— U.
Therefore,
F âˆ—F

=

U Râˆ— RU = U 2

FFâˆ—

=

RU U Râˆ— = U RRâˆ— U = U 2

which shows F is normal.
Now suppose F is normal. Is RU = U R? Since F is normal,
F F âˆ— = RU U Râˆ— = RU 2 Râˆ—
and

F âˆ— F = U Râˆ— RU = U 2 .

Therefore, RU 2 Râˆ— = U 2 , and both are nonnegative and self adjoint. Therefore, the square
roots of both sides must be equal by the uniqueness part of the theorem on fractional powers.
It follows that the square root of the ï¬rst, RU Râˆ— must equal the square root of the second,
U. Therefore, RU Râˆ— = U and so RU = U R. This proves the theorem in one case. The other
case in which W and Q commute is left as an exercise. 

13.7

An Application To Statistics

A random vector is a function X : â„¦ â†’ Rp where â„¦ is a probability space. This means
that there exists a Ïƒ algebra of measurable sets F and a probability measure P : F â†’ [0, 1].
In practice, people often donâ€™t worry too much about the underlying probability space and
instead pay more attention to the distribution measure of the random variable. For E a
suitable subset of Rp , this measure gives the probability that X has values in E. There
are often excellent reasons for believing that a random vector is normally distributed. This
means that the probability that X has values in a set E is given by
(
)
âˆ«
1
1
âˆ— âˆ’1
exp âˆ’ (x âˆ’ m) Î£ (x âˆ’ m) dx
p/2
1/2
2
E (2Ï€)
det (Î£)
The expression in the integral is called the normal probability density function. There are
two parameters, m and Î£ where m is called the mean and Î£ is called the covariance matrix.
It is a symmetric matrix which has all real eigenvalues which are all positive. While it may
be reasonable to assume this is the distribution, in general, you wonâ€™t know m and Î£ and
in order to use this formula to predict anything, you would need to know these quantities.
What people do to estimate these is to take n independent observations x1 , Â· Â· Â· , xn and
try to predict what m and Î£ should be based on these observations. One criterion used for
making this determination is the method of maximum likelihood. In this method, you seek
to choose the two parameters in such a way as to maximize the likelihood which is given as
(
)
n
âˆ
1
1
âˆ— âˆ’1
exp
âˆ’
(x
âˆ’m)
Î£
(x
âˆ’m)
.
i
i
1/2
2
i=1 det (Î£)
For convenience the term (2Ï€)

p/2

was ignored. This leads to the estimate for m as
1âˆ‘
xi â‰¡ x.
n i=1
n

m=

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

326

SELF ADJOINT OPERATORS

This part follows fairly easily from taking the ln and then setting partial derivatives equal to
0. The estimation of Î£ is harder. However, it is not too hard using the theorems presented
above. I am following a nice discussion given in Wikipedia. It will make use of Theorem
7.5.3 on the trace as well as the theorem about the square root of a linear transformation
given above. First note that by Theorem 7.5.3,
(
)
âˆ—
âˆ—
(xi âˆ’m) Î£âˆ’1 (xi âˆ’m) = trace (xi âˆ’m) Î£âˆ’1 (xi âˆ’m)
(
)
âˆ—
= trace (xi âˆ’m) (xi âˆ’m) Î£âˆ’1
Therefore, the thing to maximize is
n
âˆ
i=1

=

=

1
det (Î£)

1/2

(
)
(
1
âˆ— âˆ’1 )
exp âˆ’ trace (xi âˆ’m) (xi âˆ’m) Î£
2

)
n
âˆ‘
1
âˆ— âˆ’1
det Î£
exp âˆ’ trace
(xi âˆ’m) (xi âˆ’m) Î£
2
i=1
ï£«
ï£¶
S
}|
{
z
n
ï£¬ 1
ï£·
âˆ‘
(
)n/2
ï£¬
ï£·
âˆ—
det Î£âˆ’1
exp ï£¬âˆ’ trace
(xi âˆ’m) (xi âˆ’m) Î£âˆ’1 ï£·
ï£­ 2
ï£¸
i=1
(

)
âˆ’1 n/2

(

)
âˆ’1 n/2

â‰¡ det Î£

(

(

(
)
1
exp âˆ’ trace SÎ£âˆ’1
2

)

where S is the p Ã— p matrix indicated above. Now S is symmetric and has eigenvalues which
are all nonnegative because (Sy, y) â‰¥ 0. Therefore, S has a unique self adjoint square root.
Using Theorem 7.5.3 again, the above equals
(
(
))
( âˆ’1 )n/2
1
1/2 âˆ’1 1/2
det Î£
exp âˆ’ trace S Î£ S
2
Let B = S 1/2 Î£âˆ’1 S 1/2 and assume det (S) Ì¸= 0. Then Î£âˆ’1 = S âˆ’1/2 BS âˆ’1/2 . The above
equals
(
)
(
)
1
n/2
det S âˆ’1 det (B)
exp âˆ’ trace (B)
2
(
)
Of course the thing to estimate is only found in B. Therefore, det S âˆ’1 can be discarded
in trying to maximize things. Since B is symmetric, it is similar to a diagonal matrix D
which has Î»1 , Â· Â· Â· , Î»n down the diagonal. Thus it is desired to maximize
( p
(
)n/2
)
p
âˆ
1âˆ‘
Î»i
exp âˆ’
Î»i
2 i=1
i=1
Taking ln it follows that it suï¬ƒces to maximize
1âˆ‘
nâˆ‘
ln Î»i âˆ’
Î»i
2 i=1
2 i=1
p

p

Taking the derivative with respect to Î»i ,
1
n 1
âˆ’ =0
2 Î»i
2

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

13.8. THE SINGULAR VALUE DECOMPOSITION

327

and so Î»i = n. It follows from the above that
Î£ = S 1/2 B âˆ’1 S 1/2
where B âˆ’1 has only the eigenvalues 1/n. It follows B âˆ’1 must equal the diagonal matrix
which has 1/n down the diagonal. The reason for this is that B is similar to a diagonal
matrix because it is symmetric. Thus B = P âˆ’1 n1 IP = n1 I because the identity commutes
with every matrix. But now it follows that
Î£=

1
S
n

Of course this is just an estimate and so we write Î£Ì‚ instead of Î£.
This has shown that the maximum likelihood estimate for Î£ is
1âˆ‘
âˆ—
Î£Ì‚ =
(xi âˆ’m) (xi âˆ’m)
n i=1
n

13.8

The Singular Value Decomposition

In this section, A will be an m Ã— n matrix. To begin with, here is a simple lemma.
Lemma 13.8.1 Let A be an m Ã— n matrix. Then Aâˆ— A is self adjoint and all its eigenvalues
are nonnegative.
Proof: It is obvious that Aâˆ— A is self adjoint. Suppose Aâˆ— Ax = Î»x. Then Î» |x| =
(Î»x, x) = (Aâˆ— Ax, x) = (Ax,Ax) â‰¥ 0. 
2

Deï¬nition 13.8.2 Let A be an m Ã— n matrix. The singular values of A are the square roots
of the positive eigenvalues of Aâˆ— A.
With this deï¬nition and lemma here is the main theorem on the singular value decomposition. In all that follows, I will write the following partitioned matrix
(
)
Ïƒ 0
0 0
where Ïƒ denotes an r Ã— r diagonal matrix of the form
ï£«
ï£¶
Ïƒ1
0
ï£¬
ï£·
..
ï£­
ï£¸
.
0
Ïƒk
and the bottom row of zero matrices in the partitioned matrix, as well as the right columns
of zero matrices are each of the right size so that the resulting matrix is m Ã— n. Either
could vanish completely. However, I will write it in the above form. It is easy to make the
necessary adjustments in the other two cases.
Theorem 13.8.3 Let A be an m Ã— n matrix. Then there exist unitary matrices, U and V
of the appropriate size such that
(
)
Ïƒ 0
âˆ—
U AV =
0 0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

328

SELF ADJOINT OPERATORS

where Ïƒ is of the form

ï£«
ï£¬
Ïƒ=ï£­

Ïƒ1

0
..

.

0

ï£¶
ï£·
ï£¸

Ïƒk

for the Ïƒ i the singular values of A, arranged in order of decreasing size.
Proof: By the above lemma and Theorem 13.3.3 there exists an orthonormal basis,
n
{vi }i=1 such that Aâˆ— Avi = Ïƒ 2i vi where Ïƒ 2i > 0 for i = 1, Â· Â· Â· , k, (Ïƒ i > 0) , and equals zero if
i > k. Thus for i > k, Avi = 0 because
(Avi , Avi ) = (Aâˆ— Avi , vi ) = (0, vi ) = 0.
For i = 1, Â· Â· Â· , k, deï¬ne ui âˆˆ Fm by
ui â‰¡ Ïƒ âˆ’1
i Avi .
Thus Avi = Ïƒ i ui . Now
(ui , uj )

)
) ( âˆ’1
âˆ’1 âˆ—
âˆ’1
Ïƒ âˆ’1
i Avi , Ïƒ j Avj = Ïƒ i vi , Ïƒ j A Avj
(
) Ïƒj
âˆ’1 2
= Ïƒ âˆ’1
(vi , vj ) = Î´ ij .
i vi , Ïƒ j Ïƒ j vj =
Ïƒi

=

(

k

Thus {ui }i=1 is an orthonormal set of vectors in Fm . Also,
âˆ’1
âˆ’1
âˆ—
2
2
AAâˆ— ui = AAâˆ— Ïƒ âˆ’1
i Avi = Ïƒ i AA Avi = Ïƒ i AÏƒ i vi = Ïƒ i ui .
k

m

Now extend {ui }i=1 to an orthonormal basis for all of Fm , {ui }i=1 and let
(
)
U â‰¡ u1 Â· Â· Â· um
while
V â‰¡

(

v1

Â·Â·Â·

vn

)

.

Thus U is the matrix which has the ui as columns and V is deï¬ned as the matrix which has
the vi as columns. Then
ï£« âˆ— ï£¶
u1
ï£¬ .. ï£·
ï£¬ . ï£·
ï£¬ âˆ— ï£· (
)
âˆ—
ï£·
U AV = ï£¬
ï£¬ u k ï£· A v1 Â· Â· Â· vn
ï£¬ . ï£·
ï£­ .. ï£¸
uâˆ—m
ï£«

ï£¶
uâˆ—1
ï£¬ .. ï£·
ï£¬ . ï£·
ï£¬ âˆ— ï£·(
ï£·
=ï£¬
ï£¬ uk ï£· Ïƒ 1 u1
ï£¬ . ï£·
ï£­ .. ï£¸
uâˆ—m

Â·Â·Â·

Ïƒ k uk

0 Â·Â·Â·

0

)

(
=

Ïƒ
0

0
0

)

where Ïƒ is given in the statement of the theorem. 
The singular value decomposition has as an immediate corollary the following interesting
result.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

13.9. APPROXIMATION IN THE FROBENIUS NORM

329

Corollary 13.8.4 Let A be an m Ã— n matrix. Then the rank of A and Aâˆ— equals the number
of singular values.
Proof: Since V and U are unitary, they are each one to one and onto and so it follows
that
)
(
Ïƒ 0
âˆ—
rank (A) = rank (U AV ) = rank
= number of singular values.
0 0
Also since U, V are unitary,
(
âˆ—)
rank (Aâˆ— ) = rank (V âˆ— Aâˆ— U ) = rank (U âˆ— AV )
((

Ïƒ
0

= rank

13.9

0
0

)âˆ— )
= number of singular values. 

Approximation In The Frobenius Norm

The Frobenius norm is one of many norms for a matrix. It is arguably the most obvious of
all norms. Here is its deï¬nition.
Deï¬nition 13.9.1 Let A be a complex m Ã— n matrix. Then
||A||F â‰¡ (trace (AAâˆ— ))

1/2

Also this norm comes from the inner product
(A, B)F â‰¡ trace (AB âˆ— )
2

Thus ||A||F is easily seen to equal
in FmÃ—n .

âˆ‘

2

ij

|aij | so essentially, it treats the matrix as a vector

Lemma 13.9.2 Let A be an m Ã— n complex matrix with singular matrix
(
)
Ïƒ 0
Î£=
0 0
with Ïƒ as deï¬ned above. Then
2

2

||Î£||F = ||A||F

(13.20)

and the following hold for the Frobenius norm. If U, V are unitary and of the right size,
||U A||F = ||A||F , ||U AV ||F = ||A||F .

(13.21)

Proof: From the deï¬nition and letting U, V be unitary and of the right size,
||U A||F â‰¡ trace (U AAâˆ— U âˆ— ) = trace (AAâˆ— ) = ||A||F
2

Also,

2

||AV ||F â‰¡ trace (AV V âˆ— Aâˆ— ) = trace (AAâˆ— ) = ||A||F .
2

2

It follows
2

2

2

||U AV ||F = ||AV ||F = ||A||F .
Now consider (13.20). From what was just shown,
||A||F = ||U Î£V âˆ— ||F = ||Î£||F . 
2

Saylor URL: http://www.saylor.org/courses/ma212/

2

2

The Saylor Foundation

330

SELF ADJOINT OPERATORS

Of course, this shows that
2

||A||F =

âˆ‘

Ïƒ 2i ,

i

the sum of the squares of the singular values of A.
Why is the singular value decomposition important? It implies
(
)
Ïƒ 0
A=U
Vâˆ—
0 0
where Ïƒ is the diagonal matrix having the singular values down the diagonal. Now sometimes
A is a huge matrix, 1000Ã—2000 or something like that. This happens in applications to
situations where the entries of A describe a picture. What also happens is that most of the
singular values are very small. What if you deleted those which were very small, say for all
i â‰¥ l and got a new matrix
( â€²
)
Ïƒ 0
Aâ€² â‰¡ U
V âˆ—?
0 0
Then the entries of Aâ€² would end up being close to the entries of A but there is much less
information to keep track of. This turns out to be very useful. More precisely, letting
ï£«
ï£¶
Ïƒ1
0
(
)
Ïƒ 0
ï£¬
ï£·
âˆ—
.
.
Ïƒ=ï£­
,
ï£¸ , U AV =
.
0 0
0
Ïƒr
||A âˆ’ Aâ€² ||F = U
2

(

Ïƒ âˆ’ Ïƒâ€²
0

â€²

0
0

)

Vâˆ—

2

=
F

r
âˆ‘

Ïƒ 2k

k=l+1

â€²

Thus A is approximated by A where A has rank l < r. In fact, it is also true that out
of all matrices of rank l, this Aâ€² is the one which is closest to A in the Frobenius norm. Here
is why.
Let B be a matrix which has rank l. Then from Lemma 13.9.2
(
)
2
Ïƒ 0
2
2
2
||A âˆ’ B||F = ||U âˆ— (A âˆ’ B) V ||F = ||U âˆ— AV âˆ’ U âˆ— BV ||F =
âˆ’ U âˆ— BV
0 0
F
and since the singular values of A decrease from the upper left to the lower right, it follows
that for B to be closest as possible to A in the Frobenius norm,
( â€²
)
Ïƒ 0
âˆ—
U BV =
0 0
which implies B = Aâ€² above. This is really obvious if you look at a simple example. Say
ï£«
ï£¶
(
)
3 0 0 0
Ïƒ 0
=ï£­ 0 2 0 0 ï£¸
0 0
0 0 0 0
for example. Then what rank 1 matrix would
Obviously
ï£«
3 0
ï£­ 0 0
0 0

Saylor URL: http://www.saylor.org/courses/ma212/

be closest to this one in the Frobenius norm?
0
0
0

ï£¶
0
0 ï£¸
0

The Saylor Foundation

13.10. LEAST SQUARES AND SINGULAR VALUE DECOMPOSITION

13.10

331

Least Squares And Singular Value Decomposition

The singular value decomposition also has a very interesting connection to the problem of
least squares solutions. Recall that it was desired to ï¬nd x such that |Ax âˆ’ y| is as small as
possible. Lemma 12.5.1 shows that there is a solution to this problem which can be found by
solving the system Aâˆ— Ax = Aâˆ— y. Each x which solves this system solves the minimization
problem as was shown in the lemma just mentioned. Now consider this equation for the
solutions of the minimization problem in terms of the singular value decomposition.
Aâˆ—

Aâˆ—

A

z ( }| ) {z ( }| ) {
z ( }| ) {
Ïƒ 0
Ïƒ
0
Ïƒ 0
V
U âˆ—U
V âˆ—x = V
U âˆ— y.
0 0
0 0
0 0
Therefore, this yields the following upon using block multiplication and multiplying on the
left by V âˆ— .
( 2
)
(
)
Ïƒ 0
Ïƒ 0
V âˆ—x =
U âˆ— y.
(13.22)
0 0
0 0
One solution to this equation which is very easy to spot is
( âˆ’1
)
Ïƒ
0
x=V
U âˆ— y.
0
0

13.11

(13.23)

The Moore Penrose Inverse

The particular solution of the least squares problem given in (13.23) is important enough
that it motivates the following deï¬nition.
Deï¬nition 13.11.1 Let A be an m Ã— n matrix. Then the Moore Penrose inverse of A,
denoted by A+ is deï¬ned as
( âˆ’1
)
Ïƒ
0
+
A â‰¡V
U âˆ—.
0
0
(

Here
âˆ—

U AV =

Ïƒ
0

)

0
0

as above.
Thus A+ y is a solution to the minimization problem to ï¬nd x which minimizes |Ax âˆ’ y| .
In fact, one can say more about this. In the following picture My denotes the set of least
squares solutions x such that Aâˆ— Ax = Aâˆ— y.
A+ (y)
I
My
x



ker(Aâˆ— A)

Then A+ (y) is as given in the picture.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

332

SELF ADJOINT OPERATORS

Proposition 13.11.2 A+ y is the solution to the problem of minimizing |Ax âˆ’ y| for all x
which has smallest norm. Thus
AA+ y âˆ’ y â‰¤ |Ax âˆ’ y| for all x
and if x1 satisï¬es |Ax1 âˆ’ y| â‰¤ |Ax âˆ’ y| for all x, then |A+ y| â‰¤ |x1 | .
Proof: Consider x satisfying (13.22), equivalently Aâˆ— Ax =Aâˆ— y,
( 2
)
(
)
Ïƒ 0
Ïƒ 0
V âˆ—x =
U âˆ—y
0 0
0 0
which has smallest norm. This is equivalent to making |V âˆ— x| as small as possible because
V âˆ— is unitary and so it preserves norms. For z a vector, denote by (z)k the vector in Fk
which consists of the ï¬rst k entries of z. Then if x is a solution to (13.22)
( 2 âˆ—
) (
)
Ïƒ (V x)k
Ïƒ (U âˆ— y)k
=
0
0
and so (V âˆ— x)k = Ïƒ âˆ’1 (U âˆ— y)k . Thus the ï¬rst k entries of V âˆ— x are determined. In order to
make |V âˆ— x| as small as possible, the remaining n âˆ’ k entries should equal zero. Therefore,
(
) ( âˆ’1 âˆ—
) ( âˆ’1
)
(V âˆ— x)k
Ïƒ (U y)k
Ïƒ
0
âˆ—
V x=
=
=
U âˆ—y
0
0
0
0
(

and so
x=V

Ïƒ âˆ’1
0

0
0

)

U âˆ— y â‰¡ A+ y 

Lemma 13.11.3 The matrix A+ satisï¬es the following conditions.
AA+ A = A, A+ AA+ = A+ , A+ A and AA+ are Hermitian.
Proof: This is routine. Recall

(
A=U
(

and
+

A =V

Ïƒ
0
Ïƒ âˆ’1
0

0
0

)

0
0

(13.24)

Vâˆ—
)

Uâˆ—

so you just plug in and verify it works. 
A much more interesting observation is that A+ is characterized as being the unique
matrix which satisï¬es (13.24). This is the content of the following Theorem. The conditions
are sometimes called the Penrose conditions.
Theorem 13.11.4 Let A be an m Ã— n matrix. Then a matrix A0 , is the Moore Penrose
inverse of A if and only if A0 satisï¬es
AA0 A = A, A0 AA0 = A0 , A0 A and AA0 are Hermitian.

(13.25)

Proof: From the above lemma, the Moore Penrose inverse satisï¬es (13.25). Suppose
then that A0 satisï¬es (13.25). It is necessary to verify that A0 = A+ . Recall that from the
singular value decomposition, there exist unitary matrices, U and V such that
(
)
Ïƒ 0
âˆ—
U AV = Î£ â‰¡
, A = U Î£V âˆ— .
0 0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

13.11. THE MOORE PENROSE INVERSE

333

Let
V âˆ— A0 U =

(

P
R

)

Q
S

(13.26)

where P is k Ã— k.
Next use the ï¬rst equation of (13.25) to write
A0

z
}|
{ A
A
z }| { ( P Q ) z }| { z }| {
âˆ—
âˆ—
âˆ—
U Î£V V
U U Î£V = U Î£V âˆ— .
R S
A

Then multiplying both sides on the left by U âˆ— and on the right by V,
(
)(
)(
) (
)
Ïƒ 0
P Q
Ïƒ 0
Ïƒ 0
=
0 0
R S
0 0
0 0
Now this requires

(

ÏƒP Ïƒ
0

0
0

)

(
=

Ïƒ
0

)

0
0

.

(13.27)

Therefore, P = Ïƒ âˆ’1 . From the requirement that AA0 is Hermitian,
A0

}|
{
z
(
z }| { ( P Q )
Ïƒ
âˆ—
âˆ—
U Î£V V
U =U
R S
0
A

0
0

)(

P
R

Q
S

)

Uâˆ—

must be Hermitian. Therefore, it is necessary that
(
)(
)
(
)
Ïƒ 0
P Q
ÏƒP ÏƒQ
=
0 0
R S
0
0
(
)
I ÏƒQ
=
0 0
(

is Hermitian. Then

I
0

ÏƒQ
0

Thus

)

(
=

I
Qâˆ— Ïƒ

0
0

)

Qâˆ— Ïƒ = 0

and so multiplying both sides on the right by Ïƒ âˆ’1 , it follows Qâˆ— = 0 and so Q = 0.
From the requirement that A0 A is Hermitian, it is necessary that
A0

z (
}| ) { A
z }| {
P Q
V
U âˆ— U Î£V âˆ— =
R S

(
V
(

=
is Hermitian. Therefore, also

(

I
RÏƒ

0
0

V

0
0

I
RÏƒ

0
0

)
)

Vâˆ—
Vâˆ—

)

is Hermitian. Thus R = 0 because this equals
(
)âˆ— (
I
0
I
=
RÏƒ 0
0

Saylor URL: http://www.saylor.org/courses/ma212/

PÏƒ
RÏƒ

Ïƒ âˆ— Râˆ—
0

)

The Saylor Foundation

334

SELF ADJOINT OPERATORS

which requires RÏƒ = 0. Now multiply on right by Ïƒ âˆ’1 to ï¬nd that R = 0.
Use (13.26) and the second equation of (13.25) to write
A0

A0

A0

z (
}| ) { A z (
}| ) { z (
}| ) {
z }| {
P Q
P
Q
P
Q
U âˆ— U Î£V âˆ— V
Uâˆ— = V
U âˆ—.
V
R S
R S
R S
which implies

(

P
R

Q
S

)(

Ïƒ
0

0
0

)(

P
R

Q
S

)

(
=

P
R

Q
S

)
.

This yields from the above in which is was shown that R, Q are both 0
( âˆ’1
)(
) ( âˆ’1
)
( âˆ’1
)
Ïƒ
0
Ïƒ 0
Ïƒ
0
Ïƒ
0
=
0
S
0
0
0
S
0 0
( âˆ’1
)
Ïƒ
0
=
.
0
S
Therefore, S = 0 also and so

(

âˆ—

V A0 U â‰¡
which says

(
A0 = V

P
R
Ïƒ âˆ’1
0

Q
S
0
0

)

(
=

)

Ïƒ âˆ’1
0

0
0

(13.28)
(13.29)

)

U âˆ— â‰¡ A+ . 

The theorem is signiï¬cant because there is no mention of eigenvalues or eigenvectors in
the characterization of the Moore Penrose inverse given in (13.25). It also shows immediately
that the Moore Penrose inverse is a generalization of the usual inverse. See Problem 3.

13.12

Exercises
âˆ—

âˆ—

1. Show (Aâˆ— ) = A and (AB) = B âˆ— Aâˆ— .
2. Prove Corollary 13.3.9.
3. Show that if A is an n Ã— n matrix which has an inverse then A+ = Aâˆ’1 .
4. Using the singular value decomposition, show that for any square matrix A, it follows
that Aâˆ— A is unitarily similar to AAâˆ— .
5. Let A, B be a m Ã— n matrices. Deï¬ne an inner product on the set of m Ã— n matrices
by
(A, B)F â‰¡ trace (AB âˆ— ) .
Show this is an inner product
âˆ‘nsatisfying all the inner product axioms. Recall for M an
n Ã— n matrix, trace (M ) â‰¡ i=1 Mii . The resulting norm, ||Â·||F is called the Frobenius
norm and it can be used to measure the distance between two matrices.
âˆ‘
2
6. Let A be an m Ã— n matrix. Show ||A||F â‰¡ (A, A)F = j Ïƒ 2j where the Ïƒ j are the
singular values of A.
7. If A is a general n Ã— n matrix having possibly repeated eigenvalues, show there is a
sequence {Ak } of n Ã— n matrices having distinct eigenvalues which has the property
that the ij th entry of Ak converges to the ij th entry of A for all ij. Hint: Use Schurâ€™s
theorem.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

13.12. EXERCISES

335

8. Prove the Cayley Hamilton theorem as follows. First suppose A has a basis of eigenn
vectors {vk }k=1 , Avk = Î»k vk . Let p (Î») be the characteristic polynomial. Show
p (A) vk = p (Î»k ) vk = 0. Then since {vk } is a basis, it follows p (A) x = 0 for all
x and so p (A) = 0. Next in the general case, use Problem 7 to obtain a sequence {Ak }
of matrices whose entries converge to the entries of A such that Ak has n distinct
eigenvalues and therefore by Theorem 7.1.7 Ak has a basis of eigenvectors. Therefore, from the ï¬rst part and for pk (Î») the characteristic polynomial for Ak , it follows
pk (Ak ) = 0. Now explain why and the sense in which limkâ†’âˆ pk (Ak ) = p (A) .
9. Prove that Theorem 13.4.6 and Corollary 13.4.7 can be strengthened so that the
condition
( on
) the Ak is necessary as well as suï¬ƒcient. Hint: Consider vectors of the
x
form
where x âˆˆ Fk .
0
10. Show directly that if A is an n Ã— n matrix and A = Aâˆ— (A is Hermitian) then all the
eigenvalues are real and eigenvectors can be assumed to be real and that eigenvectors
associated with distinct eigenvalues are orthogonal, (their inner product is zero).
11. Let v1 , Â· Â· Â· , vn be an orthonormal basis for Fn . Let Q be a matrix whose ith column
is vi . Show
Qâˆ— Q = QQâˆ— = I.
12. Show that an n Ã— n matrix Q is unitary if and only if it preserves distances. This
means |Qv| = |v| . This was done in the text but you should try to do it for yourself.
13. Suppose {v1 , Â· Â· Â· , vn } and {w1 , Â· Â· Â· , wn } are two orthonormal bases for Fn and suppose Q is an n Ã— n matrix satisfying Qvi = wi . Then show Q is unitary. If |v| = 1,
show there is a unitary transformation which maps v to e1 .
14. Finish the proof of Theorem 13.6.5.
15. Let A be a Hermitian matrix so A = Aâˆ— and suppose all eigenvalues of A are larger
than Î´ 2 . Show
2
(Av, v) â‰¥ Î´ 2 |v|
âˆ‘n
Where here, the inner product is (v, u) â‰¡ j=1 vj uj .
16. Suppose A + Aâˆ— has all negative eigenvalues. Then show that the eigenvalues of A
have all negative real parts.
17. The discrete Fourier transform maps Cn â†’ Cn as follows.
nâˆ’1
1 âˆ‘ âˆ’i 2Ï€ jk
F (x) = z where zk = âˆš
e n xj .
n j=0

Show that F âˆ’1 exists and is given by the formula
nâˆ’1
1 âˆ‘ i 2Ï€ jk
e n zk
F âˆ’1 (z) = x where xj = âˆš
n j=0

Here is one way to approach this problem. Note z = U x
ï£«
2Ï€
2Ï€
2Ï€
eâˆ’i n 0Â·0
eâˆ’i n 1Â·0
eâˆ’i n 2Â·0
2Ï€
2Ï€
2Ï€
ï£¬
eâˆ’i n 1Â·1
eâˆ’i n 2Â·1
ï£¬ eâˆ’i n 0Â·1
2Ï€
2Ï€
1 ï£¬ eâˆ’i 2Ï€
n 0Â·2
eâˆ’i n 1Â·2
eâˆ’i n 2Â·2
U=âˆš ï£¬
ï£¬
nï£¬
..
..
..
ï£­
.
.
.
2Ï€
2Ï€
2Ï€
eâˆ’i n 0Â·(nâˆ’1) eâˆ’i n 1Â·(nâˆ’1) eâˆ’i n 2Â·(nâˆ’1)

Saylor URL: http://www.saylor.org/courses/ma212/

where
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·

eâˆ’i n (nâˆ’1)Â·0
2Ï€
eâˆ’i n (nâˆ’1)Â·1
2Ï€
eâˆ’i n (nâˆ’1)Â·2
..
.

Â·Â·Â·

eâˆ’i n (nâˆ’1)Â·(nâˆ’1)

2Ï€

ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

2Ï€

The Saylor Foundation

336

SELF ADJOINT OPERATORS

Now argue U is unitary and use this to establish the result. To show this verify
each row has length 1 and the inner product of two diï¬€erent rows gives 0. Now
2Ï€
2Ï€
Ukj = eâˆ’i n jk and so (U âˆ— )kj = ei n jk .
18. Let f be a periodic function having period 2Ï€. The Fourier series of f is an expression
of the form
âˆ
n
âˆ‘
âˆ‘
ck eikx â‰¡ lim
ck eikx
nâ†’âˆ

k=âˆ’âˆ

k=âˆ’n

and the idea is to ï¬nd ck such that the above sequence converges in some way to f . If
f (x) =

âˆ
âˆ‘

ck eikx

k=âˆ’âˆ

and you formally multiply both sides by eâˆ’imx and then integrate from 0 to 2Ï€,
interchanging the integral with the sum without any concern for whether this makes
sense, show it is reasonable from this to expect
âˆ« 2Ï€
1
cm =
f (x) eâˆ’imx dx.
2Ï€ 0
Now suppose you only know f (x) at equally spaced points 2Ï€j/n for j = 0, 1, Â· Â· Â· , n.
Consider the Riemann sum for this integral obtained
{
}n from using the left endpoint of
the subintervals determined from the partition 2Ï€
j
. How does this compare with
n
j=0
the discrete Fourier transform? What happens as n â†’ âˆ to this approximation?
19. Suppose A is a real 3 Ã— 3 orthogonal matrix (Recall this means AAT = AT A = I. )
having determinant 1. Show it must have an eigenvalue equal to 1. Note this shows
there exists a vector x Ì¸= 0 such that Ax = x. Hint: Show ï¬rst or recall that any
orthogonal matrix must preserve lengths. That is, |Ax| = |x| .
20. Let A be a complex m Ã— n matrix. Using the description of the Moore Penrose inverse
in terms of the singular value decomposition, show that
âˆ’1

lim (Aâˆ— A + Î´I)

Î´â†’0+

Aâˆ— = A+

where the convergence happens in the Frobenius norm. Also verify, using the singular
value decomposition, that the inverse exists in the above formula.
21. Show that A+ = (Aâˆ— A) Aâˆ— . Hint: You might use the description of A+ in terms of
the singular value decomposition.
+

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Norms For Finite Dimensional
Vector Spaces
In this chapter, X and Y are ï¬nite dimensional vector spaces which have a norm. The
following is a deï¬nition.
Deï¬nition 14.0.1 A linear space X is a normed linear space if there is a norm deï¬ned on
X, ||Â·|| satisfying
||x|| â‰¥ 0, ||x|| = 0 if and only if x = 0,
||x + y|| â‰¤ ||x|| + ||y|| ,
||cx|| = |c| ||x||
whenever c is a scalar. A set, U âŠ† X, a normed linear space is open if for every p âˆˆ U,
there exists Î´ > 0 such that
B (p, Î´) â‰¡ {x : ||x âˆ’ p|| < Î´} âŠ† U.
Thus, a set is open if every point of the set is an interior point.
To begin with recall the Cauchy Schwarz inequality which is stated here for convenience
in terms of the inner product space, Cn .
Theorem 14.0.2 The following inequality holds for ai and bi âˆˆ C.
n
âˆ‘

(
ai bi â‰¤

i=1

n
âˆ‘

|ai |

2

)1/2 ( n
âˆ‘

i=1

)1/2
2

|bi |

.

(14.1)

i=1
âˆ

Deï¬nition 14.0.3 Let (X, ||Â·||) be a normed linear space and let {xn }n=1 be a sequence of
vectors. Then this is called a Cauchy sequence if for all Îµ > 0 there exists N such that if
m, n â‰¥ N, then
||xn âˆ’ xm || < Îµ.
This is written more brieï¬‚y as
lim ||xn âˆ’ xm || = 0.

m,nâ†’âˆ

Deï¬nition 14.0.4 A normed linear space, (X, ||Â·||) is called a Banach space if it is complete. This means that, whenever, {xn } is a Cauchy sequence there exists a unique x âˆˆ X
such that limnâ†’âˆ ||x âˆ’ xn || = 0.
337

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

338

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

Let X be a ï¬nite dimensional normed linear space with norm ||Â·|| where the ï¬eld of
scalars is denoted by F and is understood to be either R or C. Let {v1 ,Â· Â· Â· , vn } be a basis
for X. If x âˆˆ X, denote by xi the ith component of x with respect to this basis. Thus
n
âˆ‘

x=

xi vi .

i=1

Deï¬nition 14.0.5 For x âˆˆ X and {v1 , Â· Â· Â· , vn } a basis, deï¬ne a new norm by
( n
âˆ‘

|x| â‰¡

)1/2
|xi |

2

.

i=1

where

n
âˆ‘

x=

xi vi .

i=1

Similarly, for y âˆˆ Y with basis {w1 , Â· Â· Â· , wm }, and yi its components with respect to this
basis,
(m
)1/2
âˆ‘
2
|y| â‰¡
|yi |
i=1

For A âˆˆ L (X, Y ) , the space of linear mappings from X to Y,
||A|| â‰¡ sup{|Ax| : |x| â‰¤ 1}.

(14.2)

The ï¬rst thing to show is that the two norms, ||Â·|| and |Â·| , are equivalent. This means
the conclusion of the following theorem holds.
Theorem 14.0.6 Let (X, ||Â·||) be a ï¬nite dimensional normed linear space and let |Â·| be
described above relative to a given basis, {v1 , Â· Â· Â· , vn } . Then |Â·| is a norm and there exist
constants Î´, âˆ† > 0 independent of x such that
Î´ ||x|| â‰¤ |x| â‰¤âˆ† ||x|| .

(14.3)

Proof: All of the above properties of a norm are obvious except the second, the triangle
inequality. To establish this inequality, use the Cauchy Schwarz inequality to write
|x + y|

2

n
âˆ‘

â‰¡

2

|xi + yi | â‰¤

i=1

n
âˆ‘

2

|xi | +

i=1

2

2

2

2

â‰¤ |x| + |y| + 2

( n
âˆ‘

|xi |

2

n
âˆ‘

2

|yi | + 2 Re

i=1
)1/2 ( n
âˆ‘

i=1

n
âˆ‘

xi y i

i=1
)1/2
2

|yi |

i=1
2

= |x| + |y| + 2 |x| |y| = (|x| + |y|)

and this proves the second property above.
It remains to show the equivalence of the two norms. By the Cauchy Schwarz inequality
again,
n
âˆ‘

||x|| â‰¡
â‰¡

Î´

i=1
âˆ’1

xi vi â‰¤

n
âˆ‘
i=1

(
|xi | ||vi || â‰¤ |x|

n
âˆ‘

)1/2
2

||vi ||

i=1

|x| .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

339
This proves the ï¬rst half of the inequality.
Suppose the second half of the inequality is not valid. Then there exists a sequence
xk âˆˆ X such that
xk > k xk , k = 1, 2, Â· Â· Â· .
Then deï¬ne
yk â‰¡

xk
.
|xk |

It follows
yk = 1,

yk > k yk .

(14.4)

Letting yik be the components of yk with respect to the given basis, it follows the vector
( k
)
y1 , Â· Â· Â· , ynk
is a unit vector in Fn . By the Heine Borel theorem, there exists a subsequence, still denoted
by k such that
( k
)
y1 , Â· Â· Â· , ynk â†’ (y1 , Â· Â· Â· , yn ) .
It follows from (14.4) and this that for
y=

n
âˆ‘

yi vi ,

i=1

0 = lim

kâ†’âˆ

y

k

= lim

kâ†’âˆ

n
âˆ‘

yik vi

=

i=1

n
âˆ‘

yi vi

i=1

but not all the yi equal zero. This contradicts the assumption that {v1 , Â· Â· Â· , vn } is a basis
and proves the second half of the inequality. 
Corollary 14.0.7 If (X, ||Â·||) is a ï¬nite dimensional normed linear space with the ï¬eld of
scalars F = C or R, then X is complete.
Proof: Let {xk } be a Cauchy sequence. Then letting the components of xk with respect
to the given basis be
xk1 , Â· Â· Â· , xkn ,
it follows from Theorem 14.0.6, that
( k
)
x1 , Â· Â· Â· , xkn
is a Cauchy sequence in Fn and so
( k
)
x1 , Â· Â· Â· , xkn â†’ (x1 , Â· Â· Â· , xn ) âˆˆ Fn .
Thus,
xk =

n
âˆ‘

xki vi â†’

i=1

n
âˆ‘

xi vi âˆˆ X. 

i=1

Corollary 14.0.8 Suppose X is a ï¬nite dimensional linear space with the ï¬eld of scalars
either C or R and ||Â·|| and |||Â·||| are two norms on X. Then there exist positive constants, Î´
and âˆ†, independent of x âˆˆ X such that
Î´ |||x||| â‰¤ ||x|| â‰¤ âˆ† |||x||| .
Thus any two norms are equivalent.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

340

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

This is very important because it shows that all questions of convergence can be considered relative to any norm with the same outcome.
Proof: Let {v1 , Â· Â· Â· , vn } be a basis for X and let |Â·| be the norm taken with respect to
this basis which was described earlier. Then by Theorem 14.0.6, there are positive constants
Î´ 1 , âˆ†1 , Î´ 2 , âˆ†2 , all independent of x âˆˆX such that
Î´ 2 |||x||| â‰¤ |x| â‰¤ âˆ†2 |||x||| ,
Î´ 1 ||x|| â‰¤ |x| â‰¤ âˆ†1 ||x|| .
Then
Î´ 2 |||x||| â‰¤ |x| â‰¤ âˆ†1 ||x|| â‰¤
and so

âˆ†1
âˆ†1 âˆ†2
|x| â‰¤
|||x|||
Î´1
Î´1

Î´2
âˆ†2
|||x||| â‰¤ ||x|| â‰¤
|||x||| 
âˆ†1
Î´1

Deï¬nition 14.0.9 Let X and Y be normed linear spaces with norms ||Â·||X and ||Â·||Y respectively. Then L (X, Y ) denotes the space of linear transformations, called bounded linear
transformations, mapping X to Y which have the property that
||A|| â‰¡ sup {||Ax||Y : ||x||X â‰¤ 1} < âˆ.
Then ||A|| is referred to as the operator norm of the bounded linear transformation A.
It is an easy exercise to verify that ||Â·|| is a norm on L (X, Y ) and it is always the case
that
||Ax||Y â‰¤ ||A|| ||x||X .
Furthermore, you should verify that you can replace â‰¤ 1 with = 1 in the deï¬nition. Thus
||A|| â‰¡ sup {||Ax||Y : ||x||X = 1} .
Theorem 14.0.10 Let X and Y be ï¬nite dimensional normed linear spaces of dimension
n and m respectively and denote by ||Â·|| the norm on either X or Y . Then if A is any linear
function mapping X to Y, then A âˆˆ L (X, Y ) and (L (X, Y ) , ||Â·||) is a complete normed
linear space of dimension nm with
||Ax|| â‰¤ ||A|| ||x|| .
Proof: It is necessary to show the norm deï¬ned on linear transformations really is a
norm. Again the ï¬rst and third properties listed above for norms are obvious. It remains to
show the second and verify ||A|| < âˆ. Letting {v1 , Â· Â· Â· , vn } be a basis and |Â·| deï¬ned with
respect to this basis as above, there exist constants Î´, âˆ† > 0 such that
Î´ ||x|| â‰¤ |x| â‰¤ âˆ† ||x|| .
Then,
||A + B||

â‰¡ sup{||(A + B) (x)|| : ||x|| â‰¤ 1}
â‰¤ sup{||Ax|| : ||x|| â‰¤ 1} + sup{||Bx|| : ||x|| â‰¤ 1}
â‰¡ ||A|| + ||B|| .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

341
Next consider the claim that ||A|| < âˆ. This follows from
)
( n
n
âˆ‘
âˆ‘
||A (x)|| = A
xi vi
â‰¤
|xi | ||A (vi )||
i=1

(
â‰¤ |x|

n
âˆ‘

i=1

)1/2
2

||A (vi )||

(
â‰¤ âˆ† ||x||

i=1

n
âˆ‘

)1/2
||A (vi )||

2

< âˆ.

i=1

(âˆ‘
)1/2
n
2
Thus ||A|| â‰¤ âˆ†
.
i=1 ||A (vi )||
Next consider the assertion about the dimension of L (X, Y ) . It follows from Theorem
9.2.3. By Corollary 14.0.7 (L (X, Y ) , ||Â·||) is complete. If x Ì¸= 0,
||Ax||

1
x
= A
||x||
||x||

â‰¤ ||A|| 

Note by Corollary 14.0.8 you can deï¬ne a norm any way desired on any ï¬nite dimensional
linear space which has the ï¬eld of scalars R or C and any other way of deï¬ning a norm on
this space yields an equivalent norm. Thus, it doesnâ€™t much matter as far as notions of
convergence are concerned which norm is used for a ï¬nite dimensional space. In particular
in the space of m Ã— n matrices, you can use the operator norm deï¬ned above, or some
other way of giving this space a norm. A popular choice for a norm is the Frobenius norm
discussed earlier but reviewed here.
Deï¬nition 14.0.11 Make the space of m Ã— n matrices into a Hilbert space by deï¬ning
(A, B) â‰¡ tr (AB âˆ— ) .
Another way of describing a norm for an n Ã— n matrix is as follows.
Deï¬nition 14.0.12 Let A be an m Ã— n matrix. Deï¬ne the spectral norm of A, written as
||A||2 to be
{
}
max Î»1/2 : Î» is an eigenvalue of Aâˆ— A .
That is, the largest singular value of A. (Note the eigenvalues of Aâˆ— A are all positive because
if Aâˆ— Ax = Î»x, then
Î» (x, x) = (Aâˆ— Ax, x) = (Ax,Ax) â‰¥ 0.)
Actually, this is nothing new. It turns out that ||Â·||2 is nothing more than the operator
norm for A taken with respect to the usual Euclidean norm,
( n
)1/2
âˆ‘
2
|x| =
|xk |
.
k=1

Proposition 14.0.13 The following holds.
||A||2 = sup {|Ax| : |x| = 1} â‰¡ ||A|| .
Proof: Note that Aâˆ— A is Hermitian and so by Corollary 13.3.5,
{
}
1/2
||A||2 = max (Aâˆ— Ax, x)
: |x| = 1
{
}
1/2
= max (Ax,Ax)
: |x| = 1
= max {|Ax| : |x| = 1} = ||A|| . 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

342

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

Here is another proof of this
Recall there are unitary matrices of the right
( proposition.
)
Ïƒ 0
size U, V such that A = U
V âˆ— where the matrix on the inside is as described
0 0
in the section on the singular value decomposition. Then since unitary matrices preserve
norms,
(
||A|| =

sup U
|x|â‰¤1

=

(

sup U
|y|â‰¤1

Ïƒ
0

0
0

Ïƒ
0

0
0

)

(

V âˆ—x =

sup
|V âˆ— x|â‰¤1

(

)

y = sup
|y|â‰¤1

)
Ïƒ 0
V âˆ—x
0 0
)
0
y = Ïƒ 1 â‰¡ ||A||2
0

U
Ïƒ
0

This completes the alternate proof.
From now on, ||A||2 will mean either the operator norm of A taken with respect to the
usual Euclidean norm or the largest singular value of A, whichever is most convenient.
An interesting application of the notion of equivalent norms on Rn is the process of
giving a norm on a ï¬nite Cartesian product of normed linear spaces.
Deï¬nition 14.0.14 Let Xi , i = 1, Â· Â· Â· , n be normed linear spaces with norms, ||Â·||i . For
x â‰¡ (x1 , Â· Â· Â· , xn ) âˆˆ

n
âˆ

Xi

i=1

deï¬ne Î¸ :

âˆn
i=1

Xi â†’ Rn by

Î¸ (x) â‰¡ (||x1 ||1 , Â· Â· Â· , ||xn ||n )
âˆn
Then if ||Â·|| is any norm on Rn , deï¬ne a norm on i=1 Xi , also denoted by ||Â·|| by
||x|| â‰¡ ||Î¸x|| .
The following theorem follows immediately from Corollary 14.0.8.
Theorem
14.0.15 Let Xi and ||Â·||i be given in the above deï¬nition and consider the norms
âˆn
X
described there in terms of norms on Rn . Then any two of these norms on
on
i
i=1
âˆn
i=1 Xi obtained in this way are equivalent.
For example, deï¬ne
||x||1 â‰¡

n
âˆ‘

|xi | ,

i=1

||x||âˆ â‰¡ max {|xi | , i = 1, Â· Â· Â· , n} ,
or

(
||x||2 =

and all three are equivalent norms on

âˆn
i=1

Saylor URL: http://www.saylor.org/courses/ma212/

n
âˆ‘

)1/2
|xi |

2

i=1

Xi .

The Saylor Foundation

14.1. THE P NORMS

14.1

343

The p Norms

In addition to ||Â·||1 and ||Â·||âˆ mentioned above, it is common to consider the so called p
norms for x âˆˆ Cn .
Deï¬nition 14.1.1 Let x âˆˆ Cn . Then deï¬ne for p â‰¥ 1,
||x||p â‰¡

( n
âˆ‘

)1/p
|xi |

p

i=1

The following inequality is called Holderâ€™s inequality.
Proposition 14.1.2 For x, y âˆˆ Cn ,
n
âˆ‘

(
|xi | |yi | â‰¤

i=1

n
âˆ‘

)1/p (
|xi |

p

i=1

n
âˆ‘

)1/pâ€²
pâ€²

|yi |

i=1

The proof will depend on the following lemma.
Lemma 14.1.3 If a, b â‰¥ 0 and pâ€² is deï¬ned by

1
p

+

1
pâ€²

= 1, then

â€²

ab â‰¤

ap
bp
+ â€².
p
p

Proof of the Proposition: If x or y equals the zero vector there is nothing to
âˆ‘n
p 1/p
prove. Therefore, assume they are both nonzero. Let A = ( i=1 |xi | )
and B =
â€²
)
(âˆ‘
â€² 1/p
n
p
. Then using Lemma 14.1.3,
i=1 |yi |
n
âˆ‘
|xi | |yi |
i=1

A B

â‰¤
=

n
n
1 1 âˆ‘
1 1 âˆ‘
p
pâ€²
|x
|
+
|yi |
i
p
â€²
p
p A i=1
p B i=1

=

1
1
+
=1
p pâ€²

and so
n
âˆ‘
i=1

[ (
)p
(
)pâ€² ]
n
âˆ‘
1 |xi |
1 |yi |
+ â€²
p A
p
B
i=1

(
|xi | |yi | â‰¤ AB =

n
âˆ‘
i=1

)1/p ( n
)1/pâ€²
âˆ‘
â€²
p
p
|xi |
|yi |
.
i=1

Theorem 14.1.4 The p norms do indeed satisfy the axioms of a norm.
Proof: It is obvious that ||Â·||p does indeed satisfy most of the norm axioms. The only
one that is not clear is the triangle inequality. To save notation write ||Â·|| in place of ||Â·||p

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

344

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

||x + y||

p

=

n
âˆ‘

= p âˆ’ 1. Then using the Holder inequality,

p
pâ€²

in what follows. Note also that

|xi + yi |

p

|xi + yi |

pâˆ’1

i=1

â‰¤
=

n
âˆ‘
i=1
n
âˆ‘

p

( n
âˆ‘

|xi + yi |
(

p/pâ€²

ï£°

p

|yi |

n
âˆ‘

)1/p
|xi |

p

+

( n
âˆ‘

i=1

||x||p + ||y||p

)1/p ï£¹
|yi |

p

ï£»

i=1

)

, it follows
âˆ’p/pâ€²

p

||x + y|| ||x + y||
(

pâˆ’1

p

i=1

||x + y||

p/pâ€²

so dividing by ||x + y||

|xi + yi |

|xi + yi | pâ€² |yi |

)1/pâ€² ï£®(

i=1

=

n
âˆ‘

i=1
n
âˆ‘

|xi + yi | pâ€² |xi | +

i=1

â‰¤

|xi | +

= ||x + y|| â‰¤ ||x||p + ||y||p

(
)
)
p âˆ’ ppâ€² = p 1 âˆ’ p1â€² = p p1 = 1. . 
It only remains to prove Lemma 14.1.3.
Proof of the lemma: Let pâ€² = q to save on notation and consider the following picture:
x
b
x = tpâˆ’1
t = xqâˆ’1
t
a
âˆ«
ab â‰¤

âˆ«

a

b

tpâˆ’1 dt +
0

xqâˆ’1 dx =
0

ap
bq
+ .
p
q

Note equality occurs when ap = bq .
Alternate proof of the lemma: Let
f (t) â‰¡

1
1
p
(at) +
p
q

( )q
b
, t>0
t

You see right away it is decreasing for a while, having an asymptote at t = 0 and then
reaches a minimum and increases from then on. Take its derivative.
( )qâˆ’1 ( )
b
âˆ’b
pâˆ’1
â€²
f (t) = (at)
a+
t
t2
Set it equal to 0. This happens when
tp+q =

Saylor URL: http://www.saylor.org/courses/ma212/

bq
.
ap

(14.5)

The Saylor Foundation

14.2. THE CONDITION NUMBER

345

Thus

bq/(p+q)
ap/(p+q)

t=
and so at this value of t,
at = (ab)

q/(p+q)

,

( )
b
p/(p+q)
= (ab)
.
t

Thus the minimum of f is
)p 1 (
)q
1(
q/(p+q)
p/(p+q)
pq/(p+q)
(ab)
(ab)
= (ab)
+
p
q
but recall 1/p + 1/q = 1 and so pq/ (p + q) = 1. Thus the minimum value of f is ab. Letting
t = 1, this shows
ap
bq
ab â‰¤
+ .
p
q
Note that equality occurs when the minimum value happens for t = 1 and this indicates
from (14.5) that ap = bq . 
Now ||A||p may be considered as the operator norm of A taken with respect to ||Â·||p . In
the case when p = 2, this is just the spectral norm. There is an easy estimate for ||A||p in
terms of the entries of A.
Theorem 14.1.5 The following holds.
ï£«

ï£«
ï£¶q/p ï£¶1/q
âˆ‘
âˆ‘
ï£¬
ï£·
p
ï£­
||A||p â‰¤ ï£­
|Ajk | ï£¸ ï£¸
j

k

Proof: Let ||x||p â‰¤ 1 and let A = (a1 , Â· Â· Â· , an ) where the ak are the columns of A. Then
(
Ax =

âˆ‘

)
xk ak

k

and so by Holderâ€™s inequality,
||Ax||p

â‰¡

âˆ‘
(

â‰¤

k

âˆ‘

ï£«

â‰¤

x k ak
p

âˆ‘

)1/p (
|xk |

p

k

|xk | ||ak ||p

k

âˆ‘

)1/q
q
||ak ||p

k

ï£¶q/p ï£¶1/q
âˆ‘
âˆ‘
ï£¬
ï£·
p
ï£­
â‰¤ ï£­
|Ajk | ï£¸ ï£¸

ï£«

k

14.2

j

The Condition Number

Let A âˆˆ L (X, X) be a linear transformation where X is a ï¬nite dimensional vector space
and consider the problem Ax = b where it is assumed there is a unique solution to this
problem. How does the solution change if A is changed a little bit and if b is changed a

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

346

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

little bit? This is clearly an interesting question because you often do not know A and b
exactly. If a small change in these quantities results in a large change in the solution, x,
then it seems clear this would be undesirable. In what follows ||Â·|| when applied to a linear
transformation will always refer to the operator norm.
Lemma 14.2.1 Let A, B âˆˆ L (X, X) where X is a normed vector space as above. Then for
||Â·|| denoting the operator norm,
||AB|| â‰¤ ||A|| ||B|| .
Proof: This follows from the deï¬nition. Letting ||x|| â‰¤ 1, it follows from Theorem
14.0.10
||ABx|| â‰¤ ||A|| ||Bx|| â‰¤ ||A|| ||B|| ||x|| â‰¤ ||A|| ||B||
and so
||AB|| â‰¡ sup ||ABx|| â‰¤ ||A|| ||B|| . 
||x||â‰¤1

Lemma 14.2.2 Let A, B âˆˆ L (X, X) , Aâˆ’1 âˆˆ L (X, X) , and suppose ||B|| < 1/ Aâˆ’1 .
âˆ’1
Then (A + B) exists and
âˆ’1

(A + B)

The above formula makes sense because

â‰¤ Aâˆ’1

1
.
1 âˆ’ ||Aâˆ’1 B||

Aâˆ’1 B < 1.

Proof: By Lemma 14.2.1,
Aâˆ’1 B â‰¤ Aâˆ’1 ||B|| < Aâˆ’1
(

1
=1
||Aâˆ’1 ||

(
)
Suppose) (A + B) x = 0. Then 0 = A I + Aâˆ’1 B x and so since A is one to one,
I + Aâˆ’1 B x = 0. Therefore,
0

(
)
I + Aâˆ’1 B x â‰¥ ||x|| âˆ’ Aâˆ’1 Bx
(
)
â‰¥ ||x|| âˆ’ Aâˆ’1 B ||x|| = 1 âˆ’ Aâˆ’1 B ||x|| > 0

=

(
)
âˆ’1
a contradiction. This also shows I + Aâˆ’1 B is one to one. Therefore, both (A + B) and
(
)
âˆ’1
I + Aâˆ’1 B
are in L (X, X). Hence
(A + B)

âˆ’1

( (
))âˆ’1 (
)âˆ’1 âˆ’1
= A I + Aâˆ’1 B
= I + Aâˆ’1 B
A

Now if

(
)âˆ’1
y
x = I + Aâˆ’1 B

for ||y|| â‰¤ 1, then
and so

(

)
I + Aâˆ’1 B x = y

(
)
||x|| 1 âˆ’ Aâˆ’1 B â‰¤ x + Aâˆ’1 Bx â‰¤ ||y|| = 1

and so
||x|| =

(

I + Aâˆ’1 B

Saylor URL: http://www.saylor.org/courses/ma212/

)âˆ’1

y â‰¤

1
1 âˆ’ ||Aâˆ’1 B||

The Saylor Foundation

14.2. THE CONDITION NUMBER

347

Since ||y|| â‰¤ 1 is arbitrary, this shows
(
)âˆ’1
I + Aâˆ’1 B
â‰¤

1
1 âˆ’ ||Aâˆ’1 B||

Therefore,
âˆ’1

(A + B)

=

(
)âˆ’1 âˆ’1
I + Aâˆ’1 B
A

â‰¤

Aâˆ’1

(

I + Aâˆ’1 B

)âˆ’1

1

1 âˆ’ ||Aâˆ’1 B||

â‰¤ Aâˆ’1

Proposition 14.2.3 Suppose A is invertible, b Ì¸= 0, Ax = b, and A1 x1 = b1 where
||A âˆ’ A1 || < 1/ Aâˆ’1 . Then
(
)
||x1 âˆ’ x||
1
||A1 âˆ’ A|| ||b âˆ’ b1 ||
âˆ’1
â‰¤
||A||
A
+
.
(14.6)
||x||
(1 âˆ’ ||Aâˆ’1 (A1 âˆ’ A)||)
||A||
||b||
Proof: It follows from the assumptions that
Ax âˆ’ A1 x + A1 x âˆ’ A1 x1 = b âˆ’ b1 .
Hence
A1 (x âˆ’ x1 ) = (A1 âˆ’ A) x + b âˆ’ b1 .
Now A1 = (A + (A1 âˆ’ A)) and so by the above lemma, Aâˆ’1
1 exists and so
âˆ’1
(x âˆ’ x1 ) = Aâˆ’1
1 (A1 âˆ’ A) x + A1 (b âˆ’ b1 )
âˆ’1

= (A + (A1 âˆ’ A))

âˆ’1

(A1 âˆ’ A) x + (A + (A1 âˆ’ A))

(b âˆ’ b1 ) .

By the estimate in Lemma 14.2.2,
||x âˆ’ x1 || â‰¤

Aâˆ’1
(||A1 âˆ’ A|| ||x|| + ||b âˆ’ b1 ||) .
1 âˆ’ ||Aâˆ’1 (A1 âˆ’ A)||

Dividing by ||x|| ,

(
)
Aâˆ’1
||x âˆ’ x1 ||
||b âˆ’ b1 ||
â‰¤
||A1 âˆ’ A|| +
||x||
1 âˆ’ ||Aâˆ’1 (A1 âˆ’ A)||
||x||
( âˆ’1 )
âˆ’1
Now b = Ax = A A b and so ||b|| â‰¤ ||A|| A b and so

(14.7)

||x|| = Aâˆ’1 b â‰¥ ||b|| / ||A|| .
Therefore, from (14.7),
||x âˆ’ x1 ||
||x||

â‰¤
â‰¤

Aâˆ’1
1 âˆ’ ||Aâˆ’1 (A1 âˆ’ A)||

(

||A|| ||A1 âˆ’ A|| ||A|| ||b âˆ’ b1 ||
+
||A||
||b||
(
)
Aâˆ’1 ||A||
||A1 âˆ’ A|| ||b âˆ’ b1 ||
+
1 âˆ’ ||Aâˆ’1 (A1 âˆ’ A)||
||A||
||b||

)

which proves the proposition. 
This shows that the number, Aâˆ’1 ||A|| , controls how sensitive the relative change in
the solution of Ax = b is to small changes in A and b. This number is called the condition
number. It is bad when it is large because a small relative change in b, for example could
yield a large relative change in x.
Recall that for A an n Ã— n matrix, ||A||2 = Ïƒ 1 where Ïƒ 1 is the largest singular value. The
largest singular value of Aâˆ’1 is therefore, 1/Ïƒ n where Ïƒ n is the smallest singular value of A.
Therefore, the condition number reduces to Ïƒ 1 /Ïƒ n , the ratio of the largest to the smallest
singular value of A.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

348

14.3

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

The Spectral Radius

Even though it is in general impractical to compute the Jordan form, its existence is all that
is needed in order to prove an important theorem about something which is relatively easy
to compute. This is the spectral radius of a matrix.
Deï¬nition 14.3.1 Deï¬ne Ïƒ (A) to be the eigenvalues of A. Also,
Ï (A) â‰¡ max (|Î»| : Î» âˆˆ Ïƒ (A))
The number, Ï (A) is known as the spectral radius of A.
Recall the following symbols and their meaning.
lim sup an , lim inf an
nâ†’âˆ

nâ†’âˆ

They are respectively the largest and smallest limit points of the sequence {an } where Â±âˆ
is allowed in the case where the sequence is unbounded. They are also deï¬ned as
lim sup an
nâ†’âˆ

lim inf an
nâ†’âˆ

â‰¡
â‰¡

lim (sup {ak : k â‰¥ n}) ,

nâ†’âˆ

lim (inf {ak : k â‰¥ n}) .

nâ†’âˆ

Thus, the limit of the sequence exists if and only if these are both equal to the same real
number.
Lemma 14.3.2 Let J be a p Ã— p Jordan matrix
ï£«
J1
ï£¬
..
J =ï£­
.

ï£¶
ï£·
ï£¸
Js

where each Jk is of the form
Jk = Î»k I + Nk
in which Nk is a nilpotent matrix having zeros down the main diagonal and ones down the
super diagonal. Then
1/n
lim ||J n ||
=Ï
nâ†’âˆ

where Ï = max {|Î»k | , k = 1, . . . , n}. Here the norm is deï¬ned to equal
||B|| = max {|Bij | , i, j} .
Proof: Suppose ï¬rst that Ï Ì¸= 0. First note that for this norm, if B, C are p Ã— p matrices,
||BC|| â‰¤ p ||B|| ||C||
which follows from a simple computation. Now
ï£«
1/n

||J n ||

ï£¬
= ï£­

ï£¶

n

(Î»1 I + N1 )

..

1/n

ï£·
ï£¸

.
n

(Î»s I + Ns )

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

14.3. THE SPECTRAL RADIUS
ï£« (

349

Î»1
Ï I

ï£¬
ï£¬
=Ï ï£¬
ï£­

+ Ï1 N1

)n

ï£¶
..

.

(

Î»2
Ï I

+ Ï1 N2

)n

1/n

ï£·
ï£·
ï£·
ï£¸

(14.8)

From the deï¬nition of Ï, at least one of the Î»k /Ï has absolute value equal to 1. Therefore,
ï£« (
ï£¬
ï£¬
ï£¬
ï£­

Î»1
Ï I

+ Ï1 N1

)n

ï£¶
..

.

(

Î»2
Ï I

+ Ï1 N2

)n

1/n

ï£·
ï£·
ï£·
ï£¸

âˆ’ 1 â‰¡ en â‰¥ 0

because each Nk has only zero terms on the main diagonal. Therefore, some term in the
matrix has absolute value at least as large as 1. Now also, since Nkp = 0, the norm of
the matrix in the above is dominated by an expression of the form Cnp where C is some
constant which does not depend on n. This is because a typical block in the above matrix
is of the form
)nâˆ’i
p ( )(
âˆ‘
n
Î»k
Nki
Ï
i
i=1
and each |Î»k | â‰¤ Ï.
It follows that for n > p + 1,
(
n

Cn â‰¥ (1 + en ) â‰¥
p

and so

(

Cnp
( n )

)
n
ep+1
p+1 n

)1/(p+1)
â‰¥ en â‰¥ 0

p+1

Therefore, limnâ†’âˆ en = 0. It follows from (14.8) that the expression in the norms in this
equation converges to 1 and so
1/n
lim ||J n ||
= Ï.
nâ†’âˆ

In case Ï = 0 so that all the eigenvalues equal zero, it follows that J n = 0 for all n > p.
Therefore, the limit still exists and equals Ï. 
The following theorem is due to Gelfand around 1941.
Theorem 14.3.3 (Gelfand) Let A be a complex p Ã— p matrix. Then if Ï is the absolute
value of its largest eigenvalue,
1/n
lim ||An ||
= Ï.
nâ†’âˆ

Here ||Â·|| is any norm on L (C , C ).
n

n

Proof: First assume ||Â·|| is the special norm of the above lemma. Then letting J denote
the Jordan form of A, S âˆ’1 AS = J, it follows from Lemma 14.3.2
1/n

lim sup ||An ||
nâ†’âˆ

=

lim sup
nâ†’âˆ

â‰¤ lim sup

nâ†’âˆ

Saylor URL: http://www.saylor.org/courses/ma212/

SJ n S âˆ’1

1/n

(( 2 )
)1/n n 1/n
||J ||
=Ï
p ||S|| S âˆ’1

The Saylor Foundation

350

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES
n

= lim inf ||J n ||
= lim inf S âˆ’1 An S
nâ†’âˆ
nâ†’âˆ
(( 2 )
)1/n
1/n
1/n
âˆ’1
= lim inf
p ||S|| S
||An ||
= lim inf ||An ||
1/n

nâ†’âˆ

nâ†’âˆ

1/n

1/n

1/n

If follows that lim inf nâ†’âˆ ||An ||
= lim supnâ†’âˆ ||An ||
= limnâ†’âˆ ||An ||
= Ï.
Now by equivalence of norms, if |||Â·||| is any other norm for the set of complex p Ã— p
matrices, there exist constants Î´, âˆ† such that
Î´ ||An || â‰¤ |||An ||| â‰¤ âˆ† ||An ||
Then raising to the 1/n power and taking a limit,
Ï â‰¤ lim inf |||An |||
nâ†’âˆ

ï£«

9
Example 14.3.4 Consider ï£­ âˆ’2
1
eigenvalue.

1/n

1/n

â‰¤ lim sup |||An |||
nâ†’âˆ

â‰¤Ï 

ï£¶
âˆ’1 2
8 4 ï£¸ . Estimate the absolute value of the largest
1 8

A laborious computation reveals the eigenvalues are 5, and 10. Therefore, the right
1/7
answer in this case is 10. Consider A7
where the norm is obtained by taking the
maximum of all the absolute values of the entries. Thus
ï£«

9
ï£­ âˆ’2
1

ï£¶7 ï£«
âˆ’1 2
8015 625
8 4 ï£¸ = ï£­ âˆ’3968 750
1 8
1984 375

âˆ’1984 375
6031 250
1984 375

ï£¶
3968 750
7937 500 ï£¸
6031 250

and taking the seventh root of the largest entry gives
Ï (A) â‰ˆ 8015 6251/7 = 9. 688 951 236 71.
Of course the interest lies primarily in matrices for which the exact roots to the characteristic
equation are not known and in the theoretical signiï¬cance.

14.4

Series And Sequences Of Linear Operators

Before beginning this discussion, it is necessary to deï¬ne what is meant by convergence in
L (X, Y ) .
âˆ

Deï¬nition 14.4.1 Let {Ak }k=1 be a sequence in L (X, Y ) where X, Y are ï¬nite dimensional normed linear spaces. Then limnâ†’âˆ Ak = A if for every Îµ > 0 there exists N such
that if n > N, then
||A âˆ’ An || < Îµ.
Here the norm refers to any of the norms deï¬ned on L (X, Y ) . By Corollary 14.0.8 and
Theorem 9.2.3 it doesnâ€™t matter which one is used. Deï¬ne the symbol for an inï¬nite sum in
the usual way. Thus
âˆ
n
âˆ‘
âˆ‘
Ak â‰¡ lim
Ak
k=1

Saylor URL: http://www.saylor.org/courses/ma212/

nâ†’âˆ

k=1

The Saylor Foundation

14.4. SERIES AND SEQUENCES OF LINEAR OPERATORS

351

âˆ

Lemma 14.4.2 Suppose {Ak }k=1 is a sequence in L (X, Y ) where X, Y are ï¬nite dimensional normed linear spaces. Then if
âˆ
âˆ‘

||Ak || < âˆ,

k=1

It follows that

âˆ
âˆ‘

Ak

(14.9)

k=1

exists. In words, absolute convergence implies convergence.
Proof: For p â‰¤ m â‰¤ n,
n
âˆ‘

Ak âˆ’

k=1

m
âˆ‘

Ak

â‰¤

âˆ
âˆ‘

||Ak ||

k=p

k=1

and so for p large enough, this term on the right in the above inequality is less than Îµ. Since
Îµ is arbitrary, this shows the partial sums of (14.9) are a Cauchy sequence. Therefore by
Corollary 14.0.7 it follows that these partial sums converge. 
As a special case, suppose Î» âˆˆ C and consider
âˆ k k
âˆ‘
t Î»

k!

k=0
k

k

where t âˆˆ R. In this case, Ak = t k!Î» and you can think of it as being in L (C, C). Then the
following corollary is of great interest.
Corollary 14.4.3 Let
f (t) â‰¡

âˆ k k
âˆ‘
t Î»
k=0

k!

â‰¡1+

âˆ k k
âˆ‘
t Î»
k=1

k!

Then this function is a well deï¬ned complex valued function and furthermore, it satisï¬es the
initial value problem,
y â€² = Î»y, y (0) = 1
Furthermore, if Î» = a + ib,
|f | (t) = eat .
Proof: That f (t) makes sense follows right away from Lemma 14.4.2.
âˆ
âˆ
k
k
âˆ‘
âˆ‘
tk Î»k
|t| |Î»|
=
= e|t||Î»|
k!
k!

k=0

k=0

It only remains to verify f satisï¬es the diï¬€erential equation because it is obvious from the
series that f (0) = 1.
(
)
k
k
âˆ
(t
+
h)
âˆ’
t
Î»k
âˆ‘
f (t + h) âˆ’ f (t)
1
=
h
h
k!
k=1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

352

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

and by the mean value theorem this equals an expression of the following form where Î¸k is
a number between 0 and 1.
âˆ
kâˆ’1 k
âˆ‘
k (t + Î¸k h)
Î»

=

k!

k=1

âˆ
kâˆ’1 k
âˆ‘
(t + Î¸k h)
Î»
k=1
âˆ
âˆ‘

= Î»

k=0

(k âˆ’ 1)!
k

(t + Î¸k h) Î»k
k!

It only remains to verify this converges to
Î»

âˆ k k
âˆ‘
t Î»
k=0

k!

= Î»f (t)

as h â†’ 0.
âˆ
k
âˆ‘
(t + Î¸k h) Î»k
k=0

k!

âˆ’

âˆ k k
âˆ‘
t Î»
k=0

k!

=

(
)
k
âˆ
(t + Î¸k h) âˆ’ tk Î»k
âˆ‘
k!

k=0

and by the mean value theorem again and the triangle inequality
â‰¤

âˆ
âˆ
kâˆ’1
k
kâˆ’1
k
âˆ‘
âˆ‘
k |(t + Î· k )|
|Î»|
k |(t + Î· k )|
|h| |Î»|
â‰¤ |h|
k!
k!
k=0

k=0

where Î· k is between 0 and 1. Thus
â‰¤ |h|

âˆ
kâˆ’1
k
âˆ‘
k (|t| + 1)
|Î»|

k!

k=0

= |h| C (t)

It follows f â€² (t) = Î»f (t) . This proves the ï¬rst part.
Next note that for f (t) = u (t) + iv (t) , both u, v are diï¬€erentiable. This is because
u=

f +f
f âˆ’f
, v=
.
2
2i

Then from the diï¬€erential equation,
(a + ib) (u + iv) = uâ€² + iv â€²
and equating real and imaginary parts,
uâ€² = au âˆ’ bv, v â€² = av + bu.
Then a short computation shows
(
) (
)
( 2
)â€²
u + v 2 = 2a u2 + v 2 , u2 + v 2 (0) = 1.
Now in general, if

y â€² = cy, y (0) = 1,

with c real it follows y (t) = ect . To see this,
y â€² âˆ’ cy = 0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

14.4. SERIES AND SEQUENCES OF LINEAR OPERATORS

353

and so, multiplying both sides by eâˆ’ct you get
d ( âˆ’ct )
ye
=0
dt
and so yeâˆ’ct equals a constant which must be 1 because of the initial condition y (0) = 1.
Thus
( 2
)
u + v 2 (t) = e2at
and taking square roots yields the desired conclusion. 
Deï¬nition 14.4.4 The function in Corollary 14.4.3 given by that power series is denoted
as
exp (Î»t) or eÎ»t .
The next lemma is normally discussed in advanced calculus courses but is proved here
for the convenience of the reader. It is known as the root test.
Deï¬nition 14.4.5 For {an } any sequence of real numbers
lim sup an â‰¡ lim (sup {ak : k â‰¥ n})
nâ†’âˆ

nâ†’âˆ

Similarly
lim inf an â‰¡ lim (inf {ak : k â‰¥ n})
nâ†’âˆ

nâ†’âˆ

In case An is an increasing (decreasing) sequence which is unbounded above (below) then it
is understood that limnâ†’âˆ An = âˆ(âˆ’âˆ) respectively. Thus either of lim sup or lim inf can
equal +âˆ or âˆ’âˆ. However, the important thing about these is that unlike the limit, these
always exist.
It is convenient to think of these as the largest point which is the limit of some subsequence of {an } and the smallest point which is the limit of some subsequence of {an }
respectively. Thus limnâ†’âˆ an exists and equals some point of [âˆ’âˆ, âˆ] if and only if the
two are equal.
Lemma 14.4.6 Let {ap } be a sequence of nonnegative terms and let
r = lim sup a1/p
p .
pâ†’âˆ

âˆ‘âˆ

Then if r < 1, it follows the series, k=1 ak converges and if r > 1, then ap fails to converge
to 0 so the series diverges. If A is an n Ã— n matrix and
1 < lim sup ||Ap ||

1/p

,

(14.10)

pâ†’âˆ

then

âˆ‘âˆ
k=0

Ak fails to converge.

Proof: Suppose r < 1. Then there exists N such that if p > N,
a1/p
<R
p
where r < R < âˆ‘
1. Therefore, for
all such p, ap < Rp and so by comparison with the
âˆ‘âˆ
p
geometric series,
R , it follows p=1 ap converges.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

354

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

Next suppose r > 1. Then letting 1 < R < r, it follows there are inï¬nitely many values
of p at which
R < a1/p
p
which implies Rp < ap , showing that ap cannot converge to 0 and so the series cannot
converge either.
p
To see the last claim, if {(14.10)
||
} then from the ï¬rst part of this lemma,
âˆ‘m holds,
âˆ‘âˆ ||A
k âˆ
k
fails to converge to 0 and so
A
is
not
a
Cauchy
sequence.
Hence
A
â‰¡
k=0
k=0
m=0
âˆ‘m
limmâ†’âˆ k=0 Ak cannot exist. 
p
Now denote by Ïƒ (A) the collection of all numbers of the form Î»p where Î» âˆˆ Ïƒ (A) .
Lemma 14.4.7 Ïƒ (Ap ) = Ïƒ (A)

p

Proof: In dealing with Ïƒ (Ap ) , is suï¬ƒces to deal with Ïƒ (J p ) where J is the Jordan form
of A because J p and Ap are similar. Thus if Î» âˆˆ Ïƒ (Ap ) , then Î» âˆˆ Ïƒ (J p ) and so Î» = Î±
where Î± is one of the entries on the main diagonal of J p . These entries are of the form Î»p
p
p
where Î» âˆˆ Ïƒ (A). Thus Î» âˆˆ Ïƒ (A) and this shows Ïƒ (Ap ) âŠ† Ïƒ (A) .
p
Now take Î± âˆˆ Ïƒ (A) and consider Î± .
(
)
Î±p I âˆ’ Ap = Î±pâˆ’1 I + Â· Â· Â· + Î±Apâˆ’2 + Apâˆ’1 (Î±I âˆ’ A)
and so Î±p I âˆ’ Ap fails to be one to one which shows that Î±p âˆˆ Ïƒ (Ap ) which shows that
p
Ïƒ (A) âŠ† Ïƒ (Ap ) . 

14.5

Iterative Methods For Linear Systems

Consider the problem of solving the equation
Ax = b

(14.11)

where A is an n Ã— n matrix. In many applications, the matrix A is huge and composed
mainly of zeros. For such matrices, the method of Gauss elimination (row operations) is
not a good way to solve the system because the row operations can destroy the zeros and
storing all those zeros takes a lot of room in a computer. These systems are called sparse.
To solve them, it is common to use an iterative technique. I am following the treatment
given to this subject by Nobel and Daniel [20].
Deï¬nition 14.5.1 The Jacobi iterative technique, also called the method of simultaneous
corrections is deï¬ned as follows. Let x1 be an initial vector, say the zero vector or some
other vector. The method generates a succession of vectors, x2 , x3 , x4 , Â· Â· Â· and hopefully this
sequence of vectors will converge to the solution to (14.11). The vectors in this list are called
iterates and they are obtained according to the following procedure. Letting A = (aij ) ,
âˆ‘
aii xr+1
=âˆ’
aij xrj + bi .
(14.12)
i
jÌ¸=i

In terms of matrices, letting

ï£«

âˆ—
ï£¬ ..
A=ï£­ .
âˆ—

Saylor URL: http://www.saylor.org/courses/ma212/

Â·Â·Â·
..
.
Â·Â·Â·

ï£¶
âˆ—
.. ï£·
. ï£¸
âˆ—

The Saylor Foundation

14.5. ITERATIVE METHODS FOR LINEAR SYSTEMS

355

The iterates are deï¬ned as
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­

âˆ—

0

0
..
.

âˆ—
..
.

0 Â·Â·Â·
ï£«
0 âˆ—
ï£¬
ï£¬ âˆ— 0
= âˆ’ï£¬
ï£¬ . .
..
ï£­ ..
âˆ— Â·Â·Â·

ï£¶ï£«
0
xr+1
.. ï£· ï£¬ 1r+1
x2
. ï£·
ï£·ï£¬
.
ï£·ï£¬
ï£­ ..
0 ï£¸
xr+1
n
âˆ—
ï£¶ï£«
Â·Â·Â· âˆ—
xr
.. ï£· ï£¬ x1r
..
. . ï£·
2
ï£·ï£¬
ï£¬ ..
ï£·
..
. âˆ— ï£¸ï£­ .
xrn
âˆ— 0

Â·Â·Â·
..
.
..
.
0

ï£¶
ï£·
ï£·
ï£·
ï£¸
ï£¶

ï£«

ï£· ï£¬
ï£· ï£¬
ï£·+ï£¬
ï£¸ ï£­

b1
b2
..
.

ï£¶
ï£·
ï£·
ï£·
ï£¸

(14.13)

bn

The matrix on the left in (14.13) is obtained by retaining the main diagonal of A and
setting every other entry equal to zero. The matrix on the right in (14.13) is obtained from
A by setting every diagonal entry equal to zero and retaining all the other entries unchanged.
Example 14.5.2 Use the Jacobi method to solve the system
ï£«
ï£¶ï£«
ï£¶ ï£«
3 1 0 0
x1
1
ï£¬ 1 4 1 0 ï£· ï£¬ x2 ï£· ï£¬ 2
ï£¬
ï£·ï£¬
ï£· ï£¬
ï£­ 0 2 5 1 ï£¸ ï£­ x3 ï£¸ = ï£­ 3
0 0 2 4
x4
4

ï£¶
ï£·
ï£·
ï£¸

Of course this is solved most easily using row reductions. The Jacobi method is useful
when the matrix is 1000Ã—1000 or larger. This example is just to illustrate how the method
works. First lets solve it using row operations. The augmented matrix is
ï£«
ï£¶
3 1 0 0 1
ï£¬ 1 4 1 0 2 ï£·
ï£¬
ï£·
ï£­ 0 2 5 1 3 ï£¸
0 0 2 4 4
The row reduced echelon form is
ï£«

1
ï£¬ 0
ï£¬
ï£­ 0
0

0
1
0
0

0 0
0 0
1 0
0 1

6
29
11
29
8
29
25
29

ï£¶
ï£·
ï£·
ï£¸

which in terms of decimals is approximately equal to
ï£«
1.0 0
0
0 . 206
ï£¬ 0 1.0 0
0 . 379
ï£¬
ï£­ 0
0 1.0 0 . 275
0
0
0 1.0 . 862
In terms of the
ï£«
3
ï£¬ 0
ï£¬
ï£­ 0
0

ï£¶
ï£·
ï£·.
ï£¸

matrices, the Jacobi iteration is of the form
ï£«
ï£¶ ï£« r+1 ï£¶
ï£¶ï£«
x1
0 0 0
0 1 0 0
ï£¬
ï£¬ r+1 ï£·
ï£·ï£¬
4 0 0 ï£·
ï£· ï£¬ x2r+1 ï£· = âˆ’ ï£¬ 1 0 1 0 ï£· ï£¬
ï£¸
ï£­
ï£¸
ï£­
0 5 0
0 2 0 1 ï£¸ï£­
x3
r+1
0 0 4
0 0 2 0
x4

Saylor URL: http://www.saylor.org/courses/ma212/

ï£¶ ï£«
xr1
1
ï£¬ 2
xr2 ï£·
ï£·+ï£¬
xr3 ï£¸ ï£­ 3
xr4
4

ï£¶
ï£·
ï£·.
ï£¸

The Saylor Foundation

356

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

Multiplying by the inverse of the matrix on the left, 1 this iteration reduces to
ï£« r+1 ï£¶
ï£«
ï£¶ï£« r ï£¶ ï£« 1 ï£¶
x1
0 31 0 0
x1
3
ï£¬ xr+1 ï£·
ï£¬ 1 0 1 0 ï£· ï£¬ xr2 ï£· ï£¬ 1 ï£·
ï£¬ 2r+1 ï£· = âˆ’ ï£¬ 4 2 4 1 ï£· ï£¬ r ï£· + ï£¬ 23 ï£· .
ï£­ x
ï£¸
ï£­ 0
0 5 ï£¸ ï£­ x3 ï£¸ ï£­ 5 ï£¸
3
5
1
xr4
1
0
0
0
xr+1
4
2

(14.14)

Now iterate this starting with
ï£«

ï£¶
0
ï£¬ 0 ï£·
ï£·
x1 â‰¡ ï£¬
ï£­ 0 ï£¸.
0
ï£«

Thus

1
3

0

ï£¬
x2 = âˆ’ ï£¬
ï£­

1
4

0
1
4

0
2
5

0
0

0
1
2

0

Then
ï£«
ï£¬
x3 = âˆ’ ï£¬
ï£­

ï£¶ï£«
0
ï£¬
0 ï£·
ï£·ï£¬
1 ï£¸ï£­
5
0

ï£¶ ï£« 1
0
3
ï£¬ 1
0 ï£·
ï£· + ï£¬ 23
0 ï£¸ ï£­ 5
0
1

x2

0

1
3

1
4

0

0
0

0

2
5

0
1
4

0
1
2

ï£¶{
ï£¶ zï£« }|
1

ï£« 1
0
3
3
ï£¬ 1 ï£· ï£¬ 1
0 ï£·
2 ï£·+ï£¬ 2
ï£¬
ï£·
1 ï£¸ï£­ 3 ï£¸
ï£­ 3
5
5
5
1
1
0

ï£¶

ï£«

ï£· ï£¬
ï£·=ï£¬
ï£¸ ï£­

1
3
1
2
3
5

ï£¶
ï£·
ï£·
ï£¸

1

ï£¶
. 166
ï£· ï£¬ . 26 ï£·
ï£·=ï£¬
ï£·
ï£¸ ï£­ .2 ï£¸
.7
ï£¶

ï£«

Continuing this way one ï¬nally gets
ï£«
ï£¬
x6 = âˆ’ ï£¬
ï£­

0

1
3

1
4

0

0
0

0

2
5

0
1
4

0
1
2

x5

}|
0
. 197
ï£¬
0 ï£·
ï£· ï£¬ . 351
1 ï£¸ï£­
. 256 6
5
. 822
0
z
ï£¶ï£«

ï£¶{

ï£¶
. 216
ï£· ï£¬
ï£· ï£¬ . 386 ï£·
ï£·+ï£¬
ï£·=ï£¬
ï£·
ï£¸ ï£­
ï£¸ ï£­ . 295 ï£¸ .
. 871
1
ï£«

1
3
1
2
3
5

ï£¶

ï£«

You can keep going like this. Recall the solution is approximately equal to
ï£«
ï£¶
. 206
ï£¬ . 379 ï£·
ï£¬
ï£·
ï£­ . 275 ï£¸
. 862
so you see that with no care at all and only 6 iterations, an approximate solution has been
obtained which is not too far oï¬€ from the actual solution.
It is important to realize that a computer would use (14.12) directly. Indeed, writing
the problem in terms of matrices as I have done above destroys every beneï¬t of the method.
However, it makes it a little easier to see what is happening and so this is why I have
presented it in this way.
Deï¬nition 14.5.3 The Gauss Seidel method, also called the method of successive corrections is given as follows. For A = (aij ) , the iterates for the problem Ax = b are obtained
according to the formula
i
n
âˆ‘
âˆ‘
r+1
aij xj = âˆ’
aij xrj + bi .
(14.15)
j=1

j=i+1

1 You certainly would not compute the invese in solving a large system. This is just to show you how the
method works for this simple example. You would use the ï¬rst description in terms of indices.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

14.5. ITERATIVE METHODS FOR LINEAR SYSTEMS
In terms of matrices, letting

ï£«

âˆ—
ï£¬ ..
A=ï£­ .
âˆ—

357

ï£¶
âˆ—
.. ï£·
. ï£¸
âˆ—

Â·Â·Â·
..
.
Â·Â·Â·

The iterates are deï¬ned as
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­

âˆ—

0

âˆ—
..
.

âˆ—
..
.

âˆ—
ï£«

Â·Â·Â·

ï£¬
ï£¬
= âˆ’ï£¬
ï£¬
ï£­

ï£¶ï£«
0
xr+1
.. ï£· ï£¬ 1r+1
x2
. ï£·
ï£·ï£¬
.
ï£·ï£¬
ï£­ ..
0 ï£¸
xr+1
n
âˆ—
ï£¶ï£«
Â·Â·Â· âˆ—
xr
.. ï£· ï£¬ x1r
..
. . ï£·
2
ï£·ï£¬
..
ï£·ï£¬
..
. âˆ— ï£¸ï£­ .
xrn
0 0

Â·Â·Â·
..
.
..
.
âˆ—

0

âˆ—

0
..
.

0
..
.

0 Â·Â·Â·

ï£¶
ï£·
ï£·
ï£·
ï£¸
ï£¶

ï£«

ï£· ï£¬
ï£· ï£¬
ï£·+ï£¬
ï£¸ ï£­

b1
b2
..
.

ï£¶
ï£·
ï£·
ï£·
ï£¸

(14.16)

bn

In words, you set every entry in the original matrix which is strictly above the main
diagonal equal to zero to obtain the matrix on the left. To get the matrix on the right,
you set every entry of A which is on or below the main diagonal equal to zero. Using the
iteration procedure of (14.15) directly, the Gauss Seidel method makes use of the very latest
information which is available at that stage of the computation.
The following example is the same as the example used to illustrate the Jacobi method.
Example 14.5.4 Use the Gauss Seidel method to
ï£«
ï£¶ï£«
3 1 0 0
x1
ï£¬ 1 4 1 0 ï£· ï£¬ x2
ï£¬
ï£·ï£¬
ï£­ 0 2 5 1 ï£¸ ï£­ x3
0 0 2 4
x4
In terms of
ï£«
3
ï£¬ 1
ï£¬
ï£­ 0
0

matrices, this procedure is
ï£¶ ï£« r+1 ï£¶
ï£«
x1
0 0 0
ï£¬ r+1 ï£·
ï£¬
4 0 0 ï£·
ï£· ï£¬ x2r+1 ï£· = âˆ’ ï£¬
ï£¸
ï£­
ï£¸
ï£­
2 5 0
x3
r+1
0 2 4
x4

0
0
0
0

solve the system
ï£¶ ï£« ï£¶
1
ï£· ï£¬ 2 ï£·
ï£·=ï£¬ ï£·
ï£¸ ï£­ 3 ï£¸
4

1
0
0
0

0
1
0
0

ï£¶ï£« r
0
x1
ï£¬ xr2
0 ï£·
ï£·ï£¬
1 ï£¸ ï£­ xr3
0
xr4

ï£¶

ï£¶
1
ï£· ï£¬ 2 ï£·
ï£· + ï£¬ ï£·.
ï£¸ ï£­ 3 ï£¸
4

Multiplying by the inverse of the matrix on the left2 this yields
ï£¶ï£« r ï£¶ ï£«
ï£«
ï£« r+1 ï£¶
1
x1
0
0
0
x1
3
1
ï£· ï£¬ xr2 ï£· ï£¬
ï£¬ xr+1 ï£·
ï£¬ 0 âˆ’1
0
12
4
ï£·ï£¬ r ï£· + ï£¬
ï£¬ 2r+1 ï£· = âˆ’ ï£¬
1
1
1
ï£¸ ï£­ x3 ï£¸ ï£­
ï£¸
ï£­ 0
ï£­ x
âˆ’ 10
3
30
5
1
1
1
xr4
0
âˆ’
âˆ’
xr+1
4
60
20
10

ï£«

1
3
5
12
13
30
47
60

ï£¶
ï£·
ï£·
ï£¸

As before, I will be totally unoriginal in the choice of x1 . Let it equal the zero vector.
Therefore,
ï£« 1 ï£¶
ï£¬
x =ï£¬
ï£­
2

3
5
12
13
30
47
60

ï£·
ï£·.
ï£¸

2 As in the case of the Jacobi iteration, the computer would not do this. It would use the iteration
procedure in terms of the entries of the matrix directly. Otherwise all beneï¬t to using this method is lost.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

358

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

Now

x2

ï£«

1
0
3
1
ï£¬
0 âˆ’ 12
x3 = âˆ’ ï£¬
1
ï£­ 0
30
1
0 âˆ’ 60

It follows

ï£«

0
ï£¬
0
x5 = âˆ’ ï£¬
ï£­ 0
0

1
3
1
âˆ’ 12
1
30
1
âˆ’ 60

0
0

1
4
1
âˆ’ 10
1
20

ï£«

1
0
3
ï£¬ 0 âˆ’1
4
12
ï£¬
x = âˆ’ï£­
1
0
30
1
0 âˆ’ 60

and so

0

1
5
1
âˆ’ 10

0
1
4
1
âˆ’ 10
1
20

0
1
4
1
âˆ’ 10
1
20

ï£¶ zï£« }|
ï£¶{
1

1
5
1
âˆ’ 10

1
5
1
âˆ’ 10

ï£·ï£¬
ï£·ï£¬
ï£¸ï£­

ï£«

ï£· ï£¬
ï£·+ï£¬
ï£¸ ï£­

ï£¶ï£«

0
0

0
0

3
5
12
13
30
47
60

ï£¶ ï£«
. 194
ï£· ï£¬ . 343 ï£· ï£¬
ï£·ï£¬
ï£· ï£¬
ï£¸ ï£­ . 306 ï£¸ + ï£­
. 846

ï£¶ï£«

ï£¶ ï£«
. 219
ï£· ï£¬ . 368 75 ï£· ï£¬
ï£·ï£¬
ï£· ï£¬
ï£¸ ï£­ . 283 3 ï£¸ + ï£­
. 858 35

1
3
5
12
13
30
47
60

1
3
5
12
13
30
47
60

1
3
5
12
13
30
47
60

ï£¶

ï£«

ï£¶

ï£«

ï£¶

ï£«

ï£¶
. 194
ï£· ï£¬ . 343 ï£·
ï£·=ï£¬
ï£·
ï£¸ ï£­ . 306 ï£¸ .
. 846
ï£¶
. 219
ï£· ï£¬ . 368 75 ï£·
ï£·=ï£¬
ï£·
ï£¸ ï£­ . 283 3 ï£¸
. 858 35
ï£¶
. 210 42
ï£· ï£¬ . 376 57 ï£·
ï£·=ï£¬
ï£·
ï£¸ ï£­ . 277 7 ï£¸ .
. 861 15

ï£«

ï£¶
. 206
ï£¬ . 379 ï£·
ï£¬
ï£·
ï£­ . 275 ï£¸
. 862

Recall the answer is

so the iterates are already pretty close to the answer. You could continue doing these iterates
and it appears they converge to the solution. Now consider the following example.
Example 14.5.5 Use the Gauss Seidel method to
ï£¶ï£«
ï£«
1 4 0 0
x1
ï£¬ 1 4 1 0 ï£· ï£¬ x2
ï£¬
ï£·ï£¬
ï£­ 0 2 5 1 ï£¸ ï£­ x3
0 0 2 4
x4

solve the system
ï£¶ ï£« ï£¶
1
ï£· ï£¬ 2 ï£·
ï£·=ï£¬ ï£·
ï£¸ ï£­ 3 ï£¸
4

The exact solution is given by doing row
this is done the row echelon form is
ï£«
1 0 0
ï£¬ 0 1 0
ï£¬
ï£­ 0 0 1
0 0 0

ï£¶
6
âˆ’ 54 ï£·
ï£·
1 ï£¸

operations on the augmented matrix. When
0
0
0
1

1
2

and so the solution is approximately
ï£«

ï£¶ ï£«
6
6.0
ï£¬ âˆ’ 5 ï£· ï£¬ âˆ’1. 25
ï£¬ 4 ï£·=ï£¬
ï£­ 1 ï£¸ ï£­ 1.0
1
.5
2

The Gauss Seidel iterations are of the form
ï£«
ï£«
ï£¶ ï£« r+1 ï£¶
x1
1 0 0 0
ï£¬ 1 4 0 0 ï£· ï£¬ xr+1 ï£·
ï£¬
ï£¬
ï£·
ï£¬
ï£·ï£¬ 2
ï£­ 0 2 5 0 ï£¸ ï£­ xr+1 ï£¸ = âˆ’ ï£­
3
0 0 2 4
xr+1
4

Saylor URL: http://www.saylor.org/courses/ma212/

0
0
0
0

4
0
0
0

0
1
0
0

ï£¶
ï£·
ï£·
ï£¸

ï£¶ï£« r
0
x1
ï£¬ xr2
0 ï£·
ï£·ï£¬
1 ï£¸ ï£­ xr3
0
xr4

ï£¶

ï£«

ï£¶
1
ï£· ï£¬ 2 ï£·
ï£·+ï£¬ ï£·
ï£¸ ï£­ 3 ï£¸
4

The Saylor Foundation

14.5. ITERATIVE METHODS FOR LINEAR SYSTEMS
and so, multiplying by the inverse of the matrix on
following in terms of matrix multiplication.
ï£«
0 4
0
0
1
ï£¬ 0 âˆ’1
0
r+1
4
x
= âˆ’ï£¬
1
1
2
ï£­ 0
âˆ’
5
10
5
1
1
1
0 âˆ’5
âˆ’ 10
20

359

the left, the iteration reduces to the
ï£¶

ï£«

ï£· r ï£¬
ï£·x + ï£¬
ï£¸
ï£­

1
1
4
1
2
3
4

ï£¶
ï£·
ï£·.
ï£¸

This time, I will pick an initial vector close to the answer. Let
ï£«
ï£¶
6
ï£¬ âˆ’1 ï£·
ï£·
x1 = ï£¬
ï£­ 1 ï£¸
1
2

This is very close to the answer. Now lets
ï£«
0 4
0
0
1
ï£¬
0
âˆ’1
0
4
x2 = âˆ’ ï£¬
2
1
1
ï£­ 0
âˆ’
5
10
5
1
1
1
0 âˆ’5
âˆ’ 10
20

see what the Gauss
ï£¶ï£«
ï£¶ ï£«
1
6
ï£· ï£¬ âˆ’1 ï£· ï£¬ 1
ï£·ï£¬
ï£· ï£¬ 4
ï£¸ï£­ 1 ï£¸ + ï£­ 1
1
2

2
3
4

Seidel iteration does to it.
ï£¶ ï£«
ï£¶
5.0
ï£· ï£¬ âˆ’1.0 ï£·
ï£·=ï£¬
ï£·
ï£¸ ï£­ .9 ï£¸
. 55

You canâ€™t expect to be real close after only one iteration. Lets do another.
ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶ ï£«
ï£¶
0 4
0
0
1
5.0
5.0
1
ï£¬ 0 âˆ’1
ï£¬
ï£· ï£¬ 1 ï£· ï£¬
ï£·
0 ï£·
4
ï£· ï£¬ âˆ’1.0 ï£· + ï£¬ 41 ï£· = ï£¬ âˆ’. 975 ï£·
x3 = âˆ’ ï£¬
2
1
1
ï£­ 0
ï£¸ï£­ .9 ï£¸ ï£­
ï£¸ ï£­ . 88 ï£¸
âˆ’ 10
5
5
2
1
1
3
.
55
. 56
0 âˆ’ 15
âˆ’
20
10
4
ï£¶ ï£«
ï£¶
ï£¶ï£«
ï£¶ ï£«
ï£«
0
0
1
0 4
5.0
4. 9
1
ï£¬
ï£· ï£¬ 1 ï£· ï£¬
ï£·
ï£¬ 0 âˆ’1
0 ï£·
4
ï£· ï£¬ âˆ’. 975 ï£· + ï£¬ 41 ï£· = ï£¬ âˆ’. 945 ï£·
x4 = âˆ’ ï£¬
2
1
1
ï£¸
ï£­
ï£¸
ï£­
ï£¸
ï£­
ï£­ 0
. 88
. 866 ï£¸
âˆ’ 10
5
5
2
1
1
1
3
. 56
. 567
0 âˆ’5
âˆ’ 10
20
4
The iterates seem to be getting farther from the actual solution. Why is the process which
worked so well in the other examples not working here? A better question might be: Why
does either process ever work at all?
Both iterative procedures for solving
Ax = b

(14.17)

are of the form
Bxr+1 = âˆ’Cxr + b
where A = B + C. In the Jacobi procedure, the matrix C was obtained by setting the
diagonal of A equal to zero and leaving all other entries the same while the matrix B was
obtained by making every entry of A equal to zero other than the diagonal entries which are
left unchanged. In the Gauss Seidel procedure, the matrix B was obtained from A by making
every entry strictly above the main diagonal equal to zero and leaving the others unchanged
and C was obtained from A by making every entry on or below the main diagonal equal to
zero and leaving the others unchanged. Thus in the Jacobi procedure, B is a diagonal matrix
while in the Gauss Seidel procedure, B is lower triangular. Using matrices to explicitly solve
for the iterates, yields
xr+1 = âˆ’B âˆ’1 Cxr + B âˆ’1 b.
(14.18)
This is what you would never have the computer do but this is what will allow the statement
of a theorem which gives the condition for convergence of these and all other similar methods.
Recall the deï¬nition of the spectral radius of M, Ï (M ) , in Deï¬nition 14.3.1 on Page 348.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

360

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

(
)
Theorem 14.5.6 Suppose Ï B âˆ’1 C < 1. Then the iterates in (14.18) converge to the
unique solution of (14.17).
I will prove this theorem in the next section. The proof depends on analysis which should
not be surprising because it involves a statement about convergence of sequences.
What is an easy to verify suï¬ƒcient condition which will imply the above holds? It is easy
to give one in the
âˆ‘case of the Jacobi method. Suppose the matrix A is diagonally dominant.
That is |aii | > jÌ¸=i |aij | . Then B would be the diagonal matrix consisting of the entries
|aii | . You can see then that every entry of B âˆ’1 C has absolute value less than 1. Thus if
you let the norm B âˆ’1 C âˆ be given by the maximum of the absolute values of the entries
of the matrix, then B âˆ’1 C âˆ = r < 1. Also, by equivalence of norms it follows there exist
positive constants Î´, âˆ† such that
Î´ ||Â·|| â‰¤ ||Â·||âˆ â‰¤ âˆ† ||Â·||
(
)âˆ’1
where here ||Â·|| is an operator norm. It follows that if |Î»| â‰¥ 1, then Î»I âˆ’ B âˆ’1 C
exists.
In fact it equals
( âˆ’1 )k
âˆ
âˆ‘
B C
Î»âˆ’1
,
Î»
k=0

the series converging because
n ( âˆ’1 )k
âˆ‘
B C
Î»

k=m

â‰¤

âˆ
âˆ‘

(
âˆ†

k=m

B âˆ’1 C
Î»

â‰¤
âˆ

( âˆ’1 )k
âˆ
âˆ‘
B C
Î»

k=m

)k
âˆ

âˆ
âˆ‘

(
âˆ†

k=m

B âˆ’1 C
Î»

)

k

( m )
âˆ
âˆ†
r
âˆ† âˆ‘ k
r â‰¤
Î´
Î´
1
âˆ’r
âˆ
k=m
k=m
(
)
which shows the partial sums form a Cauchy sequence. Therefore, Ï B âˆ’1 C < 1 in this
case.
You might try a similar argument in the case of the Gauss Seidel method.
â‰¤

14.6

âˆ
âˆ‘

âˆ†
Î´

(

B âˆ’1 C
Î»

)

â‰¤

âˆ

k

â‰¤

Theory Of Convergence

Deï¬nition 14.6.1 A normed vector space, E with norm ||Â·|| is called a Banach space if it
is also complete. This means that every Cauchy sequence converges. Recall that a sequence
âˆ
{xn }n=1 is a Cauchy sequence if for every Îµ > 0 there exists N such that whenever m, n > N,
||xn âˆ’ xm || < Îµ.
Thus whenever {xn } is a Cauchy sequence, there exists x such that
lim ||x âˆ’ xn || = 0.

nâ†’âˆ

Example 14.6.2 Let â„¦ be a nonempty subset of a normed linear space, F. Denote by
BC (â„¦; E) the set of bounded continuous functions having values in E where E is a Banach
space. Then deï¬ne the norm on BC (â„¦; E) by
||f || â‰¡ sup {||f (x)||E : x âˆˆ â„¦} .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

14.6. THEORY OF CONVERGENCE

361

Lemma 14.6.3 The space BC (â„¦; E) with the given norm is a Banach space.
Proof: It is obvious ||Â·|| is a norm. It only remains to verify BC (â„¦; E) is complete. Let
{fn } be a Cauchy sequence. Then pick x âˆˆ â„¦.
||fn (x) âˆ’ fm (x)||E â‰¤ ||fn âˆ’ fm || < Îµ
whenever m, n are large enough. Thus, for each x, {fn (x)} is a Cauchy sequence in E.
Since E is complete, it follows there exists a function, f deï¬ned on â„¦ such that f (x) =
limnâ†’âˆ fn (x).
It remains to verify that f âˆˆ BC (â„¦; E) and that ||f âˆ’ fn || â†’ 0. I will ï¬rst show that
(
)
lim sup {||f (x) âˆ’ fn (x)||E } = 0.
(14.19)
nâ†’âˆ

xâˆˆâ„¦

From this it will follow that f is bounded. Then I will show that f is continuous and
||f âˆ’ fn || â†’ 0. Let Îµ > 0 be given and let N be such that for m, n > N
||fn âˆ’ fm || < Îµ/3.
Then it follows that for all x,
||f (x) âˆ’ fm (x)||E = lim ||fn (x) âˆ’ fm (x)||E â‰¤ Îµ/3
nâ†’âˆ

Therefore, for m > N,
sup {||f (x) âˆ’ fm (x)||E } â‰¤

xâˆˆâ„¦

Îµ
< Îµ.
3

This proves (14.19). Then by the triangle inequality and letting N be as just described,
pick m > N. Then for any x âˆˆ â„¦
||f (x)||E â‰¤ ||fm (x)||E + Îµ â‰¤ ||fm || + Îµ.
Hence f is bounded. Now pick x âˆˆ â„¦ and let Îµ > 0 be given and N be as above. Then
||f (x) âˆ’ f (y)||E

â‰¤ ||f (x) âˆ’ fm (x)||E + ||fm (x) âˆ’ fm (y)||E + ||fm (y) âˆ’ f (y)||E
Îµ
Îµ
â‰¤
+ ||fm (x) âˆ’ fm (y)||E + .
3
3

Now by continuity of fm , the middle term is less than Îµ/3 whenever ||x âˆ’ y|| is suï¬ƒciently
small. Therefore, f is also continuous. Finally, from the above,
||f âˆ’ fn || â‰¤

Îµ
3

whenever n > N and so limnâ†’âˆ ||f âˆ’ fn || = 0 as claimed. 
The most familiar example of a Banach space is Fn . The following lemma is of great
importance so it is stated in general.
Lemma 14.6.4 Suppose T : E â†’ E where E is a Banach space with norm |Â·|. Also suppose
|T x âˆ’ T y| â‰¤ r |x âˆ’ y|

(14.20)

for some r âˆˆ (0, 1). Then there exists a unique ï¬xed point, x âˆˆ E such that
T x = x.

Saylor URL: http://www.saylor.org/courses/ma212/

(14.21)

The Saylor Foundation

362

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

Letting x1 âˆˆ E, this ï¬xed point, x, is the limit of the sequence of iterates,
x1 , T x1 , T 2 x1 , Â· Â· Â· .

(14.22)

In addition to this, there is a nice estimate which tells how close x1 is to x in terms of
things which can be computed.
x1 âˆ’ x â‰¤

1
x1 âˆ’ T x1 .
1âˆ’r

(14.23)

{
}âˆ
Proof: This follows easily when it is shown that the above sequence, T k x1 k=1 is a
Cauchy sequence. Note that
T 2 x1 âˆ’ T x1 â‰¤ r T x1 âˆ’ x1 .
Suppose
T k x1 âˆ’ T kâˆ’1 x1 â‰¤ rkâˆ’1 T x1 âˆ’ x1 .

(14.24)

Then
T k+1 x1 âˆ’ T k x1

â‰¤ r T k x1 âˆ’ T kâˆ’1 x1
â‰¤ rrkâˆ’1 T x1 âˆ’ x1 = rk T x1 âˆ’ x1 .

By induction, this shows that for all k â‰¥ 2, (14.24) is valid. Now let k > l â‰¥ N.
T k x1 âˆ’ T l x1

=

kâˆ’1
âˆ‘

kâˆ’1
âˆ‘
( j+1 1
)
T
x âˆ’ T j x1 â‰¤
T j+1 x1 âˆ’ T j x1

j=l

â‰¤

kâˆ’1
âˆ‘

j=l

rj T x1 âˆ’ x1 â‰¤ T x1 âˆ’ x1

j=N

rN
1âˆ’r

which converges to 0 as N â†’ âˆ. Therefore, this is a Cauchy sequence so it must converge
to x âˆˆ E. Then
x = lim T k x1 = lim T k+1 x1 = T lim T k x1 = T x.
kâ†’âˆ

kâ†’âˆ

kâ†’âˆ

This shows the existence of the ï¬xed point. To show it is unique, suppose there were
another one, y. Then
|x âˆ’ y| = |T x âˆ’ T y| â‰¤ r |x âˆ’ y|
and so x = y.
It remains to verify the estimate.
x1 âˆ’ x

â‰¤

x1 âˆ’ T x1 + T x1 âˆ’ x = x1 âˆ’ T x1 + T x1 âˆ’ T x

â‰¤

x1 âˆ’ T x1 + r x1 âˆ’ x

and solving the inequality for x1 âˆ’ x gives the estimate desired. 
The following corollary is what will be used to prove the convergence condition for the
various iterative procedures.
Corollary 14.6.5 Suppose T : E â†’ E, for some constant C
|T x âˆ’ T y| â‰¤ C |x âˆ’ y| ,
for all x, y âˆˆ E, and for some N âˆˆ N,
T N x âˆ’ T N y â‰¤ r |x âˆ’ y| ,
for all x, y âˆˆ E where r âˆˆ{(0, 1).}Then there exists a unique ï¬xed point for T and it is still
the limit of the sequence, T k x1 for any choice of x1 .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

14.7. EXERCISES

363

Proof: From Lemma 14.6.4 there exists a unique ï¬xed point for T N denoted here as x.
Therefore, T N x = x. Now doing T to both sides,
T N T x = T x.
By uniqueness, T x = x because the above equation shows T x is a ï¬xed point of T N and
there is only one ï¬xed point of T N . In fact, there is only one ï¬xed point of T because a
ï¬xed point of T is automatically a ï¬xed point of T N .
It remains to show T k x1 â†’ x, the unique ï¬xed point of T N . If this does not happen,
there exists Îµ > 0 and a subsequence, still denoted by T k such that
T k x1 âˆ’ x â‰¥ Îµ
Now k = jk N + rk where rk âˆˆ {0, Â· Â· Â· , N âˆ’ 1} and jk is a positive integer such that
limkâ†’âˆ jk = âˆ. Then there exists a single r âˆˆ {0, Â· Â· Â· , N âˆ’ 1} such that for inï¬nitely
many k, rk = r. Taking a further subsequence, still denoted by T k it follows
T jk N +r x1 âˆ’ x â‰¥ Îµ

(14.25)

However,
T jk N +r x1 = T r T jk N x1 â†’ T r x = x
and this contradicts (14.25). 
(
)
Theorem 14.6.6 Suppose Ï B âˆ’1 C < 1. Then the iterates in (14.18) converge to the
unique solution of (14.17).
Proof: Consider the iterates in (14.18). Let T x = B âˆ’1 Cx + b. Then
T kx âˆ’ T ky =

(

B âˆ’1 C

)k

(
)k
(
)k
x âˆ’ B âˆ’1 C y â‰¤ B âˆ’1 C
|x âˆ’ y| .

Here ||Â·|| refers to any of the operator norms. It doesnâ€™t matter which one you pick because
they are all equivalent. I am writing the (proof to
) indicate the operator norm taken with
respect to the usual norm on E. Since Ï B âˆ’1 C < 1, it follows from Gelfandâ€™s theorem,
Theorem 14.3.3 on Page 349, there exists N such that if k â‰¥ N, then for some r1/k < 1,
(

B âˆ’1 C

)k

1/k

< r1/k < 1.

Consequently,
T N x âˆ’ T N y â‰¤ r |x âˆ’ y| .
Also |T x âˆ’ T y| â‰¤ B âˆ’1 C |x âˆ’ y| and so Corollary 14.6.5 applies and gives the conclusion
of this theorem. 

14.7

Exercises

1. Solve the system

ï£«

4
ï£­ 1
0

1
5
2

ï£¶ï£«
ï£¶ ï£«
ï£¶
1
x
1
2 ï£¸ï£­ y ï£¸ = ï£­ 2 ï£¸
6
z
3

using the Gauss Seidel method and the Jacobi method. Check your answer by also
solving it using row operations.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

364

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

2. Solve the system

ï£«

4
ï£­ 1
0

1
7
2

ï£¶ï£«
ï£¶ ï£«
ï£¶
1
x
1
2 ï£¸ï£­ y ï£¸ = ï£­ 2 ï£¸
4
z
3

using the Gauss Seidel method and the Jacobi method. Check your answer by also
solving it using row operations.
3. Solve the system

ï£«

5
ï£­ 1
0

1
7
2

ï£¶ï£«
ï£¶ ï£«
ï£¶
1
x
1
2 ï£¸ï£­ y ï£¸ = ï£­ 2 ï£¸
4
z
3

using the Gauss Seidel method and the Jacobi method. Check your answer by also
solving it using row operations.
4. If you are considering a system of the form Ax = b and Aâˆ’1 does not exist, will either
the Gauss Seidel or Jacobi methods work? Explain. What does this indicate about
ï¬nding eigenvectors for a given eigenvalue?
5. For ||x||âˆ â‰¡ max {|xj | : j = 1, 2, Â· Â· Â· , n} , the parallelogram identity does not hold.
Explain.
6. A norm ||Â·|| is said to be strictly convex if whenever ||x|| = ||y|| , x Ì¸= y, it follows
x+y
2

< ||x|| = ||y|| .

Show the norm |Â·| which comes from an inner product is strictly convex.
7. A norm ||Â·|| is said to be uniformly convex if whenever ||xn || , ||yn || are equal to 1 for
all n âˆˆ N and limnâ†’âˆ ||xn + yn || = 2, it follows limnâ†’âˆ ||xn âˆ’ yn || = 0. Show the
norm |Â·| coming from an inner product is always uniformly convex. Also show that
uniform convexity implies strict convexity which is deï¬ned in Problem 6.
8. Suppose A : Cn â†’ Cn is a one to one and onto matrix. Deï¬ne
||x|| â‰¡ |Ax| .
Show this is a norm.
9. If X is a ï¬nite dimensional normed vector space and A, B âˆˆ L (X, X) such that
||B|| < ||A|| , can it be concluded that Aâˆ’1 B < 1?
10. Let X be a vector space with a norm ||Â·|| and let V = span (v1 , Â· Â· Â· , vm ) be a ï¬nite
dimensional subspace of X such that {v1 , Â· Â· Â· , vm } is a basis for V. Show V is a closed
subspace of X. This means that if wn â†’ w and each wn âˆˆ V, then so is w. Next show
that if w âˆˆ
/ V,
dist (w, V ) â‰¡ inf {||w âˆ’ v|| : v âˆˆ V } > 0
is a continuous function of w and
|dist (w, V ) âˆ’ dist (w1 , V )| â‰¤ âˆ¥w1 âˆ’ wâˆ¥
Next show that if w âˆˆ
/ V, there exists z such that ||z|| = 1 and dist (z, V ) > 1/2. For
those who know some advanced calculus, show that if X is an inï¬nite dimensional
vector space having norm ||Â·|| , then the closed unit ball in X cannot be compact.
Thus closed and bounded is never compact in an inï¬nite dimensional normed vector
space.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

14.7. EXERCISES

365

11. Suppose Ï (A) < 1 for A âˆˆ L (V, V ) where V is a p dimensional vector space having
a norm ||Â·||. You can use Rp or Cp if you like. Show there exists a new norm |||Â·|||
such that with respect to this new norm, |||A||| < 1 where |||A||| denotes the operator
norm of A taken with respect to this new norm on V ,
|||A||| â‰¡ sup {|||Ax||| : |||x||| â‰¤ 1}
Hint: You know from Gelfandâ€™s theorem that
||An ||

1/n

<r<1

provided n is large enough, this operator norm taken with respect to ||Â·||. Show there
exists 0 < Î» < 1 such that
( )
A
Ï
< 1.
Î»
You can do this by arguing the eigenvalues of A/Î» are the scalars Âµ/Î» where Âµ âˆˆ Ïƒ (A).
Now let Z+ denote the nonnegative integers.
|||x||| â‰¡ sup
nâˆˆZ+

An
x
Î»n

First show this is actually a norm. Next explain why
|||Ax||| â‰¡ Î» sup
nâˆˆZ+

An+1
x â‰¤ Î» |||x||| .
Î»n+1

12. Establish a similar result to Problem 11 without using Gelfandâ€™s theorem. Use an
argument which depends directly on the Jordan form or a modiï¬cation of it.
13. Using Problem 11 give an easier proof of Theorem 14.6.6 without having to use Corollary 14.6.5. It would suï¬ƒce to use a diï¬€erent norm of this problem and the contraction
mapping principle of Lemma 14.6.4.
âˆ‘
14. A matrix A is diagonally dominant if |aii | > jÌ¸=i |aij | . Show that the Gauss Seidel
method converges if A is diagonally dominant.
âˆ‘âˆ
15. Suppose f (Î») = k=0 an Î»n converges if |Î»| < R. Show that if Ï (A) < R where A is
an n Ã— n matrix, then
âˆ
âˆ‘
f (A) â‰¡
an An
kâˆ’0

converges in L (F , F ) . Hint: Use Gelfandâ€™s theorem and the root test.
n

n

16. Referring to Corollary 14.4.3, for Î» = a + ib show
exp (Î»t) = eat (cos (bt) + i sin (bt)) .
Hint: Let y (t) = exp (Î»t) and let z (t) = eâˆ’at y (t) . Show
z â€²â€² + b2 z = 0, z (0) = 1, z â€² (0) = ib.
Now letting z = u + iv where u, v are real valued, show
uâ€²â€² + b2 u

= 0, u (0) = 1, uâ€² (0) = 0

v â€²â€² + b2 v

= 0, v (0) = 0, v â€² (0) = b.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

366

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

Next show u (t) = cos (bt) and v (t) = sin (bt) work in the above and that there is at
most one solution to
wâ€²â€² + b2 w = 0 w (0) = Î±, wâ€² (0) = Î².
Thus z (t) = cos (bt) + i sin (bt) and so y (t) = eat (cos (bt) + i sin (bt)). To show there
is at most one solution to the above problem, suppose you have two, w1 , w2 . Subtract
them. Let f = w1 âˆ’ w2 . Thus
f â€²â€² + b2 f = 0
and f is real valued. Multiply both sides by f â€² and conclude
(
)
2
2
d (f â€² )
2f
+b
=0
dt
2
2
Thus the expression in parenthesis is constant. Explain why this constant must equal
0.
17. Let A âˆˆ L (Rn , Rn ) . Show the following power series converges in L (Rn , Rn ).
âˆ k k
âˆ‘
t A
k=0

k!

You might want to use Lemma 14.4.2. This is how you can deï¬ne exp (tA). Next show
using arguments like those of Corollary 14.4.3
d
exp (tA) = A exp (tA)
dt
so that this is a matrix valued solution to the diï¬€erential equation and initial condition
Î¨â€² (t) = AÎ¨ (t) , Î¨ (0) = I.
This Î¨ (t) is called a fundamental matrix for the diï¬€erential equation yâ€² = Ay. Show
t â†’ Î¨ (t) y0 gives a solution to the initial value problem
yâ€² = Ay, y (0) = y0 .
18. In Problem 17 Î¨ (t) is deï¬ned by the given series. Denote by exp (tÏƒ (A)) the numbers
exp (tÎ») where Î» âˆˆ Ïƒ (A) . Show exp (tÏƒ (A)) = Ïƒ (Î¨ (t)) . This is like Lemma 14.4.7.
Letting J be the Jordan canonical form for A, explain why
Î¨ (t) â‰¡

âˆ k k
âˆ‘
t A
k=0

k!

=S

âˆ k k
âˆ‘
t J
k=0

k!

S âˆ’1

and you note that in J k , the diagonal entries are of the form Î»k for Î» an eigenvalue
of A. Also J = D + N where N is nilpotent and commutes with D. Argue then that
âˆ k k
âˆ‘
t J
k=0

k!

is an upper triangular matrix which has on the diagonal the expressions eÎ»t where
Î» âˆˆ Ïƒ (A) . Thus conclude
Ïƒ (Î¨ (t)) âŠ† exp (tÏƒ (A))

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

14.7. EXERCISES

367

Next take etÎ» âˆˆ exp (tÏƒ (A)) and argue it must be in Ïƒ (Î¨ (t)) . You can do this as
follows:
âˆ k k
âˆ k k
âˆ k (
)
âˆ‘
âˆ‘
âˆ‘
t Î»
t
t A
Î¨ (t) âˆ’ etÎ» I =
âˆ’
I=
Ak âˆ’ Î»k I
k!
k!
k!
k=0
k=0
k=0
ï£¶
ï£«
âˆ k kâˆ’1
âˆ‘
t âˆ‘ kâˆ’j j ï£¸
= ï£­
A Î» (A âˆ’ Î»I)
k! j=1
k=0

Now you need to argue
âˆ k kâˆ’1
âˆ‘
t âˆ‘
k=0

k!

Akâˆ’j Î»j

j=1

converges to something in L (R , R ). To do this, use the ratio test and Lemma 14.4.2
after ï¬rst using the triangle inequality. Since Î» âˆˆ Ïƒ (A) , Î¨ (t) âˆ’ etÎ» I is not one to one
and so this establishes the other inclusion. You ï¬ll in the details. This theorem is a
special case of theorems which go by the name â€œspectral mapping theoremâ€.
n

n

19. Suppose Î¨ (t) âˆˆ L (V, W ) where V, W are ï¬nite dimensional inner product spaces and
t â†’ Î¨ (t) is continuous for t âˆˆ [a, b]: For every Îµ > 0 there there exists Î´ > 0 such that
if |s âˆ’ t| < Î´ then ||Î¨ (t) âˆ’ Î¨ (s)|| < Îµ. Show t â†’ (Î¨ (t) v, w) is continuous. Here it is
the inner product in W. Also deï¬ne what it means for t â†’ Î¨ (t) v to be continuous
and show this is continuous. Do it all for diï¬€erentiable in place of continuous. Next
show t â†’ ||Î¨ (t)|| is continuous.
20. If z (t) âˆˆ W, a ï¬nite dimensional inner product space, what does it mean for t â†’ z (t)
to be continuous or diï¬€erentiable? If z is continuous, deï¬ne
âˆ« b
z (t) dt âˆˆ W
a

(

as follows.

âˆ«

)

b

w,

z (t) dt

âˆ«

b

â‰¡

a

(w, z (t)) dt.
a

Show that this deï¬nition is well deï¬ned and furthermore the triangle inequality,
âˆ« b
âˆ« b
z (t) dt â‰¤
|z (t)| dt,
a

a

and fundamental theorem of calculus,
(âˆ« t
)
d
z (s) ds = z (t)
dt
a
hold along with any other interesting properties of integrals which are true.
21. For V, W two inner product spaces, deï¬ne
âˆ« b
Î¨ (t) dt âˆˆ L (V, W )
a

as follows.

(

âˆ«
w,

)

b

Î¨ (t) dt (v)
a

Saylor URL: http://www.saylor.org/courses/ma212/

âˆ«
â‰¡

b

(w, Î¨ (t) v) dt.
a

The Saylor Foundation

368

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

âˆ«b
Show this is well deï¬ned and does indeed give a Î¨ (t) dt âˆˆ L (V, W ) . Also show the
triangle inequality
âˆ« b
âˆ« b
Î¨ (t) dt â‰¤
||Î¨ (t)|| dt
a

a

where ||Â·|| is the operator norm and verify the fundamental theorem of calculus holds.
(âˆ« t
)â€²
Î¨ (s) ds = Î¨ (t) .
a

Also verify the usual properties of integrals continue to hold such as the fact the
integral is linear and
âˆ« b
âˆ« c
âˆ« c
Î¨ (t) dt +
Î¨ (t) dt =
Î¨ (t) dt
a

b

a

and similar things. Hint: On showing the triangle inequality, it will help if you use
the fact that
|w|W = sup |(w, v)| .
|v|â‰¤1

You should show this also.
22. Prove Gronwallâ€™s inequality. Suppose u (t) â‰¥ 0 and for all t âˆˆ [0, T ] ,
âˆ« t
u (t) â‰¤ u0 +
Ku (s) ds.
0

where K is some nonnegative constant. Then
u (t) â‰¤ u0 eKt .

âˆ«t
Hint: w (t) = 0 u (s) ds. Then using the fundamental theorem of calculus, w (t)
satisï¬es the following.
u (t) âˆ’ Kw (t) = wâ€² (t) âˆ’ Kw (t) â‰¤ u0 , w (0) = 0.
Now use the usual techniques you saw in an introductory diï¬€erential equations class.
Multiply both sides of the above inequality by eâˆ’Kt and note the resulting left side is
now a total derivative. Integrate both sides from 0 to t and see what you have got. If
you have problems, look ahead in the book. This inequality is proved later in Theorem
C.4.3.
23. With Gronwallâ€™s inequality and the integral deï¬ned in Problem 21 with its properties
listed there, prove there is at most one solution to the initial value problem
yâ€² = Ay, y (0) = y0 .
Hint: If there are two solutions, subtract them and call the result z. Then
zâ€² = Az, z (0) = 0.
It follows

âˆ«
z (t) = 0+

t

Az (s) ds
0

and so

âˆ«
||z (t)|| â‰¤

t

âˆ¥Aâˆ¥ ||z (s)|| ds
0

Now consider Gronwallâ€™s inequality of Problem 22.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

14.7. EXERCISES

369

24. Suppose A is a matrix which has the property that whenever Âµ âˆˆ Ïƒ (A) , Re Âµ < 0.
Consider the initial value problem
yâ€² = Ay, y (0) = y0 .
The existence and uniqueness of a solution to this equation has been established above
in preceding problems, Problem 17 to 23. Show that in this case where the real parts
of the eigenvalues are all negative, the solution to the initial value problem satisï¬es
lim y (t) = 0.

tâ†’âˆ

Hint: A nice way to approach this problem is to show you can reduce it to the
consideration of the initial value problem
zâ€² = JÎµ z, z (0) = z0
where JÎµ is the modiï¬ed Jordan canonical form where instead of ones down the main
diagonal, there are Îµ down the main diagonal (Problem 19). Then
zâ€² = Dz + NÎµ z
where D is the diagonal matrix obtained from the eigenvalues of A and NÎµ is a nilpotent
matrix commuting with D which is very small provided Îµ is chosen very small. Now
let Î¨ (t) be the solution of
Î¨â€² = âˆ’DÎ¨, Î¨ (0) = I
described earlier as

âˆ
k
âˆ‘
(âˆ’1) tk Dk

k!

k=0

.

Thus Î¨ (t) commutes with D and NÎµ . Tell why. Next argue
â€²

(Î¨ (t) z) = Î¨ (t) NÎµ z (t)
and integrate from 0 to t. Then
âˆ«

t

Î¨ (t) z (t) âˆ’ z0 =

Î¨ (s) NÎµ z (s) ds.
0

It follows

âˆ«

t

||Î¨ (t) z (t)|| â‰¤ ||z0 || +

||NÎµ || ||Î¨ (s) z (s)|| ds.
0

It follows from Gronwallâ€™s inequality
||Î¨ (t) z (t)|| â‰¤ ||z0 || e||NÎµ ||t
Now look closely at the form of Î¨ (t) to get an estimate which is interesting. Explain
why
ï£« Âµt
ï£¶
e 1
0
ï£¬
ï£·
..
Î¨ (t) = ï£­
ï£¸
.
Âµn t
0
e
and now observe that if Îµ is chosen small enough, ||NÎµ || is so small that each component
of z (t) converges to 0.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

370

NORMS FOR FINITE DIMENSIONAL VECTOR SPACES

25. Using Problem 24 show that if A is a matrix having the real parts of all eigenvalues
less than 0 then if
Î¨â€² (t) = AÎ¨ (t) , Î¨ (0) = I
it follows
lim Î¨ (t) = 0.

tâ†’âˆ

Hint: Consider the columns of Î¨ (t)?
26. Let Î¨ (t) be a fundamental matrix satisfying
Î¨â€² (t) = AÎ¨ (t) , Î¨ (0) = I.
Show Î¨ (t) = Î¨ (nt) . Hint: Subtract and show the diï¬€erence satisï¬es Î¦â€² = AÎ¦, Î¦ (0) =
0. Use uniqueness.
n

27. If the real parts of the eigenvalues of A are all negative, show that for every positive
t,
lim Î¨ (nt) = 0.
nâ†’âˆ

Hint: Pick Re (Ïƒ (A)) < âˆ’Î» < 0 and use Problem 18 about the spectrum of Î¨ (t)
and Gelfandâ€™s theorem for the spectral radius along with Problem 26 to argue that
Î¨ (nt) /eâˆ’Î»nt < 1 for all n large enough.
28. Let H be a Hermitian matrix. (H = H âˆ— ) . Show that eiH â‰¡

âˆ‘âˆ
n=0

(iH)n
n!

is unitary.

29. Show the converse of the above exercise. If V is unitary, then V = eiH for some H
Hermitian.
30. If U is unitary and does not have âˆ’1 as an eigenvalue so that (I + U )
that
âˆ’1
H = i (I âˆ’ U ) (I + U )

âˆ’1

exists, show

is Hermitian. Then, verify that
U = (I + iH) (I âˆ’ iH)

âˆ’1

.

31. Suppose that A âˆˆ L (V, V ) where V is a normed linear space. Also suppose that
âˆ¥Aâˆ¥ < 1 where this refers to the operator norm on A. Verify that
âˆ’1

(I âˆ’ A)

=

âˆ
âˆ‘

Ai

i=0

This is called the Neumann series. Suppose now that you only know the algebraic
âˆ’1
condition Ï (A) < 1. Is it still the case that the Neumann series converges to (I âˆ’ A) ?

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Numerical Methods For Finding
Eigenvalues
15.1

The Power Method For Eigenvalues

This chapter discusses numerical methods for ï¬nding eigenvalues. However, to do this
correctly, you must include numerical analysis considerations which are distinct from linear
algebra. The purpose of this chapter is to give an introduction to some numerical methods
without leaving the context of linear algebra. In addition, some examples are given which
make use of computer algebra systems. For a more thorough discussion, you should see
books on numerical methods in linear algebra like some listed in the references.
Let A be a complex p Ã— p matrix and suppose that it has distinct eigenvalues
{Î»1 , Â· Â· Â· , Î»m }
and that |Î»1 | > |Î»k | for all k. Also let the Jordan form of A be
ï£«
ï£¶
J1
ï£¬
ï£·
..
J =ï£­
ï£¸
.
Jm
with
Jk = Î»k Ik + Nk
where

Nkrk

Ì¸= 0 but

Nkrk +1

= 0. Also let
P âˆ’1 AP = J, A = P JP âˆ’1 .

Now ï¬x x âˆˆ Fp . Take Ax and let s1 be the entry of the vector Ax which has largest
absolute value. Thus Ax/s1 is a vector y1 which has a component of 1 and every other
entry of this vector has magnitude no larger than 1. If the scalars {s1 , Â· Â· Â· , snâˆ’1 } and
vectors {y1 , Â· Â· Â· , ynâˆ’1 } have been obtained, let
yn â‰¡

Aynâˆ’1
sn

where sn is the entry of Aynâˆ’1 which has largest absolute value. Thus
yn =

An x
AAynâˆ’2
Â·Â·Â· =
sn snâˆ’1
sn snâˆ’1 Â· Â· Â· s1

(15.1)

371

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

372

NUMERICAL METHODS FOR FINDING EIGENVALUES

Consider one of the blocks in the Jordan form.
rk ( ) nâˆ’i
âˆ‘
n Î»k
n
i
Jkn = Î»n1
n Nk â‰¡ Î»1 K (k, n)
i
Î»
1
i=0
Then from the above,
ï£«
Î»n1

n

A
ï£¬
=P
ï£­
sn snâˆ’1 Â· Â· Â· s1
sn snâˆ’1 Â· Â· Â· s1

ï£¶

K (1, n)
..

ï£· âˆ’1
ï£¸P

.
K (m, n)

Consider one of the terms in the sum for K (k, n) for k > 1. Letting the norm of a matrix
be the maximum of the absolute values of its entries,
( ) nâˆ’i
n Î»k
Nki
i Î»n1

â‰¤ nrk

Î»k
Î»1

n

prk C

where C depends on the eigenvalues but is independent of n. Then this converges to 0
because the inï¬nite sum of these converges due to the root test. Thus each of the matrices
K (k, n) converges to 0 for each k > 1 as n â†’ âˆ.
Now what about K (1, n)? It equals
( )âˆ‘
r1 (( ) ( ))
n
n
n
i
/
Î»âˆ’i
1 N1
r1 i=0
i
r1
=

( )
)
n ( âˆ’r1 r1
Î»1 N1 + m (n)
r1

where limnâ†’âˆ m (n) = 0. This follows from
(( ) ( ))
n
n
lim
/
= 0, i < r1
nâ†’âˆ
i
r1
It follows that (15.1) is of the form
( ) ( ( âˆ’r1 r1
)
Î»n1
n
Î»1 N1 + m (n)
yn =
P
0
sn snâˆ’1 Â· Â· Â· s1 r1

)

Aynâˆ’1
sn
( âˆ’1 )
where the entries of En converge to 0 as n â†’ âˆ. Now denote by P x m1 the ï¬rst m1
entries of P âˆ’1 x where it is assumed that Î»1 has multiplicity m1 . Assume that
( âˆ’1 )
/ ker N1r1
P x m1 âˆˆ
0
En

P âˆ’1 x =

This will be the case unless you have made an extremely unfortunate choice of x. Then yn
is of the form
)(
)
( ) ( ( âˆ’r1 r1
)
Î»n1
n
Î»1 N1 + m (n) P âˆ’1 x m1
yn =
P
(15.2)
zn
sn snâˆ’1 Â· Â· Â· s1 r1
( )
where rn1 zn â†’ 0. Also, from the construction, there is a single entry of yn equal to 1 and
all other entries of the above vector have absolute value no larger than 1. It follows that
( )
n
Î»n1
sn snâˆ’1 Â· Â· Â· s1 r1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

15.1. THE POWER METHOD FOR EIGENVALUES

373

must be bounded independent of n.
Then it follows from this observation, that for large n, the above vector yn is approximately equal to
( ) ( âˆ’r1 r1 ( âˆ’1 )
)
Î»n1
n
Î»1 N1 P x m
1
P
0
sn snâˆ’1 Â· Â· Â· s1 r1
( nâˆ’r1 ( n ) r1
)
1
Î»1
N1
0
r
1
=
P
P âˆ’1 x
(15.3)
0
0
sn snâˆ’1 Â· Â· Â· s1
(
)
If P âˆ’1 x m âˆˆ
/ ker (N1r1 ) , then the above vector is also not equal to 0. What happens when
1
it is multiplied on the left by A âˆ’ Î»1 I = P (J âˆ’ Î»1 I) P âˆ’1 ? This results in
( nâˆ’r1 ( n ) r1
)
1
Î»1
N1 r1 N1
0
P
P âˆ’1 x = 0
0
0
sn snâˆ’1 Â· Â· Â· s1
because N1r1 +1 = 0. Therefore, the vector in (15.3) is an eigenvector and yn is approximately
equal to this eigenvector.
With this preparation, here is a theorem.
Theorem 15.1.1 Let A be a complex p Ã— p matrix such that the eigenvalues are
{Î»1 , Î»2 , Â· Â· Â· , Î»r }
with |Î»1 | > |Î»j | for all j Ì¸= 1. Then for x a given vector, let
y1 =

Ax
s1

where s1 is an entry of Ax which has the largest absolute value. If the scalars {s1 , Â· Â· Â· , snâˆ’1 }
and vectors {y1 , Â· Â· Â· , ynâˆ’1 } have been obtained, let
yn â‰¡

Aynâˆ’1
sn

where sn is the entry of Aynâˆ’1 which has largest absolute value. Then it is probably the case
that {sn } will converge to Î»1 and {yn } will converge to an eigenvector associated with Î»1 .
Proof: Consider the claim about sn+1 . It was shown above that
( âˆ’r1 r1 ( âˆ’1 )
)
Î»1 N1 P x m1
zâ‰¡P
0
is an eigenvector for Î»1 . Let zl be the entry of z which has largest absolute value. Then for
large n, it will probably be the case that the entry of yn which has largest absolute value
will also be in the lth slot. This follows from (15.2) because for large n, zn will be very
small, smaller than the largest entry of the top part of the vector in that expression. Then,
since m (n) is very small, the result follows if z has a well deï¬ned entry which has largest
absolute value. Now from the above construction,
( )
Î»n+1
n
1
sn+1 yn+1 â‰¡ Ayn â‰ˆ
z
sn Â· Â· Â· s1 r1
Applying a similar formula to sn and the above observation, about the largest entry, it
follows that for large n
( )
(
)
Î»n+1
n
Î»n1
nâˆ’1
1
sn+1 â‰ˆ
zl , sn â‰ˆ
zl
sn Â· Â· Â· s1 r1
snâˆ’1 Â· Â· Â· s1
r1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

374

NUMERICAL METHODS FOR FINDING EIGENVALUES

Therefore, for large n,

Î»1 n Â· Â· Â· (n âˆ’ r1 + 1)
sn+1
Î»1
â‰ˆ
â‰ˆ
sn
sn (n âˆ’ 1) Â· Â· Â· (n âˆ’ r1 )
sn

which shows that sn+1 â‰ˆ Î»1 .
Now from the construction and the formula in (15.2), for large n
)(
)
(
) ( ( âˆ’r1 r1
)
Î»n+1
n+1
Î»1 N1 + m (n) P âˆ’1 x m1
1
yn+1 =
P
zn
sn+1 snâˆ’1 Â· Â· Â· s1
r1
)(
)
(
) ( ( âˆ’r1 r1
)
n
Î»1
Î»1
n+1
Î»1 N1 + m (n) P âˆ’1 x m1
=
P
zn
sn+1 sn snâˆ’1 Â· Â· Â· s1
r1
(n+1)
)(
)
( ) ( ( âˆ’r1 r1
)
n
n
Î»1
Î»1 N1 + m (n) P âˆ’1 x m1
r1
P
â‰ˆ (n)
zn
sn snâˆ’1 Â· Â· Â· s1 r1
r1
(n+1)
=

(rn1 ) yn â‰ˆ yn
r1

Thus {yn } is a Cauchy sequence and must converge to a vector v. Now from the construction,
Î»1 v = lim sn+1 yn+1 = lim Ayn = Av. 
nâ†’âˆ

nâ†’âˆ

In summary, here is the procedure.
Finding the largest eigenvalue with its eigenvector.
1. Start with a vector, u1 which you hope is not unlucky.
2. If uk is known,
uk+1 =

Auk
sk+1

where sk+1 is the entry of Auk which has largest absolute value.
3. When the scaling factors sk are not changing much, sk+1 will be close to the eigenvalue
and uk+1 will be close to an eigenvector.
4. Check your answer to see if it worked well.
ï£«

5 âˆ’14
4
Example 15.1.2 Find the largest eigenvalue of A = ï£­ âˆ’4
3
6

ï£¶
11
âˆ’4 ï£¸ .
âˆ’3

T

You can begin with u1 = (1, Â· Â· Â· , 1) and apply the above procedure. However, you can
accelerate the process if you begin with An u1 and then divide by the largest entry to get
the ï¬rst approximate eigenvector. Thus
ï£«
ï£¶20 ï£« ï£¶ ï£«
ï£¶
5 âˆ’14 11
1
2. 555 8 Ã— 1021
ï£­ âˆ’4
4
âˆ’4 ï£¸ ï£­ 1 ï£¸ = ï£­ âˆ’1. 277 9 Ã— 1021 ï£¸
3
6
âˆ’3
1
âˆ’3. 656 2 Ã— 1015
Divide by the largest entry to obtain a good aproximation.
ï£«
ï£¶
ï£«
ï£¶
2. 555 8 Ã— 1021
1.0
1
ï£­ âˆ’1. 277 9 Ã— 1021 ï£¸
ï£¸
âˆ’0.5
=ï£­
2. 555 8 Ã— 1021
15
âˆ’3. 656 2 Ã— 10
âˆ’1. 430 6 Ã— 10âˆ’6

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

15.1. THE POWER METHOD FOR EIGENVALUES

375

Now begin with this one.
ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
5 âˆ’14 11
1.0
12. 000
ï£­ âˆ’4
ï£¸=ï£­
ï£¸
4
âˆ’4 ï£¸ ï£­
âˆ’0.5
âˆ’6. 000 0
âˆ’6
âˆ’6
3
6
âˆ’3
âˆ’1. 430 6 Ã— 10
4. 291 8 Ã— 10
Divide by 12 to get the next iterate.
ï£«
ï£«
ï£¶
ï£¶
1.0
12. 000
ï£­
ï£¸ 1 =ï£­
ï£¸
âˆ’0.5
âˆ’6. 000 0
12
3. 576 5 Ã— 10âˆ’7
4. 291 8 Ã— 10âˆ’6
Another iteration will reveal that the scaling factor is still 12. Thus this is an approximate
eigenvalue. In fact, it is the largest eigenvalue and the corresponding eigenvector is
ï£«
ï£¶
1.0
ï£­ âˆ’0.5 ï£¸
0
The process has worked very well.

15.1.1

The Shifted Inverse Power Method

This method can ï¬nd various eigenvalues and eigenvectors. It is a signiï¬cant generalization
of the above simple procedure and yields very good results. One can ï¬nd complex eigenvalues
using this method. The situation is this: You have a number, Î± which is close to Î», some
eigenvalue of an n Ã— n matrix A. You donâ€™t know Î» but you know that Î± is closer to Î»
than to any other eigenvalue. Your problem is to ï¬nd both Î» and an eigenvector which goes
with Î». Another way to look at this is to start with Î± and seek the eigenvalue Î», which is
closest to Î± along with an eigenvector associated with Î». If Î± is an eigenvalue of A, then
you have what you want. Therefore, I will always assume Î± is not an eigenvalue of A and
âˆ’1
so (A âˆ’ Î±I) exists. The method is based on the following lemma.
n

Lemma 15.1.3 Let {Î»k }k=1 be the eigenvalues of A. If xk is an eigenvector of A for the
âˆ’1
eigenvalue Î»k , then xk is an eigenvector for (A âˆ’ Î±I)
corresponding to the eigenvalue
1
if
.
Conversely,
Î»k âˆ’Î±
1
âˆ’1
(A âˆ’ Î±I) y =
y
(15.4)
Î»âˆ’Î±
and y Ì¸= 0, then Ay = Î»y.
Proof: Let Î»k and xk be as described in the statement of the lemma. Then
(A âˆ’ Î±I) xk = (Î»k âˆ’ Î±) xk
and so

1
âˆ’1
xk = (A âˆ’ Î±I) xk .
Î»k âˆ’ Î±

1
[Ay âˆ’ Î±y] . Solving for Ay leads to Ay = Î»y. 
Suppose (15.4). Then y = Î»âˆ’Î±
1
Now assume Î± is closer to Î» than to any other eigenvalue. Then the magnitude of Î»âˆ’Î±
âˆ’1
is greater than the magnitude of all the other eigenvalues of (A âˆ’ Î±I) . Therefore, the
âˆ’1
1
1
power method applied to (A âˆ’ Î±I) will yield Î»âˆ’Î± . You end up with sn+1 â‰ˆ Î»âˆ’Î±
and
solve for Î».

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

376

NUMERICAL METHODS FOR FINDING EIGENVALUES

15.1.2

The Explicit Description Of The Method

Here is how you use this method to ï¬nd the eigenvalue and eigenvector closest
to Î±.
1. Find (A âˆ’ Î±I)

âˆ’1

.

2. Pick u1 . If you are not phenomenally unlucky, the iterations will converge.
3. If uk has been obtained,

âˆ’1

uk+1 =
where sk+1 is the entry of (A âˆ’ Î±I)

âˆ’1

(A âˆ’ Î±I)
sk+1

uk

uk which has largest absolute value.

4. When the scaling factors, sk are not changing much and the uk are not changing much,
ï¬nd the approximation to the eigenvalue by solving
sk+1 =

1
Î»âˆ’Î±

for Î». The eigenvector is approximated by uk+1 .
5. Check your work by multiplying by the original matrix to see how well what you have
found works.
âˆ’1

Thus this amounts to the power method for the matrix (A âˆ’ Î±I) .
ï£¶
ï£«
5 âˆ’14 11
4
âˆ’4 ï£¸ which is closest to âˆ’7.
Example 15.1.4 Find the eigenvalue of A = ï£­ âˆ’4
3
6
âˆ’3
Also ï¬nd an eigenvector which goes with this eigenvalue.
In this case the eigenvalues are âˆ’6, 0, and 12 so the correct answer is âˆ’6 for the eigenvalue. Then from the above procedure, I will start with an initial vector,
ï£« ï£¶
1
u1 â‰¡ ï£­ 1 ï£¸ .
1
Then I must solve the following equation.
ï£«ï£«
ï£¶
ï£«
5 âˆ’14 11
1
ï£­ï£­ âˆ’4
4
âˆ’4 ï£¸ + 7 ï£­ 0
3
6
âˆ’3
0

0
1
0

ï£¶ï£¶ ï£«
ï£¶ ï£«
ï£¶
0
x
1
0 ï£¸ï£¸ ï£­ y ï£¸ = ï£­ 1 ï£¸
1
z
1

Simplifying the matrix on the left, I must solve
ï£«
ï£¶ï£«
ï£¶ ï£« ï£¶
12 âˆ’14 11
x
1
ï£­ âˆ’4 11 âˆ’4 ï£¸ ï£­ y ï£¸ = ï£­ 1 ï£¸
3
6
4
z
1
and then divide by the entry which has largest absolute value to obtain
ï£«
ï£¶
1.0
u2 = ï£­ . 184 ï£¸
âˆ’. 76

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

15.1. THE POWER METHOD FOR EIGENVALUES
Now solve

ï£«

12
ï£­ âˆ’4
3

377

ï£¶ï£«
ï£¶ ï£«
ï£¶
âˆ’14 11
x
1.0
11 âˆ’4 ï£¸ ï£­ y ï£¸ = ï£­ . 184 ï£¸
6
4
z
âˆ’. 76

and divide by the largest entry, 1. 051 5 to get
ï£«

ï£¶
1.0
u3 = ï£­ .0 266 ï£¸
âˆ’. 970 61

Solve

ï£«

12
ï£­ âˆ’4
3

ï£¶ï£«
ï£¶ ï£«
ï£¶
âˆ’14 11
x
1.0
11 âˆ’4 ï£¸ ï£­ y ï£¸ = ï£­ .0 266 ï£¸
6
4
z
âˆ’. 970 61

and divide by the largest entry, 1. 01 to get
ï£«

ï£¶
1.0
u4 = ï£­ 3. 845 4 Ã— 10âˆ’3 ï£¸ .
âˆ’. 996 04

These scaling factors are pretty close after these few iterations. Therefore, the predicted
eigenvalue is obtained by solving the following for Î».
1
= 1.01
Î»+7
which gives Î» = âˆ’6. 01. You see this is pretty close. In this case the eigenvalue closest to
âˆ’7 was âˆ’6.
How would you know what to start with for an initial guess? You might apply Gerschgorinâ€™s theorem.
ï£«
ï£¶
1 2 3
Example 15.1.5 Consider the symmetric matrix A = ï£­ 2 1 4 ï£¸ . Find the middle
3 4 2
eigenvalue and an eigenvector which goes with it.
Since A is symmetric, it follows it has three real eigenvalues
ï£¶ ï£«
ï£« ï£«
1 0 0
1 2
p (Î») = det ï£­Î» ï£­ 0 1 0 ï£¸ âˆ’ ï£­ 2 1
3 4
0 0 1

which are solutions to
ï£¶ï£¶
3
4 ï£¸ï£¸
2

= Î»3 âˆ’ 4Î»2 âˆ’ 24Î» âˆ’ 17 = 0
If you use your graphing calculator to graph this polynomial, you ï¬nd there is an eigenvalue
somewhere between âˆ’.9 and âˆ’.8 and that this is the middle eigenvalue. Of course you could
zoom in and ï¬nd it very accurately without much trouble but what about the eigenvector
which goes with it? If you try to solve
ï£«
ï£«
ï£¶ ï£«
ï£¶ï£¶ ï£«
ï£¶ ï£« ï£¶
1 2 3
x
0
1 0 0
ï£­(âˆ’.8) ï£­ 0 1 0 ï£¸ âˆ’ ï£­ 2 1 4 ï£¸ï£¸ ï£­ y ï£¸ = ï£­ 0 ï£¸
0 0 1
3 4 2
z
0
there will be only the zero solution because the matrix on the left will be invertible and the
same will be true if you replace âˆ’.8 with a better approximation like âˆ’.86 or âˆ’.855. This is

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

378

NUMERICAL METHODS FOR FINDING EIGENVALUES

because all these are only approximations to the eigenvalue and so the matrix in the above
is nonsingular for all of these. Therefore, you will only get the zero solution and
Eigenvectors are never equal to zero!
However, there exists such an eigenvector and you can ï¬nd it using the shifted inverse power
method. Pick Î± = âˆ’.855. Then you solve
ï£«ï£«
ï£¶
ï£«
ï£¶ï£¶ ï£«
ï£¶ ï£« ï£¶
1 2 3
1 0 0
x
1
ï£­ï£­ 2 1 4 ï£¸ + .855 ï£­ 0 1 0 ï£¸ï£¸ ï£­ y ï£¸ = ï£­ 1 ï£¸
3 4 2
0 0 1
z
1
or in other words,
ï£«

ï£¶ï£«
ï£¶ ï£« ï£¶
1. 855
2.0
3.0
x
1
ï£­ 2.0
1. 855
4.0 ï£¸ ï£­ y ï£¸ = ï£­ 1 ï£¸
3.0
4.0
2. 855
z
1
and after ï¬nding the solution, divide by the largest entry âˆ’67. 944, to obtain
ï£«
ï£¶
1. 0
u2 = ï£­ âˆ’. 589 21 ï£¸
âˆ’. 230 44
After a couple more iterations, you obtain
ï£«

ï£¶
1. 0
u3 = ï£­ âˆ’. 587 77 ï£¸
âˆ’. 227 14

(15.5)

Then doing it again, the scaling factor is âˆ’513. 42 and the next iterate is
ï£«
ï£¶
1. 0
u4 = ï£­ âˆ’. 587 78 ï£¸
âˆ’. 227 14
Clearly the uk are not changing much. This suggests an approximate eigenvector for this
eigenvalue which is close to âˆ’.855 is the above u3 and an eigenvalue is obtained by solving
1
= âˆ’514. 01,
Î» + .855
which yields Î» = âˆ’. 856 9 Lets
ï£«
1 2
ï£­ 2 1
3 4

check this.
ï£¶ï£«
ï£¶ ï£«
ï£¶
3
1. 0
âˆ’. 856 96
4 ï£¸ ï£­ âˆ’. 587 77 ï£¸ = ï£­ . 503 67 ï£¸ .
2
âˆ’. 227 14
. 194 64
ï£«

ï£¶ ï£«
ï£¶
1. 0
âˆ’. 856 9
âˆ’. 856 9 ï£­ âˆ’. 587 77 ï£¸ = ï£­ . 503 7 ï£¸
âˆ’. 227 14
. 194 6
Thus the vector of (15.5) is very close to the desired eigenvector, just as âˆ’. 856 9 is very
close to the desired eigenvalue. For practical purposes, I have found both the eigenvector
and the eigenvalue.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

15.1. THE POWER METHOD FOR EIGENVALUES

379
ï£«

2
Example 15.1.6 Find the eigenvalues and eigenvectors of the matrix A = ï£­ 2
3

1
1
2

ï£¶
3
1 ï£¸.
1

This is only a 3Ã—3 matrix and so it is not hard to estimate the eigenvalues. Just get
the characteristic equation, graph it using a calculator and zoom in to ï¬nd the eigenvalues.
If you do this, you ï¬nd there is an eigenvalue near âˆ’1.2, one near âˆ’.4, and one near 5.5.
(The characteristic equation is 2 + 8Î» + 4Î»2 âˆ’ Î»3 = 0.) Of course I have no idea what the
eigenvectors are.
Lets ï¬rst try to ï¬nd the eigenvector and a better approximation for the eigenvalue near
âˆ’1.2. In this case, let Î± = âˆ’1.2. Then
ï£«
ï£¶
âˆ’25. 357 143 âˆ’33. 928 571 50.0
âˆ’1
12. 5
17. 5
âˆ’25.0 ï£¸ .
(A âˆ’ Î±I) = ï£­
23. 214 286
30. 357 143 âˆ’45.0
As before, it helps to get things started if you raise to a power and then go from the
approximate eigenvector obtained.
ï£«
ï£¶7 ï£« ï£¶ ï£«
ï£¶
âˆ’25. 357 143 âˆ’33. 928 571 50.0
1
âˆ’2. 295 6 Ã— 1011
ï£­
12. 5
17. 5
âˆ’25.0 ï£¸ ï£­ 1 ï£¸ = ï£­ 1. 129 1 Ã— 1011 ï£¸
23. 214 286
30. 357 143 âˆ’45.0
1
2. 086 5 Ã— 1011
Then the next iterate will be
ï£«
ï£¶
ï£¶
ï£«
âˆ’2. 295 6 Ã— 1011
1.0
1
ï£­ 1. 129 1 Ã— 1011 ï£¸
= ï£­ âˆ’0.491 85 ï£¸
âˆ’2. 295 6 Ã— 1011
11
2. 086 5 Ã— 10
âˆ’0.908 91
Next iterate:
ï£«
âˆ’25. 357 143
ï£­
12. 5
23. 214 286
Divide by largest entry

âˆ’33. 928 571
17. 5
30. 357 143

ï£¶ï£«
ï£¶ ï£«
ï£¶
50.0
1.0
âˆ’54. 115
âˆ’25.0 ï£¸ ï£­ âˆ’0.491 85 ï£¸ = ï£­ 26. 615 ï£¸
âˆ’45.0
âˆ’0.908 91
49. 184

ï£¶
ï£«
ï£¶
âˆ’54. 115
1.0
1
ï£­ 26. 615 ï£¸
= ï£­ âˆ’0.491 82 ï£¸
âˆ’54. 115
49. 184
âˆ’0.908 88
ï£«

You can see the vector didnâ€™t change much and so the next scaling factor will not be much
diï¬€erent than this one. Hence you need to solve for Î»
1
= âˆ’54. 115
Î» + 1.2
Then Î» = âˆ’1. 218 5 is an approximate eigenvalue and
ï£«
ï£¶
1.0
ï£­ âˆ’0.491 82 ï£¸
âˆ’0.908 88
is an approximate eigenvector. How well does it work?
ï£«
ï£¶ï£«
ï£¶
2 1 3
1.0
ï£­ 2 1 1 ï£¸ ï£­ âˆ’0.491 82 ï£¸ =
3 2 1
âˆ’0.908 88
ï£«
ï£¶
1.0
(âˆ’1. 218 5) ï£­ âˆ’0.491 82 ï£¸ =
âˆ’0.908 88

Saylor URL: http://www.saylor.org/courses/ma212/

ï£«

ï£¶
âˆ’1. 218 5
ï£­ 0.599 3 ï£¸
1. 107 5
ï£«
ï£¶
âˆ’1. 218 5
ï£­ 0.599 28 ï£¸
1. 107 5

The Saylor Foundation

380

NUMERICAL METHODS FOR FINDING EIGENVALUES

You can see that for practical purposes, this has found the eigenvalue closest to âˆ’1. 218 5
and the corresponding eigenvector.
The other eigenvectors and eigenvalues can be found similarly. In the case of âˆ’.4, you
could let Î± = âˆ’.4 and then
ï£«
ï£¶
8. 064 516 1 Ã— 10âˆ’2 âˆ’9. 274 193 5 6. 451 612 9
âˆ’1
âˆ’. 403 225 81
11. 370 968 âˆ’7. 258 064 5 ï£¸ .
(A âˆ’ Î±I) = ï£­
. 403 225 81
3. 629 032 3 âˆ’2. 741 935 5
Following the procedure of the power method, you ï¬nd that after about 5 iterations, the
scaling factor is 9. 757 313 9, they are not changing much, and
ï£«
ï£¶
âˆ’. 781 224 8
ï£¸.
1. 0
u5 = ï£­
. 264 936 88
Thus the approximate eigenvalue is
1
= 9. 757 313 9
Î» + .4
which shows Î» = âˆ’. 297 512 78 is an approximation to the eigenvalue near .4. How well does
it work?
ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
2 1 3
âˆ’. 781 224 8
. 232 361 04
ï£­ 2 1 1 ï£¸ï£­
ï£¸ = ï£­ âˆ’. 297 512 72 ï£¸ .
1. 0
3 2 1
. 264 936 88
âˆ’.0 787 375 2
ï£«
ï£¶ ï£«
ï£¶
âˆ’. 781 224 8
. 232 424 36
ï£¸=ï£­
ï£¸.
1. 0
âˆ’. 297 512 78
âˆ’. 297 512 78 ï£­
. 264 936 88
âˆ’7. 882 210 8 Ã— 10âˆ’2
It works pretty well. For practical purposes, the eigenvalue and eigenvector have now been
found. If you want better accuracy, you could just continue iterating.
Next I will ï¬nd the eigenvalue and eigenvector for the eigenvalue near 5.5. In this case,
ï£¶
ï£«
29. 2 16. 8 23. 2
âˆ’1
(A âˆ’ Î±I) = ï£­ 19. 2 10. 8 15. 2 ï£¸ .
28.0 16.0 22.0
T

As before, I have no idea what the eigenvector is but I am tired of always using (1, 1, 1)
and I donâ€™t want to give the impression that you always need to start with this vector.
T
Therefore, I shall let u1 = (1, 2, 3) . Also, I will begin by raising the matrix to a power.
ï£«
ï£¶9 ï£« ï£¶ ï£«
ï£¶
29. 2 16. 8 23. 2
1
3. 009 Ã— 1016
ï£­ 19. 2 10. 8 15. 2 ï£¸ ï£­ 2 ï£¸ = ï£­ 1. 968 2 Ã— 1016 ï£¸ .
28.0 16.0 22.0
3
2. 870 6 Ã— 1016
Divide by largest entry to get the next iterate.
ï£«
ï£¶
ï£¶
ï£«
3. 009 Ã— 1016
1.0
1
ï£­ 1. 968 2 Ã— 1016 ï£¸
= ï£­ 0.654 1 ï£¸
3. 009 Ã— 1016
2. 870 6 Ã— 1016
0.954
Now

ï£«

29. 2 16. 8
ï£­ 19. 2 10. 8
28.0 16.0

ï£¶ï£«
ï£¶ ï£«
ï£¶
23. 2
1.0
62. 322
15. 2 ï£¸ ï£­ 0.654 1 ï£¸ = ï£­ 40. 765 ï£¸
22.0
0.954
59. 454

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

15.1. THE POWER METHOD FOR EIGENVALUES
Then the next iterate is

381

ï£«

ï£¶
ï£«
ï£¶
1.0
62. 322
ï£­ 40. 765 ï£¸ 1
= ï£­ 0.654 1 ï£¸
62.322
59. 454
0.953 98

This is very close to the eigenvector given above and so the next scaling factor will also be
close to 62.322. Thus the approximate eigenvalue is obtained by solving
1
= 62.322
Î» âˆ’ 5.5
An approximate eigenvalue
vector. How well does it work?
ï£«
2 1
ï£­ 2 1
3 2

is Î» = 5. 516 and an approximate eigenvector is the above

ï£¶ï£«
ï£¶ ï£«
ï£¶
3
1.0
5. 516
1 ï£¸ ï£­ 0.654 1 ï£¸ = ï£­ 3. 608 1 ï£¸
1
0.953 98
5. 262 2
ï£¶ ï£«
ï£¶
ï£«
1.0
5. 516
5. 516 ï£­ 0.654 1 ï£¸ = ï£­ 3. 608 ï£¸
0.953 98
5. 262 2

It appears this is very close.

15.1.3

Complex Eigenvalues

What about complex eigenvalues? If your matrix is real, you wonâ€™t see these by graphing
the characteristic equation on your calculator. Will the shifted inverse power method ï¬nd
these eigenvalues and their associated eigenvectors? The answer is yes. However, for a real
matrix, you must pick Î± to be complex. This is because the eigenvalues occur in conjugate
pairs so if you donâ€™t pick it complex, it will be the same distance between any conjugate
pair of complex numbers and so nothing in the above argument for convergence implies you
will get convergence to a complex number. Also, the process of iteration will yield only real
vectors and scalars.
Example 15.1.7 Find the complex eigenvalues
trix
ï£«
5 âˆ’8
ï£­ 1 0
0 1

and corresponding eigenvectors for the maï£¶
6
0 ï£¸.
0

Here the characteristic equation is Î»3 âˆ’ 5Î»2 + 8Î» âˆ’ 6 = 0. One solution is Î» = 3. The
other two are 1 + i and 1 âˆ’ i. I will apply the process to Î± = i to ï¬nd the eigenvalue closest
to i.
ï£«
ï£¶
âˆ’.0 2 âˆ’ . 14i 1. 24 + . 68i âˆ’. 84 + . 12i
âˆ’1
. 12 + . 84i ï£¸
(A âˆ’ Î±I) = ï£­ âˆ’. 14 + .0 2i . 68 âˆ’ . 24i
.0 2 + . 14i âˆ’. 24 âˆ’ . 68i . 84 + . 88i
T

Then let u1 = (1, 1, 1) for lack of any insight into anything better.
ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
âˆ’.0 2 âˆ’ . 14i 1. 24 + . 68i âˆ’. 84 + . 12i
1
. 38 + . 66i
ï£­ âˆ’. 14 + .0 2i . 68 âˆ’ . 24i
. 12 + . 84i ï£¸ ï£­ 1 ï£¸ = ï£­ . 66 + . 62i ï£¸
.0 2 + . 14i âˆ’. 24 âˆ’ . 68i . 84 + . 88i
1
. 62 + . 34i

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

382

NUMERICAL METHODS FOR FINDING EIGENVALUES

s2 = . 66 + . 62i.

ï£«

ï£¶
. 804 878 05 + . 243 902 44i
ï£¸
1.0
u2 = ï£­
. 756 097 56 âˆ’ . 195 121 95i
ï£«

ï£¶
âˆ’.0 2 âˆ’ . 14i 1. 24 + . 68i âˆ’. 84 + . 12i
ï£­ âˆ’. 14 + .0 2i . 68 âˆ’ . 24i
. 12 + . 84i ï£¸ Â·
.0 2 + . 14i âˆ’. 24 âˆ’ . 68i . 84 + . 88i
ï£«
ï£¶
. 804 878 05 + . 243 902 44i
ï£­
ï£¸
1.0
. 756 097 56 âˆ’ . 195 121 95i
ï£«
ï£¶
. 646 341 46 + . 817 073 17i
ï£¸
. 817 073 17 + . 353 658 54i
= ï£­
âˆ’2
. 548 780 49 âˆ’ 6. 097 560 9 Ã— 10 i
s3 = . 646 341 46+. 817 073 17i. After more iterations, of this sort, you ï¬nd s9 = 1. 002 748 5+
2. 137 621 7 Ã— 10âˆ’4 i and
ï£«
ï£¶
1.0
ï£¸.
. 501 514 17 âˆ’ . 499 807 33i
u9 = ï£­
âˆ’3
1. 562 088 1 Ã— 10 âˆ’ . 499 778 55i
Then

ï£«

ï£¶
âˆ’.0 2 âˆ’ . 14i 1. 24 + . 68i âˆ’. 84 + . 12i
ï£­ âˆ’. 14 + .0 2i . 68 âˆ’ . 24i
. 12 + . 84i ï£¸ Â·
.0 2 + . 14i âˆ’. 24 âˆ’ . 68i . 84 + . 88i
ï£«
ï£¶
1.0
ï£­
ï£¸
. 501 514 17 âˆ’ . 499 807 33i
1. 562 088 1 Ã— 10âˆ’3 âˆ’ . 499 778 55i
ï£¶
ï£«
1. 000 407 8 + 1. 269 979 Ã— 10âˆ’3 i
ï£¸
. 501 077 31 âˆ’ . 498 893 66i
= ï£­
8. 848 928 Ã— 10âˆ’4 âˆ’ . 499 515 22i

s10 = 1. 000 407 8 + 1. 269 979 Ã— 10âˆ’3 i.
ï£«
u10

ï£¶
1.0
ï£¸
. 500 239 18 âˆ’ . 499 325 33i
=ï£­
2. 506 749 2 Ã— 10âˆ’4 âˆ’ . 499 311 92i

The scaling factors are not changing much at this point. Thus you would solve the following
for Î».
1
1. 000 407 8 + 1. 269 979 Ã— 10âˆ’3 i =
Î»âˆ’i
The approximate eigenvalue is then Î» = . 999 590 76 + . 998 731 06i. This is pretty close to
1 + i. How well does the eigenvector work?
ï£«
ï£¶ï£«
ï£¶
5 âˆ’8 6
1.0
ï£­ 1 0 0 ï£¸ï£­
ï£¸
. 500 239 18 âˆ’ . 499 325 33i
0 1 0
2. 506 749 2 Ã— 10âˆ’4 âˆ’ . 499 311 92i
ï£«
ï£¶
. 999 590 61 + . 998 731 12i
ï£¸
1.0
= ï£­
. 500 239 18 âˆ’ . 499 325 33i

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

15.1. THE POWER METHOD FOR EIGENVALUES

383

ï£«

ï£¶
1.0
ï£¸
. 500 239 18 âˆ’ . 499 325 33i
(. 999 590 76 + . 998 731 06i) ï£­
2. 506 749 2 Ã— 10âˆ’4 âˆ’ . 499 311 92i
ï£«
ï£¶
. 999 590 76 + . 998 731 06i
= ï£­ . 998 726 18 + 4. 834 203 9 Ã— 10âˆ’4 i ï£¸
. 498 928 9 âˆ’ . 498 857 22i
It took more iterations than before because Î± was not very close to 1 + i.
This illustrates an interesting topic which leads to many related topics. If you have a
polynomial, x4 + ax3 + bx2 + cx + d, you can consider it as the characteristic polynomial of
a certain matrix, called a companion matrix. In this case,
ï£«
ï£¶
âˆ’a âˆ’b âˆ’c âˆ’d
ï£¬ 1
0
0
0 ï£·
ï£¬
ï£·.
ï£­ 0
1
0
0 ï£¸
0
0
1
0
The above example was just a companion matrix for Î»3 âˆ’ 5Î»2 + 8Î» âˆ’ 6. You can see the
pattern which will enable you to obtain a companion matrix for any polynomial of the form
Î»n + a1 Î»nâˆ’1 + Â· Â· Â· + anâˆ’1 Î» + an . This illustrates that one way to ï¬nd the complex zeros
of a polynomial is to use the shifted inverse power method on a companion matrix for the
polynomial. Doubtless there are better ways but this does illustrate how impressive this
procedure is. Do you have a better way?
Note that the shifted inverse power method is a way you can begin with something close
but not equal to an eigenvalue and end up with something close to an eigenvector.

15.1.4

Rayleigh Quotients And Estimates for Eigenvalues

There are many specialized results concerning the eigenvalues and eigenvectors for Hermitian
matrices. Recall a matrix A is Hermitian if A = Aâˆ— where Aâˆ— means to take the transpose
of the conjugate of A. In the case of a real matrix, Hermitian reduces to symmetric. Recall
also that for x âˆˆ Fn ,
n
âˆ‘
2
2
|x| = xâˆ— x =
|xj | .
j=1

Recall the following corollary found on Page 179 which is stated here for convenience.
Corollary 15.1.8 If A is Hermitian, then all the eigenvalues of A are real and there exists
an orthonormal basis of eigenvectors.
n

Thus for {xk }k=1 this orthonormal basis,
xâˆ—i xj = Î´ ij â‰¡

{

1 if i = j
0 if i Ì¸= j

For x âˆˆ Fn , x Ì¸= 0, the Rayleigh quotient is deï¬ned by
xâˆ— Ax
|x|

Saylor URL: http://www.saylor.org/courses/ma212/

2

.

The Saylor Foundation

384

NUMERICAL METHODS FOR FINDING EIGENVALUES
n

Now let the eigenvalues of A be Î»1 â‰¤ Î»2 â‰¤ Â· Â· Â· â‰¤ Î»n and Axk = Î»k xk where {xk }k=1 is
the above orthonormal basis of eigenvectors mentioned in the corollary. Then if x is an
arbitrary vector, there exist constants, ai such that
x=

n
âˆ‘

ai xi .

i=1

Also,
|x|

2

=

n
âˆ‘

ai xâˆ—i

i=1

=

n
âˆ‘

aj xj

j=1

âˆ‘

ai aj xâˆ—i xj =

âˆ‘

ij

ij

Therefore,

|x|

2

n
âˆ‘

2

|ai | .

i=1

)
(âˆ‘
n
âˆ—
a
Î»
x
)
a
x
j=1 j j j
i=1 i i
âˆ‘n
2
i=1 |ai |
âˆ‘
âˆ‘
âˆ—
ij ai aj Î»j xi xj
ij ai aj Î»j Î´ ij
= âˆ‘n
âˆ‘n
2
2
i=1 |ai |
i=1 |ai |
âˆ‘n
2
i=1 |ai | Î»i
âˆ‘n
2 âˆˆ [Î»1 , Î»n ] .
i=1 |ai |
(

âˆ—

x Ax

ai aj Î´ ij =

=
=
=

âˆ‘n

In other words, the Rayleigh quotient is always between the largest and the smallest eigenvalues of A. When x = xn , the Rayleigh quotient equals the largest eigenvalue and when x = x1
the Rayleigh quotient equals the smallest eigenvalue. Suppose you calculate a Rayleigh quotient. How close is it to some eigenvalue?
Theorem 15.1.9 Let x Ì¸= 0 and form the Rayleigh quotient,
xâˆ— Ax
|x|

2

â‰¡ q.

Then there exists an eigenvalue of A, denoted here by Î»q such that
|Î»q âˆ’ q| â‰¤
Proof: Let x =

âˆ‘n

|Ax âˆ’ qx|

(15.6)

n

k=1
2

|Ax âˆ’ qx|
.
|x|

ak xk where {xk }k=1 is the orthonormal basis of eigenvectors.
âˆ—

= (Ax âˆ’ qx) (Ax âˆ’ qx)
( n
)âˆ— ( n
)
âˆ‘
âˆ‘
=
ak Î»k xk âˆ’ qak xk
ak Î»k xk âˆ’ qak xk
k=1

ï£«
= ï£­

n
âˆ‘

ï£¶(
(Î»j âˆ’

q) aj xâˆ—j ï£¸

j=1

=

=

âˆ‘

j,k
n
âˆ‘

k=1
n
âˆ‘

)

(Î»k âˆ’ q) ak xk

k=1

(Î»j âˆ’ q) aj (Î»k âˆ’ q) ak xâˆ—j xk
2

|ak | (Î»k âˆ’ q)

2

k=1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

15.1. THE POWER METHOD FOR EIGENVALUES

385

Now pick the eigenvalue Î»q which is closest to q. Then
2

|Ax âˆ’ qx| =

n
âˆ‘

2

2

|ak | (Î»k âˆ’ q) â‰¥ (Î»q âˆ’ q)

k=1

2

n
âˆ‘

2

2

|ak | = (Î»q âˆ’ q) |x|

2

k=1

which implies (15.6). 

ï£«

ï£¶
1 2 3
T
Example 15.1.10 Consider the symmetric matrix A = ï£­ 2 2 1 ï£¸ . Let x = (1, 1, 1) .
3 1 4
How close is the Rayleigh quotient to some eigenvalue of A? Find the eigenvector and eigenvalue to several decimal places.
Everything is real and so there is no need to worry about taking conjugates. Therefore,
the Rayleigh quotient is
ï£«
ï£¶ï£« ï£¶
1 2 3
1
(
)
1 1 1 ï£­ 2 2 1 ï£¸ï£­ 1 ï£¸
3 1 4
1
19
=
3
3
According to the above theorem, there is some eigenvalue of this matrix Î»q such that
ï£¶ï£« ï£¶
ï£« ï£¶
ï£«
1
1
1 2 3
ï£­ 2 2 1 ï£¸ ï£­ 1 ï£¸ âˆ’ 19 ï£­ 1 ï£¸
3
3 1 4
1
1
19
âˆš
Î»q âˆ’
â‰¤
3
3
ï£« 1 ï£¶
âˆ’3
1
= âˆš ï£­ âˆ’ 43 ï£¸
3
5
3
âˆš
(
)
( )2
4 2
1
+ 53
9 + 3
âˆš
=
= 1. 247 2
3
Could you ï¬nd this eigenvalue and associated eigenvector? Of course you could. This is
what the shifted inverse power method is all about.
Solve
ï£«ï£«
ï£¶
ï£¶ï£¶ ï£«
ï£¶ ï£« ï£¶
ï£«
1 2 3
x
1
1 0 0
19
ï£­ï£­ 2 2 1 ï£¸ âˆ’
ï£­ 0 1 0 ï£¸ï£¸ ï£­ y ï£¸ = ï£­ 1 ï£¸
3
3 1 4
0 0 1
z
1
ï£«

In other words solve

âˆ’ 16
3
ï£­ 2
3

2
âˆ’ 13
3
1

ï£¶ï£«
ï£¶ ï£« ï£¶
3
x
1
1 ï£¸ï£­ y ï£¸ = ï£­ 1 ï£¸
z
1
âˆ’ 37

and divide by the entry which is largest, 3. 870 7, to get
ï£«
ï£¶
. 699 25
u2 = ï£­ . 493 89 ï£¸
1.0
Now solve

ï£«

âˆ’ 16
3
ï£­ 2
3

2
âˆ’ 13
3
1

ï£¶ï£«
ï£¶ ï£«
ï£¶
3
x
. 699 25
1 ï£¸ ï£­ y ï£¸ = ï£­ . 493 89 ï£¸
z
1.0
âˆ’ 37

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

386

NUMERICAL METHODS FOR FINDING EIGENVALUES

and divide by the largest entry, 2. 997 9 to get
ï£«
ï£¶
. 714 73
u3 = ï£­ . 522 63 ï£¸
1. 0
Now solve

ï£«

âˆ’ 16
3
ï£­ 2
3

2
âˆ’ 13
3
1

ï£¶ï£«
ï£¶
ï£¶ ï£«
3
x
. 714 73
1 ï£¸ ï£­ y ï£¸ = ï£­ . 522 63 ï£¸
1. 0
z
âˆ’ 37

and divide by the largest entry, 3. 045 4, to get
ï£«
ï£¶
. 713 7
u4 = ï£­ . 520 56 ï£¸
1.0
Solve

ï£«

âˆ’ 16
3
ï£­ 2
3

2
âˆ’ 13
3
1

ï£¶ ï£«
ï£¶ï£«
ï£¶
3
. 713 7
x
1 ï£¸ ï£­ y ï£¸ = ï£­ . 520 56 ï£¸
z
1.0
âˆ’ 37

and divide by the largest entry, 3. 042 1 to get
ï£«
ï£¶
. 713 78
u5 = ï£­ . 520 73 ï£¸
1.0
You can see these scaling factors are not changing much. The predicted eigenvalue is then
about
1
19
+
= 6. 662 1.
3. 042 1
3
How close is this?
ï£«
ï£¶ï£«
ï£¶ ï£«
ï£¶
1 2 3
. 713 78
4. 755 2
ï£­ 2 2 1 ï£¸ ï£­ . 520 73 ï£¸ = ï£­ 3. 469 ï£¸
3 1 4
1.0
6. 662 1
while

ï£«

ï£¶ ï£«
ï£¶
. 713 78
4. 755 3
6. 662 1 ï£­ . 520 73 ï£¸ = ï£­ 3. 469 2 ï£¸ .
1.0
6. 662 1

You see that for practical purposes, this has found the eigenvalue and an eigenvector.

15.2

The QR Algorithm

15.2.1

Basic Properties And Deï¬nition

Recall the theorem about the QR factorization in Theorem 5.7.5. It says that given an nÃ—n
real matrix A, there exists a real orthogonal matrix Q and an upper triangular matrix R such
that A = QR and that this factorization can be accomplished by a systematic procedure.
One such procedure was given in proving this theorem.
There is also a way to generalize the QR factorization to the case where A is just a
complex n Ã— n matrix and Q is unitary
triangular with nonnegative entries
( while R is upper
)
on the main diagonal. Letting A = a1 Â· Â· Â· an be the matrix with the aj the columns,

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

15.2. THE QR ALGORITHM

387

each a vector in Cn , let Q1 be a unitary matrix which maps a1 to |a1 | e1 in the case that
a1 Ì¸= 0. If a1 = 0, let Q1 = I. Why does such a unitary matrix exist? Let
{a1 / |a1 | , u2 , Â· Â· Â· , un }
(
)
be an orthonormal basis and let Q1 |aa11 | = e1 , Q1 (u2 ) = e2 etc. Extend Q1 linearly. Then
Q1 preserves lengths so it is unitary by Lemma 13.6.1. Now
(
)
Q1 a1 Q1 a2 Â· Â· Â· Q1 an
Q1 A =
(
)
|a1 | e1 Q1 a2 Â· Â· Â· Q1 an
=
which is a matrix of the form

(

|a1 | b
0 A1

)

Now do the same thing for A1 obtaining an n âˆ’ 1 Ã— n âˆ’ 1 unitary matrix Qâ€²2 which when
multiplied on the left of A1 yields something of the form
(
)
a b1
0 A2
Then multiplying A on the left by the product
(
)
1 0
Q1 â‰¡ Q2 Q1
0 Qâ€²2
yields a matrix which is upper triangular with respect to the ï¬rst two columns. Continuing
this way
Qn Qnâˆ’1 Â· Â· Â· Q1 A = R
where R is upper triangular having all positive entries on the main diagonal. Then the
desired unitary matrix is
âˆ—
Q = (Qn Qnâˆ’1 Â· Â· Â· Q1 )
II
The QR algorithm is described in the following deï¬nition.
Deï¬nition 15.2.1 The QR algorithm is the following. In the description of this algorithm,
Q is unitary and R is upper triangular having nonnegative entries on the main diagonal.
Starting with A an n Ã— n matrix, form
A0 â‰¡ A = Q1 R1

(15.7)

A1 â‰¡ R1 Q1 .

(15.8)

Ak = Rk Qk ,

(15.9)

Ak = Qk+1 Rk+1 , Ak+1 = Rk+1 Qk+1

(15.10)

Then
In general given
obtain Ak+1 by
This algorithm was proposed by Francis in 1961. The sequence {Ak } is the desired
sequence of iterates. Now with the above deï¬nition of the algorithm, here are its properties.
The next lemma shows each of the Ak is unitarily similar to A and the amazing thing about
this algorithm is that often it becomes increasingly easy to ï¬nd the eigenvalues of the Ak .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

388

NUMERICAL METHODS FOR FINDING EIGENVALUES

Lemma 15.2.2 Let A be an n Ã— n matrix and let the Qk and Rk be as described in the algorithm. Then each Ak is unitarily similar to A and denoting by Q(k) the product Q1 Q2 Â· Â· Â· Qk
and R(k) the product Rk Rkâˆ’1 Â· Â· Â· R1 , it follows that
Ak = Q(k) R(k)
(The matrix on the left is A raised to the k th power.)
A = Q(k) Ak Q(k)âˆ— , Ak = Q(k)âˆ— AQ(k) .
Proof: From the algorithm, Rk+1 = Ak+1 Qâˆ—k+1 and so
Ak = Qk+1 Rk+1 = Qk+1 Ak+1 Qâˆ—k+1
Now iterating this, it follows
Akâˆ’1 = Qk Ak Qâˆ—k = Qk Qk+1 Ak+1 Qâˆ—k+1 Qâˆ—k
Akâˆ’2 = Qkâˆ’1 Akâˆ’1 Qâˆ—kâˆ’1 = Qkâˆ’1 Qk Qk+1 Ak+1 Qâˆ—k+1 Qâˆ—k Qâˆ—kâˆ’1
etc. Thus, after k âˆ’ 2 more iterations,
A = Q(k+1) Ak+1 Q(k+1)âˆ—
The product of unitary matrices is unitary and so this proves the ï¬rst claim of the lemma.
Now consider the part about Ak . From the algorithm, this is clearly true for k = 1.
1
(A = QR) Suppose then that
Ak = Q1 Q2 Â· Â· Â· Qk Rk Rkâˆ’1 Â· Â· Â· R1
What was just shown indicated
A = Q1 Q2 Â· Â· Â· Qk+1 Ak+1 Qâˆ—k+1 Qâˆ—k Â· Â· Â· Qâˆ—1
and now from the algorithm, Ak+1 = Rk+1 Qk+1 and so
A = Q1 Q2 Â· Â· Â· Qk+1 Rk+1 Qk+1 Qâˆ—k+1 Qâˆ—k Â· Â· Â· Qâˆ—1
Then
Ak+1 = AAk =
A

z
}|
{
Q1 Q2 Â· Â· Â· Qk+1 Rk+1 Qk+1 Qâˆ—k+1 Qâˆ—k Â· Â· Â· Qâˆ—1 Q1 Â· Â· Â· Qk Rk Rkâˆ’1 Â· Â· Â· R1
= Q1 Q2 Â· Â· Â· Qk+1 Rk+1 Rk Rkâˆ’1 Â· Â· Â· R1 â‰¡ Q(k+1) R(k+1) 
Here is another very interesting lemma.
Lemma 15.2.3 Suppose Q(k) , Q are unitary and Rk is upper triangular such that the diagonal entries on Rk are all positive and
Q = lim Q(k) Rk
kâ†’âˆ

Then
lim Q(k) = Q, lim Rk = I.

kâ†’âˆ

kâ†’âˆ

Also the QR factorization of A is unique whenever Aâˆ’1 exists.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

15.2. THE QR ALGORITHM
Proof: Let

389

(
)
Q = (q1 , Â· Â· Â· , qn ) , Q(k) = qk1 , Â· Â· Â· , qkn

k
where the q are the columns. Also denote by rij
the ij th entry of Rk . Thus
ï£« k
ï£¶
r11
âˆ—
(
)ï£¬
ï£·
..
Q(k) Rk = qk1 , Â· Â· Â· , qkn ï£­
ï£¸
.
k
0
rnn

It follows
k k
r11
q1 â†’ q1

and so
k
k k
r11
= r11
q1 â†’ 1

Therefore,
qk1 â†’ q1 .
Next consider the second column.
k k
k k
r12
q1 + r22
q2 â†’ q2

Taking the inner product of both sides with qk1 it follows
(
)
k
lim r12
= lim q2 Â· qk1 = (q2 Â· q1 ) = 0.
kâ†’âˆ

kâ†’âˆ

Therefore,
k k
lim r22
q2 = q2

kâ†’âˆ

k
k
â†’ 1. Hence
> 0, it follows as in the ï¬rst part that r22
and since r22

lim qk2 = q2 .

kâ†’âˆ

Continuing this way, it follows
k
=0
lim rij

kâ†’âˆ

for all i Ì¸= j and

k
= 1, lim qkj = qj .
lim rjj
kâ†’âˆ

kâ†’âˆ

Thus Rk â†’ I and Q â†’ Q. This proves the ï¬rst part of the lemma.
The second part follows immediately. If QR = Qâ€² Râ€² = A where Aâˆ’1 exists, then
(k)

âˆ’1

Qâˆ— Qâ€² = R (Râ€² )

and I need to show both sides of the above are equal to I. The left side of the above is
unitary and the right side is upper triangular having positive entries on the diagonal. This
is because the inverse of such an upper triangular matrix having positive entries on the
main diagonal is still upper triangular having positive entries on the main diagonal and
the product of two such upper triangular matrices gives another of the same form having
positive entries on the main diagonal. Suppose then that Q = R where Q is unitary and R
is upper triangular having positive entries on the main diagonal. Let Qk = Q and Rk = R.
It follows
IRk â†’ R = Q
and so from the ï¬rst part, Rk â†’ I but Rk = R and so R = I. Thus applying this to
âˆ’1
Qâˆ— Qâ€² = R (Râ€² ) yields both sides equal I. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

390

NUMERICAL METHODS FOR FINDING EIGENVALUES

A case of all this is of great
( interest. Suppose A
) has a largest eigenvalue Î» which is
real. Then An is of the form Anâˆ’1 a1 , Â· Â· Â· , Anâˆ’1 an and so likely each of these columns
will be pointing roughly in the direction of an eigenvector of A which corresponds to this
eigenvalue. Then when you do the QR factorization of this, it follows from the fact that R
is upper triangular, that the ï¬rst column of Q will be a multiple of Anâˆ’1 a1 and so will end
up being roughly parallel to the eigenvector desired. Also this will require the entries below
the top in the ï¬rst column of An = QT AQ will all be small because they will be of the form
qTi Aq1 â‰ˆ Î»qTi q1 = 0. Therefore, An will be of the form
( â€²
)
Î» a
e B
where e is small. It follows that Î»â€² will be close to Î» and q1 will be close to an eigenvector for
Î». Then if you like, you could do the same thing with the matrix B to obtain approximations
for the other eigenvalues. Finally, you could use the shifted inverse power method to get
more exact solutions.

15.2.2

The Case Of Real Eigenvalues

With these lemmas, it is possible to prove that for the QR algorithm and certain conditions,
the sequence Ak converges pointwise to an upper triangular matrix having the eigenvalues
of A down the diagonal. I will assume all the matrices are real here.
(
)
0 1
This convergence wonâ€™t always happen. Consider for example the matrix
.
1 0
You can verify quickly that the algorithm will return this matrix for each k. The problem
here is that, although the matrix has the two eigenvalues âˆ’1, 1, they have the same absolute
value. The QR algorithm works in somewhat the same way as the power method, exploiting
diï¬€erences in the size of the eigenvalues.
If A has all real eigenvalues and you are interested in ï¬nding these eigenvalues along
with the corresponding eigenvectors, you could always consider A + Î»I instead where Î» is
suï¬ƒciently large and positive that A + Î»I has all positive eigenvalues. (Recall Gerschgorinâ€™s
theorem.) Then if Âµ is an eigenvalue of A + Î»I with
(A + Î»I) x = Âµx
then
Ax = (Âµ âˆ’ Î») x
so to ï¬nd the eigenvalues of A you just subtract Î» from the eigenvalues of A + Î»I. Thus
there is no loss of generality in assuming at the outset that the eigenvalues of A are all
positive. Here is the theorem. It involves a technical condition which will often hold. The
proof presented here follows [26] and is a special case of that presented in this reference.
Before giving the proof, note that the product of upper triangular matrices is upper
triangular. If they both have positive entries on the main diagonal so will the product.
Furthermore, the inverse of an upper triangular matrix is upper triangular. I will use these
simple facts without much comment whenever convenient.
Theorem 15.2.4 Let A be a real matrix having eigenvalues
Î»1 > Î»2 > Â· Â· Â· > Î»n > 0
and let

A = SDS âˆ’1

Saylor URL: http://www.saylor.org/courses/ma212/

(15.11)

The Saylor Foundation

15.2. THE QR ALGORITHM

391
ï£«

where

ï£¬
D=ï£­

Î»1

0
..

.

0

ï£¶
ï£·
ï£¸

Î»n

and suppose S âˆ’1 has an LU factorization. Then the matrices Ak in the QR algorithm
described above converge to an upper triangular matrix T â€² having the eigenvalues of A,
Î»1 , Â· Â· Â· , Î»n descending on the main diagonal. The matrices Q(k) converge to Qâ€² , an orthogonal matrix which equals Q except for possibly having some columns multiplied by âˆ’1 for Q
the unitary part of the QR factorization of S,
S = QR,
and

lim Ak = T â€² = Qâ€²T AQâ€²

kâ†’âˆ

Proof: From Lemma 15.2.2
Ak = Q(k) R(k) = SDk S âˆ’1

(15.12)

Let S = QR where this is just a QR factorization which is known to exist and let S âˆ’1 = LU
which is assumed to exist. Thus
Q(k) R(k) = QRDk LU
and so

(15.13)

Q(k) R(k) = QRDk LU = QRDk LDâˆ’k Dk U

That matrix in the middle, Dk LDâˆ’k satisï¬es
( k
)
D LDâˆ’k ij = Î»ki Lij Î»âˆ’k
for j â‰¤ i, 0 if j > i.
j
Thus for j < i the expression converges to 0 because Î»j > Î»i when this happens. When
i = j it reduces to 1. Thus the matrix in the middle is of the form
I + Ek
where Ek â†’ 0. Then it follows
Ak = Q(k) R(k) = QR (I + Ek ) Dk U
(
)
= Q I + REk Râˆ’1 RDk U â‰¡ Q (I + Fk ) RDk U
where Fk â†’ 0. Then let I + Fk = Qk Rk where this is another QR factorization. Then it
reduces to
Q(k) R(k) = QQk Rk RDk U
This looks really interesting because by Lemma 15.2.3 Qk â†’ I and Rk â†’ I because
Qk Rk = (I + Fk ) â†’ I. So it follows QQk is an orthogonal matrix converging to Q while
)âˆ’1
(
Rk RDk U R(k)
is upper triangular, being the product of upper triangular matrices. Unfortunately, it is not
known that the diagonal entries of this matrix are nonnegative because of the U . Let Î› be
just like the identity matrix but having some of the ones replaced with âˆ’1 in such a way

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

392

NUMERICAL METHODS FOR FINDING EIGENVALUES

that Î›U is an upper triangular matrix having positive diagonal entries. Note Î›2 = I and
also Î› commutes with a diagonal matrix. Thus
Q(k) R(k) = QQk Rk RDk Î›2 U = QQk Rk RÎ›Dk (Î›U )
At this point, one does some inspired massaging to write the above in the form
]
) [(
)âˆ’1
(
QQk Î›Dk
Î›Dk
Rk RÎ›Dk (Î›U )
[(
]
)âˆ’1
= Q (Qk Î›) Dk Î›Dk
Rk RÎ›Dk (Î›U )
â‰¡Gk

=

z [
}|
{
]
(
)âˆ’1
Q (Qk Î›) Dk Î›Dk
Rk RÎ›Dk (Î›U )

Now I claim the middle matrix in [Â·] is upper triangular and has all positive entries on the
diagonal. This is because it is an upper triangular matrix which is similar to the upper
triangular matrix Rk R[and so it has the same
] eigenvalues (diagonal entries) as Rk R. Thus
(
)
k
k
k âˆ’1
R
RÎ›D
(Î›U ) is upper triangular and has all positive
the matrix Gk â‰¡ D
Î›D
k
entries on the diagonal. Multiply on the right by Gâˆ’1
k to get
â€²
Q(k) R(k) Gâˆ’1
k = QQk Î› â†’ Q

where Qâ€² is essentially equal to Q but might have some of the columns multiplied by âˆ’1.
This is because Qk â†’ I and so Qk Î› â†’ Î›. Now by Lemma 15.2.3, it follows
Q(k) â†’ Qâ€² , R(k) Gâˆ’1
k â†’ I.
It remains to verify Ak converges to an upper triangular matrix. Recall that from (15.12)
and the deï¬nition below this (S = QR)
A = SDS âˆ’1 = (QR) D (QR)

âˆ’1

= QRDRâˆ’1 QT = QT QT

Where T is an upper triangular matrix. This is because it is the product of upper triangular
matrices R, D, Râˆ’1 . Thus
QT AQ = T.
If you replace Q with Qâ€² in the above, it still results in an upper triangular matrix T â€² having
the same diagonal entries as T. This is because
T = QT AQ = (Qâ€² Î›) A (Qâ€² Î›) = Î›Qâ€²T AQâ€² Î›
T

and considering the iith entry yields
âˆ‘
( T
)
(
)
(
)
(
)
Î›ij Qâ€²T AQâ€² jk Î›ki = Î›ii Î›ii Qâ€²T AQâ€² ii = Qâ€²T AQâ€² ii
Q AQ ii â‰¡
j,k

Recall from Lemma 15.2.2,
Ak = Q(k)T AQ(k)
Thus taking a limit and using the ï¬rst part,
Ak = Q(k)T AQ(k) â†’ Qâ€²T AQâ€² = T â€² . 
An easy case is for A symmetric. Recall Corollary 7.4.13. By this corollary, there exists
an orthogonal (real unitary) matrix Q such that
QT AQ = D
where D is diagonal having the eigenvalues on the main diagonal decreasing in size from the
upper left corner to the lower right.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

15.2. THE QR ALGORITHM

393

Corollary 15.2.5 Let A be a real symmetric n Ã— n matrix having eigenvalues
Î»1 > Î»2 > Â· Â· Â· > Î»n > 0
and let Q be deï¬ned by
QDQT = A, D = QT AQ,

(15.14)

where Q is orthogonal and D is a diagonal matrix having the eigenvalues on the main
diagonal decreasing in size from the upper left corner to the lower right. Let QT have an
LU factorization. Then in the QR algorithm, the matrices Q(k) converge to Qâ€² where Qâ€² is
the same as Q except having some columns multiplied by (âˆ’1) . Thus the columns of Qâ€² are
eigenvectors of A. The matrices Ak converge to D.
Proof: This follows from Theorem 15.2.4. Here S = Q, S âˆ’1 = QT . Thus
Q = S = QR
and R = I. By Theorem 15.2.4 and Lemma 15.2.2,
Ak = Q(k)T AQ(k) â†’ Qâ€²T AQâ€² = QT AQ = D.
because formula (15.14) is unaï¬€ected by replacing Q with Qâ€² . 
When using the QR algorithm, it is not necessary to check technical condition about
S âˆ’1 having an LU factorization. The algorithm delivers a sequence of matrices which are
similar to the original one. If that sequence converges to an upper triangular matrix, then
the algorithm worked. Furthermore, the technical condition is suï¬ƒcient but not necessary.
The algorithm will work even without the technical condition.
Example 15.2.6 Find the eigenvalues and eigenvectors of the matrix
ï£«
ï£¶
5 1 1
A=ï£­ 1 3 2 ï£¸
1 2 1
It is a symmetric matrix but other than that, I just pulled it out of the air. By Lemma
15.2.2 it follows Ak = Q(k)T AQ(k) . And so to get to the answer quickly I could have the
computer raise A to a power and then take the QR factorization of what results to get the
k th iteration using the above formula. Lets pick k = 10.
ï£«

5 1
ï£­ 1 3
1 2

ï£¶10 ï£«
1
4. 227 3 Ã— 107
2 ï£¸ = ï£­ 2. 595 9 Ã— 107
1. 861 1 Ã— 107
1

2. 595 9 Ã— 107
1. 607 2 Ã— 107
1. 150 6 Ã— 107

ï£¶
1. 861 1 Ã— 107
1. 150 6 Ã— 107 ï£¸
8. 239 6 Ã— 106

Now take QR factorization of this. The computer will do that also.
This yields
ï£«
ï£¶
. 797 85 âˆ’. 599 12 âˆ’6. 694 3 Ã— 10âˆ’2
ï£­ . 489 95 . 709 12
ï£¸Â·
âˆ’. 507 06
. 351 26 . 371 76
. 859 31
ï£«
ï£¶
5. 298 3 Ã— 107 3. 262 7 Ã— 107 2. 338 Ã— 107
ï£­
ï£¸
0
1. 217 2 Ã— 105
71946.
0
0
277. 03

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

394

NUMERICAL METHODS FOR FINDING EIGENVALUES

Next it follows
ï£«

A10

ï£¶T
. 797 85 âˆ’. 599 12 âˆ’6. 694 3 Ã— 10âˆ’2
ï£¸ Â·
âˆ’. 507 06
= ï£­ . 489 95 . 709 12
. 351 26 . 371 76
. 859 31
ï£«
ï£¶ï£«
ï£¶
5 1 1
. 797 85 âˆ’. 599 12 âˆ’6. 694 3 Ã— 10âˆ’2
ï£­ 1 3 2 ï£¸ ï£­ . 489 95 . 709 12
ï£¸
âˆ’. 507 06
. 351 26 . 371 76
. 859 31
1 2 1

and this equals
ï£«

6. 057 1
ï£­ 3. 698 Ã— 10âˆ’3
3. 434 6 Ã— 10âˆ’5

3. 698 Ã— 10âˆ’3
3. 200 8
âˆ’4. 064 3 Ã— 10âˆ’4

ï£¶
3. 434 6 Ã— 10âˆ’5
âˆ’4. 064 3 Ã— 10âˆ’4 ï£¸
âˆ’. 257 9

By Gerschgorinâ€™s theorem, the eigenvalues are pretty close to the diagonal entries of the
above matrix. Note I didnâ€™t use the theorem, just Lemma 15.2.2 and Gerschgorinâ€™s theorem
to verify the eigenvalues are close to the above numbers. The eigenvectors are close to
ï£¶
ï£¶ ï£«
ï£¶ ï£«
ï£«
âˆ’6. 694 3 Ã— 10âˆ’2
âˆ’. 599 12
. 797 85
ï£¸
ï£­ . 489 95 ï£¸ , ï£­ . 709 12 ï£¸ , ï£­
âˆ’. 507 06
. 859 31
. 371 76
. 351 26
Lets check one of these.
ï£«ï£«

ï£¶
ï£«
5 1 1
1 0
ï£­ï£­ 1 3 2 ï£¸ âˆ’ 6. 057 1 ï£­ 0 1
1 2 1
0 0
ï£«
ï£¶ ï£« ï£¶
âˆ’2. 197 2 Ã— 10âˆ’3
0
= ï£­ 2. 543 9 Ã— 10âˆ’3 ï£¸ â‰ˆ ï£­ 0 ï£¸
1. 393 1 Ã— 10âˆ’3
0

Now lets see how well
ï£«ï£«
5 1
ï£­ï£­ 1 3
1 2

ï£¶ï£¶ ï£«
ï£¶
0
. 797 85
0 ï£¸ï£¸ ï£­ . 489 95 ï£¸
1
. 351 26

the smallest approximate eigenvalue and eigenvector works.
ï£¶
ï£«
ï£¶ï£¶ ï£«
ï£¶
1
1 0 0
âˆ’6. 694 3 Ã— 10âˆ’2
ï£¸
2 ï£¸ âˆ’ (âˆ’. 257 9) ï£­ 0 1 0 ï£¸ï£¸ ï£­
âˆ’. 507 06
1
0 0 1
. 859 31
ï£«
ï£¶
ï£«
ï£¶
2. 704 Ã— 10âˆ’4
0
= ï£­ âˆ’2. 737 7 Ã— 10âˆ’4 ï£¸ â‰ˆ ï£­ 0 ï£¸
âˆ’1. 369 5 Ã— 10âˆ’4
0

For practical purposes, this has found the eigenvalues and eigenvectors.

15.2.3

The QR Algorithm In The General Case

In the case where A has distinct positive eigenvalues it was shown above that under reasonable conditions related to a certain matrix having an LU factorization the QR algorithm
produces a sequence of matrices {Ak } which converges to an upper triangular matrix. What
if A is just an nÃ—n matrix having possibly complex eigenvalues but A is nondefective? What
happens with the QR algorithm in this case? The short answer to this question is that the
Ak of the algorithm typically cannot converge. However, this does not mean the algorithm is not useful in ï¬nding eigenvalues. It turns out the sequence of matrices {Ak } have
the appearance of a block upper triangular matrix for large k in the sense that the entries

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

15.2. THE QR ALGORITHM

395

below the blocks on the main diagonal are small. Then looking at these blocks gives a way
to approximate the eigenvalues. An important example of the concept of a block triangular
matrix is the real Schur form for a matrix discussed in Theorem 7.4.6 but the concept as
described here allows for any size block centered on the diagonal.
First it is important to note a simple fact about unitary diagonal matrices. In what
follows Î› will denote a unitary matrix which is also a diagonal matrix. These matrices
are just the identity matrix with some of the ones replaced with a number of the form eiÎ¸
for some Î¸. The important property of multiplication of any matrix by Î› on either side
is that it leaves all the zero entries the same and also preserves the absolute values of the
other entries. Thus a block triangular matrix multiplied by Î› on either side is still block
triangular. If the matrix is close to being block triangular this property of being close to a
block triangular matrix is also preserved by multiplying on either side by Î›. Other patterns
depending only on the size of the absolute value occurring in the matrix are also preserved
by multiplying on either side by Î›. In other words, in looking for a pattern in a matrix,
multiplication by Î› is irrelevant.
Now let A be an n Ã— n matrix having real or complex entries. By Lemma 15.2.2 and the
assumption that A is nondefective, there exists an invertible S,
Ak = Q(k) R(k) = SDk S âˆ’1
where

ï£«
ï£¬
D=ï£­

Î»1

0
..

0

.

(15.15)

ï£¶
ï£·
ï£¸

Î»n

and by rearranging the columns of S, D can be made such that
|Î»1 | â‰¥ |Î»2 | â‰¥ Â· Â· Â· â‰¥ |Î»n | .
Assume S âˆ’1 has an LU factorization. Then
Ak = SDk LU = SDk LDâˆ’k Dk U.
Consider the matrix in the middle, Dk LDâˆ’k . The ij th entry is of the form
ï£± k
if j < i
ï£² Î»i Lij Î»âˆ’k
j
( k
)
âˆ’k
D LD
=
1
if
i
=
j
ij
ï£³
0 if j > i
and these all converge to 0 whenever |Î»i | < |Î»j | . Thus
Dk LDâˆ’k = (Lk + Ek )
where Lk is a lower triangular matrix which has all ones down the diagonal and some
subdiagonal terms of the form
(15.16)
Î»ki Lij Î»âˆ’k
j
for which |Î»i | = |Î»j | while Ek â†’ 0. (Note the entries of Lk are all bounded independent of
k but some may fail to converge.) Then
Q(k) R(k) = S (Lk + Ek ) Dk U
Let
SLk = Qk Rk

Saylor URL: http://www.saylor.org/courses/ma212/

(15.17)

The Saylor Foundation

396

NUMERICAL METHODS FOR FINDING EIGENVALUES

where this is the QR factorization of SLk . Then
Q(k) R(k)

= (Qk Rk + SEk ) Dk U
)
(
= Qk I + Qâˆ—k SEk Rkâˆ’1 Rk Dk U
= Qk (I + Fk ) Rk Dk U

where Fk â†’ 0. Let I + Fk = Qâ€²k Rkâ€² . Then
Q(k) R(k) = Qk Qâ€²k Rkâ€² Rk Dk U
By Lemma 15.2.3

Qâ€²k â†’ I and Rkâ€² â†’ I.

(15.18)

Now let Î›k be a diagonal unitary matrix which has the property that
Î›âˆ—k Dk U
is an upper triangular matrix which has all the diagonal entries positive. Then
Q(k) R(k) = Qk Qâ€²k Î›k (Î›âˆ—k Rkâ€² Rk Î›k ) Î›âˆ—k Dk U
That matrix in the middle has all positive diagonal entries because it is itself an upper
triangular matrix, being the product of such, and is similar to the matrix Rkâ€² Rk which is
upper triangular with positive diagonal entries. By Lemma 15.2.3 again, this time using the
uniqueness assertion,
Q(k) = Qk Qâ€²k Î›k , R(k) = (Î›âˆ—k Rkâ€² Rk Î›k ) Î›âˆ—k Dk U
Note the term Qk Qâ€²k Î›k must be real because the algorithm gives all Q(k) as real matrices.
By (15.18) it follows that for k large enough
Q(k) â‰ˆ Qk Î›k
where â‰ˆ means the two matrices are close. Recall
Ak = Q(k)T AQ(k)
and so for large k,

âˆ—

Ak â‰ˆ (Qk Î›k ) A (Qk Î›k ) = Î›âˆ—k Qâˆ—k AQk Î›k

As noted above, the form of Î›âˆ—k Qâˆ—k AQk Î›k in terms of which entries are large and small is
not aï¬€ected by the presence of Î›k and Î›âˆ—k . Thus, in considering what form this is in, it
suï¬ƒces to consider Qâˆ—k AQk .
This could get pretty complicated but I will consider the case where
if |Î»i | = |Î»i+1 | , then |Î»i+2 | < |Î»i+1 | .

(15.19)

This is typical of the situation where the eigenvalues are all distinct and the matrix A is real
so the eigenvalues occur as conjugate pairs. Then in this case, Lk above is lower triangular
with some nonzero terms on the diagonal right below the main diagonal but zeros everywhere
else. Thus maybe
(Lk )s+1,s Ì¸= 0
Recall (15.17) which implies

Qk = SLk Rkâˆ’1

Saylor URL: http://www.saylor.org/courses/ma212/

(15.20)

The Saylor Foundation

15.2. THE QR ALGORITHM

397

where Rkâˆ’1 is upper triangular. Also recall that from the deï¬nition of S in (15.15),
S âˆ’1 AS = D
and so the columns of S are eigenvectors of A, the ith being an eigenvector for Î»i . Now
from the form of Lk , it follows Lk Rkâˆ’1 is a block upper triangular matrix denoted by TB and
so Qk = STB . It follows from the above construction in (15.16) and the given assumption
on the sizes of the eigenvalues, there are ï¬nitely many 2 Ã— 2 blocks centered on the main
diagonal along with possibly some diagonal entries. Therefore, for large k the matrix
Ak = Q(k)T AQ(k)
is approximately of the same form as that of
Qâˆ—k AQk = TBâˆ’1 S âˆ’1 ASTB = TBâˆ’1 DTB
which is a block upper triangular matrix. As explained above, multiplication by the various
diagonal unitary matrices does not aï¬€ect this form. Therefore, for large k, Ak is approximately a block upper triangular matrix.
How would this change if the above assumption on the size of the eigenvalues were relaxed
but the matrix was still nondefective with appropriate matrices having an LU factorization
as above? It would mean the blocks on the diagonal would be larger. This immediately
makes the problem more cumbersome to deal with. However, in the case that the eigenvalues
of A are distinct, the above situation really is typical of what occurs and in any case can be
quickly reduced to this case.
To see this, suppose condition (15.19) is violated and Î»j , Â· Â· Â· , Î»j+p are complex eigenvalues having nonzero imaginary parts such that each has the same absolute value but they
are all distinct. Then let Âµ > 0 and consider the matrix A + ÂµI. Thus the corresponding eigenvalues of A + ÂµI are Î»j + Âµ, Â· Â· Â· , Î»j+p + Âµ. A short computation shows shows
|Î»j + Âµ| , Â· Â· Â· , |Î»j+p + Âµ| are all distinct and so the above situation of (15.19) is obtained. Of
course, if there are repeated eigenvalues, it may not be possible to reduce to the case above
and you would end up with large blocks on the main diagonal which could be diï¬ƒcult to
deal with.
So how do you identify the eigenvalues? You know Ak and behold that it is close to a
block upper triangular matrix TBâ€² . You know Ak is also similar to A. Therefore, TBâ€² has
eigenvalues which are close to the eigenvalues of Ak and hence those of A provided k is
suï¬ƒciently large. See Theorem 7.9.2 which depends on complex analysis or the exercise on
Page 197 which gives another way to see this. Thus you ï¬nd the eigenvalues of this block
triangular matrix TBâ€² and assert that these are good approximations of the eigenvalues of
Ak and hence to those of A. How do you ï¬nd the eigenvalues of a block triangular matrix?
This is easy from Lemma 7.4.5. Say
ï£«
ï£¶
B1 Â· Â· Â·
âˆ—
ï£¬
.. ï£·
..
TBâ€² = ï£­
.
. ï£¸
0

Bm

Then forming Î»I âˆ’ TBâ€² and taking the determinant, it follows from Lemma 7.4.5 this equals
m
âˆ

det (Î»Ij âˆ’ Bj )

j=1

and so all you have to do is take the union of the eigenvalues for each Bj . In the case
emphasized here this is very easy because these blocks are just 2 Ã— 2 matrices.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

398

NUMERICAL METHODS FOR FINDING EIGENVALUES

How do you identify approximate eigenvectors from this? First try to ï¬nd the approximate eigenvectors for Ak . Pick an approximate eigenvalue Î», an exact eigenvalue for TBâ€² .
Then ï¬nd v solving TBâ€² v = Î»v. It follows since TBâ€² is close to Ak that
Ak v â‰ˆ Î»v
and so
Q(k) AQ(k)T v = Ak v â‰ˆ Î»v
Hence
AQ(k)T v â‰ˆ Î»Q(k)T v
and so Q(k)T v is an approximation to the eigenvector which goes with the eigenvalue of A
which is close to Î».
Example 15.2.7 Here is a matrix.
ï£«

3
2
ï£­ âˆ’2 0
âˆ’2 âˆ’2

ï£¶
1
âˆ’1 ï£¸
0

It happens that the eigenvalues of this matrix are 1, 1 + i, 1 âˆ’ i. Lets apply the QR algorithm
as if the eigenvalues were not known.

Applying the QR algorithm to this matrix yields the following sequence of matrices.
ï£«
ï£¶
1. 235 3
1. 941 2
4. 365 7
A1 = ï£­ âˆ’. 392 15 1. 542 5 5. 388 6 Ã— 10âˆ’2 ï£¸
âˆ’. 161 69 âˆ’. 188 64
. 222 22
..
.

ï£«

A12

9. 177 2 Ã— 10âˆ’2
âˆ’2. 855 6
=ï£­
1. 078 6 Ã— 10âˆ’2

. 630 89
1. 908 2
3. 461 4 Ã— 10âˆ’4

ï£¶
âˆ’2. 039 8
âˆ’3. 104 3 ï£¸
1.0

At this point the bottom two terms on the left part of the bottom row are both very
small so it appears the real eigenvalue is near 1.0. The complex eigenvalues are obtained
from solving
( (
) (
))
1 0
9. 177 2 Ã— 10âˆ’2 . 630 89
det Î»
âˆ’
=0
0 1
âˆ’2. 855 6
1. 908 2
This yields
Î» = 1.0 âˆ’ . 988 28i, 1.0 + . 988 28i
Example 15.2.8 The equation x4 + x3 + 4x2 + xâˆ’ 2 = 0 has exactly two real solutions. You
can see this by graphing it. However, the rational root theorem from algebra shows neither
of these solutions are rational. Also, graphing it does not yield any information about the
complex solutions. Lets use the QR algorithm to approximate all the solutions, real and
complex.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

15.2. THE QR ALGORITHM

399

A matrix whose characteristic polynomial is the
ï£«
âˆ’1 âˆ’4 âˆ’1
ï£¬ 1
0
0
ï£¬
ï£­ 0
1
0
0
0
1
Using the QR algorithm yields the following
ï£«
. 999 99 âˆ’2. 592 7
ï£¬ 2. 121 3 âˆ’1. 777 8
A1 = ï£¬
ï£­
0
. 342 46
0
0

given polynomial is
ï£¶
2
0 ï£·
ï£·
0 ï£¸
0

sequence of iterates for Ak
ï£¶
âˆ’1. 758 8 âˆ’1. 297 8
âˆ’1. 604 2 âˆ’. 994 15 ï£·
ï£·
âˆ’. 327 49 âˆ’. 917 99 ï£¸
âˆ’. 446 59 . 105 26

..
.
ï£«

âˆ’. 834 12
âˆ’4. 168 2
ï£¬ 1. 05
. 145 14
A9 = ï£¬
ï£­
0
4. 026 4 Ã— 10âˆ’4
0
0

âˆ’1. 939
. 217 1
âˆ’. 850 29
âˆ’1. 826 3 Ã— 10âˆ’2

ï£¶
âˆ’. 778 3
2. 547 4 Ã— 10âˆ’2 ï£·
ï£·
ï£¸
âˆ’. 616 08
. 539 39

Now this is similar to A and the eigenvalues are close to the eigenvalues obtained from
the two blocks on the diagonal. Of course the lower left corner of the bottom block is
vanishing but it is still fairly large so the eigenvalues are approximated by the solution to
( (
) (
))
1 0
âˆ’. 850 29
âˆ’. 616 08
det Î»
âˆ’
=0
0 1
âˆ’1. 826 3 Ã— 10âˆ’2 . 539 39
The solution to this is
Î» = âˆ’. 858 34, . 547 44
and for the complex eigenvalues,
( (
) (
1 0
âˆ’. 834 12
det Î»
âˆ’
0 1
1. 05

âˆ’4. 168 2
. 145 14

))
=0

The solution is
Î» = âˆ’. 344 49 âˆ’ 2. 033 9i, âˆ’. 344 49 + 2. 033 9i
How close are the complex eigenvalues just obtained to giving a solution to the original
equation? Try âˆ’. 344 49 + 2. 033 9i . When this is plugged in it yields
âˆ’.00 12 + 2. 006 8 Ã— 10âˆ’4 i
which is pretty close to 0. The real eigenvalues are also very close to the corresponding real
solutions to the original equation.
It seems like most of the attention to the QR algorithm has to do with ï¬nding ways
to get it to â€œconvergeâ€ faster. Great and marvelous are the clever tricks which have been
proposed to do this but my intent is to present the basic ideas, not to go in to the numerous
reï¬nements of this algorithm. However, there is one thing which is usually done. It involves
reducing to the case of an upper Hessenberg matrix which is one which is zero below the
main sub diagonal. To see that every matrix is unitarily similar to an upper Hessenberg
matrix , see Problem 1 on Page 273. What follows is a construction which also proves this.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

400

NUMERICAL METHODS FOR FINDING EIGENVALUES

Let A be an invertible n Ã— n matrix. Let Qâ€²1 be a unitary matrix
ï£« âˆšâˆ‘
ï£¶ ï£«
ï£¶
n
2
ï£«
ï£¶
a
|aj1 |
j=2
a21
ï£¬
ï£· ï£¬
ï£· ï£¬ 0 ï£·
0
ï£· ï£¬
ï£¬
ï£·â‰¡ï£¬ . ï£·
Qâ€²1 ï£­ ... ï£¸ = ï£¬
ï£¬
ï£· ï£­ . ï£·
..
. ï£¸
ï£­
ï£¸
.
an1
0
0
The vector Qâ€²1 is multiplying is just the bottom n âˆ’ 1 entries of the ï¬rst column of A. Then
let Q1 be
(
)
1 0
0 Qâ€²1
It follows

ï£«
Q1 AQâˆ—1

(
=

1 0
0 Qâ€²1

)

ï£¬
ï£¬
=ï£¬
ï£­

AQâˆ—1

a11
a
..
.

a12

Â·Â·Â·
Aâ€²1

a1n

ï£¶
)
ï£·(
ï£· 1 0
ï£·
ï£¸ 0 Qâ€²âˆ—
1

0
ï£«
ï£¬
ï£¬
=ï£¬
ï£­

âˆ— âˆ—
a
..
.

Â·Â·Â·

âˆ—

A1

ï£¶
ï£·
ï£·
ï£·
ï£¸

0
Now let Qâ€²2 be the n âˆ’ 2 Ã— n âˆ’ 2 matrix which does to the ï¬rst column of A1 the same sort
of thing that the n âˆ’ 1 Ã— n âˆ’ 1 matrix Qâ€²1 did to the ï¬rst column of A. Let
(
)
I 0
Q2 â‰¡
0 Qâ€²2
where I is the 2 Ã— 2 identity. Then applying block multiplication,
ï£«
ï£¶
âˆ— âˆ— Â·Â·Â·
âˆ— âˆ—
ï£¬ âˆ— âˆ— Â·Â·Â·
âˆ— âˆ— ï£·
ï£¬
ï£·
ï£¬
ï£·
Q2 Q1 AQâˆ—1 Qâˆ—2 = ï£¬ 0 âˆ—
ï£·
ï£¬ .. ..
ï£·
ï£­ . .
ï£¸
A2
0 0
where A2 is now an n âˆ’ 2 Ã— n âˆ’ 2 matrix. Continuing this way you eventually get a unitary
matrix Q which is a product of those discussed above such that
ï£«
ï£¶
âˆ— âˆ— Â·Â·Â·
âˆ— âˆ—
ï£¬ âˆ— âˆ— Â·Â·Â·
âˆ— âˆ— ï£·
ï£¬
ï£·
ï£¬
.. ï£·
ï£·
0
âˆ—
âˆ—
.
QAQT = ï£¬
ï£¬
ï£·
ï£¬ . . .
ï£·
.
..
.. âˆ— ï£¸
ï£­ .. ..
0 0
âˆ— âˆ—
This matrix equals zero below the subdiagonal. It is called an upper Hessenberg matrix.
It happens that in the QR algorithm, if Ak is upper Hessenberg, so is Ak+1 . To see this,
note that the matrix is upper Hessenberg means that Aij = 0 whenever i âˆ’ j â‰¥ 2.
Ak+1 = Rk Qk

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

15.3. EXERCISES

401

where Ak = Qk Rk . Therefore as shown before,
Ak+1 = Rk Ak Rkâˆ’1
Let the ij th entry of Ak be akij . Then if i âˆ’ j â‰¥ 2
ak+1
=
ij

j
n âˆ‘
âˆ‘

âˆ’1
rip akpq rqj

p=i q=1

It is given that akpq = 0 whenever p âˆ’ q â‰¥ 2. However, from the above sum,
pâˆ’q â‰¥iâˆ’j â‰¥2
and so the sum equals 0.
Since upper Hessenberg matrices stay that way in the algorithm and it is closer to
being upper triangular, it is reasonable to suppose the QR algorithm will yield good results
more quickly for this upper Hessenberg matrix than for the original matrix. This would be
especially true if the matrix is good sized. The other important thing to observe is that,
starting with an upper Hessenberg matrix, the algorithm will restrict the size of the blocks
which occur to being 2 Ã— 2 blocks which are easy to deal with. These blocks allow you to
identify the complex roots.

15.3

Exercises

In these exercises which call for a computation, donâ€™t waste time on them unless you use a
computer or calculator which can raise matrices to powers and take QR factorizations.
1. In Example 15.1.10 an eigenvalue was found correct to several decimal places along
with an eigenvector. Find the other eigenvalues along with their eigenvectors.
ï£«
ï£¶
3 2 1
2. Find the eigenvalues and eigenvectors of the matrix A = ï£­ 2 1 3 ï£¸ numerically.
1 3 2
âˆš
In this case the exact eigenvalues are Â± 3, 6. Compare with the exact answers.
ï£«
ï£¶
3 2 1
3. Find the eigenvalues and eigenvectors of the matrix A = ï£­ 2 5 3 ï£¸ numerically.
1 3 2
âˆš
âˆš
The exact eigenvalues are 2, 4 + 15, 4 âˆ’ 15. Compare your numerical results with
the exact values. Is it much fun to compute the exact eigenvectors?
ï£«
ï£¶
0 2 1
4. Find the eigenvalues and eigenvectors of the matrix A = ï£­ 2 5 3 ï£¸ numerically.
1 3 2
I donâ€™t know the exact eigenvalues in this case. Check your answers by multiplying
your numerically computed eigenvectors by the matrix.
ï£«
ï£¶
0 2 1
5. Find the eigenvalues and eigenvectors of the matrix A = ï£­ 2 0 3 ï£¸ numerically.
1 3 2
I donâ€™t know the exact eigenvalues in this case. Check your answers by multiplying
your numerically computed eigenvectors by the matrix.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

402

NUMERICAL METHODS FOR FINDING EIGENVALUES

ï£«

ï£¶
3 2 3
T
6. Consider the matrix A = ï£­ 2 1 4 ï£¸ and the vector (1, 1, 1) . Find the shortest
3 4 0
distance between the Rayleigh quotient determined by this vector and some eigenvalue
of A.
ï£¶
ï£«
1 2 1
T
7. Consider the matrix A = ï£­ 2 1 4 ï£¸ and the vector (1, 1, 1) . Find the shortest
1 4 5
distance between the Rayleigh quotient determined by this vector and some eigenvalue
of A.
ï£«
ï£¶
3 2 3
T
8. Consider the matrix A = ï£­ 2 6 4 ï£¸ and the vector (1, 1, 1) . Find the shortest
3 4 âˆ’3
distance between the Rayleigh quotient determined by this vector and some eigenvalue
of A.
9. ï£«
Using
3
ï£­ 2
3

Gerschgorinâ€™s
theorem, ï¬nd upper and lower bounds for the eigenvalues of A =
ï£¶
2 3
6 4 ï£¸.
4 âˆ’3

10. Tell how to ï¬nd a matrix whose characteristic polynomial is a given monic polynomial.
This is called a companion matrix. Find the roots of the polynomial x3 + 7x2 + 3x + 7.
11. Find the roots to x4 + 3x3 + 4x2 + x + 1. It has two complex roots.
12. Suppose A is a real symmetric matrix and the technique of reducing to an upper
Hessenberg matrix is followed. Show the resulting upper Hessenberg matrix is actually
equal to 0 on the top as well as the bottom.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Positive Matrices
Earlier theorems about Markov matrices were presented. These were matrices in which all
the entries were nonnegative and either the columns or the rows added to 1. It turns out
that many of the theorems presented can be generalized to positive matrices. When this is
done, the resulting theory is mainly due to Perron and Frobenius. I will give an introduction
to this theory here following Karlin and Taylor [18].
Deï¬nition A.0.1 For A a matrix or vector, the notation, A >> 0 will mean every entry
of A is positive. By A > 0 is meant that every entry is nonnegative and at least one is
positive. By A â‰¥ 0 is meant that every entry is nonnegative. Thus the matrix or vector
consisting only of zeros is â‰¥ 0. An expression like A >> B will mean A âˆ’ B >> 0 with
similar modiï¬cations for > and â‰¥.
T
For the sake of this section only, deï¬ne the following for x = (x1 , Â· Â· Â· , xn ) , a vector.
T

|x| â‰¡ (|x1 | , Â· Â· Â· , |xn |) .
Thus |x| is the vector which results by replacing each entry of x with its absolute value1 .
Also deï¬ne for x âˆˆ Cn ,
âˆ‘
|xk | .
||x||1 â‰¡
k

Lemma A.0.2 Let A >> 0 and let x > 0. Then Ax >> 0.
âˆ‘
Proof: (Ax)i = j Aij xj > 0 because all the Aij > 0 and at least one xj > 0.
Lemma A.0.3 Let A >> 0. Deï¬ne
S â‰¡ {Î» : Ax > Î»x for some x >> 0} ,
and let
K â‰¡ {x â‰¥ 0 such that ||x||1 = 1} .
Now deï¬ne
S1 â‰¡ {Î» : Ax â‰¥ Î»x for some x âˆˆ K} .
Then
sup (S) = sup (S1 ) .
1 This notation is just about the most abominable thing imaginable. However, it saves space in the
presentation of this theory of positive matrices and avoids the use of new symbols. Please forget about it
when you leave this section.

403

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

404

POSITIVE MATRICES

Proof: Let Î» âˆˆ S. Then there exists x >> 0 such that Ax > Î»x. Consider y â‰¡ x/ ||x||1 .
Then ||y||1 = 1 and Ay > Î»y. Therefore, Î» âˆˆ S1 and so S âŠ† S1 . Therefore, sup (S) â‰¤
sup (S1 ) .
Now let Î» âˆˆ S1 . Then there exists x â‰¥ 0 such that ||x||1 = 1 so x > 0 and Ax > Î»x.
Letting y â‰¡ Ax, it follows from Lemma A.0.2 that Ay >> Î»y and y >> 0. Thus Î» âˆˆ S
and so S1 âŠ† S which shows that sup (S1 ) â‰¤ sup (S) . 
This lemma is signiï¬cant because the set, {x â‰¥ 0 such that ||x||1 = 1} â‰¡ K is a compact
set in Rn . Deï¬ne
Î»0 â‰¡ sup (S) = sup (S1 ) .
(1.1)
The following theorem is due to Perron.
Theorem A.0.4 Let A >> 0 be an n Ã— n matrix and let Î»0 be given in (1.1). Then
1. Î»0 > 0 and there exists x0 >> 0 such that Ax0 = Î»0 x0 so Î»0 is an eigenvalue for A.
2. If Ax = Âµx where x Ì¸= 0, and Âµ Ì¸= Î»0 . Then |Âµ| < Î»0 .
3. The eigenspace for Î»0 has dimension 1.
T

Proof: To see Î»0 > 0, consider the vector, e â‰¡ (1, Â· Â· Â· , 1) . Then
âˆ‘
(Ae)i =
Aij > 0
j

and so Î»0 is at least as large as
min
i

âˆ‘

Aij .

j

Let {Î»k } be an increasing sequence of numbers from S1 converging to Î»0 . Letting xk be
the vector from K which occurs in the deï¬nition of S1 , these vectors are in a compact set.
Therefore, there exists a subsequence, still denoted by xk such that xk â†’ x0 âˆˆ K and
Î»k â†’ Î»0 . Then passing to the limit,
Ax0 â‰¥ Î»0 x0 , x0 > 0.
If Ax0 > Î»0 x0 , then letting y â‰¡ Ax0 , it follows from Lemma A.0.2 that Ay >> Î»0 y and
y >> 0. But this contradicts the deï¬nition of Î»0 as the supremum of the elements of S
because since Ay >> Î»0 y, it follows Ay >> (Î»0 + Îµ) y for Îµ a small positive number.
Therefore, Ax0 = Î»0 x0 . It remains to verify that x0 >> 0. But this follows immediately
from
âˆ‘
0<
Aij x0j = (Ax0 )i = Î»0 x0i .
j

This proves 1.
Next suppose Ax = Âµx and x Ì¸= 0 and Âµ Ì¸= Î»0 . Then |Ax| = |Âµ| |x| . But this implies
A |x| â‰¥ |Âµ| |x| . (See the above abominable deï¬nition of |x|.)
Case 1: |x| Ì¸= x and |x| Ì¸= âˆ’x.
In this case, A |x| > |Ax| = |Âµ| |x| and letting y = A |x| , it follows y >> 0 and
Ay >> |Âµ| y which shows Ay >> (|Âµ| + Îµ) y for suï¬ƒciently small positive Îµ and veriï¬es
|Âµ| < Î»0 .
Case 2: |x| = x or |x| = âˆ’x
In this case, the entries of x are all real and have the same sign. Therefore, A |x| =
|Ax| = |Âµ| |x| . Now let y â‰¡ |x| / ||x||1 . Then Ay = |Âµ| y and so |Âµ| âˆˆ S1 showing that

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

405
|Âµ| â‰¤ Î»0 . But also, the fact the entries of x all have the same sign shows Âµ = |Âµ| and so
Âµ âˆˆ S1 . Since Âµ Ì¸= Î»0 , it must be that Âµ = |Âµ| < Î»0 . This proves 2.
It remains to verify 3. Suppose then that Ay = Î»0 y and for all scalars Î±, Î±x0 Ì¸= y. Then
A Re y = Î»0 Re y, A Im y = Î»0 Im y.
If Re y = Î±1 x0 and Im y = Î±2 x0 for real numbers, Î±i ,then y = (Î±1 + iÎ±2 ) x0 and it is
assumed this does not happen. Therefore, either
t Re y Ì¸= x0 for all t âˆˆ R
or
t Im y Ì¸= x0 for all t âˆˆ R.
Assume the ï¬rst holds. Then varying t âˆˆ R, there exists a value of t such that x0 +t Re y > 0
but it is not the case that x0 +t Re y >> 0. Then A (x0 + t Re y) >> 0 by Lemma A.0.2. But
this implies Î»0 (x0 + t Re y) >> 0 which is a contradiction. Hence there exist real numbers,
Î±1 and Î±2 such that Re y = Î±1 x0 and Im y = Î±2 x0 showing that y = (Î±1 + iÎ±2 ) x0 . This
proves 3.
It is possible to obtain a simple corollary to the above theorem.
Corollary A.0.5 If A > 0 and Am >> 0 for some m âˆˆ N, then all the conclusions of the
above theorem hold.
Proof: There exists Âµ0 > 0 such that Am y0 = Âµ0 y0 for y0 >> 0 by Theorem A.0.4 and
Âµ0 = sup {Âµ : Am x â‰¥ Âµx for some x âˆˆ K} .
Let Î»m
0 = Âµ0 . Then
(
)
(A âˆ’ Î»0 I) Amâˆ’1 + Î»0 Amâˆ’2 + Â· Â· Â· + Î»mâˆ’1
I y0 = (Am âˆ’ Î»m
0
0 I) y0 = 0
(
)
and so letting x0 â‰¡ Amâˆ’1 + Î»0 Amâˆ’2 + Â· Â· Â· + Î»mâˆ’1
I y0 , it follows x0 >> 0 and Ax0 =
0
Î»0 x0 .
Suppose now that Ax = Âµx for x Ì¸= 0 and Âµ Ì¸= Î»0 . Suppose |Âµ| â‰¥ Î»0 . Multiplying both
m
sides by A, it follows Am x = Âµm x and |Âµm | = |Âµ| â‰¥ Î»m
0 = Âµ0 and so from Theorem A.0.4,
m
m
m
since |Âµ | â‰¥ Âµ0 , and Âµ is an eigenvalue of A , it follows that Âµm = Âµ0 . But by Theorem
A.0.4 again, this implies x = cy0 for some scalar, c and hence Ay0 = Âµy0 . Since y0 >> 0,
it follows Âµ â‰¥ 0 and so Âµ = Î»0 , a contradiction. Therefore, |Âµ| < Î»0 .
Finally, if Ax = Î»0 x, then Am x = Î»m
0 x and so x = cy0 for some scalar, c. Consequently,
( mâˆ’1
)
(
)
A
+ Î»0 Amâˆ’2 + Â· Â· Â· + Î»mâˆ’1
I x = c Amâˆ’1 + Î»0 Amâˆ’2 + Â· Â· Â· + Î»mâˆ’1
I y0
0
0
= cx0 .
Hence
x = cx0
mÎ»mâˆ’1
0
which shows the dimension of the eigenspace for Î»0 is one. 
The following corollary is an extremely interesting convergence result involving the powers of positive matrices.
Corollary A.0.6 Let A > 0 and Am >> 0 for some(m âˆˆ
)mN. Then for Î»0 given in (1.1),
A
there exists a rank one matrix P such that limmâ†’âˆ
âˆ’ P = 0.
Î»0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

406

POSITIVE MATRICES

Proof: Considering AT , and the fact that A and AT have the same eigenvalues, Corollary
A.0.5 implies the existence of a vector, v >> 0 such that
AT v = Î»0 v.
Also let x0 denote the vector such that Ax0 = Î»0 x0 with x0 >> 0. First note that xT0 v > 0
because both these vectors have all entries positive. Therefore, v may be scaled such that
vT x0 = xT0 v = 1.

(1.2)

Deï¬ne
P â‰¡ x0 v T .
Thanks to (1.2),
A
P = x0 vT = P, P
Î»0

(

A
Î»0

)

(
= x0 v

T

A
Î»0

)
= x0 vT = P,

(1.3)

and
P 2 = x0 vT x0 vT = vT x0 = P.

(1.4)

Therefore,
(

A
âˆ’P
Î»0

)2

(
=
(
=

A
Î»0
A
Î»0

)2

(
âˆ’2

)2

A
Î»0

)
P + P2

âˆ’ P.

Continuing this way, using (1.3) repeatedly, it follows
(( )
)m ( )m
A
A
âˆ’P
=
âˆ’ P.
(1.5)
Î»0
Î»0
( )
The eigenvalues of Î»A0 âˆ’ P are of interest because it is powers of this matrix which
( )m
determine the convergence of Î»A0
to P. Therefore, let Âµ be a nonzero eigenvalue of this
matrix. Thus
(( )
)
A
âˆ’ P x = Âµx
(1.6)
Î»0
for x Ì¸= 0, and Âµ Ì¸= 0. Applying P to both sides and using the second formula of (1.3) yields
( ( )
)
A
0 = (P âˆ’ P ) x = P
âˆ’ P 2 x = ÂµP x.
Î»0
But since P x = 0, it follows from (1.6) that
Ax = Î»0 Âµx
which implies Î»0 Âµ is an eigenvalue of A. Therefore, by Corollary A.0.5 it follows that either
Î»0 Âµ = Î»0 in which case Âµ = 1, or Î»0 |Âµ| < Î»0 which implies |Âµ| < 1. But if Âµ = 1, then x is
a multiple of x0 and (1.6) would yield
)
(( )
A
âˆ’ P x0 = x0
Î»0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

407
which says x0 âˆ’x0 vT x0 = x0 and so by (1.2), x0 = 0 contrary to the property that x0 >> 0.
Therefore,
|Âµ| < 1 and so this has shown that the absolute values of all eigenvalues of
( )
A
Î»0

âˆ’ P are less than 1. By Gelfandâ€™s theorem, Theorem 14.3.3, it follows
((

A
Î»0

)

)m

1/m

âˆ’P

<r<1

whenever m is large enough. Now by (1.5) this yields
( )m
)m
(( )
A
A
âˆ’P
âˆ’P =
â‰¤ rm
Î»0
Î»0
whenever m is large enough. It follows
(
lim

mâ†’âˆ

A
Î»0

)m
âˆ’P

=0

as claimed.
What about the case when A > 0 but maybe it is not the case that A >> 0? As before,
K â‰¡ {x â‰¥ 0 such that ||x||1 = 1} .
Now deï¬ne
S1 â‰¡ {Î» : Ax â‰¥ Î»x for some x âˆˆ K}
and
Î»0 â‰¡ sup (S1 )

(1.7)

Theorem A.0.7 Let A > 0 and let Î»0 be deï¬ned in (1.7). Then there exists x0 > 0 such
that Ax0 = Î»0 x0 .
Proof: Let E consist of the matrix which has a one in every entry. Then from Theorem
A.0.4 it follows there exists xÎ´ >> 0 , ||xÎ´ ||1 = 1, such that (A + Î´E) xÎ´ = Î»0Î´ xÎ´ where
Î»0Î´ â‰¡ sup {Î» : (A + Î´E) x â‰¥ Î»x for some x âˆˆ K} .
Now if Î± < Î´
{Î» : (A + Î±E) x â‰¥ Î»x for some x âˆˆ K} âŠ†
{Î» : (A + Î´E) x â‰¥ Î»x for some x âˆˆ K}
and so Î»0Î´ â‰¥ Î»0Î± because Î»0Î´ is the sup of the second set and Î»0Î± is the sup of the ï¬rst. It
follows the limit, Î»1 â‰¡ limÎ´â†’0+ Î»0Î´ exists. Taking a subsequence and using the compactness
of K, there exists a subsequence, still denoted by Î´ such that as Î´ â†’ 0, xÎ´ â†’ x âˆˆ K.
Therefore,
Ax = Î»1 x
and so, in particular, Ax â‰¥ Î»1 x and so Î»1 â‰¤ Î»0 . But also, if Î» â‰¤ Î»0 ,
Î»x â‰¤ Ax < (A + Î´E) x
showing that Î»0Î´ â‰¥ Î» for all such Î». But then Î»0Î´ â‰¥ Î»0 also. Hence Î»1 â‰¥ Î»0 , showing these
two numbers are the same. Hence Ax = Î»0 x. 
If Am >> 0 for some m and A > 0, it follows that the dimension of the eigenspace for
Î»0 is one and that the absolute value of every other eigenvalue of A is less than Î»0 . If it is
only assumed that A > 0, not necessarily >> 0, this is no longer true. However, there is
something which is very interesting which can be said. First here is an interesting lemma.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

408

POSITIVE MATRICES

Lemma A.0.8 Let M be a matrix of the form
(
)
A 0
M=
B C
(

or
M=

A
0

)

B
C

where A is an r Ã— r matrix and C is an (n âˆ’ r) Ã— (n âˆ’ r) matrix. Then det (M ) =
det (A) det (B) and Ïƒ (M ) = Ïƒ (A) âˆª Ïƒ (C) .
Proof: To verify the claim about the determinants, note
(
) (
)(
)
A 0
A 0
I 0
=
B C
0 I
B C
Therefore,

(
det

A
B

0
C

)

(
= det

A 0
0 I

)

(
det

I
B

0
C

)
.

But it is clear from the method of Laplace expansion that
(
)
A 0
det
= det A
0 I
and from the multilinear properties of the determinant and row operations that
(
)
(
)
I 0
I 0
det
= det
= det C.
B C
0 C
The case where M is upper block triangular is similar.
This immediately implies Ïƒ (M ) = Ïƒ (A) âˆª Ïƒ (C) .
Theorem A.0.9 Let A > 0 and let Î»0 be given in (1.7). If Î» is an eigenvalue for A such
m
that |Î»| = Î»0 , then Î»/Î»0 is a root of unity. Thus (Î»/Î»0 ) = 1 for some m âˆˆ N.
Proof: Applying Theorem A.0.7 to AT , there exists v > 0 such that AT v = Î»0 v. In
the ï¬rst part of the argument it is assumed v >> 0. Now suppose Ax = Î»x, x Ì¸= 0 and that
|Î»| = Î»0 . Then
A |x| â‰¥ |Î»| |x| = Î»0 |x|
and it follows that if A |x| > |Î»| |x| , then since v >> 0,
(
)
Î»0 (v, |x|) < (v,A |x|) = AT v, |x| = Î»0 (v, |x|) ,
a contradiction. Therefore,
A |x| = Î»0 |x| .

(1.8)

It follows that
âˆ‘

Aij xj = Î»0 |xi | =

âˆ‘

j

Aij |xj |

j

and so the complex numbers,
Aij xj , Aik xk

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

409
must have the same argument for every k, j because equality holds in the triangle inequality.
Therefore, there exists a complex number, Âµi such that
Aij xj = Âµi Aij |xj |
and so, letting r âˆˆ N,

(1.9)

Aij xj Âµrj = Âµi Aij |xj | Âµrj .

Summing on j yields

âˆ‘

Aij xj Âµrj = Âµi

âˆ‘

j

Aij |xj | Âµrj .

(1.10)

j

Also, summing (1.9) on j and using that Î» is an eigenvalue for x, it follows from (1.8) that
âˆ‘
âˆ‘
Î»xi =
Aij xj = Âµi
Aij |xj | = Âµi Î»0 |xi | .
(1.11)
j

j

From (1.10) and (1.11),
âˆ‘

Aij xj Âµrj

= Âµi

âˆ‘

j

Aij |xj | Âµrj

j

= Âµi

âˆ‘

see (1.11)

z }| {
Aij Âµj |xj | Âµrâˆ’1
j

j

= Âµi

âˆ‘

(
Aij

j

(
= Âµi

Î»
Î»0

Î»
Î»0

)âˆ‘

)
xj Âµrâˆ’1
j

Aij xj Âµrâˆ’1
j

j

Now from (1.10) with r replaced by r âˆ’ 1, this equals
( )âˆ‘
( )âˆ‘
Î»
Î»
râˆ’1
2
2
Âµi
Aij |xj | Âµj
Aij Âµj |xj | Âµrâˆ’2
= Âµi
j
Î»0
Î»
0
j
j
( )2 âˆ‘
Î»
= Âµ2i
Aij xj Âµrâˆ’2
.
j
Î»0
j
Continuing this way,

âˆ‘

(
Aij xj Âµrj = Âµki

j

Î»
Î»0

)k âˆ‘

Aij xj Âµrâˆ’k
j

j

and eventually, this shows
âˆ‘

(
Aij xj Âµrj

Âµri

=

j

(
=

(
and this says

Î»
Î»0

)r+1

(
is an eigenvalue for

A
Î»0

Î»
Î»0

Î»
Î»0
)r

)r âˆ‘

Aij xj

j

Î» (xi Âµri )

)
with the eigenvector being
T

(x1 Âµr1 , Â· Â· Â· , xn Âµrn ) .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

410

POSITIVE MATRICES

( )2 ( )3 ( )4
Now recall that r âˆˆ N was arbitrary and so this has shown that Î»Î»0 , Î»Î»0 , Î»Î»0 , Â· Â· Â·
( )
are each eigenvalues of Î»A0 which has only ï¬nitely many and hence this sequence must
( )
repeat. Therefore, Î»Î»0 is a root of unity as claimed. This proves the theorem in the case
that v >> 0.
Now it is necessary to consider the case where v > 0 but it is not the case that v >> 0.
Then in this case, there exists a permutation matrix P such that
ï£«
ï£¶
v1
ï£¬ .. ï£·
ï£¬ . ï£·
ï£¬
ï£· (
)
ï£¬ vr ï£·
u
ï£¬
ï£·
â‰¡ v1
Pv = ï£¬
ï£·â‰¡
0
ï£¬ 0 ï£·
ï£¬ . ï£·
ï£­ .. ï£¸
0
Then
Î»0 v = AT v = AT P v1 .
Therefore,
Î»0 v1 = P AT P v1 = Gv1
Now P 2 = I because it is a permutation matrix. Therefore, the matrix G â‰¡ P AT P and A
are similar. Consequently, they have the same eigenvalues and it suï¬ƒces from now on to
consider the matrix G rather than A. Then
(
) (
)(
)
u
M1 M2
u
Î»0
=
0
M3 M4
0
where M1 is r Ã— r and M4 is (n âˆ’ r) Ã— (n âˆ’ r) . It follows from block multiplication and the
assumption that A and hence G are > 0 that
( â€²
)
A B
G=
.
0 C
Now let Î» be an eigenvalue of G such that |Î»| = Î»0 . Then from Lemma A.0.8, either
Î» âˆˆ Ïƒ (Aâ€² ) or Î» âˆˆ Ïƒ (C) . Suppose without loss of generality that Î» âˆˆ Ïƒ (Aâ€² ) . Since Aâ€² > 0
it has a largest positive eigenvalue Î»â€²0 which is obtained from (1.7). Thus Î»â€²0 â‰¤ Î»0 but Î»
being an eigenvalue of Aâ€² , has its absolute value bounded by Î»â€²0 and so Î»0 = |Î»| â‰¤ Î»â€²0 â‰¤ Î»0
showing that Î»0 âˆˆ Ïƒ (Aâ€² ) . Now if there exists v >> 0 such that Aâ€²T v = Î»0 v, then the ï¬rst
part of this proof applies to the matrix A and so (Î»/Î»0 ) is a root of unity. If such a vector,
v does not exist, then let Aâ€² play the role of A in the above argument and reduce to the
consideration of
( â€²â€²
)
A
Bâ€²
Gâ€² â‰¡
0 Câ€²
where Gâ€² is similar to Aâ€² and Î», Î»0 âˆˆ Ïƒ (Aâ€²â€² ) . Stop if Aâ€²â€²T v = Î»0 v for some v >> 0.
Otherwise, decompose Aâ€²â€² similar to the above and add another prime. Continuing this way
T
you must eventually obtain the situation where (Aâ€²Â·Â·Â·â€² ) v = Î»0 v for some v >> 0. Indeed,
â€²Â·Â·Â·â€²
this happens no later than when A
is a 1 Ã— 1 matrix. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Functions Of Matrices
The existence of the Jordan form also makes it possible to deï¬ne various functions of matrices. Suppose
âˆ
âˆ‘
f (Î») =
an Î»n
(2.1)
n=0

âˆ‘âˆ
for all |Î»| < R. There is a formula for f (A) â‰¡ n=0 an An which makes sense whenever
Ï (A) < R. Thus you can speak of sin (A) or eA for A an n Ã— n matrix. To begin with, deï¬ne
fP (Î») â‰¡

P
âˆ‘

an Î»n

n=0

so for k < P
(k)

fP (Î») =

P
âˆ‘
n=k

=

P
âˆ‘
n=k

Thus

an n Â· Â· Â· (n âˆ’ k + 1) Î»nâˆ’k
( )
n
an
k!Î»nâˆ’k .
k

(2.2)

( )
P
(k)
âˆ‘
fP (Î»)
n nâˆ’k
=
an
Î»
k!
k

(2.3)

n=k

To begin with consider f (Jm (Î»)) where Jm (Î») is an m Ã— m Jordan block. Thus Jm (Î») =
D + N where N m = 0 and N commutes with D. Therefore, letting P > m
P
âˆ‘

n

an Jm (Î»)

n=0

n ( )
âˆ‘
n
Dnâˆ’k N k
k
n=0
k=0
P
P
âˆ‘ âˆ‘ (n)
=
an
Dnâˆ’k N k
k
k=0 n=k
( )
mâˆ’1
P
âˆ‘
âˆ‘
n
k
=
N
an
Dnâˆ’k .
k
P
âˆ‘

=

an

k=0

(2.4)

n=k

From (2.3) this equals
mâˆ’1
âˆ‘
k=0

(
k

N diag

(k)

(k)

f (Î»)
fP (Î»)
,Â·Â·Â· , P
k!
k!

)
(2.5)

411

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

412

FUNCTIONS OF MATRICES

where for k = 0, Â· Â· Â· , m âˆ’ 1, deï¬ne diagk (a1 , Â· Â· Â· , amâˆ’k ) the m Ã— m matrix which equals zero
everywhere except on the k th super diagonal where this diagonal is ï¬lled with the numbers,
{a1 , Â· Â· Â· , amâˆ’k } from the upper left to the lower right. With no subscript, it is just the
diagonal matrices having the indicated entries. Thus in 4 Ã— 4 matrices, diag2 (1, 2) would
be the matrix
ï£«
ï£¶
0 0 1 0
ï£¬ 0 0 0 2 ï£·
ï£¬
ï£·
ï£­ 0 0 0 0 ï£¸.
0 0 0 0
Then from (2.5) and (2.2),
P
âˆ‘

n

an Jm (Î») =

n=0

Therefore,

âˆ‘P

mâˆ’1
âˆ‘

(
diag k

k=0

(k)

(k)

fP (Î»)
f (Î»)
,Â·Â·Â· , P
k!
k!

)
.

n

n=0

an Jm (Î») =
ï£«
â€²
fP
(Î»)
f (Î»)
1!
ï£¬ P
ï£¬
ï£¬
fP (Î»)
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­

(2)

fP (Î»)
2!
â€²
fP
(Î»)
1!

fP (Î»)

(mâˆ’1)

fP
(Î»)
(mâˆ’1)!

Â·Â·Â·
..
.
..
.
..
.

..
.
(2)

fP (Î»)
2!
â€²
fP
(Î»)
1!
fP (Î»)

0

ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

(2.6)

Now let A be an n Ã— n matrix with Ï (A) < R where R is given above. Then the Jordan
form of A is of the form
ï£«
ï£¶
J1
0
ï£¬
ï£·
J2
ï£¬
ï£·
J =ï£¬
(2.7)
ï£·
.
..
ï£­
ï£¸
0
Jr
where Jk = Jmk (Î»k ) is an mk Ã— mk Jordan block and A = S âˆ’1 JS. Then, letting P > mk
for all k,
P
P
âˆ‘
âˆ‘
an An = S âˆ’1
an J n S,
n=0

n=0

and because of block multiplication of matrices,
ï£« âˆ‘P
n
n=0 an J1
ï£¬
..
P
âˆ‘
ï£¬
.
n
ï£¬
an J = ï£¬
ï£­
n=0
0
and from (2.6)

âˆ‘P
n=0

ï£¶

0
..

.

âˆ‘P
n=0

ï£·
ï£·
ï£·
ï£·
ï£¸
an Jrn

an Jkn converges as P â†’ âˆ to the mk Ã— mk matrix
ï£¶
ï£«
â€²
f (mâˆ’1) (Î»k )
f (2) (Î»k )
k)
Â·
Â·
Â·
f (Î»k ) f (Î»
1!
2!
(mk âˆ’1)!
ï£·
ï£¬
..
â€²
ï£·
ï£¬
..
f
(Î»
)
k
ï£·
ï£¬
.
0
f
(Î»
)
.
k
1!
ï£·
ï£¬
ï£·
ï£¬
(2)
.
f
(Î»
)
.
k
ï£·
ï£¬
.
0
0
f (Î»k )
2!
ï£·
ï£¬
ï£·
ï£¬
..
â€²
..
..
f (Î»k )
ï£¸
ï£­
.
.
.
1!
0
f (Î»k )
0
0
Â·Â·Â·

Saylor URL: http://www.saylor.org/courses/ma212/

(2.8)

The Saylor Foundation

413
There is no convergence problem because |Î»| < R for all Î» âˆˆ Ïƒ (A) . This has proved the
following theorem.
Theorem B.0.10 Let f be given by (2.1) and suppose Ï (A) < R where R is the radius of
convergence of the power series in (2.1). Then the series,
âˆ
âˆ‘

an An

(2.9)

k=0

converges in the space L (Fn , Fn ) with respect to any of the norms on this space and furthermore,
ï£« âˆ‘âˆ
ï£¶
n
0
n=0 an J1
ï£¬
ï£·
..
âˆ
âˆ‘
ï£¬
ï£·
.
ï£·S
an An = S âˆ’1 ï£¬
ï£·
ï£¬
..
ï£­
ï£¸
.
k=0
âˆ‘âˆ
n
0
a
J
n=0 n r
âˆ‘âˆ
n
where n=0 an Jk is an mk Ã— mk matrix of the form given in (2.8) where A = S âˆ’1 JS and
the Jordan form of A, J is given by (2.7). Therefore, you can deï¬ne f (A) by the series in
(2.9).
Here is a simple example.
ï£«

ï£¶
âˆ’1 1
0 âˆ’1 ï£·
ï£·.
1 âˆ’1 ï£¸
1
4

4
1
ï£¬ 1
1
Example B.0.11 Find sin (A) where A = ï£¬
ï£­ 0 âˆ’1
âˆ’1 2

In this case, the Jordan canonical form of the matrix is
ï£¶ ï£«
ï£«
4
1 âˆ’1 1
2
0
ï£¬ 1
ï£· ï£¬ 1 âˆ’4
1
0
âˆ’1
ï£¬
ï£· ï£¬
ï£­ 0 âˆ’1 1 âˆ’1 ï£¸ = ï£­ 0
0
âˆ’1 2
1
4
âˆ’1 4
ï£«

4
ï£¬ 0
ï£¬
ï£­ 0
0

0
2
0
0

0
1
2
0

ï£¶ï£« 1
0
2
ï£¬ 1
0 ï£·
ï£·ï£¬ 8
1 ï£¸ï£­ 0
2
0

1
2
âˆ’ 38
1
4
1
2

0
0
âˆ’ 14
1
2

Then from the above theorem sin (J) is given by
ï£«
ï£¶ ï£«
sin 4
0
4 0 0 0
ï£¬ 0 2 1 0 ï£· ï£¬ 0
sin
2
ï£· ï£¬
sin ï£¬
ï£­ 0 0 2 1 ï£¸=ï£­ 0
0
0 0 0 2
0
0
Therefore, sin (A) =
ï£«
2
0 âˆ’2 âˆ’1
ï£¬ 1 âˆ’4 âˆ’2 âˆ’1
ï£¬
ï£­ 0
0 âˆ’2 1
âˆ’1 4
4
2

ï£¶ï£«

sin 4
0
ï£·ï£¬ 0
sin
2
ï£·ï£¬
ï£¸ï£­ 0
0
0
0

Saylor URL: http://www.saylor.org/courses/ma212/

0
cos 2
sin 2
0

0

not too hard to ï¬nd.
ï£¶
âˆ’2 âˆ’1
âˆ’2 âˆ’1 ï£·
ï£·Â·
âˆ’2 1 ï£¸
4
2
ï£¶

1
2
âˆ’ 18
1
4
1
2

ï£·
ï£·.
ï£¸

0
cos 2
sin 2
0
ï£¶ï£«

0

ï£·
ï£·.
cos 2 ï£¸
sin 2

âˆ’ sin 2
2

1
2
1
8

ï£·ï£¬
ï£·ï£¬
cos 2 ï£¸ ï£­ 0
0
sin 2

âˆ’ sin 2
2

ï£¶

1
2
âˆ’ 38
1
4
1
2

0
0
âˆ’ 14
1
2

1
2
âˆ’ 18
1
4
1
2

ï£¶
ï£·
ï£·=M
ï£¸

The Saylor Foundation

414

FUNCTIONS OF MATRICES

where the columns of M are as follows from left to right,
ï£«
ï£¶ ï£«
sin 4
sin 4 âˆ’ sin 2 âˆ’ cos 2
ï£¬ 1 sin 4 âˆ’ 1 sin 2 ï£· ï£¬ 1 sin 4 + 3 sin 2 âˆ’ 2 cos 2
2
2
ï£¬ 2
ï£·,ï£¬ 2
ï£­
ï£¸ ï£­
0
âˆ’ cos 2
âˆ’ 1 sin 4 + 12 sin 2
âˆ’ 12 sin 4 âˆ’ 12 sin 2 + 3 cos 2
ï£« 2
ï£¶
sin 4 âˆ’ sin 2 âˆ’ cos 2
ï£¬ 1 sin 4 + 1 sin 2 âˆ’ 2 cos 2 ï£·
2
ï£¬ 2
ï£·.
ï£¸
ï£­
âˆ’ cos 2
1
1
âˆ’ 2 sin 4 + 2 sin 2 + 3 cos 2

ï£¶ ï£«

ï£¶
âˆ’ cos 2
ï£· ï£¬
ï£·
sin 2
ï£·,ï£¬
ï£·
ï£¸ ï£­ sin 2 âˆ’ cos 2 ï£¸
cos 2 âˆ’ sin 2

Perhaps this isnâ€™t the ï¬rst thing you would think of. Of course the ability to get this nice
closed form description of sin (A) was dependent on being able to ï¬nd the Jordan form along
with a similarity transformation which will yield the Jordan form.
The following corollary is known as the spectral mapping theorem.
Corollary B.0.12 Let A be an n Ã— n matrix and let Ï (A) < R where for |Î»| < R,
f (Î») =

âˆ
âˆ‘

an Î»n .

n=0

Then f (A) is also an n Ã— n matrix and furthermore, Ïƒ (f (A)) = f (Ïƒ (A)) . Thus the eigenvalues of f (A) are exactly the numbers f (Î») where Î» is an eigenvalue of A. Furthermore,
the algebraic multiplicity of f (Î») coincides with the algebraic multiplicity of Î».
All of these things can be generalized to linear transformations deï¬ned on inï¬nite dimensional spaces and when this is done the main tool is the Dunford integral along with
the methods of complex analysis. It is good to see it done for ï¬nite dimensional situations
ï¬rst because it gives an idea of what is possible. Actually, some of the most interesting
functions in applications do not come in the above form as a power series expanded about
0. One example of this situation has already been encountered in the proof of the right
polar decomposition with the square root of an Hermitian transformation which had all
nonnegative eigenvalues. Another example is that of taking the positive part of an Hermitian matrix. This is important in some physical models where something may depend on
the positive part of the strain which is a symmetric real matrix. Obviously there is no way
to consider this as a power series expanded about 0 because the function f (r) = r+ is not
even diï¬€erentiable at 0. Therefore, a totally diï¬€erent approach must be considered. First
the notion of a positive part is deï¬ned.
Deï¬nition B.0.13 Let A be an Hermitian matrix. Thus it suï¬ƒces to consider A as an
element of L (Fn , Fn ) according to the usual notion of matrix multiplication. Then there
exists an orthonormal basis of eigenvectors, {u1 , Â· Â· Â· , un } such that
A=

n
âˆ‘

Î» j uj âŠ— uj ,

j=1

for Î»j the eigenvalues of A, all real. Deï¬ne
A+ â‰¡

n
âˆ‘

Î»+
j uj âŠ— uj

j=1

where Î»+ â‰¡

|Î»|+Î»
2 .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

415
This gives us a nice deï¬nition of what is meant but it turns out to be very important in
the applications to determine how this function depends on the choice of symmetric matrix
A. The following addresses this question.
Theorem B.0.14 If A, B be Hermitian matrices, then for |Â·| the Frobenius norm,
A+ âˆ’ B + â‰¤ |A âˆ’ B| .
âˆ‘
âˆ‘
Proof: Let A = i Î»i vi âŠ— vi and let B = j Âµj wj âŠ— wj where {vi } and {wj } are
orthonormal bases of eigenvectors.
ï£«
A+ âˆ’ B

+ 2

= trace ï£­

âˆ‘

Î»+
i vi âŠ— vi âˆ’

âˆ‘

ï£¶2
ï£¸ =
Âµ+
j wj âŠ— wj

j

i

ï£®
âˆ‘ ( )2
âˆ‘ ( )2
Âµ+
wj âŠ— wj
trace ï£°
Î»+
vi âŠ— vi +
i
j
i

âˆ’

âˆ‘

j

+
Î»+
i Âµj (wj , vi ) vi âŠ— wj âˆ’

i,j

âˆ‘

ï£¹
+
ï£»
Î»+
i Âµj (vi , wj ) wj âŠ— vi

i,j

Since the trace of vi âŠ— wj is (vi , wj ) , a fact which follows from (vi , wj ) being the only
possibly nonzero eigenvalue,
âˆ‘ ( )2 âˆ‘ ( )2
âˆ‘
2
+
=
Î»+
Âµ+
Î»+
+
âˆ’2
(2.10)
i
i Âµj |(vi , wj )| .
j
i

j

i,j

Since these are orthonormal bases,
âˆ‘
âˆ‘
2
2
|(vi , wj )| = 1 =
|(vi , wj )|
i

j

and so (2.10) equals
=

âˆ‘ âˆ‘ ((
i

Î»+
i

)2

)
( )2
2
+
+ Âµ+
âˆ’ 2Î»+
|(vi , wj )| .
i Âµj
j

j

Similarly,
2

|A âˆ’ B| =

âˆ‘âˆ‘(
i

)
( )2
2
2
(Î»i ) + Âµj âˆ’ 2Î»i Âµj |(vi , wj )| .

j

( )2
( )2 ( + )2
2
+
Now it is easy to check that (Î»i ) + Âµj âˆ’ 2Î»i Âµj â‰¥ Î»+
+ Âµj âˆ’ 2Î»+
i Âµj . 
i

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

416

Saylor URL: http://www.saylor.org/courses/ma212/

FUNCTIONS OF MATRICES

The Saylor Foundation

Applications To Diï¬€erential
Equations
C.1

Theory Of Ordinary Diï¬€erential Equations

Here I will present fundamental existence and uniqueness theorems for initial value problems
for the diï¬€erential equation,
xâ€² = f (t, x) .
Suppose that f : [a, b] Ã— Rn â†’ Rn satisï¬es the following two conditions.
|f (t, x) âˆ’ f (t, x1 )| â‰¤ K |x âˆ’ x1 | ,

(3.1)

f is continuous.

(3.2)

The ï¬rst of these conditions is known as a Lipschitz condition.
Lemma C.1.1 Suppose x : [a, b] â†’ Rn is a continuous function and c âˆˆ [a, b]. Then x is a
solution to the initial value problem,
xâ€² = f (t, x) , x (c) = x0
if and only if x is a solution to the integral equation,
âˆ« t
x (t) = x0 +
f (s, x (s)) ds.

(3.3)

(3.4)

c

Proof: If x solves (3.4), then since f is continuous, we may apply the fundamental
theorem of calculus to diï¬€erentiate both sides and obtain xâ€² (t) = f (t, x (t)) . Also, letting
t = c on both sides, gives x (c) = x0 . Conversely, if x is a solution of the initial value
problem, we may integrate both sides from c to t to see that x solves (3.4). 
Theorem C.1.2 Let f satisfy (3.1) and (3.2). Then there exists a unique solution to the
initial value problem, (3.3) on the interval [a, b].
{
}
Proof: Let ||x||Î» â‰¡ sup eÎ»t |x (t)| : t âˆˆ [a, b] . Then this norm is equivalent to the usual
norm on BC ([a, b] , Fn ) described in Example 14.6.2. This means that for ||Â·|| the norm given
there, there exist constants Î´ and âˆ† such that
||x||Î» Î´ â‰¤ ||x|| â‰¤ âˆ† ||x||

417

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

418

APPLICATIONS TO DIFFERENTIAL EQUATIONS

for all x âˆˆBC ([a, b] , Fn ) . In fact, you can take Î´ â‰¡ eÎ»a and âˆ† â‰¡ eÎ»b in case Î» > 0 with the
two reversed in case Î» < 0. Thus BC ([a, b] , Fn ) is a Banach space with this norm, ||Â·||Î» .
Then let F : BC ([a, b] , Fn ) â†’ BC ([a, b] , Fn ) be deï¬ned by
âˆ« t
F x (t) â‰¡ x0 +
f (s, x (s)) ds.
c

Let Î» < 0. It follows
âˆ«
e |F x (t) âˆ’ F y (t)| â‰¤
Î»t

e

âˆ«
â‰¤

t

|f (s, x (s)) âˆ’ f (s, y (s))| ds

Î»t
c
t

KeÎ»(tâˆ’s) |x (s) âˆ’ y (s)| eÎ»s ds
c

âˆ«
â‰¤ ||x âˆ’ y||Î»

a

t

KeÎ»(tâˆ’s) ds â‰¤ ||x âˆ’ y||Î»

K
|Î»|

and therefore,
||F x âˆ’ F y||Î» â‰¤ ||x âˆ’ y||

K
.
|Î»|

If |Î»| is chosen larger than K, this implies F is a contraction mapping on BC ([a, b] , Fn ) .
Therefore, there exists a unique ï¬xed point. With Lemma C.1.1 this proves the theorem. 

C.2

Linear Systems

As an example of the above theorem, consider for t âˆˆ [a, b] the system
xâ€² = A (t) x (t) + g (t) , x (c) = x0

(3.5)

where A (t) is an n Ã— n matrix whose entries are continuous functions of t, (aij (t)) and
g (t) is a vector whose components are continuous functions of t satisï¬es the conditions
T
of Theorem C.1.2 with f (t, x) = A (t) x + g (t) . To see this, let x = (x1 , Â· Â· Â· , xn ) and
T
x1 = (x11 , Â· Â· Â· x1n ) . Then letting M = max {|aij (t)| : t âˆˆ [a, b] , i, j â‰¤ n} ,
|f (t, x) âˆ’ f (t, x1 )| = |A (t) (x âˆ’ x1 )|
ï£«
ï£¶
ï£«
ï£«
ï£¶2 ï£¶1/2
2 1/2
n âˆ‘
n
n
n
âˆ‘
âˆ‘
âˆ‘
ï£¬
ï£·
ï£¬
ï£·
ï£­
= ï£­
aij (t) (xj âˆ’ x1j ) ï£¸
â‰¤M ï£­
|xj âˆ’ x1j |ï£¸ ï£¸
i=1 j=1

i=1

j=1

ï£«

ï£¶1/2
ï£«
ï£¶1/2
n
n
n
âˆ‘
âˆ‘
âˆ‘
2
2
â‰¤M ï£­
n
|xj âˆ’ x1j | ï£¸
= Mn ï£­
|xj âˆ’ x1j | ï£¸
= M n |x âˆ’ x1 | .
i=1

j=1

j=1

Therefore, let K = M n. This proves
Theorem C.2.1 Let A (t) be a continuous n Ã— n matrix and let g (t) be a continuous vector
for t âˆˆ [a, b] and let c âˆˆ [a, b] and x0 âˆˆ Fn . Then there exists a unique solution to (3.5)
valid for t âˆˆ [a, b] .
This includes more examples of linear equations than are typically encountered in an
entire diï¬€erential equations course.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

C.3. LOCAL SOLUTIONS

C.3

419

Local Solutions

Lemma C.3.1 Let D (x0 , r) â‰¡ {x âˆˆ Fn : |x âˆ’ x0 | â‰¤ r} and suppose U is an open set containing D (x0 , r) such that f : U â†’ Fn is C 1 (U ) . (Recall this means all partial derivatives of
f exist and are continuous.) Then for K = M n, where M denotes the maximum of
for z âˆˆ D (x0 , r) , it follows that for all x, y âˆˆ D (x0 , r) ,

âˆ‚f
âˆ‚xi

(z)

|f (x) âˆ’ f (y)| â‰¤ K |x âˆ’ y| .
Proof: Let x, y âˆˆ D (x0 , r) and consider the line segment joining these two points,
x+t (y âˆ’ x) for t âˆˆ [0, 1] . Letting h (t) = f (x+t (y âˆ’ x)) for t âˆˆ [0, 1] , then
âˆ«

1

f (y) âˆ’ f (x) = h (1) âˆ’ h (0) =

hâ€² (t) dt.

0

Also, by the chain rule,
hâ€² (t) =

n
âˆ‘
âˆ‚f
(x+t (y âˆ’ x)) (yi âˆ’ xi ) .
âˆ‚x
i
i=1

Therefore,
|f (y) âˆ’ f (x)| =
âˆ«
âˆ«

n
1âˆ‘
0

i=1
n
1âˆ‘

â‰¤
0

â‰¤ M

i=1
n
âˆ‘

âˆ‚f
(x+t (y âˆ’ x)) (yi âˆ’ xi ) dt
âˆ‚xi
âˆ‚f
(x+t (y âˆ’ x)) |yi âˆ’ xi | dt
âˆ‚xi

|yi âˆ’ xi | â‰¤ M n |x âˆ’ y| . 

i=1

Now consider the map, P which maps all of Rn to D (x0 , r) given as follows. For
x âˆˆ D (x0 , r) , P x = x. For x âˆˆD
/ (x0 , r) , P x will be the closest point in D (x0 , r) to x. Such
a closest point exists because D (x0 , r) is a closed and bounded set. Taking f (y) â‰¡ |y âˆ’ x| ,
it follows f is a continuous function deï¬ned on D (x0 , r) which must achieve its minimum
value by the extreme value theorem from calculus.
x

Px
9
z
D(x0 , r)
x0

Lemma C.3.2 For any pair of points, x, y âˆˆ Fn , |P x âˆ’ P y| â‰¤ |x âˆ’ y| .
Proof: The above picture suggests the geometry of what is going on. Letting z âˆˆ
D (x0 , r) , it follows that for all t âˆˆ [0, 1] ,
2

|x âˆ’ P x| â‰¤ |xâˆ’ (P x + t (zâˆ’P x))|

Saylor URL: http://www.saylor.org/courses/ma212/

2

The Saylor Foundation

420

APPLICATIONS TO DIFFERENTIAL EQUATIONS
2

= |xâˆ’P x| + 2t Re ((x âˆ’ P x) Â· (P x âˆ’ z)) + t2 |zâˆ’P x|

2

Hence
2

2t Re ((x âˆ’ P x) Â· (P x âˆ’ z)) + t2 |zâˆ’P x| â‰¥ 0
and this can only happen if
Re ((x âˆ’ P x) Â· (P x âˆ’ z)) â‰¥ 0.
Therefore,
Re ((x âˆ’ P x) Â· (P xâˆ’P y)) â‰¥ 0
Re ((y âˆ’ P y) Â· (P yâˆ’P x)) â‰¥ 0
and so
Re (x âˆ’ P x âˆ’ (y âˆ’ P y)) Â· (P xâˆ’P y) â‰¥ 0
which implies
Re (x âˆ’ y) Â· (P x âˆ’ P y) â‰¥ |P x âˆ’ P y|

2

Then using the Cauchy Schwarz inequality it follows
|x âˆ’ y| â‰¥ |P x âˆ’ P y| .

With this here is the local existence and uniqueness theorem.
Theorem C.3.3 Let [a, b] be a closed interval and let U be an open subset of Fn . Let
âˆ‚f
f : [a, b] Ã— U â†’ Fn be continuous and suppose that for each t âˆˆ [a, b] , the map x â†’ âˆ‚x
(t, x)
i
is continuous. Also let x0 âˆˆ U and c âˆˆ [a, b] . Then there exists an interval, I âŠ† [a, b] such
that c âˆˆ I and there exists a unique solution to the initial value problem,
xâ€² = f (t, x) , x (c) = x0

(3.6)

valid for t âˆˆ I.
Proof: Consider the following picture.
U
x0
D(x0 , r)
The large dotted circle represents U and the little solid circle represents D (x0 , r) as
indicated. Here r is so small that D (x0 , r) is contained in U as shown. Now let P denote
the projection map deï¬ned above. Consider the initial value problem
xâ€² = f (t, P x) , x (c) = x0 .

(3.7)

âˆ‚f
From Lemma C.3.1 and the continuity of x â†’ âˆ‚x
(t, x) , there exists a constant, K such
i
that if x, y âˆˆ D (x0 , r) , then |f (t, x) âˆ’ f (t, y)| â‰¤ K |x âˆ’ y| for all t âˆˆ [a, b] . Therefore, by
Lemma C.3.2
|f (t, P x) âˆ’ f (t, P y)| â‰¤ K |P xâˆ’P y| â‰¤ K |x âˆ’ y| .

It follows from Theorem C.1.2 that (3.7) has a unique solution valid for t âˆˆ [a, b] . Since
x is continuous, it follows that there exists an interval, I containing c such that for t âˆˆ I,

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

C.4. FIRST ORDER LINEAR SYSTEMS

421

x (t) âˆˆ D (x0 , r) . Therefore, for these values of t, f (t, P x) = f (t, x) and so there is a unique
solution to (3.6) on I. 
Now suppose f has the property that for every R > 0 there exists a constant, KR such
that for all x, x1 âˆˆ B (0, R),
|f (t, x) âˆ’ f (t, x1 )| â‰¤ KR |x âˆ’ x1 | .

(3.8)

Corollary C.3.4 Let f satisfy (3.8) and suppose also that (t, x) â†’ f (t, x) is continuous.
Suppose now that x0 is given and there exists an estimate of the form |x (t)| < R for all
t âˆˆ [0, T ) where T â‰¤ âˆ on the local solution to
xâ€² = f (t, x) , x (0) = x0 .

(3.9)

Then there exists a unique solution to the initial value problem, (3.9) valid on [0, T ).
Proof: Replace f (t, x) with f (t, P x) where P is the projection onto B (0, R). Then by
Theorem C.1.2 there exists a unique solution to the system
xâ€² = f (t, P x) , x (0) = x0
valid on [0, T1 ] for every T1 < T. Therefore, the above system has a unique solution on [0, T )
and from the estimate, P x = x. 

C.4

First Order Linear Systems

Here is a discussion of linear systems of the form
xâ€² = Ax + f (t)
where A is a constant n Ã— n matrix and f is a vector valued function having all entries
continuous. Of course the existence theory is a very special case of the general considerations
above but I will give a self contained presentation based on elementary ï¬rst order scalar
diï¬€erential equations and linear algebra.
Deï¬nition C.4.1 Suppose t â†’ M (t) is a matrix valued function of t. Thus M (t) =
(mij (t)) . Then deï¬ne
)
(
M â€² (t) â‰¡ mâ€²ij (t) .
In words, the derivative of M (t) is the matrix whose entries consist of the derivatives of the
entries of M (t) . Integrals of matrices are deï¬ned the same way. Thus
(âˆ«
)
âˆ«
b

b

M (t) di â‰¡

mij (t) dt .
a

a

In words, the integral of M (t) is the matrix obtained by replacing each entry of M (t) by the
integral of that entry.
With this deï¬nition, it is easy to prove the following theorem.
Theorem C.4.2 Suppose M (t) and N (t) are matrices for which M (t) N (t) makes sense.
Then if M â€² (t) and N â€² (t) both exist, it follows that
â€²

(M (t) N (t)) = M â€² (t) N (t) + M (t) N â€² (t) .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

422

APPLICATIONS TO DIFFERENTIAL EQUATIONS

Proof:
(

(M (t) N

â€²)
(t)) ij

â‰¡
=

(
(M (t) N (t))ij
âˆ‘

(
=

âˆ‘

)â€²
M (t)ik N (t)kj

k

(
)â€²
â€²
(M (t)ik ) N (t)kj + M (t)ik N (t)kj

k

â‰¡

)â€²

âˆ‘(

M (t)

â€²)
ik

(
â€²)
N (t)kj + M (t)ik N (t) kj

k

â‰¡ (M â€² (t) N (t) + M (t) N â€² (t))ij 
In the study of diï¬€erential equations, one of the most important theorems is Gronwallâ€™s
inequality which is next.
Theorem C.4.3 Suppose u (t) â‰¥ 0 and for all t âˆˆ [0, T ] ,
âˆ« t
u (t) â‰¤ u0 +
Ku (s) ds.

(3.10)

0

where K is some nonnegative constant. Then
u (t) â‰¤ u0 eKt .

(3.11)

âˆ«t
Proof: Let w (t) = 0 u (s) ds. Then using the fundamental theorem of calculus, (3.10)
w (t) satisï¬es the following.
u (t) âˆ’ Kw (t) = wâ€² (t) âˆ’ Kw (t) â‰¤ u0 , w (0) = 0.

(3.12)

Multiply both sides of this inequality by eâˆ’Kt and using the product rule and the chain
rule,
)
d ( âˆ’Kt
eâˆ’Kt (wâ€² (t) âˆ’ Kw (t)) =
e
w (t) â‰¤ u0 eâˆ’Kt .
dt
Integrating this from 0 to t,
( âˆ’tK
)
âˆ« t
e
âˆ’1
eâˆ’Kt w (t) â‰¤ u0
eâˆ’Ks ds = u0 âˆ’
.
K
0
Now multiply through by eKt to obtain
( âˆ’tK
)
u0
e
âˆ’ 1 Kt
u0
w (t) â‰¤ u0 âˆ’
e = âˆ’ + etK .
K
K
K
Therefore, (3.12) implies
( u
)
u0
0
u (t) â‰¤ u0 + K âˆ’ + etK = u0 eKt .
K
K

With Gronwallâ€™s inequality, here is a theorem on uniqueness of solutions to the initial
value problem,
xâ€² = Ax + f (t) , x (a) = xa ,
(3.13)
in which A is an n Ã— n matrix and f is a continuous function having values in Cn .
Theorem C.4.4 Suppose x and y satisfy (3.13). Then x (t) = y (t) for all t.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

C.4. FIRST ORDER LINEAR SYSTEMS

423

Proof: Let z (t) = x (t + a) âˆ’ y (t + a). Then for t â‰¥ 0,
zâ€² = Az, z (0) = 0.

(3.14)

Note that for K = max {|aij |} , where A = (aij ) ,
|(Az, z)| =

âˆ‘

aij zj zi â‰¤ K

ij

âˆ‘

|zi | |zj | â‰¤ K

ij
2

(For x and y real numbers, xy â‰¤ x2 +
2
Similarly, |(z,Az)| â‰¤ nK |z| .Thus,

âˆ‘
ij

2

y
2

(

2

|zj |
|zi |
+
2
2

2

)
2

= nK |z| .
2

because this is equivalent to saying (x âˆ’ y) â‰¥ 0.)
2

|(z,Az)| , |(Az, z)| â‰¤ nK |z| .

(3.15)

Now multiplying (3.14) by z and observing that
d ( 2)
|z| = (zâ€² , z) + (z, zâ€² ) = (Az, z) + (z,Az) ,
dt
it follows from (3.15) and the observation that z (0) = 0,
âˆ« t
2
2
|z (t)| â‰¤
2nK |z (s)| ds
0
2

and so by Gronwallâ€™s inequality, |z (t)| = 0 for all t â‰¥ 0. Thus,
x (t) = y (t)
for all t â‰¥ a.
Now let w (t) = x (a âˆ’ t) âˆ’ y (a âˆ’ t) for t â‰¥ 0. Then wâ€² (t) = (âˆ’A) w (t) and you can
repeat the argument which was just given to conclude that x (t) = y (t) for all t â‰¤ a. 
Deï¬nition C.4.5 Let A be an n Ã— n matrix. We say Î¦ (t) is a fundamental matrix for A
if
Î¦â€² (t) = AÎ¦ (t) , Î¦ (0) = I,
(3.16)
âˆ’1

and Î¦ (t)

exists for all t âˆˆ R.

Why should anyone care about a fundamental matrix? The reason is that such a matrix
valued function makes possible a convenient description of the solution of the initial value
problem,
xâ€² = Ax + f (t) , x (0) = x0 ,
(3.17)
on the interval, [0, T ] . First consider the special case where n = 1. This is the ï¬rst order
linear diï¬€erential equation,
râ€² = Î»r + g, r (0) = r0 ,
(3.18)
where g is a continuous scalar valued function. First consider the case where g = 0.
Lemma C.4.6 There exists a unique solution to the initial value problem,
râ€² = Î»r, r (0) = 1,

(3.19)

and the solution for Î» = a + ib is given by
r (t) = eat (cos bt + i sin bt) .

(3.20)

This solution to the initial value problem is denoted as eÎ»t . (If Î» is real, eÎ»t as deï¬ned here
reduces to the usual exponential function so there is no contradiction between this and earlier
notation seen in Calculus.)

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

424

APPLICATIONS TO DIFFERENTIAL EQUATIONS

Proof: From the uniqueness theorem presented above, Theorem C.4.4, applied to the
case where n = 1, there can be no more than one solution to the initial value problem,
(3.19). Therefore, it only remains to verify (3.20) is a solution to (3.19). However, this is
an easy calculus exercise. 
Note the diï¬€erential equation in (3.19) says
d ( Î»t )
e
= Î»eÎ»t .
dt

(3.21)

With this lemma, it becomes possible to easily solve the case in which g Ì¸= 0.
Theorem C.4.7 There exists a unique solution to (3.18) and this solution is given by the
formula,
âˆ« t
r (t) = eÎ»t r0 + eÎ»t
eâˆ’Î»s g (s) ds.
(3.22)
0

Proof: By the uniqueness theorem, Theorem C.4.4, there is no moreâˆ« than one solution.
0
It only remains to verify that (3.22) is a solution. But r (0) = eÎ»0 r0 + 0 eâˆ’Î»s g (s) ds = r0
and so the initial condition is satisï¬ed. Next diï¬€erentiate this expression to verify the
diï¬€erential equation is also satisï¬ed. Using (3.21), the product rule and the fundamental
theorem of calculus,
âˆ« t
râ€² (t) = Î»eÎ»t r0 + Î»eÎ»t
eâˆ’Î»s g (s) ds + eÎ»t eâˆ’Î»t g (t) = Î»r (t) + g (t) . 
0

Now consider the question of ï¬nding a fundamental matrix for A. When this is done, it
will be easy to give a formula for the general solution to (3.17) known as the variation of
constants formula, arguably the most important result in diï¬€erential equations.
The next theorem gives a formula for the fundamental matrix (3.16). It is known as
Putzerâ€™s method [1],[21].
Theorem C.4.8 Let A be an n Ã— n matrix whose eigenvalues are {Î»1 , Â· Â· Â· , Î»n } . Deï¬ne
Pk (A) â‰¡

k
âˆ

(A âˆ’ Î»m I) , P0 (A) â‰¡ I,

m=1

and let the scalar valued functions, rk (t) be deï¬ned as
value problem
ï£¶ ï£«
ï£« â€²
ï£¶
r0 (t)
0
ï£¬ r1â€² (t) ï£· ï£¬ Î»1 r1 (t) + r0 (t) ï£·
ï£¬ â€²
ï£· ï£¬
ï£·
ï£¬ r2 (t) ï£· ï£¬ Î»2 r2 (t) + r1 (t) ï£·
ï£¬
ï£·=ï£¬
ï£·,
ï£¬ .. ï£· ï£¬
ï£·
..
ï£­ . ï£¸ ï£­
ï£¸
.
rnâ€² (t)

Î»n rn (t) + rnâˆ’1 (t)

the solutions to the following initial
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­

r0 (0)
r1 (0)
r2 (0)
..
.
rn (0)

ï£¶

ï£«

ï£· ï£¬
ï£· ï£¬
ï£· ï£¬
ï£·=ï£¬
ï£· ï£¬
ï£¸ ï£­

0
1
0
..
.

ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

0

Note the system amounts to a list of single ï¬rst order linear diï¬€erential equations. Now
deï¬ne
nâˆ’1
âˆ‘
Î¦ (t) â‰¡
rk+1 (t) Pk (A) .
k=0

Then

Î¦â€² (t) = AÎ¦ (t) , Î¦ (0) = I.

Furthermore, if Î¦ (t) is a solution to (3.23) for all t, then it follows Î¦ (t)
and Î¦ (t) is the unique fundamental matrix for A.

Saylor URL: http://www.saylor.org/courses/ma212/

(3.23)
âˆ’1

exists for all t

The Saylor Foundation

C.4. FIRST ORDER LINEAR SYSTEMS

425

Proof: The ï¬rst part of this follows from a computation. First note that by the Cayley
Hamilton theorem, Pn (A) = 0. Now for the computation:
Î¦â€² (t) =

nâˆ’1
âˆ‘

â€²
rk+1
(t) Pk (A) =

k=0
nâˆ’1
âˆ‘

nâˆ’1
âˆ‘

Î»k+1 rk+1 (t) Pk (A) +

k=0

nâˆ’1
âˆ‘

rk (t) Pk (A) =

k=0
nâˆ’1
âˆ‘

nâˆ’1
âˆ‘

nâˆ’1
âˆ‘

(Î»k+1 I âˆ’ A) rk+1 (t) Pk (A) +

k=0

rk (t) Pk (A) +

k=0

=âˆ’

(Î»k+1 rk+1 (t) + rk (t)) Pk (A) =

k=0

nâˆ’1
âˆ‘

Ark+1 (t) Pk (A)

k=0

rk+1 (t) Pk+1 (A) +

k=0

nâˆ’1
âˆ‘

rk (t) Pk (A) + A

k=0

nâˆ’1
âˆ‘

rk+1 (t) Pk (A) .

(3.24)

k=0

Now using r0 (t) = 0, the ï¬rst term equals
âˆ’

n
âˆ‘

rk (t) Pk (A) = âˆ’

k=1

nâˆ’1
âˆ‘

rk (t) Pk (A) = âˆ’

k=1

nâˆ’1
âˆ‘

rk (t) Pk (A)

k=0

and so (3.24) reduces to
A

nâˆ’1
âˆ‘

rk+1 (t) Pk (A) = AÎ¦ (t) .

k=0

This shows Î¦â€² (t) = AÎ¦ (t) . That Î¦ (0) = 0 follows from
Î¦ (0) =

nâˆ’1
âˆ‘

rk+1 (0) Pk (A) = r1 (0) P0 = I.

k=0
âˆ’1

It remains to verify that if (3.23) holds, then Î¦ (t) exists for all t. To do so, consider
v Ì¸= 0 and suppose for some t0 , Î¦ (t0 ) v = 0. Let x (t) â‰¡ Î¦ (t0 + t) v. Then
xâ€² (t) = AÎ¦ (t0 + t) v = Ax (t) , x (0) = Î¦ (t0 ) v = 0.
But also z (t) â‰¡ 0 also satisï¬es
zâ€² (t) = Az (t) , z (0) = 0,
and so by the theorem on uniqueness, it must be the case that z (t) = x (t) for all t, showing
that Î¦ (t + t0 ) v = 0 for all t, and in particular for t = âˆ’t0 . Therefore,
Î¦ (âˆ’t0 + t0 ) v = Iv = 0
and so v = 0, a contradiction. It follows that Î¦ (t) must be one to one for all t and so,
âˆ’1
Î¦ (t) exists for all t.
It only remains to verify the solution to (3.23) is unique. Suppose Î¨ is another fundamental matrix solving (3.23). Then letting v be an arbitrary vector,
z (t) â‰¡ Î¦ (t) v, y (t) â‰¡ Î¨ (t) v
both solve the initial value problem,
xâ€² = Ax, x (0) = v,

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

426

APPLICATIONS TO DIFFERENTIAL EQUATIONS

and so by the uniqueness theorem, z (t) = y (t) for all t showing that Î¦ (t) v = Î¨ (t) v for
all t. Since v is arbitrary, this shows that Î¦ (t) = Î¨ (t) for every t. 
It is useful to consider the diï¬€erential equations for the rk for k â‰¥ 1. As noted above,
r0 (t) = 0 and r1 (t) = eÎ»1 t .
â€²
rk+1
= Î»k+1 rk+1 + rk , rk+1 (0) = 0.

Thus

âˆ«

t

eÎ»k+1 (tâˆ’s) rk (s) ds.

rk+1 (t) =
0

Therefore,

âˆ«

t

eÎ»2 (tâˆ’s) eÎ»1 s ds =

r2 (t) =
0

eÎ»1 t âˆ’ eÎ»2 t
âˆ’Î»2 + Î»1

assuming Î»1 Ì¸= Î»2 .
Sometimes people deï¬ne a fundamental matrix to be a matrix Î¦ (t) such that Î¦â€² (t) =
AÎ¦ (t) and det (Î¦ (t)) Ì¸= 0 for all t. Thus this avoids the initial condition, Î¦ (0) = I. The
next proposition has to do with this situation.
Proposition C.4.9 Suppose A is an n Ã— n matrix and suppose Î¦ (t) is an n Ã— n matrix for
each t âˆˆ R with the property that
Î¦â€² (t) = AÎ¦ (t) .
(3.25)
Then either Î¦ (t)

âˆ’1

âˆ’1

exists for all t âˆˆ R or Î¦ (t)

fails to exist for all t âˆˆ R.

âˆ’1

Proof: Suppose Î¦ (0) exists and (3.25) holds. Let Î¨ (t) â‰¡ Î¦ (t) Î¦ (0)
I and
âˆ’1
âˆ’1
Î¨â€² (t) = Î¦â€² (t) Î¦ (0) = AÎ¦ (t) Î¦ (0) = AÎ¨ (t)
âˆ’1

âˆ’1

. Then Î¨ (0) =

âˆ’1

so by Theorem C.4.8, Î¨ (t) exists for all t. Therefore, Î¦ (t) also exists for all t.
âˆ’1
âˆ’1
Next suppose Î¦ (0) does not exist. I need to show Î¦ (t) does not exist for any t.
âˆ’1
âˆ’1
Suppose then that Î¦ (t0 ) does exist. Then letÎ¨ (t) â‰¡ Î¦ (t0 + t) Î¦ (t0 ) . Then Î¨ (0) =
âˆ’1
I and Î¨â€² = AÎ¨ so by Theorem C.4.8 it follows Î¨ (t)
exists for all t and so for all
âˆ’1
âˆ’1
t, Î¦ (t + t0 ) must also exist, even for t = âˆ’t0 which implies Î¦ (0) exists after all. 
The conclusion of this proposition is usually referred to as the Wronskian alternative and
another way to say it is that if (3.25) holds, then either det (Î¦ (t)) = 0 for all t or det (Î¦ (t))
is never equal to 0. The Wronskian is the usual name of the function, t â†’ det (Î¦ (t)).
The following theorem gives the variation of constants formula,.
Theorem C.4.10 Let f be continuous on [0, T ] and let A be an n Ã— n matrix and x0 a
vector in Cn . Then there exists a unique solution to (3.17), x, given by the variation of
constants formula,
âˆ«
t

x (t) = Î¦ (t) x0 + Î¦ (t)

âˆ’1

Î¦ (s)

f (s) ds

(3.26)

0
âˆ’1

for Î¦ (t) the fundamental matrix for A. Also, Î¦ (t) = Î¦ (âˆ’t) and Î¦ (t + s) = Î¦ (t) Î¦ (s)
for all t, s and the above variation of constants formula can also be written as
âˆ« t
x (t) = Î¦ (t) x0 +
Î¦ (t âˆ’ s) f (s) ds
(3.27)
0
âˆ« t
Î¦ (s) f (t âˆ’ s) ds
(3.28)
= Î¦ (t) x0 +
0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

C.4. FIRST ORDER LINEAR SYSTEMS

427

Proof: From the uniqueness theorem there is at most one solution to (3.17). Therefore,
if (3.26) solves (3.17), the theorem is proved. The veriï¬cation that the given formula works
is identical with the veriï¬cation that the scalar formula given in Theorem C.4.7 solves the
âˆ’1
initial value problem given there. Î¦ (s) is continuous because of the formula for the inverse
of a matrix in terms of the transpose of the cofactor matrix. Therefore, the integrand in
(3.26) is continuous and the fundamental theorem of calculus applies. To verify the formula
for the inverse, ï¬x s and consider x (t) = Î¦ (s + t) v, and y (t) = Î¦ (t) Î¦ (s) v. Then
xâ€² (t) = AÎ¦ (t + s) v = Ax (t) , x (0) = Î¦ (s) v
yâ€² (t) = AÎ¦ (t) Î¦ (s) v = Ay (t) , y (0) = Î¦ (s) v.
By the uniqueness theorem, x (t) = y (t) for all t. Since s and v are arbitrary, this shows
âˆ’1
Î¦ (t + s) = Î¦ (t) Î¦ (s) for all t, s. Letting s = âˆ’t and using Î¦ (0) = I veriï¬es Î¦ (t)
=
Î¦ (âˆ’t) .
âˆ’1
Next, note that this also implies Î¦ (t âˆ’ s) Î¦ (s) = Î¦ (t) and so Î¦ (t âˆ’ s) = Î¦ (t) Î¦ (s) .
Therefore, this yields (3.27) and then (3.28)follows from changing the variable. 
âˆ’1
If Î¦â€² = AÎ¦ and Î¦ (t) exists for all t, you should verify that the solution to the initial
value problem
xâ€² = Ax + f , x (t0 ) = x0
is given by

âˆ«

t

x (t) = Î¦ (t âˆ’ t0 ) x0 +

Î¦ (t âˆ’ s) f (s) ds.
t0

Theorem C.4.10 is general enough to include all constant coeï¬ƒcient linear diï¬€erential
equations or any order. Thus it includes as a special case the main topics of an entire
elementary diï¬€erential equations class. This is illustrated in the following example. One
can reduce an arbitrary linear diï¬€erential equation to a ï¬rst order system and then apply the
above theory to solve the problem. The next example is a diï¬€erential equation of damped
vibration.
Example C.4.11 The diï¬€erential equation is y â€²â€² + 2y â€² + 2y = cos t and initial conditions,
y (0) = 1 and y â€² (0) = 0.
To solve this equation, let x1 = y and x2 = xâ€²1 = y â€² . Then, writing this in terms of these
new variables, yields the following system.
xâ€²2 + 2x2 + 2x1 = cos t
xâ€²1 = x2
This system can be written in the above form as
(
)â€² (
) (
) (
)(
) (
)
x1
x2
0
0
1
x1
0
=
+
=
+
.
x2
âˆ’2x2 âˆ’ 2x1
cos t
âˆ’2 âˆ’2
x2
cos t
and the initial condition is of the form
(
)
(
)
x1
1
(0) =
x2
0
Now P0 (A) â‰¡ I. The eigenvalues are âˆ’1 + i, âˆ’1 âˆ’ i and so
((
)
(
)) (
0
1
1 0
1âˆ’i
P1 (A) =
âˆ’ (âˆ’1 + i)
=
âˆ’2 âˆ’2
0 1
âˆ’2

Saylor URL: http://www.saylor.org/courses/ma212/

1
âˆ’1 âˆ’ i

)
.

The Saylor Foundation

428

APPLICATIONS TO DIFFERENTIAL EQUATIONS

Recall r0 (t) â‰¡ 0 and r1 (t) = e(âˆ’1+i)t . Then
r2â€² = (âˆ’1 âˆ’ i) r2 + e(âˆ’1+i)t , r2 (0) = 0
and so

e(âˆ’1+i)t âˆ’ e(âˆ’1âˆ’i)t
= eâˆ’t sin (t)
2i
Putzerâ€™s method yields the fundamental matrix as
(
)
(
)
1 0
1âˆ’i
1
(âˆ’1+i)t
âˆ’t
Î¦ (t) = e
+ e sin (t)
0 1
âˆ’2 âˆ’1 âˆ’ i
( âˆ’t
)
âˆ’t
e (cos (t) + sin (t))
e sin t
=
âˆ’2eâˆ’t sin t
eâˆ’t (cos (t) âˆ’ sin (t))
r2 (t) =

From variation of constants formula the desired solution is
(
)
( âˆ’t
)(
)
x1
e (cos (t) + sin (t))
eâˆ’t sin t
1
(t) =
x2
âˆ’2eâˆ’t sin t
eâˆ’t (cos (t) âˆ’ sin (t))
0
âˆ« t(

)(
)
eâˆ’s (cos (s) + sin (s))
eâˆ’s sin s
0
+
âˆ’2eâˆ’s sin s
eâˆ’s (cos (s) âˆ’ sin (s))
cos (t âˆ’ s)
0
( âˆ’t
) âˆ« t(
)
e (cos (t) + sin (t))
eâˆ’s sin (s) cos (t âˆ’ s)
=
+
ds
âˆ’2eâˆ’t sin t
eâˆ’s (cos s âˆ’ sin s) cos (t âˆ’ s)
0
( âˆ’t
) ( 1
)
e (cos (t) + sin (t))
âˆ’ 5 (cos t) eâˆ’t âˆ’ 35 eâˆ’t sin t + 15 cos t + 25 sin t
=
+
âˆ’2eâˆ’t sin t
âˆ’ 25 (cos t) eâˆ’t + 45 eâˆ’t sin t + 25 cos t âˆ’ 15 sin t
)
( 4
âˆ’t
+ 52 eâˆ’t sin t + 15 cos t + 52 sin t
5 (cos t) e
=
âˆ’ 65 eâˆ’t sin t âˆ’ 52 (cos t) eâˆ’t + 25 cos t âˆ’ 15 sin t
Thus y (t) = x1 (t) =

C.5

4
5

(cos t) eâˆ’t + 25 eâˆ’t sin t +

1
5

cos t +

2
5

sin t.

Geometric Theory Of Autonomous Systems

Here a suï¬ƒcient condition is given for stability of a ï¬rst order system. First of all, here is
a fundamental estimate for the entries of a fundamental matrix.
Lemma C.5.1 Let the functions, rk be given in the statement of Theorem C.4.8 and suppose that A is an n Ã— n matrix whose eigenvalues are {Î»1 , Â· Â· Â· , Î»n } . Suppose that these
eigenvalues are ordered such that
Re (Î»1 ) â‰¤ Re (Î»2 ) â‰¤ Â· Â· Â· â‰¤ Re (Î»n ) < 0.
Then if 0 > âˆ’Î´ > Re (Î»n ) is given, there exists a constant, C such that for each k =
0, 1, Â· Â· Â· , n,
|rk (t)| â‰¤ Ceâˆ’Î´t
(3.29)
for all t > 0.
Proof: This is obvious for r0 (t) because it is identically equal to 0. From the deï¬nition
of the rk , r1â€² = Î»1 r1 , r1 (0) = 1 and so r1 (t) = eÎ»1 t which implies
|r1 (t)| â‰¤ eRe(Î»1 )t .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

C.5. GEOMETRIC THEORY OF AUTONOMOUS SYSTEMS

429

Suppose for some m â‰¥ 1 there exists a constant, Cm such that
|rk (t)| â‰¤ Cm tm eRe(Î»m )t
for all k â‰¤ m for all t > 0. Then
â€²
rm+1
(t) = Î»m+1 rm+1 (t) + rm (t) , rm+1 (0) = 0

and so

âˆ«

t

rm+1 (t) = eÎ»m+1 t

eâˆ’Î»m+1 s rm (s) ds.

0

Then by the induction hypothesis,
âˆ«
|rm+1 (t)| â‰¤

t

eRe(Î»m+1 )t

â‰¤ e

âˆ«

0

âˆ«

0

t

Re(Î»m+1 )t

eâˆ’Î»m+1 s Cm sm eRe(Î»m )s ds
sm Cm eâˆ’ Re(Î»m+1 )s eRe(Î»m )s ds

t

â‰¤ eRe(Î»m+1 )t

sm Cm ds =
0

Cm m+1 Re(Î»m+1 )t
t
e
m+1

It follows by induction there exists a constant, C such that for all k â‰¤ n,
|rk (t)| â‰¤ Ctn eRe(Î»n )t
and this obviously implies the conclusion of the lemma.
The proof of the above lemma yields the following corollary.
Corollary C.5.2 Let the functions, rk be given in the statement of Theorem C.4.8 and
suppose that A is an n Ã— n matrix whose eigenvalues are {Î»1 , Â· Â· Â· , Î»n } . Suppose that these
eigenvalues are ordered such that
Re (Î»1 ) â‰¤ Re (Î»2 ) â‰¤ Â· Â· Â· â‰¤ Re (Î»n ) .
Then there exists a constant C such that for all k â‰¤ m
|rk (t)| â‰¤ Ctm eRe(Î»m )t .
With the lemma, the following sloppy estimate is available for a fundamental matrix.
Theorem C.5.3 Let A be an n Ã— n matrix and let Î¦ (t) be the fundamental matrix for A.
That is,
Î¦â€² (t) = AÎ¦ (t) , Î¦ (0) = I.
Suppose also the eigenvalues of A are {Î»1 , Â· Â· Â· , Î»n } where these eigenvalues are ordered such
that
Re (Î»1 ) â‰¤ Re (Î»2 ) â‰¤ Â· Â· Â· â‰¤ Re (Î»n ) < 0.
Then if 0 > âˆ’Î´ > Re (Î»n ) , is given, there exists a constant, C such that Î¦ (t)ij â‰¤ Ceâˆ’Î´t
for all t > 0. Also
|Î¦ (t) x| â‰¤ Cn3/2 eâˆ’Î´t |x| .
(3.30)

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

430

APPLICATIONS TO DIFFERENTIAL EQUATIONS

Proof: Let
M â‰¡ max

{

}
Pk (A)ij for all i, j, k .

Then from Putzerâ€™s formula for Î¦ (t) and Lemma C.5.1, there exists a constant, C such that
Î¦ (t)ij â‰¤

nâˆ’1
âˆ‘

Ceâˆ’Î´t M.

k=0

Let the new C be given by nCM. 
Next,
ï£«
ï£¶2
ï£«
ï£¶2
n
n
n
n
âˆ‘
âˆ‘
âˆ‘
âˆ‘
2
ï£­
ï£­
|Î¦ (t) x| â‰¡
Î¦ij (t) xj ï£¸ â‰¤
|Î¦ij (t)| |xj |ï£¸
i=1

â‰¤

n
âˆ‘
i=1

ï£«
ï£­

n
âˆ‘

j=1

i=1

ï£¶2
Ceâˆ’Î´t |x|ï£¸ = C 2 eâˆ’2Î´t

n
âˆ‘

j=1

(n |x|) = C 2 eâˆ’2Î´t n3 |x|
2

2

i=1

j=1

This proves (3.30) and completes the proof.
Deï¬nition C.5.4 Let f : U â†’ Rn where U is an open subset of Rn such that a âˆˆ U and
f (a) = 0. A point, a where f (a) = 0 is called an equilibrium point. Then a is asymptotically
stable if for any Îµ > 0 there exists r > 0 such that whenever |x0 âˆ’ a| < r and x (t) the
solution to the initial value problem,
xâ€² = f (x) , x (0) = x0 ,
it follows
lim x (t) = a, |x (t) âˆ’ a| < Îµ

tâ†’âˆ

A diï¬€erential equation of the form xâ€² = f (x) is called autonomous as opposed to a nonautonomous equation of the form xâ€² = f (t, x) . The equilibrium point a is stable if for every
Îµ > 0 there exists Î´ > 0 such that if |x0 âˆ’ a| < Î´, then if x is the solution of
xâ€² = f (x) , x (0) = x0 ,

(3.31)

then |x (t) âˆ’ a| < Îµ for all t > 0.
Obviously asymptotic stability implies stability.
An ordinary diï¬€erential equation is called almost linear if it is of the form
xâ€² = Ax + g (x)
where A is an n Ã— n matrix and
lim

xâ†’0

g (x)
= 0.
|x|

Now the stability of an equilibrium point of an autonomous system, xâ€² = f (x) can
always be reduced to the consideration of the stability of 0 for an almost linear system.
Here is why. If you are considering the equilibrium point, a for xâ€² = f (x) , you could
deï¬ne a new variable, y by a + y = x. Then asymptotic stability would involve |y (t)| < Îµ
and limtâ†’âˆ y (t) = 0 while stability would only require |y (t)| < Îµ. Then since a is an
equilibrium point, y solves the following initial value problem.
yâ€² = f (a + y) âˆ’ f (a) , y (0) = y0 ,

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

C.5. GEOMETRIC THEORY OF AUTONOMOUS SYSTEMS

431

where y0 = x0 âˆ’ a.
Let A = Df (a) . Then from the deï¬nition of the derivative of a function,
yâ€² = Ay + g (y) , y (0) = y0
where
lim

yâ†’0

(3.32)

g (y)
= 0.
|y|

Thus there is never any loss of generality in considering only the equilibrium point 0 for an
almost linear system.1 Therefore, from now on I will only consider the case of almost linear
systems and the equilibrium point 0.
Theorem C.5.5 Consider the almost linear system of equations,
xâ€² = Ax + g (x)
where
lim

xâ†’0

(3.33)

g (x)
=0
|x|

and g is a C 1 function. Suppose that for all Î» an eigenvalue of A, Re Î» < 0. Then 0 is
asymptotically stable.
Proof: By Theorem C.5.3 there exist constants Î´ > 0 and K such that for Î¦ (t) the
fundamental matrix for A,
|Î¦ (t) x| â‰¤ Keâˆ’Î´t |x| .
Let Îµ > 0 be given and let r be small enough that Kr < Îµ and for |x| < (K + 1) r, |g (x)| <
Î· |x| where Î· is so small that KÎ· < Î´, and let |y0 | < r. Then by the variation of constants
formula, the solution to (3.33), at least for small t satisï¬es
âˆ«

t

Î¦ (t âˆ’ s) g (y (s)) ds.

y (t) = Î¦ (t) y0 +
0

The following estimate holds.
âˆ« t
âˆ« t
|y (t)| â‰¤ Keâˆ’Î´t |y0 | +
Keâˆ’Î´(tâˆ’s) Î· |y (s)| ds < Keâˆ’Î´t r +
Keâˆ’Î´(tâˆ’s) Î· |y (s)| ds.
0

0

Therefore,

âˆ«
e |y (t)| < Kr +

t

KÎ·eÎ´s |y (s)| ds.

Î´t

0

By Gronwallâ€™s inequality,
eÎ´t |y (t)| < KreKÎ·t
and so
|y (t)| < Kre(KÎ·âˆ’Î´)t < Îµe(KÎ·âˆ’Î´)t
Therefore, |y (t)| < Kr < Îµ for all t and so from Corollary C.3.4, the solution to (3.33)
exists for all t â‰¥ 0 and since KÎ· âˆ’ Î´ < 0,
lim |y (t)| = 0.

tâ†’âˆ

1 This is no longer true when you study partial diï¬€erential equations as ordinary diï¬€erential equations in
inï¬nite dimensional spaces.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

432

C.6

APPLICATIONS TO DIFFERENTIAL EQUATIONS

General Geometric Theory

Here I will consider the case where the matrix A has both positive and negative eigenvalues.
First here is a useful lemma.
Lemma C.6.1 Suppose A is an n Ã— n matrix and there exists Î´ > 0 such that
0 < Î´ < Re (Î»1 ) â‰¤ Â· Â· Â· â‰¤ Re (Î»n )
where {Î»1 , Â· Â· Â· , Î»n } are the eigenvalues of A, with possibly some repeated. Then there exists
a constant, C such that for all t < 0,
|Î¦ (t) x| â‰¤ CeÎ´t |x|
Proof: I want an estimate on the solutions to the system
Î¦â€² (t) = AÎ¦ (t) , Î¦ (0) = I.
for t < 0. Let s = âˆ’t and let Î¨ (s) = Î¦ (t) . Then writing this in terms of Î¨,
Î¨â€² (s) = âˆ’AÎ¨ (s) , Î¨ (0) = I.
Now the eigenvalues of âˆ’A have real parts less than âˆ’Î´ because these eigenvalues are
obtained from the eigenvalues of A by multiplying by âˆ’1. Then by Theorem C.5.3 there
exists a constant, C such that for any x,
|Î¨ (s) x| â‰¤ Ceâˆ’Î´s |x| .
Therefore, from the deï¬nition of Î¨,
|Î¦ (t) x| â‰¤ CeÎ´t |x| .
Here is another essential lemma which is found in Coddington and Levinson [6]
Lemma C.6.2 Let pj (t) be polynomials with complex coeï¬ƒcients and let
f (t) =

m
âˆ‘

pj (t) eÎ»j t

j=1

where m â‰¥ 1, Î»j Ì¸= Î»k for j Ì¸= k, and none of the pj (t) vanish identically. Let
Ïƒ = max (Re (Î»1 ) , Â· Â· Â· , Re (Î»m )) .
Then there exists a positive number, r and arbitrarily large positive values of t such that
eâˆ’Ïƒt |f (t)| > r.
In particular, |f (t)| is unbounded.
Proof: Suppose the largest exponent of any of the pj is M and let Î»j = aj + ibj . First
assume each aj = 0. This is convenient because Ïƒ = 0 in this case and the largest of the
Re (Î»j ) occurs in every Î»j .
Then arranging the above sum as a sum of decreasing powers of t,
f (t) = tM fM (t) + Â· Â· Â· + tf1 (t) + f0 (t) .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

C.6. GENERAL GEOMETRIC THEORY

433

Then
tâˆ’M f (t) = fM (t) + O
where the last term means that tO

(1)

( )
1
t

is bounded. Then

t

fM (t) =

m
âˆ‘

cj eibj t

j=1

It canâ€™t be the case that all the cj are equal to 0 because then M would not be the highest
power exponent. Suppose ck Ì¸= 0. Then
1
lim
T â†’âˆ T

âˆ«

T

t

âˆ’M

f (t) e

âˆ’ibk t

0

m
âˆ‘

1
dt =
cj
T
j=1

âˆ«

T

ei(bj âˆ’bk )t dt = ck Ì¸= 0.

0

Letting r = |ck /2| , it follows tâˆ’M f (t) eâˆ’ibk t > r for arbitrarily large values of t. Thus it
is also true that |f (t)| > r for arbitrarily large values of t.
Next consider the general case in which Ïƒ is given above. Thus
âˆ‘
eâˆ’Ïƒt f (t) =
pj (t) ebj t + g (t)
j:aj =Ïƒ

âˆ‘
where limtâ†’âˆ g (t) = 0, g (t) being of the form s ps (t) e(as âˆ’Ïƒ+ibs )t where as âˆ’ Ïƒ < 0. Then
this reduces to the case above in which Ïƒ = 0. Therefore, there exists r > 0 such that
eâˆ’Ïƒt f (t) > r
for arbitrarily large values of t. 
Next here is a Banach space which will be useful.
Lemma C.6.3 For Î³ > 0, let
{
}
EÎ³ = x âˆˆ BC ([0, âˆ), Fn ) : t â†’ eÎ³t x (t) is also in BC ([0, âˆ), Fn )
and let the norm be given by
||x||Î³ â‰¡ sup

{

eÎ³t x (t) : t âˆˆ [0, âˆ)

}

Then EÎ³ is a Banach space.
Proof: Let {xk } be a Cauchy sequence in EÎ³ . Then since BC ([0, âˆ), Fn ) is a Banach
space, there exists y âˆˆ BC ([0, âˆ), Fn ) such that eÎ³t xk (t) converges uniformly on [0, âˆ) to
y (t). Therefore eâˆ’Î³t eÎ³t xk (t) = xk (t) converges uniformly to eâˆ’Î³t y (t) on [0, âˆ). Deï¬ne
x (t) â‰¡ eâˆ’Î³t y (t) . Then y (t) = eÎ³t x (t) and by deï¬nition,
||xk âˆ’ x||Î³ â†’ 0.


Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

434

C.7

APPLICATIONS TO DIFFERENTIAL EQUATIONS

The Stable Manifold
(

Here assume
A=

Aâˆ’
0

0
A+

)
(3.34)

where Aâˆ’ and A+ are square matrices of size k Ã— k and (n âˆ’ k) Ã— (n âˆ’ k) respectively. Also
assume Aâˆ’ has eigenvalues whose real parts are all less than âˆ’Î± while A+ has eigenvalues
whose real parts are all larger than Î±. Assume also that each of Aâˆ’ and A+ is upper
triangular.
Also, I will use the following convention. For v âˆˆ Fn ,
(
)
vâˆ’
v=
v+
where vâˆ’ consists of the ï¬rst k entries of v.
Then from Theorem C.5.3 and Lemma C.6.1 the following lemma is obtained.
Lemma C.7.1 Let A be of the form given in (3.34) as explained above and let Î¦+ (t) and
Î¦âˆ’ (t) be the fundamental matrices corresponding to A+ and Aâˆ’ respectively. Then there
exist positive constants, Î± and Î³ such that
|Î¦+ (t) y| â‰¤ CeÎ±t for all t < 0

(3.35)

|Î¦âˆ’ (t) y| â‰¤ Ceâˆ’(Î±+Î³)t for all t > 0.

(3.36)

Also for any nonzero x âˆˆ Cnâˆ’k ,
|Î¦+ (t) x| is unbounded.

(3.37)

Proof: The ï¬rst two claims have been established already. It suï¬ƒces to pick Î± and Î³
such that âˆ’ (Î± + Î³) is larger than all eigenvalues of Aâˆ’ and Î± is smaller than all eigenvalues
of A+ . It remains to verify (3.37). From the Putzer formula for Î¦+ (t) ,
Î¦+ (t) x =

nâˆ’1
âˆ‘

rk+1 (t) Pk (A) x

k=0

where P0 (A) â‰¡ I. Now each rk is a polynomial (possibly a constant) times an exponential.
This follows easily from the deï¬nition of the rk as solutions of the diï¬€erential equations
â€²
rk+1
= Î»k+1 rk+1 + rk .

Now by assumption the eigenvalues have positive real parts so
Ïƒ â‰¡ max (Re (Î»1 ) , Â· Â· Â· , Re (Î»nâˆ’k )) > 0.
It can also be assumed
Re (Î»1 ) â‰¥ Â· Â· Â· â‰¥ Re (Î»nâˆ’k )
By Lemma C.6.2 it follows |Î¦+ (t) x| is unbounded. This follows because
Î¦+ (t) x = r1 (t) x +

nâˆ’1
âˆ‘

rk+1 (t) yk , r1 (t) = eÎ»1 t .

k=1

Since x Ì¸= 0, it has a nonzero entry, say xm Ì¸= 0. Consider the mth entry of the vector
Î¦+ (t) x. By this Lemma the mth entry is unbounded and this is all it takes for x (t) to be
unbounded. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

C.7. THE STABLE MANIFOLD

435

Lemma C.7.2 Consider the initial value problem for the almost linear system
xâ€² = Ax + g (x) , x (0) = x0 ,
where g is C 1 and A is of the special form
(
Aâˆ’
A=
0

0
A+

)

in which Aâˆ’ is a k Ã— k matrix which has eigenvalues for which the real parts are all negative
and A+ is a (n âˆ’ k) Ã— (n âˆ’ k) matrix for which the real parts of all the eigenvalues are
positive. Then 0 is not stable. More precisely, there exists a set of points (aâˆ’ , Ïˆ (aâˆ’ )) for
aâˆ’ small such that for x0 on this set,
lim x (t, x0 ) = 0

tâ†’âˆ

and for x0 not on this set, there exists a Î´ > 0 such that |x (t, x0 )| cannot remain less than
Î´ for all positive t.
Proof: Consider the initial value problem for the almost linear equation,
(
)
aâˆ’
â€²
x = Ax + g (x) , x (0) = a =
.
a+
Then by the variation of constants formula, a local solution has the form
(
)(
)
Î¦âˆ’ (t)
0
aâˆ’
x (t, a) =
0
Î¦+ (t)
a+
)
âˆ« t(
Î¦âˆ’ (t âˆ’ s)
0
+
g (x (s, a)) ds
0
Î¦+ (t âˆ’ s)
0

(3.38)

Write x (t) for x (t, a) for short. Let Îµ > 0 be given and suppose Î´ is such that if |x| < Î´,
then |gÂ± (x)| < Îµ |x|. Assume from now on that |a| < Î´. Then suppose |x (t)| < Î´ for all
t > 0. Writing (3.38) diï¬€erently yields
)
(
)(
) ( âˆ«t
Î¦âˆ’ (t)
0
aâˆ’
Î¦ (t âˆ’ s) gâˆ’ (x (s, a)) ds
0 âˆ’
x (t, a) =
+
0
Î¦+ (t)
a+
0
(
)
0
+ âˆ«t
Î¦
(t
âˆ’
s)
g
+ (x (s, a)) ds
0 +
(
=
(
+

Î¦âˆ’ (t)
0
0
Î¦+ (t)
âˆ«âˆ
0

)(

aâˆ’
a+

)

( âˆ«t
+

0

Î¦âˆ’ (t âˆ’ s) gâˆ’ (x (s, a)) ds
0

0âˆ«
âˆ
Î¦+ (t âˆ’ s) g+ (x (s, a)) ds âˆ’ t Î¦+ (t âˆ’ s) g+ (x (s, a)) ds

)
)
.

These improper integrals converge thanks to the assumption that x is bounded and the
estimates (3.35) and (3.36). Continuing the rewriting,
) )
( (
âˆ«t
(
)
xâˆ’ (t)
Î¦âˆ’ (t) aâˆ’ + 0 Î¦âˆ’ (t âˆ’ s) gâˆ’ (x (s, a)) ds
=
(
)
âˆ«âˆ
x+ (t)
Î¦+ (t) a+ + 0 Î¦+ (âˆ’s) g+ (x (s, a)) ds
(
)
0
âˆ«
+
.
âˆ
âˆ’ t Î¦+ (t âˆ’ s) g+ (x (s, a)) ds

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

436

APPLICATIONS TO DIFFERENTIAL EQUATIONS

It follows from Lemma
âˆ« âˆ C.7.1 that if |x (t, a)| is bounded by Î´ as asserted, then it must be
the case that a+ + 0 Î¦+ (âˆ’s) g+ (x (s, a)) ds = 0. Consequently, it must be the case that
(
x (t) = Î¦ (t)

aâˆ’
0

)

(
+

)
âˆ«t
Î¦
(t
âˆ’
s)
g
(x
(s,
a))
ds
âˆ’
âˆ’
0
âˆ«âˆ
âˆ’ t Î¦+ (t âˆ’ s) g+ (x (s, a)) ds

(3.39)

Letting t â†’ 0, this requires that for a solution to the initial value problem to exist and also
satisfy |x (t)| < Î´ for all t > 0 it must be the case that
(
)
aâˆ’
âˆ«
x (0) =
âˆ
âˆ’ 0 Î¦+ (âˆ’s) g+ (x (s, a)) ds
where x (t, a) is the solution of
(

â€²

x = Ax + g (x) , x (0) =

âˆ’

âˆ«âˆ
0

aâˆ’
Î¦+ (âˆ’s) g+ (x (s, a)) ds

)

This is because in (3.39), if x is bounded by Î´ then the reverse steps show x is a solution of
the above diï¬€erential equation and initial condition.
T
It follows if I can show that for all aâˆ’ suï¬ƒciently small and a = (aâˆ’ , 0) , there exists a
solution to (3.39) x (s, a) on (0, âˆ) for which |x (s, a)| < Î´, then I can deï¬ne
âˆ« âˆ
Ïˆ (a) â‰¡ âˆ’
Î¦+ (âˆ’s) g+ (x (s, a)) ds
0
T

and conclude that |x (t, x0 )| < Î´ for all t > 0 if and only if x0 = (aâˆ’ , Ïˆ (aâˆ’ )) for some
suï¬ƒciently small aâˆ’ .
Let C, Î±, Î³ be the constants of Lemma C.7.1. Let Î· be a small positive number such that
CÎ·
1
<
Î±
6
Note that
then

âˆ‚g
âˆ‚xi

(0) = 0. Therefore, by Lemma C.3.1, there exists Î´ > 0 such that if |x| , |y| â‰¤ Î´,
|g (x) âˆ’ g (y)| < Î· |x âˆ’ y|

and in particular,
|gÂ± (x) âˆ’ gÂ± (y)| < Î· |x âˆ’ y|
because each

âˆ‚g
âˆ‚xi

(3.40)

(x) is very small. In particular, this implies
|gâˆ’ (x)| < Î· |x| , |g+ (x)| < Î· |x| .

For x âˆˆ EÎ³ deï¬ned in Lemma C.6.3 and |aâˆ’ | <
(
F x (t) â‰¡

Î´
2C ,

)
âˆ«t
Î¦âˆ’ (t) a
+
Î¦
(t
âˆ’
s)
g
(x
(s))
ds
âˆ’
âˆ’
âˆ’
0
âˆ«âˆ
.
âˆ’ t Î¦+ (t âˆ’ s) g+ (x (s)) ds

I need to ï¬nd a ï¬xed point of F. Letting ||x||Î³ < Î´, and using the estimates of Lemma C.7.1,
âˆ«

e |F x (t)|
Î³t

â‰¤

t

Ceâˆ’(Î±+Î³)(tâˆ’s) Î· |x (s)| ds
e |Î¦âˆ’ (t) aâˆ’ | + e
0
âˆ« âˆ
Î³t
CeÎ±(tâˆ’s) Î· |x (s)| ds
+e
Î³t

Î³t

t

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

C.7. THE STABLE MANIFOLD

437

âˆ« t
Î´ âˆ’(Î±+Î³)t
Î³t
â‰¤ e C
e
+ e ||x||Î³ CÎ·
eâˆ’(Î±+Î³)(tâˆ’s) eâˆ’Î³s ds
2C
0
âˆ« âˆ
Î³t
+e CÎ·
eÎ±(tâˆ’s) eâˆ’Î³s ds ||x||Î³
t
âˆ« t
âˆ« âˆ
Î´
âˆ’Î±(tâˆ’s)
+ Î´CÎ·
<
e
ds + CÎ·Î´
e(Î±+Î³)(tâˆ’s) ds
2
0
t
(
)
Î´CÎ·
1 CÎ·
2Î´
Î´
1
+ Î´CÎ· +
â‰¤Î´
+
< .
<
2
Î± Î±+Î³
2
Î±
3
Î³t

Thus F maps every x âˆˆ EÎ³ having ||x||Î³ < Î´ to F x where ||F x||Î³ â‰¤
Now let x, y âˆˆ EÎ³ where ||x||Î³ , ||y||Î³ < Î´. Then
âˆ«
eÎ³t |F x (t) âˆ’ F y (t)|

â‰¤

t

eÎ³t

|Î¦âˆ’ (t âˆ’ s)| Î·eâˆ’Î³s eÎ³s |x (s) âˆ’ y (s)| ds

âˆ«

0

+e

2Î´
3 .

âˆ

Î³t

|Î¦+ (t âˆ’ s)| eâˆ’Î³s eÎ³s Î· |x (s) âˆ’ y (s)| ds

t

(âˆ«
â‰¤ CÎ· ||x âˆ’ y||Î³
(
â‰¤ CÎ·

1
1
+
Î± Î±+Î³

)

t

) âˆ«
eâˆ’Î±(tâˆ’s) ds +

0

||x âˆ’ y||Î³ <

âˆ

e(Î±+Î³)(tâˆ’s) ds

t

2CÎ·
1
||x âˆ’ y||Î³ < ||x âˆ’ y||Î³ .
Î±
3

Î´
It follows from Lemma 14.6.4, for each aâˆ’ such that |aâˆ’ | < 2C
, there exists a unique solution
to (3.39) in EÎ³ .
As pointed out earlier, if
âˆ« âˆ
Ïˆ (a) â‰¡ âˆ’
Î¦+ (âˆ’s) g+ (x (s, a)) ds
0

then for x (t, x0 ) the solution to the initial value problem
xâ€² = Ax + g (x) , x (0) = x0
(
)
aâˆ’
has the property that if x0 is not of the form
, then |x (t, x0 )| cannot be less
Ïˆ (aâˆ’ )
than Î´ for all t > 0.
(
)
aâˆ’
Î´
On the other hand, if x0 =
for |aâˆ’ | < 2C
, then x (t, x0 ) ,the solution to
Ïˆ (aâˆ’ )
(3.39) is the unique solution to the initial value problem
xâ€² = Ax + g (x) , x (0) = x0 .
and it was shown that ||x (Â·, x0 )||Î³ < Î´ and so in fact,
|x (t, x0 )| â‰¤ Î´eâˆ’Î³t
showing that
lim x (t, x0 ) = 0.

tâ†’âˆ


The following theorem is the main result. It involves a use of linear algebra and the
above lemma.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

438

APPLICATIONS TO DIFFERENTIAL EQUATIONS

Theorem C.7.3 Consider the initial value problem for the almost linear system
xâ€² = Ax + g (x) , x (0) = x0
in which g is C 1 and where at there are k < n eigenvalues of A which have negative real
parts and n âˆ’ k eigenvalues of A which have positive real parts. Then 0 is not stable. More
precisely, there exists a set of points (a, Ïˆ (a)) for a small and in a k dimensional subspace
such that for x0 on this set,
lim x (t, x0 ) = 0
tâ†’âˆ

and for x0 not on this set, there exists a Î´ > 0 such that |x (t, x0 )| cannot remain less than
Î´ for all positive t.
Proof: This involves nothing more than a reduction to the situation of Lemma C.7.2.
From Theorem 10.5.2 on
( Page 10.5.2
) A is similar to a matrix of the form described in Lemma
A
0
âˆ’
C.7.2. Thus A = S âˆ’1
S. Letting y = Sx, it follows
0 A+
(
)
(
)
Aâˆ’
0
yâ€² =
y + g S âˆ’1 y
0 A+
Now |x| = S âˆ’1 Sx â‰¤ S âˆ’1 |y| and |y| = SS âˆ’1 y â‰¤ ||S|| |x| . Therefore,
1
|y| â‰¤ |x| â‰¤ S âˆ’1 |y| .
||S||
It follows all conclusions of Lemma C.7.2 are valid for this theorem. 
The set of points (a, Ïˆ (a)) for a small is called the stable manifold. Much more can be
said about the stable manifold and you should look at a good diï¬€erential equations book
for this.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Compactness And Completeness
D.0.1

The Nested Interval Lemma

First, here is the one dimensional nested interval lemma.
Lemma D.0.4 Let Ik = [ak , bk ] be closed intervals, ak â‰¤ bk , such that Ik âŠ‡ Ik+1 for all k.
Then there exists a point c which is contained in all these intervals. If limkâ†’âˆ (bk âˆ’ ak ) = 0,
then there is exactly one such point.
Proof: Note that the {ak } are an increasing sequence and that {bk } is a decreasing
sequence. Now note that if m < n, then
am â‰¤ an â‰¤ bn
while if m > n,
bn â‰¥ bm â‰¥ am .
It follows that am â‰¤ bn for any pair m, n. Therefore, each bn is an upper bound for all the
am and so if c â‰¡ sup {ak }, then for each n, it follows that c â‰¤ bn and so for all, an â‰¤ c â‰¤ bn
which shows that c is in all of these intervals.
If the condition on the lengths of the intervals holds, then if c, câ€² are in all the intervals,
then if they are not equal, then eventually, for large enough k, they cannot both be contained
in [ak , bk ] since eventually bk âˆ’ ak < |c âˆ’ câ€² |. This would be a contradiction. Hence c = câ€² .

Deï¬nition D.0.5 The diameter of a set S, is deï¬ned as
diam (S) â‰¡ sup {|x âˆ’ y| : x, y âˆˆ S} .
Thus diam (S) is just a careful description of what you would think of as the diameter.
It measures how stretched out the set is.
Here is a multidimensional version of the nested interval lemma.
] {
[
]}
âˆp [
Lemma D.0.6 Let Ik = i=1 aki , bki â‰¡ x âˆˆ Rp : xi âˆˆ aki , bki
and suppose that for all
k = 1, 2, Â· Â· Â· ,
Ik âŠ‡ Ik+1 .
Then there exists a point c âˆˆ Rp which is an element of every Ik . If limkâ†’âˆ diam (Ik ) = 0,
then the point c is unique.
[ k k]
[ k+1 k+1 ]
Proof: For each
and so, by Lemma D.0.4, there
[ ki =k ]1, Â· Â· Â· , p, ai , bi âŠ‡ ai , bi
exists a point ci âˆˆ ai , bi for all k. Then letting c â‰¡ (c1 , Â· Â· Â· , cp ) it follows c âˆˆ [Ik for ]all k.
If the condition on the diameters holds, then the lengths of the intervals limkâ†’âˆ aki , bki = 0
and so by the same lemma, each ci is unique. Hence c is unique. 
439

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

440

D.0.2

COMPACTNESS AND COMPLETENESS

Convergent Sequences, Sequential Compactness

A mapping f : {k, k + 1, k + 2, Â· Â· Â· } â†’ Rp is called a sequence. We usually write it in the
form {aj } where it is understood that aj â‰¡ f (j).
Deï¬nition D.0.7 A sequence, {ak } is said to converge to a if for every Îµ > 0 there exists
nÎµ such that if n > nÎµ , then |a âˆ’ an | < Îµ. The usual notation for this is limnâ†’âˆ an = a
although it is often written as an â†’ a. A closed set K âŠ† Rn is one which has the property
âˆ
that if {kj }j=1 is a sequence of points of K which converges to x, then x âˆˆ K.
One can also deï¬ne a subsequence.
Deï¬nition D.0.8 {ank } is a subsequence of {an } if n1 < n2 < Â· Â· Â· .
The following theorem says the limit, if it exists, is unique.
Theorem D.0.9 If a sequence, {an } converges to a and to b then a = b.
Proof: There exists nÎµ such that if n > nÎµ then |an âˆ’ a| <
|an âˆ’ b| < 2Îµ . Then pick such an n.
|a âˆ’ b| < |a âˆ’ an | + |an âˆ’ b| <

Îµ
2

and if n > nÎµ , then

Îµ Îµ
+ = Îµ.
2 2

Since Îµ is arbitrary, this proves the theorem. 
The following is the deï¬nition of a Cauchy sequence in Rp .
Deï¬nition D.0.10 {an } is a Cauchy sequence if for all Îµ > 0, there exists nÎµ such that
whenever n, m â‰¥ nÎµ , if follows that |an âˆ’am | < Îµ.
A sequence is Cauchy, means the terms are â€œbunching up to each otherâ€ as m, n get
large.
Theorem D.0.11 The set of terms in a Cauchy sequence in Rp is bounded in the sense
that for all n, |an | < M for some M < âˆ.
Proof: Let Îµ = 1 in the deï¬nition of a Cauchy sequence and let n > n1 . Then from the
deï¬nition, |an âˆ’ an1 | < 1.It follows that for all n > n1 , |an | < 1 + |an1 | .Therefore, for all n,
|an | â‰¤ 1 + |an1 | +

n1
âˆ‘

|ak | . 

k=1

Theorem D.0.12 If a sequence {an } in Rp converges, then the sequence is a Cauchy sequence. Also, if some subsequence of a Cauchy sequence converges, then the original sequence
converges.
Proof: Let Îµ > 0 be given and suppose an â†’ a. Then from the deï¬nition of convergence,
there exists nÎµ such that if n > nÎµ , it follows that |an âˆ’a| < 2Îµ . Therefore, if m, n â‰¥ nÎµ + 1,
it follows that
Îµ Îµ
|an âˆ’am | â‰¤ |an âˆ’a| + |a âˆ’ am | < + = Îµ
2 2
showing that, since Îµ > 0 is arbitrary, {an } is a Cauchy sequence. It remains to that the
last claim.
âˆ
Suppose then that {an } is a Cauchy sequence and a = limkâ†’âˆ ank where {ank }k=1
is a subsequence. Let Îµ > 0 be given. Then there exists K such that if k, l â‰¥ K, then

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

441
|ak âˆ’ al | < 2Îµ . Then if k > K, it follows nk > K because n1 , n2 , n3 , Â· Â· Â· is strictly increasing
as the subscript increases. Also, there exists K1 such that if k > K1 , |ank âˆ’ a| < 2Îµ . Then
letting n > max (K, K1 ), pick k > max (K, K1 ). Then
|a âˆ’ an | â‰¤ |a âˆ’ ank | + |ank âˆ’ an | <

Îµ Îµ
+ = Îµ.
2 2

Therefore, the sequence converges. 
Deï¬nition D.0.13 A set K in Rp is said to be sequentially compact if every sequence
in K has a subsequence which converges to a point of K.
âˆp
Theorem D.0.14 If I0 = i=1 [ai , bi ] where ai â‰¤ bi , then I0 is sequentially compact.
âˆp
âˆ
Proof: Let[ {ak }k=1] âŠ† I0 and consider
all] sets of the form i=1 [ci , di ] where [ci , di ]
[
p
i
i
equals either ai , ai +b
or [ci , di ] = ai +b
2
2 , bi . Thus there are 2 of these sets because
th
there are two choices for the i slot for i = 1, Â· Â· Â· , p. Also, if x and y are two points in one
(âˆ‘
)1/2
p
2
of these sets, |xi âˆ’ yi | â‰¤ 2âˆ’1 |bi âˆ’ ai | where diam (I0 ) =
|b
âˆ’
a
|
,
i
i
i=1
(
|x âˆ’ y| =

p
âˆ‘

)1/2
|xi âˆ’ yi |

2

âˆ’1

â‰¤2

i=1

( p
âˆ‘

)1/2
|bi âˆ’ ai |

2

â‰¡ 2âˆ’1 diam (I0 ) .

i=1

In particular, since d â‰¡ (d1 , Â· Â· Â· , dp ) and c â‰¡ (c1 , Â· Â· Â· , cp ) are two such points,
(
D1 â‰¡

p
âˆ‘

)1/2
|di âˆ’ ci |

2

â‰¤ 2âˆ’1 diam (I0 )

i=1

Denote by {J1 , Â· Â· Â· , J2p } these sets determined above. Since the union of these sets equals
all of I0 â‰¡ I, it follows that for some Jk , the sequence, {ai } is contained in Jk for inï¬nitely
many k. Let that one be called I1 . Next do for I1 what was done for I0 to get I2 âŠ† I1
such that the diameter is half that of I1 and I2 contains {ak } for inï¬nitely many values
of k. Continue in this way obtaining a nested sequence {Ik } such that Ik âŠ‡ Ik+1 , and if
x, y âˆˆ Ik , then |x âˆ’ y| â‰¤ 2âˆ’k diam (I0 ), and In contains {ak } for inï¬nitely many values of
k for each n. Then by the nested interval lemma, there exists c such that c is contained in
each Ik . Pick an1 âˆˆ I1 . Next pick n2 > n1 such that an2 âˆˆ I2 . If an1 , Â· Â· Â· , ank have been
chosen, let ank+1 âˆˆ Ik+1 and nk+1 > nk . This can be done because in the construction, In
contains {ak } for inï¬nitely many k. Thus the distance between ank and c is no larger than
2âˆ’k diam (I0 ), and so limkâ†’âˆ ank = c âˆˆ I0 . 
Corollary D.0.15 Let K be a closed and bounded set of points in Rp . Then K is sequentially compact.
âˆp
Proof: Since K is closed and bounded, there exists a closed rectangle, k=1 [ak , bk ]
which contains K. Now let {xk } be a sequence of âˆ
points in K. By Theorem D.0.14, there
p
exists a subsequence {xnk } such that xnk â†’ x âˆˆ k=1 [ak , bk ]. However, K is closed and
each xnk is in K so x âˆˆ K. 
Theorem D.0.16 Every Cauchy sequence in Rp converges.
âˆp
Proof: Let {ak } be a Cauchy sequence. By Theorem D.0.11, there is some box i=1 [ai , bi ]
containing allâˆthe terms of {ak }. Therefore, by Theorem D.0.14, a subsequence converges
p
to a point of i=1 [ai , bi ]. By Theorem D.0.12, the original sequence converges. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

442

Saylor URL: http://www.saylor.org/courses/ma212/

COMPACTNESS AND COMPLETENESS

The Saylor Foundation

The Fundamental Theorem Of
Algebra
The fundamental theorem of algebra states that every non constant polynomial having
coeï¬ƒcients in C has a zero in C. If C is replaced by R, this is not true because of the
example, x2 + 1 = 0. This theorem is a very remarkable result and notwithstanding its title,
all the best proofs of it depend on either analysis or topology. It was proved by Gauss in
1797 then proved with no loose ends by Argand in 1806 although others also worked on
it. The proof given here follows Rudin [22]. See also Hardy [12] for another proof, more
discussion and references. Recall De Moivreâ€™s theorem on Page 17 which is listed below for
convenience.
Theorem E.0.17 Let r > 0 be given. Then if n is a positive integer,
n

[r (cos t + i sin t)] = rn (cos nt + i sin nt) .
Now from this theorem, the following corollary on Page 1.5.5 is obtained.
Corollary E.0.18 Let z be a non zero complex number and let k be a positive integer. Then
there are always exactly k k th roots of z in C.
âˆ‘n
Lemma E.0.19 Let ak âˆˆ C for k = 1, Â· Â· Â· , n and let p (z) â‰¡ k=1 ak z k . Then p is continuous.
Proof:
|az n âˆ’ awn | â‰¤ |a| |z âˆ’ w| z nâˆ’1 + z nâˆ’2 w + Â· Â· Â· + wnâˆ’1 .
Then for |z âˆ’ w| < 1, the triangle inequality implies |w| < 1 + |z| and so if |z âˆ’ w| < 1,
n

|az n âˆ’ awn | â‰¤ |a| |z âˆ’ w| n (1 + |z|) .
If Îµ > 0 is given, let

(
Î´ < min 1,

Îµ
n
|a| n (1 + |z|)

)
.

It follows from the above inequality that for |z âˆ’ w| < Î´, |az n âˆ’ awn | < Îµ. The function of
the lemma is just the sum of functions of this sort and so it follows that it is also continuous.
Theorem E.0.20 (Fundamental theorem of Algebra) Let p (z) be a nonconstant polynomial.
Then there exists z âˆˆ C such that p (z) = 0.

443

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

444

THE FUNDAMENTAL THEOREM OF ALGEBRA

Proof: Suppose not. Then
p (z) =

n
âˆ‘

ak z k

k=0

where an Ì¸= 0, n > 0. Then
n

|p (z)| â‰¥ |an | |z| âˆ’

nâˆ’1
âˆ‘

|ak | |z|

k

k=0

and so
lim |p (z)| = âˆ.

(5.1)

|z|â†’âˆ

Now let
Î» â‰¡ inf {|p (z)| : z âˆˆ C} .
By (5.1), there exists an R > 0 such that if |z| > R, it follows that |p (z)| > Î» + 1. Therefore,
Î» â‰¡ inf {|p (z)| : z âˆˆ C} = inf {|p (z)| : |z| â‰¤ R} .
The set {z : |z| â‰¤ R} is a closed and bounded set and so this inï¬mum is achieved at some
point w with |w| â‰¤ R. A contradiction is obtained if |p (w)| = 0 so assume |p (w)| > 0. Then
consider
p (z + w)
q (z) â‰¡
.
p (w)
It follows q (z) is of the form
q (z) = 1 + ck z k + Â· Â· Â· + cn z n
where ck Ì¸= 0, because q (0) = 1. It is also true that |q (z)| â‰¥ 1 by the assumption that
|p (w)| is the smallest value of |p (z)| . Now let Î¸ âˆˆ C be a complex number with |Î¸| = 1 and
k

Î¸ck wk = âˆ’ |w| |ck | .
If
w Ì¸= 0, Î¸ =

âˆ’ wk |ck |
wk ck

and if w = 0, Î¸ = 1 will work. Now let Î· k = Î¸ and let t be a small positive number.
k

n

q (tÎ·w) â‰¡ 1 âˆ’ tk |w| |ck | + Â· Â· Â· + cn tn (Î·w)
which is of the form
k

1 âˆ’ tk |w| |ck | + tk (g (t, w))
where limtâ†’0 g (t, w) = 0. Letting t be small enough,
k

|g (t, w)| < |w| |ck | /2
and so for such t,
k

k

|q (tÎ·w)| < 1 âˆ’ tk |w| |ck | + tk |w| |ck | /2 < 1,
a contradiction to |q (z)| â‰¥ 1. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Fields And Field Extensions
F.1

The Symmetric Polynomial Theorem

First here is a deï¬nition of polynomials in many variables which have coeï¬ƒcients in a
commutative ring. A commutative ring would be a ï¬eld except you donâ€™t know that every
nonzero element has a multiplicative inverse. If you like, let these coeï¬ƒcients be in a ï¬eld
it is still interesting. A good example of a commutative ring is the integers. In particular,
every ï¬eld is a commutative ring.
Deï¬nition F.1.1 Let k â‰¡ (k1 , k2 , Â· Â· Â· , kn ) where each ki is a nonnegative integer. Let
âˆ‘
ki
|k| â‰¡
i

Polynomials of degree p in the variables x1 , x2 , Â· Â· Â· , xn are expressions of the form
âˆ‘
g (x1 , x2 , Â· Â· Â· , xn ) =
ak xk11 Â· Â· Â· xknn
|k|â‰¤p

where each ak is in a commutative ring. If all ak = 0, the polynomial has no degree. Such
a polynomial is said to be symmetric if whenever Ïƒ is a permutation of {1, 2, Â· Â· Â· , n},
(
)
g xÏƒ(1) , xÏƒ(2) , Â· Â· Â· , xÏƒ(n) = g (x1 , x2 , Â· Â· Â· , xn )
An example of a symmetric polynomial is
s1 (x1 , x2 , Â· Â· Â· , xn ) â‰¡

n
âˆ‘

xi

i=1

Another one is
sn (x1 , x2 , Â· Â· Â· , xn ) â‰¡ x1 x2 Â· Â· Â· xn
Deï¬nition F.1.2 The elementary symmetric polynomial sk (x1 , x2 , Â· Â· Â· , xn ) , k = 1, Â· Â· Â· , n
k
is the coeï¬ƒcient of (âˆ’1) xnâˆ’k in the following polynomial.
(x âˆ’ x1 ) (x âˆ’ x2 ) Â· Â· Â· (x âˆ’ xn )
= xn âˆ’ s1 xnâˆ’1 + s2 xnâˆ’2 âˆ’ Â· Â· Â· Â± sn
Thus
s2 =

âˆ‘
i<j

s1 = x1 + x2 + Â· Â· Â· + xn
âˆ‘
xi xj , s3 =
xi xj xk , . . . , sn = x1 x2 Â· Â· Â· xn
i<j<k

445

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

446

FIELDS AND FIELD EXTENSIONS

Then the following result is the fundamental theorem in the subject. It is the symmetric
polynomial theorem. It says that these elementary symmetric polynomials are a lot like a
basis for the symmetric polynomials.
Theorem F.1.3 Let g (x1 , x2 , Â· Â· Â· , xn ) be a symmetric polynomial. Then g (x1 , x2 , Â· Â· Â· , xn )
equals a polynomial in the elementary symmetric functions.
âˆ‘
g (x1 , x2 , Â· Â· Â· , xn ) =
ak sk11 Â· Â· Â· sknn
k

and the ak are unique.
Proof: If n = 1, it is obviously true because s1 = x1 . Suppose the theorem is true for
n âˆ’ 1 and g (x1 , x2 , Â· Â· Â· , xn ) has degree d. Let
g â€² (x1 , x2 , Â· Â· Â· , xnâˆ’1 ) â‰¡ g (x1 , x2 , Â· Â· Â· , xnâˆ’1 , 0)
By induction, there are unique ak such that
g â€² (x1 , x2 , Â· Â· Â· , xnâˆ’1 ) =

âˆ‘

â€²k

nâˆ’1
1
ak sâ€²k
1 Â· Â· Â· snâˆ’1

k

where sâ€²i is the corresponding symmetric polynomial which pertains to x1 , x2 , Â· Â· Â· , xnâˆ’1 .
Note that
sk (x1 , x2 , Â· Â· Â· , xnâˆ’1 , 0) = sâ€²k (x1 , x2 , Â· Â· Â· , xnâˆ’1 )
Now consider
g (x1 , x2 , Â· Â· Â· , xn ) âˆ’

âˆ‘

k

nâˆ’1
â‰¡ q (x1 , x2 , Â· Â· Â· , xn )
ak sk11 Â· Â· Â· snâˆ’1

k

is a symmetric polynomial and it equals 0 when xn equals 0. Since it is symmetric, it is also
0 whenever xi = 0. Therefore,
q (x1 , x2 , Â· Â· Â· , xn ) = sn h (x1 , x2 , Â· Â· Â· , xn )
and it follows that h (x1 , x2 , Â· Â· Â· , xn ) is symmetric of degree no more than d âˆ’ n and is
uniquely determined. Thus, if g (x1 , x2 , Â· Â· Â· , xn ) is symmetric of degree d,
âˆ‘
knâˆ’1
g (x1 , x2 , Â· Â· Â· , xn ) =
ak sk11 Â· Â· Â· snâˆ’1
+ sn h (x1 , x2 , Â· Â· Â· , xn )
k

where h has degree no more than d âˆ’ n. Now apply the same argument to h (x1 , x2 , Â· Â· Â· , xn )
and continue, repeatedly obtaining a sequence of symmetric polynomials hi , of strictly decreasing degree, obtaining expressions of the form
âˆ‘
knâˆ’1 kn
g (x1 , x2 , Â· Â· Â· , xn ) =
bk sk11 Â· Â· Â· snâˆ’1
sn + sn hm (x1 , x2 , Â· Â· Â· , xn )
k

Eventually hm must be a constant or zero. By induction, each step in the argument yields
uniqueness and so, the ï¬nal sum of combinations of elementary symmetric functions is
uniquely determined. 
Here is a very interesting result which I saw claimed in a paper by Steinberg and Redheï¬€er
on Lindemannnâ€™s theorem which follows from the above corollary.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.2. THE FUNDAMENTAL THEOREM OF ALGEBRA

447

Theorem F.1.4 Let Î±1 , Â· Â· Â· , Î±n be roots of the polynomial equation
an xn + anâˆ’1 xnâˆ’1 + Â· Â· Â· + a1 x + a0 = 0
where each ai is an integer. Then any symmetric polynomial in the quantities an Î±1 , Â· Â· Â· , an Î±n
having integer coeï¬ƒcients is also an integer. Also any symmetric polynomial in the quantities Î±1 , Â· Â· Â· , Î±n having rational coeï¬ƒcients is a rational number.
Proof: Let f (x1 , Â· Â· Â· , xn ) be the symmetric polynomial. Thus
f (x1 , Â· Â· Â· , xn ) âˆˆ Z [x1 Â· Â· Â· xn ]
From Corollary F.1.3 it follows there are integers ak1 Â·Â·Â·kn such that
âˆ‘
f (x1 , Â· Â· Â· , xn ) =
ak1 Â·Â·Â·kn pk11 Â· Â· Â· pknn
k1 +Â·Â·Â·+kn â‰¤m

where the pi are the elementary symmetric polynomials deï¬ned as the coeï¬ƒcients of
n
âˆ

(x âˆ’ xj )

j=1

Thus
f (an Î±1 , Â· Â· Â· , an Î±n )
âˆ‘
=
ak1 Â·Â·Â·kn pk11 (an Î±1 , Â· Â· Â· , an Î±n ) Â· Â· Â· pknn (an Î±1 , Â· Â· Â· , an Î±n )
k1 +Â·Â·Â·+kn

Now the given polynomial is of the form
an

n
âˆ

(x âˆ’ Î±j )

j=1

and so the coeï¬ƒcient of xnâˆ’k is pk (Î±1 , Â· Â· Â· , Î±n ) an = anâˆ’k . Also
pk (an Î±1 , Â· Â· Â· , an Î±n ) = akn pk (Î±1 , Â· Â· Â· , Î±n ) = akn

anâˆ’k
an

It follows
f (an Î±1 , Â· Â· Â· , an Î±n ) =

âˆ‘

(
ak1 Â·Â·Â·kn

k1 +Â·Â·Â·+kn

anâˆ’1
a1n
an

)k1 (
)k2
(
)kn
2 anâˆ’2
n a0
an
Â· Â· Â· an
an
an

which is an integer. To see the last claim follows from this, take the symmetric polynomial
in Î±1 , Â· Â· Â· , Î±n and multiply by the product of the denominators of the rational coeï¬ƒcients
to get one which has integer coeï¬ƒcients. Then by the ï¬rst part, each homogeneous term is
just an integer divided by an raised to some power. 

F.2

The Fundamental Theorem Of Algebra

This is devoted to a mostly algebraic proof of the fundamental theorem of algebra. It
depends on the interesting results about symmetric polynomials which are presented above.
I found it on the Wikipedia article about the fundamental theorem of algebra. You google

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

448

FIELDS AND FIELD EXTENSIONS

â€œfundamental theorem of algebraâ€ and go to the Wikipedia article. It gives several other
proofs in addition to this one. According to this article, the ï¬rst completely correct proof
of this major theorem is due to Argand in 1806. Gauss and others did it earlier but their
arguments had gaps in them.
You canâ€™t completely escape analysis when you prove this theorem. The necessary analysis is in the following lemma.
Lemma F.2.1 Suppose p (x) = xn + anâˆ’1 xnâˆ’1 + Â· Â· Â· + a1 x + a0 where n is odd and the
coeï¬ƒcients are real. Then p (x) has a real root.
Proof: This follows from the intermediate value theorem from calculus.
Next is an algebraic consideration. First recall some notation.
m
âˆ

ai â‰¡ a1 a2 Â· Â· Â· am

i=1

Recall a polynomial in {z1 , Â· Â· Â· , zn } is symmetric only if it can be written as a sum of
elementary symmetric polynomials raised to various powers multiplied by constants. This
follows from Proposition F.1.3 or Theorem F.1.3 both of which are the theorem on symmetric
polynomials.
The following is the main part of the theorem. In fact this is one version of the fundamental theorem of algebra which people studied earlier in the 1700â€™s.
Lemma F.2.2 Let p (x) = xn + anâˆ’1 xnâˆ’1 + Â· Â· Â· + a1 x + a0 be a polynomial with real coefï¬cients. Then it has a complex root.
Proof: It is possible to write
n = 2k m
where m is odd. If n is odd, k = 0. If n is even, keep dividing by 2 until you are left with
an odd number. If k = 0 so that n is odd, it follows from Lemma F.2.1 that p (x) has a
real, hence complex root. The proof will be by induction on k, the case k = 0 being done.
Suppose then that it works for n = 2l m where m is odd and l â‰¤ k âˆ’ 1 and let n = 2k m
where m is odd. Let {z1 , Â· Â· Â· , zn } be the roots of the polynomial in a splitting ï¬eld, the
existence of this ï¬eld being given by the above proposition. Then
p (x) =

n
âˆ

(x âˆ’ zj ) =

j=1

n
âˆ‘

k

(âˆ’1) pk xk

(6.1)

k=0

where pk is the k th elementary symmetric polynomial. Note this shows
k

anâˆ’k = pk (âˆ’1) .

(6.2)

There is another polynomial which has coeï¬ƒcients which are sums of real numbers times
the pk raised to various powers and it is
âˆ
qt (x) â‰¡
(x âˆ’ (zi + zj + tzi zj )) , t âˆˆ R
1â‰¤i<jâ‰¤n

I need to verify this is really the case for qt (x). When you switch any two of the zi in qt (x)
the polynomial does not change. For example, let n = 3 when qt (x) is
(x âˆ’ (z1 + z2 + tz1 z2 )) (x âˆ’ (z1 + z3 + tz1 z3 )) (x âˆ’ (z2 + z3 + tz2 z3 ))

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.2. THE FUNDAMENTAL THEOREM OF ALGEBRA

449

and you can observe the assertion about the polynomial is true when you switch two different zi . Thus the coeï¬ƒcients of qt (x) must be symmetric polynomials in the zi with real
coeï¬ƒcients. Hence by Proposition F.1.3 these coeï¬ƒcients are real polynomials in terms of
the elementary symmetric polynomials pk . Thus by (6.2) the coeï¬ƒcients of qt (x) are real
polynomials in terms of the ak of the original polynomial. Recall these were all real. It
follows, and this is what was wanted,(that)qt (x) has all real coeï¬ƒcients.
n
Note that the degree of qt (x) is
because there are this number of ways to pick
2
i < j out of {1, Â· Â· Â· , n}. Now
(
)
(
)
n (n âˆ’ 1)
n
= 2kâˆ’1 m 2k m âˆ’ 1
=
2
2
= 2kâˆ’1 (odd)
and so by induction, for each t âˆˆ R, qt (x) has a complex root.
There must exist s Ì¸= t such that for a single pair of indices i, j, with i < j,
(zi + zj + tzi zj ) , (zi + zj + szi zj )
are both complex. Here is why. Let A (i, j) denote those t âˆˆ R such that (zi + zj + tzi zj ) is
complex. It was just shown that every t âˆˆ R must be in some A (i, j). There are inï¬nitely
many t âˆˆ R and so some A (i, j) contains two of them.
Now for that t, s,
zi + zj + tzi zj
zi + zj + szi zj

=
=

a
b

where t Ì¸= s and so by Cramerâ€™s rule,

zi + zj =

a t
b s
1 t
1 s

âˆˆC

and also
zi zj =

1 a
1 b
1 t
1 s

âˆˆC

At this point, note that zi , zj are both solutions to the equation
x2 âˆ’ (z1 + z2 ) x + z1 z2 = 0,
which from the above has complex coeï¬ƒcients. By the quadratic formula the zi , zj are both
complex. Thus the original polynomial has a complex root. 
With this lemma, it is easy to prove the fundamental theorem of algebra. The diï¬€erence
between the lemma and this theorem is that in the theorem, the coeï¬ƒcients are only assumed
to be complex. What this means is that if you have any polynomial with complex coeï¬ƒcients
it has a complex root and so it is not irreducible. Hence the ï¬eld extension is the same ï¬eld.
Another way to say this is that for every complex polynomial there exists a factorization
into linear factors or in other words a splitting ï¬eld for a complex polynomial is the ï¬eld of
complex numbers.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

450

FIELDS AND FIELD EXTENSIONS

Theorem F.2.3 Let p (x) â‰¡ an xn + anâˆ’1 xnâˆ’1 + Â· Â· Â· + a1 x + a0 be any complex polynomial,
n â‰¥ 1, an Ì¸= 0. Then it has a complex root. Furthermore, there exist complex numbers
z1 , Â· Â· Â· , zn such that
n
âˆ
p (x) = an
(x âˆ’ zk )
k=1

Proof: First suppose an = 1. Consider the polynomial
q (x) â‰¡ p (x) p (x)
this is a polynomial and it has real coeï¬ƒcients. This is because it equals
( n
)
x + anâˆ’1 xnâˆ’1 + Â· Â· Â· + a1 x + a0 Â·
( n
)
x + anâˆ’1 xnâˆ’1 + Â· Â· Â· + a1 x + a0
The xj+k term of the above product is of the form
ak xk aj xj + ak xk aj xj = xk+j (ak aj + ak aj )
and
ak aj + ak aj = ak aj + ak aj
so it is of the form of a complex number added to its conjugate. Hence q (x) has real
coeï¬ƒcients as claimed. Therefore, by by Lemma F.2.2 it has a complex root z. Hence either
p (z) = 0 or p (z) = 0. Thus p (x) has a complex root.
Next suppose an Ì¸= 0. Then simply divide by it and get a polynomial in which an = 1.
Denote this modiï¬ed polynomial as q (x). Then by what was just shown and the Euclidean
algorithm, there exists z1 âˆˆ C such that
q (x) = (x âˆ’ z1 ) q1 (x)
where q1 (x) has complex coeï¬ƒcients. Now do the same thing for q1 (x) to obtain
q (x) = (x âˆ’ z1 ) (x âˆ’ z2 ) q2 (x)
and continue this way. Thus

n
âˆ
p (x)
=
(x âˆ’ zj ) 
an
j=1

Obviously this is a harder proof than the other proof of the fundamental theorem of
algebra presented earlier. However, this is a better proof. Consider the algebraic numbers A consisting of the real numbers which are roots of some polynomial having rational
coeï¬ƒcients. By Theorem 8.3.32 they are a ï¬eld. Now consider the ï¬eld A + iA with the
usual conventions for complex arithmetic. You could repeat the above argument with small
changes and conclude that every polynomial having coeï¬ƒcients in A + iA has a root in
A + iA. Recall from Problem 41 on Page 223 that A is countable and so this is also the case
for A + iA. Thus this gives an algebraically complete ï¬eld which is countable and so very
diï¬€erent than C. Of course there are other situations in which the above harder proof will
work and yield interesting results.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.3. TRANSCENDENTAL NUMBERS

F.3

451

Transcendental Numbers

Most numbers are like this. Here the algebraic numbers are those which are roots of a
polynomial equation having rational numbers as coeï¬ƒcients. By the fundamental theorem
of calculus, all these numbers are in C. There are only countably many of these algebraic
numbers, (Problem 41 on Page 223). Therefore, most numbers are transcendental. Nevertheless, it is very hard to prove that this or that number is transcendental. Probably the
most famous theorem about this is the Lindemannn Weierstrass theorem.
Theorem F.3.1 Let the Î±i be distinct nonzero algebraic numbers and let the ai be nonzero
algebraic numbers. Then
n
âˆ‘
ai eai Ì¸= 0
i=1

I am following the interesting Wikepedia article on this subject. You can also look at the
book by Baker [4], Transcendental Number Theory, Cambridge University Press. There are
also many other treatments which you can ï¬nd on the web including an interesting article
by Steinberg and Redheï¬€er which appeared in about 1950.
The proof makes use of the following identity. For f (x) a polynomial,
âˆ«
I (s) â‰¡

âˆ‘

deg(f )

s

esâˆ’x f (x) dx = es
0

âˆ‘

deg(f )

f (j) (0) âˆ’

j=0

f (j) (s) .

(6.3)

j=0

where f (j) denotes the j th derivative. In this formula, s âˆˆ C and the integral is deï¬ned in
the natural way as
âˆ« 1
sf (ts) esâˆ’ts dt
(6.4)
0

The identity follows from integration by parts.
âˆ« 1
âˆ« 1
sf (ts) esâˆ’ts dt = ses
f (ts) eâˆ’ts dt
0
0
[ âˆ’ts
]
âˆ« 1 âˆ’ts
e
e
= ses âˆ’
f (ts) |10 +
sf â€² (st) dt
s
s
0
[
]
âˆ« 1
âˆ’s
1
e
= ses f (s) âˆ’
f (0) +
eâˆ’ts f â€² (st) dt
s
s
0
âˆ« 1
= f (0) âˆ’ es f (s) +
sesâˆ’ts f â€² (st) dt
0
âˆ« s
esâˆ’x f â€² (x) dx
â‰¡ f (0) âˆ’ f (s) es +
0

Continuing this way establishes the identity.
Lemma F.3.2 If K and c are nonzero integers, and Î² 1 , Â· Â· Â· , Î² m are the roots of a single
polynomial with integer coeï¬ƒcients,
Q (x) = vxm + Â· Â· Â· + u
where v, u Ì¸= 0, then

(
)
K + c eÎ² 1 + Â· Â· Â· + eÎ² m Ì¸= 0.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

452

FIELDS AND FIELD EXTENSIONS

Letting
v (mâˆ’1)p Qp (x) xpâˆ’1
(p âˆ’ 1)!
and I (s) be deï¬ned in terms of f (x) as above, it follows,
f (x) =

lim

pâ†’âˆ

and

n
âˆ‘

m
âˆ‘

I (Î² i ) = 0

i=1

f (j) (0) = v p(mâˆ’1) up + m1 (p) p

j=0
m âˆ‘
n
âˆ‘

f (j) (Î² i ) = m2 (p) p

i=1 j=0

where mi (p) is some integer.
Proof: Let p be a prime number. Then consider the polynomial f (x) of degree n â‰¡
pm + p âˆ’ 1,
v (mâˆ’1)p Qp (x) xpâˆ’1
f (x) =
(p âˆ’ 1)!
From (6.3)
ï£«
ï£¶
m
m
n
n
âˆ‘
âˆ‘
âˆ‘
âˆ‘
ï£­ eÎ² i
c
I (Î² i ) = c
f (j) (0) âˆ’
f (j) (Î² i )ï£¸
(
=

i=1

K +c

m
âˆ‘

)
eÎ² i

i=1
âˆ‘m
limpâ†’âˆ c i=1 I

i=1
n
âˆ‘

j=0

f (j) (0) âˆ’ K

j=0

j=0
n
âˆ‘

f (j) (0) âˆ’ c

m âˆ‘
n
âˆ‘

j=0

f (j) (Î² i )

(6.5)

i=1 j=0

(Î² i ) = 0.
Claim 1:
( )
Proof: This follows right away from the deï¬nition of I Î² j and the deï¬nition of f (x) .
âˆ« 1
( )
( )
I Î²j â‰¤
Î² j f tÎ² j eÎ² j âˆ’tÎ² j dt
0

âˆ«

1

â‰¤

|v|

(mâˆ’1)p

0

( ) p pâˆ’1
Q tÎ² j
t
Î²j
(p âˆ’ 1)!

pâˆ’1

dt

which clearly converges to 0. This proves the claim.
The next thing to consider is the term on the end in (6.5),
K

n
âˆ‘

f (j) (0) + c

j=0

m âˆ‘
n
âˆ‘

f (j) (Î² i )

(6.6)

i=1 j=0

The idea is to show thatâˆ‘
for large enough p it is always an integer. When this is done, it
m
canâ€™t happen that K + c i=1 eÎ² i = 0 because if this were so, you would have a very small
number equal to an integer. Now
ï£¶p
ï£«
Q(x)
}|
{
z
ï£·
ï£¬
v (mâˆ’1)p ï£­v (x âˆ’ Î² 1 ) (x âˆ’ Î² 2 ) Â· Â· Â· (x âˆ’ Î² m )ï£¸ xpâˆ’1
f (x) =
=

(p âˆ’ 1)!
p
v mp ((x âˆ’ Î² 1 ) (x âˆ’ Î² 2 ) Â· Â· Â· (x âˆ’ Î² m )) xpâˆ’1
(p âˆ’ 1)!

Saylor URL: http://www.saylor.org/courses/ma212/

(6.7)

The Saylor Foundation

F.3. TRANSCENDENTAL NUMBERS

453

It follows that for j < p âˆ’ 1, f (j) (0) = 0. This is because of that term xpâˆ’1 . If j â‰¥ p, f (j) (0)
is an integer multiple of p. Here is why. The terms in this derivative which are nonzero
involve taking p âˆ’ 1 derivatives of xpâˆ’1 and this introduces a (p âˆ’ 1)! which cancels out the
denominator. Then there are some other derivatives of the product of the (x âˆ’ Î² i ) raised
to the power p. By the chain rule, these all involve a multiple of p. Thus this j th derivative
is of the form
pg (x, vÎ² 1 , Â· Â· Â· , vÎ² m ) ,
(6.8)
where g (x, vÎ² 1 , Â· Â· Â· , vÎ² m ) is a polynomial in x with coeï¬ƒcients which are symmetric polynomials in {vÎ² 1 , Â· Â· Â· , vÎ² m } having integer coeï¬ƒcients. Then derivatives of g with respect
to x also yield polynomials in x which have coeï¬ƒcients which are symmetric polynomials
in {vÎ² 1 , Â· Â· Â· , vÎ² m } having integer coeï¬ƒcients. Evaluating g at x = 0 must therefore yield
a polynomial which is symmetric in the {vÎ² 1 , Â· Â· Â· , vÎ² m } with integer coeï¬ƒcients. Since the
{Î² 1 , Â· Â· Â· , Î² m } are the roots of a polynomial having integer coeï¬ƒcients with leading coeï¬ƒcient v, it follows from Theorem F.1.4 that this last polynomial is an integer and so the
j th derivative of f given by (6.8) when evaluated at x = 0 yields an integer times p. Now
consider the case of the (p âˆ’ 1) derivative of f . The only nonzero term of f (j) (0) is the one
which comes from taking p âˆ’ 1 derivatives of xpâˆ’1 and so it reduces to
v mp (âˆ’1)

mp

(Î² 1 Î² 2 Â· Â· Â· Î² m )

m

Now Q (0) = v (âˆ’1) (Î² 1 Î² 2 Â· Â· Â· Î² m ) = u and so v p (âˆ’1)

mp

p
p

(Î² 1 Î² 2 Â· Â· Â· Î² m ) = up which yields

f (pâˆ’1) (0) = v mp up v âˆ’p = v p(mâˆ’1) up
Note this is not necessarily a multiple of p and in fact will not be so if p > u, v because p is
a prime number. It follows
n
âˆ‘

f (j) (0) = v p(mâˆ’1) up + m (p) p

j=0

where m (p) is some integer.
Now consider the other sum in (6.6),
c

m âˆ‘
n
âˆ‘

f (j) (Î² i )

i=1 j=0

Using the formula in (6.7) it follows that for j < p, f (j) (Î² i ) = 0. This is because for such
derivatives, each term will have that product of the (x âˆ’ Î² i ) in it. Next consider the case
where j â‰¥ p. In this case, the nonzero terms must involve at least p derivatives of the
expression
p
((x âˆ’ Î² 1 ) (x âˆ’ Î² 2 ) Â· Â· Â· (x âˆ’ Î² m ))
since otherwise, when evaluated at any Î² k the result would be 0. Hence the (p âˆ’ 1)! will
vanish from the denominator and so all coeï¬ƒcients of the polynomials in the Î² j and x will
be integers and in fact, there will be an extra factor of p left over. Thus the j th derivatives
for j â‰¥ p involve taking the k th derivative, k â‰¥ 0 with respect to x of
pv mp g (x, Î² 1 , Â· Â· Â· , Î² m )
where g (x, Î² 1 , Â· Â· Â· , Î² m ) is a polynomial in x having coeï¬ƒcients which are integers times
symmetric polynomials in the {Î² 1 , Â· Â· Â· , Î² m } . It follows that the k th derivative for k â‰¥ 0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

454

FIELDS AND FIELD EXTENSIONS

is also a polynomial in x having the same properties. Therefore, taking the k th derivative
where k corresponds to j â‰¥ p and adding, yields
m
âˆ‘

pv mp g,k (Î² i , Î² 1 , Â· Â· Â· , Î² m ) =

i=1

m
âˆ‘

f (j) (Î² i )

(6.9)

i=1

where g,k denotes the k th derivative of g taken with respect to x. Now
m
âˆ‘

g,k (Î² i , Î² 1 , Â· Â· Â· , Î² m )

i=1

is a symmetric polynomial in the {Î² 1 , Â· Â· Â· , Î² m } with no term having degree more than mp
and1 so by Corollary F.1.3 this is of the form
m
âˆ‘

âˆ‘

g,k (Î² i , Î² 1 , Â· Â· Â· , Î² m ) =

i=1

ak1 Â·Â·Â·km pk11 Â· Â· Â· pkmm

k1 ,Â·Â·Â· ,km

where the ak1 Â·Â·Â·km are integers and the pk are the elementary symmetric polynomials in
{Î² 1 , Â· Â· Â· , Î² m }. Recall these were roots of vxm + Â· Â· Â· + u and so from the deï¬nition of the
elementary symmetric polynomials given in Deï¬nition F.1.2, these pk are each an integer
divided by v, the integers being the coeï¬ƒcients of Q (x). Therefore, from (6.9)
m
âˆ‘

f (j) (Î² i ) = pv mp

i=1

m
âˆ‘

g,k (Î² i , Î² 1 , Â· Â· Â· , Î² m )

i=1

âˆ‘

= pv mp

ak1 Â·Â·Â·km pk11 Â· Â· Â· pkmm

k1 ,Â·Â·Â· ,km
mp

which is pv
times an expression which consists of integers times products of coeï¬ƒcients
of Q (x) divided by v raised to various powers, the sum of which is always no more than
mp. Therefore, it reduces to an integer multiple of p and so the same is true of
c

m âˆ‘
n
âˆ‘

f (j) (Î² i )

i=1 j=0

which just involves adding up these integer multiples of p. Therefore, (6.6) is of the form
Kv p(mâˆ’1) up + M (p) p
for some integer M (p). Summarizing, it follows
(
) n
m
m
âˆ‘
âˆ‘
âˆ‘
Î²i
c
I (Î² i ) = K + c
e
f (j) (0) + Kv p(mâˆ’1) up + M (p) p
i=1

i=1

j=0

where the left side is very small whenever p is large enough. Let p be larger than max (K, v, u) .
Since p is prime, it follows it cannot divide Kv p(mâˆ’1) up and so the last two terms must sum
to a nonzero integer and so the equation cannot hold unless
K +c

m
âˆ‘

eÎ² i Ì¸= 0 

i=1
1 Note

the claim about this being a symmetric polynomial is about the sum, not an individual term.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.3. TRANSCENDENTAL NUMBERS

455

Note this shows Ï€ is irrational. If Ï€ = k/m where k, m are integers, then both iÏ€ and
âˆ’iÏ€ are roots of the polynomial with integer coeï¬ƒcients,
m2 x2 + k 2
which would require from what was just shown that
0 Ì¸= 2 + eiÏ€ + eâˆ’iÏ€
which is not the case since the sum on the right equals 0.
The following corollary follows from this.
Corollary F.3.3 Let K and ci for i = 1, Â· Â· Â· , n be nonzero integers. For each k between 1
m(k)
and n let {Î² (k)i }i=1 be the roots of a polynomial with integer coeï¬ƒcients,
Qk (x) â‰¡ vk xmk + Â· Â· Â· + uk
where vk , uk Ì¸= 0. Then
ï£«
ï£¶
ï£«
ï£¶
ï£«
ï£¶
m1
m2
mn
âˆ‘
âˆ‘
âˆ‘
K + c1 ï£­
eÎ²(1)j ï£¸ + c2 ï£­
eÎ²(2)j ï£¸ + Â· Â· Â· + cn ï£­
eÎ²(n)j ï£¸ Ì¸= 0.
j=1

j=1

j=1

Proof: Deï¬ning fk (x) and Ik (s) as in Lemma F.3.2, it follows from Lemma F.3.2 that
for each k = 1, Â· Â· Â· , n,
ck

mk
âˆ‘

(
Ik (Î² (k)i ) =

Kk + ck

i=1

mk
âˆ‘

e

Î²(k)i

) deg(fk )
âˆ‘

i=1

âˆ‘

j=0

deg(fk )

âˆ’Kk

(j)

fk (0)

(j)

fk (0) âˆ’ ck

j=0

mk deg(f
âˆ‘k )
âˆ‘
i=1

(j)

fk (Î² (k)i )

j=0

This is exactly the same computation as in the beginning of that lemma except one adds
âˆ‘deg(f ) (j)
âˆ‘deg(f ) (j)
and subtracts Kk j=0 k fk (0) rather than K j=0 k fk (0) where the Kk are chosen
such that their sum equals K. By Lemma F.3.2,
(
)
mk
mk
(
)
âˆ‘
âˆ‘
(m âˆ’1)p p
Î²(k)i
ck
Ik (Î² (k)i ) = Kk + ck
e
vk k
u k + Nk p
i=1

i=1

(

(mk âˆ’1)p p
uk

âˆ’Kk vk
and so
ck

mk
âˆ‘

(
Ik (Î² (k)i ) =

Kk + ck

i=1

)
+ Nk p âˆ’ ck Nkâ€² p

mk
âˆ‘

)
e

Î²(k)i

(

(mk âˆ’1)p p
uk

vk

)
+ Nk p

i=1
(m âˆ’1)p p
âˆ’Kk vk k
uk

+ Mk p

for some integer Mk . By multiplying each Qk (x) by a suitable constant, it can be assumed
without loss of generality that all the vkmk âˆ’1 uk are equal to a constant integer U . Then the
above equals
(
)
mk
mk
âˆ‘
âˆ‘
Î²(k)i
ck
Ik (Î² (k)i ) = Kk + ck
e
(U p + Nk p)
i=1

Saylor URL: http://www.saylor.org/courses/ma212/

i=1

The Saylor Foundation

456

FIELDS AND FIELD EXTENSIONS

âˆ’Kk U p + Mk p
Adding these for all k gives
n
âˆ‘

ck

k=1

mk
âˆ‘

(
Ik (Î² (k)i ) = U p

K+

i=1

n
âˆ‘

ck

k=1

+

n
âˆ‘

(
Nk p Kk + ck

mk
âˆ‘

)
eÎ²(k)i

âˆ’ KU p + M p

i=1
mk
âˆ‘

)
Î²(k)i

e

(6.10)

i=1

k=1

For large p it follows from Lemma F.3.2 that the left side is very small. If
K+

n
âˆ‘

ck

k=1

âˆ‘n

mk
âˆ‘

eÎ²(k)i = 0

i=1

âˆ‘mk

then k=1 ck i=1 eÎ²(k)i is an integer and so the last term in (6.10) is an integer times p.
Thus for large p it reduces to
small number = âˆ’KU p + Ip
where I is an integer. Picking prime p > max (U, K) it follows âˆ’KU p + Ip is a nonzero
integer and this contradicts the left side being a small number less than 1 in absolute value.

Next is an even more interesting Lemma which follows from the above corollary.
Lemma F.3.4 If b0 , b1 , Â· Â· Â· , bn are non zero integers, and Î³ 1 , Â· Â· Â· , Î³ n are distinct algebraic
numbers, then
b0 eÎ³ 0 + b1 eÎ³ 1 + Â· Â· Â· + bn eÎ³ n Ì¸= 0
Proof: Assume
b0 eÎ³ 0 + b1 eÎ³ 1 + Â· Â· Â· + bn eÎ³ n = 0

(6.11)

Divide by eÎ³ 0 and letting K = b0 ,
K + b1 eÎ±(1) + Â· Â· Â· + bn eÎ±(n) = 0

(6.12)

where Î± (k) = Î³ k âˆ’ Î³ 0 . These are still distinct algebraic numbers none of which is 0 thanks
to Theorem 8.3.32. Therefore, Î± (k) is a root of a polynomial
vk xmk + Â· Â· Â· + uk

(6.13)

having integer coeï¬ƒcients, vk , uk Ì¸= 0. Recall algebraic numbers were deï¬ned as roots of
polynomial equations having rational coeï¬ƒcients. Just multiply by the denominators to get
one with integer coeï¬ƒcients. Let the roots of this polynomial equation be
{
}
Î± (k)1 , Â· Â· Â· , Î± (k)mk
and suppose they are listed in such a way that Î± (k)1 = Î± (k). Letting ik be an integer in
{1, Â· Â· Â· , mk } it follows from the assumption (6.11) that
(
)
âˆ
K + b1 eÎ±(1)i1 + b2 eÎ±(2)i2 + Â· Â· Â· + bn eÎ±(n)in = 0
(6.14)
(i1 ,Â·Â·Â· ,in )
ik âˆˆ{1,Â·Â·Â· ,mk }

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.3. TRANSCENDENTAL NUMBERS

457

This is because one of the factors is the one occurring in (6.12) when ik = 1 for every k. The
product is taken over all distinct ordered lists (i1 , Â· Â· Â· , in ) where ik is as indicated. Expand
this possibly huge product. This will yield something like the following.
(
)
(
)
K â€² + c1 eÎ²(1)1 + Â· Â· Â· + eÎ²(1)Âµ(1) + c2 eÎ²(2)1 + Â· Â· Â· + eÎ²(2)Âµ(2) + Â· Â· Â· +
(
)
cN eÎ²(N )1 + Â· Â· Â· + eÎ²(N )Âµ(N ) = 0

(6.15)

These integers cj come from products of the bi and K. The Î² (i)j are the distinct exponents
which result. Note that a typical term in this product (6.14) would be something like
Î²(j)

integer
z
}|r
{
z
}|
{
Î±
(k
)
+
Î±
(k
)
Â·
Â·
Â·
+
Î±
(k
)
1
2
nâˆ’p
i1
i2
inâˆ’p
K p+1 bk1 Â· Â· Â· bknâˆ’p e

the kj possibly not distinct and each ik âˆˆ {1, Â· Â· Â· , mik }. Other terms of this sort are
K p+1 bk1 Â· Â· Â· bknâˆ’p e
K

p+1

Î±(k1 )iâ€² +Î±(k2 )iâ€² Â·Â·Â·+Î±(knâˆ’p )iâ€²

bk1 Â· Â· Â· bknâˆ’p e

1

2

nâˆ’p

,

Î±(k1 )1 +Î±(k2 )1 Â·Â·Â·+Î±(knâˆ’p )1

where each iâ€²k is another index in{{1, Â· Â· Â· , mik } and
} so forth. A given j in the sum of (6.15)
corresponds to such a choice of bk1 , Â· Â· Â· , bknâˆ’p which leads to K p+1 bk1 Â· Â· Â· bknâˆ’p times a
sum of exponentials like those just described. Since the product in (6.14) is taken over all
choices ik âˆˆ {1, Â· Â· Â· , mk } , it follows that if you switch Î± (r)i and Î± (r)j , two of the roots of
the polynomial
vr xmr + Â· Â· Â· + ur
mentioned above, the result in (6.15) would be the same except for permuting the
Î² (s)1 , Î² (s)2 , Â· Â· Â· , Î² (s)Âµ(s) .
Thus a symmetric polynomial in
Î² (s)1 , Î² (s)2 , Â· Â· Â· , Î² (s)Âµ(s)
is also a symmetric polynomial in the Î± (k)1 , Î± (k)2 , Â· Â· Â· , Î± (k)mk for each k. Thus for a
given r, Î² (r)1 , Â· Â· Â· , Î² (r)Âµ(r) are roots of the polynomial
(
)
(x âˆ’ Î² (r)1 ) (x âˆ’ Î² (r)2 ) Â· Â· Â· x âˆ’ Î² (r)Âµ(r)
whose coeï¬ƒcients are symmetric polynomials in the Î² (r)j which is a symmetric polynomial
in the Î± (k)j , j = 1, Â· Â· Â· , mk for each k. Letting g be one of these symmetric polynomials
and writing it in terms of the Î± (k)i you would have
âˆ‘
l
l
l
Al1 Â·Â·Â·ln Î± (n)11 Î± (n)22 Â· Â· Â· Î± (n)mnn
l1 ,Â·Â·Â· ,ln

where Al1 Â·Â·Â·ln is a symmetric polynomial in Î± (k)j , j = 1, Â· Â· Â· , mk for each k â‰¤ n âˆ’ 1. These
coeï¬ƒcients are in the ï¬eld (Proposition 8.3.31) Q [A (1) , Â· Â· Â· , A (n âˆ’ 1)] where A (k) denotes
{
}
Î± (k)1 , Â· Â· Â· , Î± (k)mk
and so from Proposition F.1.3, the above symmetric polynomial is of the form
âˆ‘
(
)
(
)
k
Bk1 Â·Â·Â·kmn pk11 Î± (n)1 , Â· Â· Â· , Î± (n)mn Â· Â· Â· pmmnn Î± (n)1 , Â· Â· Â· , Î± (n)mn
(k1 Â·Â·Â·kmn )

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

458

FIELDS AND FIELD EXTENSIONS

where Bk1 Â·Â·Â·kmn is a symmetric polynomial in Î± (k)j , j = 1, Â· Â· Â· , mk for each k â‰¤ n âˆ’ 1. Now
do for each Bk1 Â·Â·Â·kmn what was just done for g featuring this time
{
}
Î± (n âˆ’ 1)1 , Â· Â· Â· , Î± (n âˆ’ 1)mnâˆ’1
and continuing this way, it must be the case that eventually you have a sum of integer
multiples of products of elementary symmetric polynomials in Î± (k)j , j = 1, Â· Â· Â· , mk for
each k â‰¤ n. By Theorem F.1.4, these are each rational numbers. Therefore, each such g is
a rational number and so the Î² (r)j are algebraic. Now (6.15) contradicts Corollary F.3.3.

Note this lemma is suï¬ƒcient to prove Lindemannâ€™s theorem that Ï€ is transcendental.
Here is why. If Ï€ is algebraic, then so is iÏ€ and so from this lemma, e0 + eiÏ€ Ì¸= 0 but this is
not the case because eiÏ€ = âˆ’1.
The next theorem is the main result, the Lindemannn Weierstrass theorem.
Theorem F.3.5 Suppose a (1) , Â· Â· Â· , a (n) are nonzero algebraic numbers and suppose
Î± (1) , Â· Â· Â· , Î± (n)
are distinct algebraic numbers. Then
a (1) eÎ±(1) + a (2) eÎ±(2) + Â· Â· Â· + a (n) eÎ±(n) Ì¸= 0
Proof: Suppose a (j) â‰¡ a (j)1 is a root of the polynomial
v j x mj + Â· Â· Â· + u j
where vj , uj Ì¸= 0. Let the roots of this polynomial be a (j)1 , Â· Â· Â· , a (j)mj . Suppose to the
contrary that
a (1)1 eÎ±(1) + a (2)1 eÎ±(2) + Â· Â· Â· + a (n)1 eÎ±(n) = 0
Then consider the big product
(
)
âˆ
a (1)i1 eÎ±(1) + a (2)i2 eÎ±(2) + Â· Â· Â· + a (n)in eÎ±(n)

(6.16)

(i1 ,Â·Â·Â· ,in )
ik âˆˆ{1,Â·Â·Â· ,mk }

the product taken over all ordered lists (i1 , Â· Â· Â· , in ) . This product equals
0 = b1 eÎ²(1) + b2 eÎ²(2) + Â· Â· Â· + bN eÎ²(N )

(6.17)

where the Î² (j) are the distinct exponents which result. The Î² (i) are clearly algebraic
because they are the sum of the Î± (i). Since the product in (6.16) is taken for all ordered
lists as described above, it follows that for a given k,if Î± (k)i is switched with Î± (k)j , that is,
two of the roots of vk xmk +Â· Â· Â·+uk are switched, then the product is unchanged and so (6.17)
is also unchanged. Thus each bk is a symmetric polynomial in the a (k)j , j = 1, Â· Â· Â· , mk for
each k. It follows
âˆ‘
j
j
bk =
Aj1 ,Â·Â·Â· ,jmn a (n)11 Â· Â· Â· a (n)mmnn
(j1 ,Â·Â·Â· ,jmn )

{
}
and this is symmetric in the a (n)1 , Â· Â· Â· , a (n)mn the coeï¬ƒcients Aj1 ,Â·Â·Â· ,jmn being in the
ï¬eld (Proposition 8.3.31) Q [A (1) , Â· Â· Â· , A (n âˆ’ 1)] where A (k) denotes
a (k)1 , Â· Â· Â· , a (k)mk

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.4. MORE ON ALGEBRAIC FIELD EXTENSIONS

459

and so from Proposition F.1.3,
âˆ‘
(
)
(
)
j
bk =
Bj1 ,Â·Â·Â· ,jmn pj11 a (n)1 Â· Â· Â· a (n)mn Â· Â· Â· pmmnn a (n)1 Â· Â· Â· a (n)mn
(j1 ,Â·Â·Â· ,jmn )

{
where the Bj1 ,Â·Â·Â· ,jmn are symmetric in

}mk
a (k)j

j=1

for each k â‰¤ n âˆ’ 1. Now doing to

Bj1 ,Â·Â·Â· ,jmn what was just done to bk and continuing this way, it follows bk is a ï¬nite sum of
{
}mk
integers times elementary polynomials in the various a (k)j
for k â‰¤ n. By Theorem
j=1

F.1.4 this is a rational number. Thus bk is a rational number. Multiplying by the product
of all the denominators, it follows there exist integers ci such that
0 = c1 eÎ²(1) + c2 eÎ²(2) + Â· Â· Â· + cN eÎ²(N )
which contradicts Lemma F.3.4. 
This theorem is suï¬ƒcient to show e is transcendental. If it were algebraic, then
eeâˆ’1 + (âˆ’1) e0 Ì¸= 0
but this is not the case. If a Ì¸= 1 is algebraic, then ln (a) is transcendental. To see this, note
that
1eln(a) + (âˆ’1) ae0 = 0
which cannot happen according to the above theorem. If a is algebraic and sin (a) Ì¸= 0, then
sin (a) is transcendental because
1 ia
1
e âˆ’ eâˆ’ia + (âˆ’1) sin (a) e0 = 0
2i
2i
which cannot occur if sin (a) is algebraic. There are doubtless other examples of numbers
which are transcendental by this amazing theorem.

F.4

More On Algebraic Field Extensions

The next few sections have to do with ï¬elds and ï¬eld extensions. There are many linear
algebra techniques which are used in this discussion and it seems to me to be very interesting.
However, this is deï¬nitely far removed from my own expertise so there may be some parts of
this which are not too good. I am following various algebra books in putting this together.
Consider the notion of splitting ï¬elds. It is desired to show that any two are isomorphic,
meaning that there exists a one to one and onto mapping from one to the other which
preserves all the algebraic structure. To begin with, here is a theorem about extending
homomorphisms. [17]
Deï¬nition F.4.1 Suppose F, FÌ„ are two ï¬elds and that f : F â†’ FÌ„ is a homomorphism. This
means that
f (xy) = f (x) f (y) , f (x + y) = f (x) + f (y)
An isomorphism is a homomorphism which is one to one and onto. A monomorphism is
a homomorphism which is one to one. An automorphism is an isomorphism of a single
ï¬eld. Sometimes people use the symbol â‰ƒ to indicate something is an isomorphism. Then if
p (x) âˆˆ F [x] , say
n
âˆ‘
p (x) =
ak xk ,
k=0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

460

FIELDS AND FIELD EXTENSIONS

pÌ„ (x) will be the polynomial in FÌ„ [x] deï¬ned as
pÌ„ (x) â‰¡

n
âˆ‘

f (ak ) xk .

k=0

Also consider f as a homomorphism of F [x] and FÌ„ [x] in the obvious way.
f (p (x)) = pÌ„ (x)
The following is a nice theorem which will be useful.
Theorem F.4.2 Let F be a ï¬eld and let r be algebraic over F. Let p (x) be the minimal
polynomial of r. Thus p (r) = 0 and p (x) is monic and no nonzero polynomial having
coeï¬ƒcients in F of smaller degree has r as a root. In particular, p (x) is irreducible over F.
Then deï¬ne f : F [x] â†’ F [r] , the polynomials in r by
(m
)
m
âˆ‘
âˆ‘
f
ai x i â‰¡
ai ri
i=0

i=0

Then f is a homomorphism. Also, deï¬ning g : F [x] / (p (x)) by
g ([q (x)]) â‰¡ f (q (x)) â‰¡ q (r)
it follows that g is an isomorphism from the ï¬eld F [x] / (p (x)) to F [r] .
Proof: First of all, consider why f is a homomorphism. The preservation of sums is
obvious. Consider products.
ï£«
ï£¶
ï£«
ï£¶
âˆ‘
âˆ‘
âˆ‘
âˆ‘
fï£­
ai xi
bj xj ï£¸ = f ï£­
ai bj xi+j ï£¸ =
ai bj ri+j
i

j

i,j

=

âˆ‘

ai ri

i

ij

âˆ‘

(
bj rj = f

j

âˆ‘

ï£¶
) ï£«
âˆ‘
ai xi f ï£­
bj xj ï£¸

i

j

Thus it is clear that f is a homomorphism.
First consider why g is even well deï¬ned. If [q (x)] = [q1 (x)] , this means that
q1 (x) âˆ’ q (x) = p (x) l (x)
for some l (x) âˆˆ F [x]. Therefore,
f (q1 (x))

= f (q (x)) + f (p (x) l (x))
= f (q (x)) + f (p (x)) f (l (x))
â‰¡ q (r) + p (r) l (r) = q (r) = f (q (x))

Now from this, it is obvious that g is a homomorphism.
g ([q (x)] [q1 (x)])
g ([q (x)]) g ([q1 (x)])

= g ([q (x) q1 (x)]) = f (q (x) q1 (x)) = q (r) q1 (r)
â‰¡ q (r) q1 (r)

Similarly, g preserves sums. Now why is g one to one? It suï¬ƒces to show that if g ([q (x)]) = 0,
then [q (x)] = 0. Suppose then that
g ([q (x)]) â‰¡ q (r) = 0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.4. MORE ON ALGEBRAIC FIELD EXTENSIONS

461

Then
q (x) = p (x) l (x) + Ï (x)
where the degree of Ï (x) is less than the degree of p (x) or else Ï (x) = 0. If Ï (x) Ì¸= 0, then
it follows that
Ï (r) = 0
and Ï (x) has smaller degree than that of p (x) which contradicts the deï¬nition of p (x) as the
minimal polynomial of r. Since p (x) is irreducible, F [x] / (p (x)) is a ï¬eld. It is clear that g
is onto. Therefore, F [r] is a ï¬eld also. (This was shown earlier by diï¬€erent reasoning.) 
Here is a diagram of what the following theorem says.
Extending f to g
f

F

â†’

p (x)âˆ‘
âˆˆ F [x]
n
p (x) = k=0 ak xk
p (r) = 0
F [r]

FÌ„

â‰ƒ
f

â†’ âˆ‘ pÌ„ (x) âˆˆ FÌ„ [x]
n
k
â†’
k=0 f (ak ) x = pÌ„ (x)
pÌ„ (rÌ„) = 0
g
â†’
FÌ„ [rÌ„]
â‰ƒ
g

â†’

r

rÌ„

One such g for each rÌ„
Theorem F.4.3 Let f : F â†’ FÌ„ be an isomorphism of the two ï¬elds. Let r be algebraic
over F with minimal polynomial p (x) and suppose there exists rÌ„ algebraic over FÌ„ such that
pÌ„ (rÌ„) = 0. Then there exists an isomorphism g : F [r] â†’ FÌ„ [rÌ„] which agrees with f on F. If
g : F [r] â†’ FÌ„ [rÌ„] is an isomorphism which agrees with f on F and if Î± ([k (x)]) â‰¡ k (r) is the
homomorphism mapping F [x] / (p (x)) to F [r] , then there must exist rÌ„ such that pÌ„ (rÌ„) = 0
and g = Î²Î±âˆ’1 where Î²
Î² : F [x] / (p (x)) â†’ FÌ„ [rÌ„]
is given by Î² ([k (x)]) = kÌ„ (rÌ„) . In particular, g (r) = rÌ„.
Proof: From Theorem F.4.2, there exists Î±, an isomorphism in the following picture,
Î± ([k (x)]) = k (r).
Î±

Î²

F [r] â† F [x] / (p (x)) â†’ FÌ„ [rÌ„]
where Î² ([k (x)]) â‰¡ kÌ„ (rÌ„) . (kÌ„ (x) comes from f as described in the above deï¬nition.) This Î²
is a well deï¬ned monomorphism because of the assumption that pÌ„ (rÌ„) = 0. This needs to be
veriï¬ed. Assume then that it is so. Then just let g = Î²Î±âˆ’1 .
Why is Î² well deï¬ned? Suppose [k (x)] = [k â€² (x)] so that k (x) âˆ’ k â€² (x) = l (x) p (x) . Then
since f is a homomorphism,
kÌ„ (x) âˆ’ kÌ„ â€² (x) = Â¯l (x) pÌ„ (x) , kÌ„ (rÌ„) âˆ’ gÌ„ kÌ„ â€² (rÌ„) = Â¯l (rÌ„) pÌ„ (rÌ„) = 0
so Î² is indeed well deï¬ned. It is clear from the deï¬nition that Î² is a homomorphism. Suppose
Î² ([k (x)]) = 0. Does it follow that [k (x)] = 0? By assumption, gÌ„ (rÌ„) = 0 and also,
kÌ„ (x) = pÌ„ (x) Â¯l (x) + ÏÌ„ (x)
where the degree of ÏÌ„ (x) is less than the degree of pÌ„ (x) or else it equals 0. But then, since
f is an isomorphism,
k (x) = p (x) l (x) + Ï (x)

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

462

FIELDS AND FIELD EXTENSIONS

where the degree of Ï (x) is less than the degree of p (x) . However, the above shows that
Ï (r) = 0 contrary to p (x) being the minimal polynomial. Hence Ï (x) = 0 and this implies
that [k (x)] = 0. Thus Î² is one to one and a homomorphism. Hence g = Î²Î±âˆ’1 works if it
is also onto. However, it is clear that Î±âˆ’1 is onto and that Î² is onto. Hence the desired
extension exists.
Now suppose such an isomorphism g exists. Then rÌ„ must equal g (r) and
0 = g (p (r)) = pÌ„ (g (r)) = pÌ„ (rÌ„)
Hence, Î² can be deï¬ned as above as Î² ([k (x)]) â‰¡ kÌ„ (rÌ„) relative to this rÌ„ â‰¡ g (r) and
Î²Î±âˆ’1 (k (r)) â‰¡ Î² ([k (x)]) â‰¡ kÌ„ (g (r)) = g (k (r))

What is the meaning of the above in simple terms? It says that the monomorphisms
from F [r] to a ï¬eld KÌ„ containing FÌ„ correspond to the roots of pÌ„ (x) in KÌ„. That is, for each
root of pÌ„ (x), there is a monomorphism and for each monomorphism, there is a root. Also,
for each root rÌ„ of pÌ„ (x) in KÌ„, there is an isomorphism from F [r] to FÌ„ [rÌ„].
Note that if p (x) is a monic irreducible polynomial, then it is the minimal polynomial
for each of its roots. This is the situation which is about to be considered. It involves the
splitting ï¬elds K, KÌ„ of p (x) , pÌ„ (x) where Î· is an isomorphism of F and FÌ„ as described above.
See [17]. Here is a little diagram which describes what this theorem says.
Deï¬nition F.4.4 The symbol [K : F] where K is a ï¬eld extension of F means the dimension
of the vector space K with ï¬eld of scalars F.
Î·

F

â†’

FÌ„

p (x)

â€œÎ·p (x) = pÌ„ (x)â€

pÌ„ (x)

F [r1 , Â· Â· Â· , rn ]

Î¶i

FÌ„ [r1 , Â· Â· Â· , rn ]

â‰ƒ

{
i = 1, Â· Â· Â· , m,

â†’
â‰ƒ

m â‰¤ [K : F]
m = [K : F] , rÌ„i Ì¸= rÌ„j

Theorem F.4.5 Let Î· be an isomorphism from F to FÌ„ and let K = F [r1 , Â· Â· Â· , rn ] , KÌ„ =
FÌ„ [rÌ„1 , Â· Â· Â· , rÌ„n ] be splitting ï¬elds of p (x) and pÌ„ (x) respectively. Then there exist at most
[K : F] isomorphisms Î¶ i : K â†’ KÌ„ which extend Î·. If {rÌ„1 , Â· Â· Â· , rÌ„n } are distinct, then there
exist exactly [K : F] isomorphisms of the above sort. In either case, the two splitting ï¬elds
are isomorphic with any of these Î¶ i serving as an isomorphism.
Proof: Suppose [K : F] = 1. Say a basis for K is {r} . Then {1, r} is dependent and so
there exist a, b âˆˆ F, not both zero such that a + br = 0. Then it follows that r âˆˆ F and so in
this case F = K. Then the isomorphism which extends Î· is just Î· itself and there is exactly
1 isomorphism.
Next suppose [K : F] > 1. Then p (x) has an irreducible factor over F of degree larger
than 1, q (x). If not, you would have
p (x) = xn + anâˆ’1 xnâˆ’1 + Â· Â· Â· + an
and it would factor as
= (x âˆ’ r1 ) Â· Â· Â· (x âˆ’ rn )
with each rj âˆˆ F, so F = K contrary to [K : F] > 1.Without loss of generality, let the roots
of q (x) in K be {r1 , Â· Â· Â· , rm }. Thus
q (x) =

m
âˆ

(x âˆ’ ri ) , p (x) =

i=1

Saylor URL: http://www.saylor.org/courses/ma212/

n
âˆ

(x âˆ’ ri )

i=1

The Saylor Foundation

F.4. MORE ON ALGEBRAIC FIELD EXTENSIONS

463

Now qÌ„ (x) deï¬ned analogously to p (x) , also has degree at least 2. Furthermore, it divides
pÌ„ (x) all of whose roots are in KÌ„. Denote the roots of qÌ„ (x) in K as {rÌ„1 , Â· Â· Â· , rÌ„m } where they
are counted according to multiplicity.
Then from Theorem F.4.3, there exist k â‰¤ m one to one homomorphisms Î¶ i mapping
F [r1 ] to KÌ„, one for each distinct root of qÌ„ (x) in KÌ„. If the roots of pÌ„ (x) are distinct, then
this is suï¬ƒcient to imply that the roots of qÌ„ (x) are also distinct, and k = m. Otherwise,
maybe k < m. (It is conceivable that qÌ„ (x) might have repeated roots in KÌ„.) Then
[K : F] = [K : F [r1 ]] [F [r1 ] : F]
and since the degree of q (x) > 1 and q (x) is irreducible, this shows that [F [r1 ] : F] = m > 1
and so
[K : F [r1 ]] < [K : F]
Therefore, by induction, each of these k â‰¤ m = [F [r1 ] : F] one to one homomorphisms
extends to an isomorphism from K to KÌ„ and for each of these Î¶ i , there are no more than
[K : F [r1 ]] of these isomorphisms extending F. If the roots of pÌ„ (x) are distinct, then there
are exactly m of these Î¶ i and for each, there are [K : F [r1 ]] extensions. Therefore, if the
roots of pÌ„ (x) are distinct, this has identiï¬ed
[K : F [r1 ]] m = [K : F [r1 ]] [F [r1 ] : F] = [K : F]
isomorphisms of K to KÌ„ which agree with Î· on F. If the roots of pÌ„ (x) are not distinct, then
maybe there are fewer than [K : F] extensions of Î·.
Is this all of them? Suppose Î¶ is such an isomorphism of K and KÌ„. Then consider its
restriction to F [r1 ] . By Theorem F.4.3, this restriction must coincide with one of the Î¶ i
chosen earlier. Then by induction, Î¶ is one of the extensions of the Î¶ i just mentioned. 
Deï¬nition F.4.6 Let K be a ï¬nite dimensional extension of a ï¬eld F such that every element of K is algebraic over F, that is, each element of K is a root of some polynomial
in F [x]. Then K is called a normal extension if for every k âˆˆ K all roots of the minimal
polynomial of k are contained in K.
So what are some ways to tell a ï¬eld is a normal extension? It turns out that if K is a
splitting ï¬eld of f (x) âˆˆ F [x] , then K is a normal extension. I found this in [17]. This is an
amazing result.
Proposition F.4.7 Let K be a splitting ï¬eld of f (x) âˆˆ F [x]. Then K is a normal extension.
In fact, if L is an intermediate ï¬eld between F and K, then L is also a normal extension of
F.
Proof: Let r âˆˆ K be a root of g (x), an irreducible monic polynomial in F [x]. It is
required to show that every other root of g (x) is in K. Let the roots of g (x) in a splitting
ï¬eld be {r1 = r, r2 , Â· Â· Â· , rm }. Now g (x) is the minimal polynomial of rj over F because g (x)
is irreducible. Recall why this was. If p (x) is the minimal polynomial of rj ,
g (x) = p (x) l (x) + r (x)
where r (x) either is 0 or it has degree less than the degree of p (x) . However, r (rj ) = 0 and
this is impossible if p (x) is the minimal polynomial. Hence r (x) = 0 and now it follows
that g (x) was not irreducible unless l (x) = 1.
By Theorem F.4.3, there exists an isomorphism Î· of F [r1 ] and F [rj ] which ï¬xes F and
maps r1 to rj . Now K [r1 ] and K [rj ] are splitting ï¬elds of f (x) over F [r1 ] and F [rj ] respectively. By Theorem F.4.5, the two ï¬elds K [r1 ] and K [rj ] are isomorphic, the isomorphism,
Î¶ extending Î·. Hence
[K [r1 ] : K] = [K [rj ] : K]

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

464

FIELDS AND FIELD EXTENSIONS

But r1 âˆˆ K and so K [r1 ] = K. Therefore, K = K [rj ] and so rj is also in K. Thus all the
roots of g (x) are actually in K. Consider the last assertion.
Suppose r = r1 âˆˆ L where the minimal polynomial for r is denoted by q (x). Then
letting the roots of q (x) in K be {r1 , Â· Â· Â· , rm }. By Theorem F.4.3 applied to the identity
map on L, there exists an isomorphism Î¸ : L [r1 ] â†’ L [rj ] which ï¬xes L and takes r1 to rj .
But this implies that
1 = [L [r1 ] : L] = [L [rj ] : L]
Hence rj âˆˆ L also. Since r was an arbitrary element of L, this shows that L is normal. 
Deï¬nition F.4.8 When you have F [a1 , Â· Â· Â· , am ] with each ai algebraic so that F [a1 , Â· Â· Â· , am ]
is a ï¬eld, you could consider
m
âˆ
f (x) â‰¡
fi (x)
i=1

where fi (x) is the minimal polynomial of ai . Then if K is a splitting ï¬eld for f (x) , this K
is called the normal closure. It is at least as large as F [a1 , Â· Â· Â· , am ] and it has the advantage
of being a normal extension.

F.5

The Galois Group

In the case where F = FÌ„, the above suggests the following deï¬nition.
Deï¬nition F.5.1 When K is a splitting ï¬eld for a polynomial p (x) having coeï¬ƒcients in
F, we say that K is a splitting ï¬eld of p (x) over the ï¬eld F. Let K be a splitting ï¬eld of
p (x) over the ï¬eld F. Then G (K, F) denotes the group of automorphisms of K which leave
F ï¬xed. For a ï¬nite set S, denote by |S| as the number of elements of S. More generally,
when K is a ï¬nite extension of L, denote by G (K, L) the group of automorphisms of K
which leave L ï¬xed.
It is shown later that G (K, F) really is a group according to the strict deï¬nition of a
group. For right now, just regard it as a set of automorphisms which keeps F ï¬xed. Theorem
F.4.5 implies the following important result.
Theorem F.5.2 Let K be a splitting ï¬eld of p (x) over the ï¬eld F. Then
|G (K, F)| â‰¤ [K : F]
When the roots of p (x) are distinct, equality holds in the above.
So how large is |G (K, F)| in case p (x) is a polynomial of degree n which has n distinct
roots? Let p (x) be a monic polynomial with roots in K, {r1 , Â· Â· Â· , rn } and suppose that none
of the ri is in F. Thus
xn + a1 xnâˆ’1 + a2 xnâˆ’2 + Â· Â· Â· + an
n
âˆ
=
(x âˆ’ rk ) , ai âˆˆ F

p (x) =

k=1

Thus K consists of all rational functions in the r1 , Â· Â· Â· , rn . Let Ïƒ be a mapping from
{r1 , Â· Â· Â· , rn } to {r1 , Â· Â· Â· , rn } , say rj â†’ rij . In other words Ïƒ produces a permutation of
these roots. Consider the following way of obtaining something in G (K, F) from Ïƒ. If you
have a typical thing in K, you can obtain another thing in K by replacing each rj with rij in

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.5. THE GALOIS GROUP

465

a rational function, a quotient of two polynomials which have coeï¬ƒcients in F. Furthermore,
if you do this, you can see right away that the resulting map form K to K is obviously an automorphism, preserving the operations of multiplication and addition. Does it keep F ï¬xed?
Of course. You donâ€™t change the coeï¬ƒcients of the polynomials in the rational function
which are always in F. Thus every permutation of the roots determines an automorphism
of K. Now suppose Ïƒ is an automorphism of K. Does it determine a permutation of the
roots?
0 = Ïƒ (p (ri )) = Ïƒ (p (Ïƒ (ri )))
and so Ïƒ (ri ) is also a root, say rij . Thus it is clear that each Ïƒ âˆˆ G (K, F) determines
a permutation of the roots. Since the roots are distinct, it follows that |G (K, F)| equals
the number of permutations of {r1 , Â· Â· Â· , rn } which is n! and that there is a one to one
correspondence between the permutations of the roots and G (K, F) . More will be done on
this later after discussing permutation groups.
This is a good time to make a very important observation about irreducible polynomials.
Lemma F.5.3 Suppose q (x) Ì¸= p (x) are both irreducible polynomials over a ï¬eld F. Then
for K a ï¬eld which contains all the roots of both polynomials, there is no root common to
both p (x) and q (x).
Proof: If l (x) is a monic polynomial which divides them both, then l (x) must equal
1. Otherwise, it would equal p (x) and q (x) which would require these two to be equal.
Thus p (x) and q (x) are relatively prime and there exist polynomials a (x) , b (x) having
coeï¬ƒcients in F such that
a (x) p (x) + b (x) q (x) = 1
Now if p (x) and q (x) share a root r, then (x âˆ’ r) divides both sides of the above in K [x],
but this is impossible. 
Now here is an important deï¬nition of a class of polynomials which yield equality in the
inequality of Theorem F.5.2.
Deï¬nition F.5.4 Let p (x) be a polynomial having coeï¬ƒcients in a ï¬eld F. Also let K be a
splitting ï¬eld. Then p (x) is separable if it is of the form
p (x) =

m
âˆ

qi (x)

ki

i=1

where each qi (x) is irreducible over F and each qi (x) has distinct roots in K. From the
above lemma, no two qi (x) share a root. Thus
p1 (x) â‰¡

m
âˆ

qi (x)

i=1

has distinct roots in K.
For example, consider the case where F = Q and the polynomial is of the form
(

x2 + 1

)2 ( 2
)2
x âˆ’ 2 = x8 âˆ’ 2x6 âˆ’ 3x4 + 4x2 + 4

[ âˆš ]
Then let K be the splitting ï¬eld over Q, Q i, 2 .The polynomials x2 + 1 and x2 âˆ’ 2 are
irreducible over Q and each has distinct roots in K.
This is also a convenient time to show that G (K, F) for K a ï¬nite extension of F really
is a group. First, here is the deï¬nition.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

466

FIELDS AND FIELD EXTENSIONS

Deï¬nition F.5.5 A group G is a nonempty set with an operation, denoted here as Â· such
that the following axioms hold.
1. For Î±, Î², Î³ âˆˆ G, (Î± Â· Î²) Â· Î³ = Î± Â· (Î² Â· Î³) . We usually donâ€™t bother to write the Â·.
2. There exists Î¹ âˆˆ G such that Î±Î¹ = Î¹Î± = Î±
3. For every Î± âˆˆ G, there exists Î±âˆ’1 âˆˆ G such that Î±Î±âˆ’1 = Î±âˆ’1 Î± = Î¹.
Then why is G â‰¡ G (K, F) , where K is a ï¬nite extension of F, a group? If you simply
look at the automorphisms of K then it is obvious that this is a group with the operation
being composition. Also, from Theorem F.4.5 |G (K, F)| is ï¬nite. Clearly Î¹ âˆˆ G. It is
just the automorphism which takes everything to itself. The operation in this case is just
composition. Thus the associative law is obvious. What about the existence of the inverse?
Clearly, you can deï¬ne the inverse of Î±, but does it ï¬x F? If Î± = Î¹, then the inverse is clearly
Î¹. Otherwise, consider Î±, Î±2 , Â· Â· Â· . Since |G (K, F)| is( ï¬nite,
is a repeat. Thus
)meventually there
to get Î¹ = Î±Î±nâˆ’m . Hence Î±âˆ’1 is
Î±m = Î±n , n > m. Simply multiply on the left by Î±âˆ’1
a suitable power of Î± and so Î±âˆ’1 obviously leaves F ï¬xed. Thus G (K, F) which has been
called a group all along, really is a group.
Then the following corollary is the reason why separable polynomials are so important.
Also, one can show that if F contains a ï¬eld which is isomorphic to Q then every polynomial
with coeï¬ƒcients in F is separable. This will be done later after presenting the big results.
This is equivalent to saying that the ï¬eld has characteristic zero. In addition, the property
of being separable holds in other situations which are described later.
Corollary F.5.6 Let K
separable. Then

be a splitting ï¬eld of p (x) over the ï¬eld F. Assume p (x) is
|G (K, F)| = [K : F]

Proof: Just note that K is also the splitting ï¬eld of p1 (x), the product of the distinct
irreducible factors and that from Lemma F.5.3, p1 (x) has distinct roots. Thus the conclusion
follows from Theorem F.4.5. 
What if L is an intermediate ï¬eld between F and K? Then p1 (x) still has coeï¬ƒcients in
L and distinct roots in K and so it also follows that
|G (K, L)| = [K : L]
Deï¬nition F.5.7 Let G be a group of automorphisms of a ï¬eld K. Then denote by KG the
ï¬xed ï¬eld of G. Thus
KG â‰¡ {x âˆˆ K : Ïƒ (x) = x for all Ïƒ âˆˆ G}
Thus there are two new things, the ï¬xed ï¬eld of a group of automorphisms H denoted
by KH and the Gallois group G (K, L). How are these related? First here is a simple lemma
which comes from the deï¬nitions.
Lemma F.5.8 Let K be an algebraic extension of L (each element of L is a root of some
polynomial in L) for L, K ï¬elds. Then
(
)
G (K, L) = G K, KG(K,L)
Proof: It is clear that L âŠ† KG(K,L) because if r âˆˆ L then by deï¬nition, everything in
G (K, L) ï¬xes r and so r is in KG(K,L) . Therefore,
(
)
G (K, L) âŠ‡ G K, KG(K,L) .

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.5. THE GALOIS GROUP

467

Now let Ïƒ âˆˆ G (K, L) then it is one of the automorphisms
of K which
ï¬xes everything in
(
)
the ï¬xed ï¬eld of G (K, L) . Thus, by deï¬nition, Ïƒ âˆˆ G K, KG(K,L) and so the two are the
same. 
Now the following says that you can start with L, go to the group G (K, L) and then to
the ï¬xed ï¬eld of this group and end up back where you started. More precisely,
Proposition F.5.9 If K is a splitting ï¬eld of p (x) over the ï¬eld F for separable p (x) , and
if L is a ï¬eld between K and F, then K is also a splitting ï¬eld of p (x) over L and also
L = KG(K,L)
Proof: By the above lemma, and Corollary F.5.6,
[
][
]
|G (K, L)| = [K : L] = K : KG(K,L) KG(K,L) : L
(
) [
]
[
]
= G K, KG(K,L) KG(K,L) : L = |G (K, L)| KG(K,L) : L
[
]
which shows that KG(K,L) : L = 1 and so, since L âŠ† KG(K,L) , it follows that L = KG(K,L) .

This has shown the following diagram in the context of K being a splitting ï¬eld of a
separable polynomial over F and L being an intermediate ï¬eld.
L â†’ G (K, L) â†’ KG(K,L) = L
In particular, every intermediate ï¬eld is a ï¬xed ï¬eld of a subgroup of G (K, F). Is every
subgroup of G (K, F) obtained in the form G (K, L) for some intermediate ï¬eld? This involves
another estimate which is apparently due to Artin. I also found this in [17]. There is more
there about some of these things than what I am including.
Theorem F.5.10 Let K be a ï¬eld and let G be a ï¬nite group of automorphisms of K. Then
[K : KG ] â‰¤ |G|
Proof: Let G = {Ïƒ 1 , Â· Â· Â· , Ïƒ n } , Ïƒ 1 = Î¹ the identity map and suppose {u1 , Â· Â· Â· , um } is a
linearly independent set in K with respect to the ï¬eld KG . Suppose m > n. Then consider
the system of equations
Ïƒ 1 (u1 ) x1 + Ïƒ 1 (u2 ) x2 + Â· Â· Â· + Ïƒ 1 (um ) xm = 0
Ïƒ 2 (u1 ) x1 + Ïƒ 2 (u2 ) x2 + Â· Â· Â· + Ïƒ 2 (um ) xm = 0
..
.

(6.18)

Ïƒ n (u1 ) x1 + Ïƒ n (u2 ) x2 + Â· Â· Â· + Ïƒ n (um ) xm = 0
which is of the form M x = 0 for x âˆˆ Km . Since M has more columns than rows, there
exists a nonzero solution x âˆˆ Km to the above system. Note that this could not happen if
x âˆˆ Km
G because of independence of {u1 , Â· Â· Â· , um } and the fact that Ïƒ 1 = Î¹. Let the solution
x be one which has the least possible number of nonzero entries. Without loss of generality,
some xk = 1 for some k. If Ïƒ r (xk ) = xk for all xk and for each r, then the xk are each
in KG and so the ï¬rst equation above would be impossible as just noted. Therefore, there
exists l Ì¸= k and Ïƒ r such that Ïƒ r (xl ) Ì¸= xl . For purposes of illustration, say l > k. Now
do Ïƒ r to both sides of all the above equations. This yields, after re ordering the resulting
equations a list of equations of the form
Ïƒ 1 (u1 ) Ïƒ r (x1 ) + Â· Â· Â· + Ïƒ 1 (uk ) 1 + Â· Â· Â· + Ïƒ 1 (ul ) Ïƒ r (xl ) + Â· Â· Â· + Ïƒ 1 (um ) Ïƒ r (xm ) = 0
Ïƒ 2 (u1 ) Ïƒ r (x1 ) + Â· Â· Â· + Ïƒ 2 (uk ) 1 + Â· Â· Â· + Ïƒ 2 (ul ) Ïƒ r (xl ) + Â· Â· Â· + Ïƒ 2 (um ) Ïƒ r (xm ) = 0
..
.
Ïƒ n (u1 ) Ïƒ r (x1 ) + Â· Â· Â· + Ïƒ n (uk ) 1 + Â· Â· Â· + Ïƒ n (ul ) Ïƒ r (xl ) + Â· Â· Â· + Ïƒ n (um ) Ïƒ r (xm ) = 0

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

468

FIELDS AND FIELD EXTENSIONS

This is because Ïƒ (1) = 1 if Ïƒ is an automorphism. The original system in (6.18) is of the
form
Ïƒ 1 (u1 ) x1 + Â· Â· Â· + Ïƒ 1 (uk ) 1 + Â· Â· Â· + Ïƒ 1 (ul ) xl + Â· Â· Â· + Ïƒ 1 (um ) xm = 0
Ïƒ 2 (u1 ) x1 + Â· Â· Â· + Ïƒ 2 (uk ) 1 + Â· Â· Â· + Ïƒ 1 (ul ) xl + Â· Â· Â· + Ïƒ 2 (um ) xm = 0
..
.
Ïƒ n (u1 ) x1 + Â· Â· Â· + Ïƒ n (uk ) 1 + Â· Â· Â· + Ïƒ 1 (ul ) xl + Â· Â· Â· + Ïƒ n (um ) xm = 0
Now replace the k th equation with the diï¬€erence of the k th equations in the original system
and the one in which Ïƒ r was done to both sides of the equations. Since Ïƒ r (xl ) Ì¸= xl the
result will be a linear system of the form M y = 0 where y Ì¸= 0 has fewer nonzero entries
than x, contradicting the choice of x. 
With the above estimate, here is another relation between the ï¬xed ï¬elds and subgroups
of automorphisms. It doesnâ€™t seem to depend on anything being a splitting ï¬eld of a
separable polynomial.
Proposition F.5.11 Let H be a ï¬nite group of automorphisms deï¬ned on a ï¬eld K. Then
for KH the ï¬xed ï¬eld,
G (K, KH ) = H
Proof: If Ïƒ âˆˆ H, then by deï¬nition, Ïƒ âˆˆ G (K, KH ). It is clear that H âŠ† G (K, KH ) .
Then by Proposition F.5.10 and Theorem F.5.2,
|H| â‰¥ [K : KH ] â‰¥ |G (K, KH )| â‰¥ |H|
and so H = G (K, KH ). 
This leads to the following interesting correspondence in the case where K is a splitting
ï¬eld of a separable polynomial over a ï¬eld F.
Î²

Fixed ï¬elds

L â†’ G (K, L)
Î±
KH â† H

Subgroups of G (K, F)

(6.19)

Then Î±Î²L = L and Î²Î±H = H. Thus there exists a one to one correspondence between the
ï¬xed ï¬elds and the subgroups of G (K, F). The following theorem summarizes the above
result.
Theorem F.5.12 Let K be a splitting ï¬eld of a separable polynomial over a ï¬eld F. Then
there exists a one to one correspondence between the ï¬xed ï¬elds KH for H a subgroup of
G (K, F) and the intermediate ï¬elds as described in the above. H1 âŠ† H2 if and only if
KH1 âŠ‡ KH2 . Also
|H| = [K : KH ]
Proof: The one to one correspondence is established above. The claim about the ï¬xed
ï¬elds is obvious because if the group is larger, then the ï¬xed ï¬eld must get harder because it
is more diï¬ƒcult to ï¬x everything using more automorphisms than with fewer automorphisms.
Consider the estimate. From Theorem F.5.10, |H| â‰¥ [K : KH ]. But also, H = G (K, KH )
from Proposition F.5.11 G (K, KH ) = H and from Theorem F.5.2,
|H| = |G (K, KH )| â‰¤ [K : KH ] .


Note that from the above discussion, when K is a splitting ï¬eld of p (x) âˆˆ F [x] , this
implies that if L is an intermediate ï¬eld, then it is also a ï¬xed ï¬eld of a subgroup of G (K, F).
In fact, from the above,
L = KG(K,L)

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.6. NORMAL SUBGROUPS

469

If H is a subgroup, then it is also the Galois group
H = G (K, KH ) .
By Proposition F.4.7, each of these intermediate ï¬elds L is also a normal extension of F.
Now there is also something called a normal subgroup which will end up corresponding with
these normal ï¬eld extensions consisting of the intermediate ï¬elds between F and K.

F.6

Normal Subgroups

When you look at groups, one of the ï¬rst things to consider is the notion of a normal
subgroup.
Deï¬nition F.6.1 Let G be a group. Then a subgroup N is said to be a normal subgroup if
whenever Î± âˆˆ G,
Î±âˆ’1 N Î± âŠ† N
The important thing about normal subgroups is that you can deï¬ne the quotient group
G/N .
Deï¬nition F.6.2 Let N be a subgroup of G. Deï¬ne an equivalence relation âˆ¼ as follows.
Î± âˆ¼ Î² means Î±âˆ’1 Î² âˆˆ N
Why is this an equivalence relation? It is clear that Î± âˆ¼ Î± because Î±âˆ’1 Î± = Î¹ âˆˆ N since
N is a subgroup. If Î± âˆ¼ Î², then Î±âˆ’1 Î² âˆˆ N and so, since N is a subgroup,
(

Î±âˆ’1 Î²

)âˆ’1

= Î² âˆ’1 Î± âˆˆ N

which shows that Î² âˆ¼ Î±. Now suppose Î±Î± âˆ¼ Î², then Î±âˆ’1 Î² âˆˆ N and so, since N is a
subgroup,
( âˆ’1 )âˆ’1
Î± Î²
= Î² âˆ’1 Î± âˆˆ N
which shows that Î² âˆ¼ Î±. Now suppose Î± âˆ¼ Î² and Î² âˆ¼ Î³. Then Î±âˆ’1 Î² âˆˆ N and Î² âˆ’1 Î³ âˆˆ N.
Then since N is a subgroup
Î±âˆ’1 Î²Î² âˆ’1 Î³ = Î±âˆ’1 Î³ âˆˆ N
and so Î± âˆ¼ Î³ which shows that it is an equivalence relation as claimed. Denote by [Î±] the
equivalence class determined by Î±.
Now in the case of N a normal subgroup, you can consider the quotient group.
Deï¬nition F.6.3 Let N be a normal subgroup of a group G and deï¬ne G/N as the set of
all equivalence classes with respect to the above equivalence relation. Also deï¬ne
[Î±] [Î²] â‰¡ [Î±Î²]
Proposition F.6.4 The above deï¬nition is well deï¬ned and it also makes G/N into a
group.
Proof: First consider the claim that the deï¬nition is well deï¬ned. Suppose then that
Î± âˆ¼ Î±â€² and Î² âˆ¼ Î² â€² . It is required to show that
[
]
[Î±Î²] = Î±â€² Î² â€²

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

470

FIELDS AND FIELD EXTENSIONS

But
âˆˆN

âˆ’1

(Î±Î²)

Î±â€² Î² â€²

z }| {
= Î² âˆ’1 Î±âˆ’1 Î±â€² Î² â€² = Î² âˆ’1 Î±âˆ’1 Î±â€² Î² â€²
âˆˆN

âˆˆN

z ( }| ) {z }| {
= Î² âˆ’1 Î±âˆ’1 Î±â€² Î² Î² âˆ’1 Î² â€² = n1 n2 âˆˆ N
Thus the operation[ is well
] deï¬ned. Clearly the identity is [Î¹] where Î¹ is the identity in G
and the inverse is Î±âˆ’1 where Î±âˆ’1 is the inverse for Î± in G. The associative law is also
obvious. 
Note that it was important to have the subgroup be normal in order to have the operation
deï¬ned on the quotient group.

F.7

Normal Extensions And Normal Subgroups

When K is a splitting ï¬eld of a separable polynomial having coeï¬ƒcients in F, the intermediate
ï¬elds are each normal extensions from the above. If L is one of these, what about G (L, F)?
is this a normal subgroup of G (K, F)? More generally, consider the following diagram which
has now been established in the case that K is a splitting ï¬eld of a separable polynomial in
F [x].
F â‰¡ L0
G (F, F) = {Î¹}

âŠ† L1
âŠ† G (L1 , F)

âŠ† L2
Â·Â·Â·
âŠ† G (L2 , F) Â· Â· Â·

âŠ† Lkâˆ’1
âŠ† G (Lkâˆ’1 , F)

âŠ† Lk â‰¡ K
âŠ† G (K, F)

(6.20)

The intermediate ï¬elds Li are each normal extensions of F each element of Li being algebraic.
As implied in the diagram, there is a one to one correspondence between the intermediate
ï¬elds and the Galois groups displayed. Is G (Ljâˆ’1 , F) a normal subgroup of G (Lj , F)?
Let Ïƒ âˆˆ G (Lj , F) and let Î· âˆˆ G (Ljâˆ’1 , F) . Then is Ïƒ âˆ’1 Î·Ïƒ âˆˆ G (Ljâˆ’1 , F)? Let r = r1
be something in Ljâˆ’1 and let {r1 , Â· Â· Â· , rm } be the roots of the minimal polynomial of r
denoted by f (x) , a polynomial having coeï¬ƒcients in F. Then 0 = Ïƒf (r) = f (Ïƒ (r)) and
so Ïƒ (r) = rj for some j. Since Ljâˆ’1 is normal, Ïƒ (r) âˆˆ Ljâˆ’1 . Therefore, it is ï¬xed by Î·. It
follows that
Ïƒ âˆ’1 Î·Ïƒ (r) = Ïƒ âˆ’1 Ïƒ (r) = r
and so Ïƒ âˆ’1 Î·Ïƒ âˆˆ G (Ljâˆ’1 , F). Thus G (Ljâˆ’1 , F) is a normal subgroup of G (Lj , F) as hoped.
This leads to the following fundamental theorem of Galois theory.
Theorem F.7.1 Let K be a splitting ï¬eld of a separable polynomial p (x) having coeï¬ƒcients
k
in a ï¬eld F. Let {Li }i=0 be the increasing sequence of intermediate ï¬elds between F and K
as shown above in (6.20). Then each of these is a normal extension of F and the Galois
group G (Ljâˆ’1 , F) is a normal subgroup of G (Lj , F). In addition to this,
G (Lj , F) â‰ƒ G (K, F) /G (K, Lj )
where the symbol â‰ƒ indicates the two spaces are isomorphic.
Proof: All that remains is to check that the above isomorphism is valid. Let
Î¸ : G (K, F) /G (K, Lj ) â†’ G (Lj , F) , Î¸ [Ïƒ] â‰¡ Ïƒ|Lj
In other words, this is just the restriction of Ïƒ to Lj . Is Î¸ well deï¬ned? If [Ïƒ 1 ] = [Ïƒ 2 ] , then
âˆ’1
by deï¬nition, Ïƒ 1 Ïƒ âˆ’1
ï¬xes everything in Lj . It follows that the
2 âˆˆ G (K, Lj ) and so Ïƒ 1 Ïƒ 2
restrictions of Ïƒ 1 and Ïƒ 2 to Lj are equal. Therefore, Î¸ is well deï¬ned. It is obvious that

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.8. CONDITIONS FOR SEPARABILITY

471

Î¸ is a homomorphism. Why is Î¸ onto? This follows right away from Theorem F.4.5. Note
that K is the splitting ï¬eld of p (x) over Lj since Lj âŠ‡ F. Also if Ïƒ âˆˆ G (Lj , F) so it is an
automorphism of Lj , then, since it ï¬xes F, p (x) = pÌ„ (x) in that theorem. Thus Ïƒ extends to
Î¶, an automorphism of K. Thus Î¸Î¶ = Ïƒ. Why is Î¸ one to one? If Î¸ [Ïƒ] = Î¸ [Î±] , this means
Ïƒ = Î± on Lj . Thus ÏƒÎ±âˆ’1 is the identity on Lj . Hence ÏƒÎ±âˆ’1 âˆˆ G (K, Lj ) which is what it
means for [Ïƒ] = [Î±]. 
There is an immediate application to a description of the normal closure of an algebraic
extension F [a1 , a2 , Â· Â· Â· , am ] . To begin with, recall the following deï¬nition.
Deï¬nition F.7.2 When you have F [a1 , Â· Â· Â· , am ] with each ai algebraic so that F [a1 , Â· Â· Â· , am ]
is a ï¬eld, you could consider
m
âˆ
f (x) â‰¡
fi (x)
i=1

where fi (x) is the minimal polynomial of ai . Then if K is a splitting ï¬eld for f (x) , this K
is called the normal closure. It is at least as large as F [a1 , Â· Â· Â· , am ] and it has the advantage
of being a normal extension.
Let G (K, F) = {Î· 1 , Î· 2 , Â· Â· Â· , Î· m } . The conjugate ï¬elds are the ï¬elds
Î· j (F [a1 , Â· Â· Â· , am ])
Thus each of these ï¬elds is isomorphic to any other and they are all contained in K. Let Kâ€²
denote the smallest ï¬eld contained in K which contains all of these conjugate ï¬elds. Note
that if k âˆˆ F [a1 , Â· Â· Â· , am ] so that Î· i (k) is in one of these conjugate ï¬elds, then Î· j Î· i (k) is
also in a conjugate ï¬eld because Î· j Î· i is one of the automorphisms of G (K, F). Let
{
}
S = k âˆˆ Kâ€² : Î· j (k) âˆˆ Kâ€² each j .
Then from what was just shown, each conjugate ï¬eld is in S. Suppose k âˆˆ S. What about
k âˆ’1 ?
(
)
(
)
Î· j (k) Î· j k âˆ’1 = Î· j kk âˆ’1 = Î· j (1) = 1
(
)âˆ’1
(
)
(
)âˆ’1
and so Î· j (k)
= Î· j k âˆ’1 . Now Î· j (k)
âˆˆ Kâ€² because Kâ€² is a ï¬eld. Therefore,
( âˆ’1 )
â€²
Î·j k
âˆˆ K . Thus S is closed with respect to taking inverses. It is also closed with
respect to products. Thus it is clear that S is a ï¬eld which contains each conjugate ï¬eld.
However, Kâ€² was deï¬ned as the smallest ï¬eld which contains the conjugate ï¬elds. Therefore,
S = Kâ€² and so this shows that each Î· j maps Kâ€² to itself while ï¬xing F. Thus G (K, F) âŠ†
G (Kâ€² , F) . However, since Kâ€² âŠ† K, it follows that also G (Kâ€² , F) âŠ† G (K, F) . Therefore,
G (Kâ€² , F) = G (K, F) , and by the one to one correspondence between the intermediate ï¬elds
and the Galois groups, it follows that Kâ€² = K. This proves the following lemma.
Lemma F.7.3 Let K denote the normal extension of F [a1 , Â· Â· Â· , am ] with each ai algebraic
so that F [a1 , Â· Â· Â· , am ] is a ï¬eld. Thus K is the splitting ï¬eld of the product of the minimal
polynomials of the ai . Then K is also the smallest ï¬eld containing the conjugate ï¬elds
Î· j (F [a1 , Â· Â· Â· , am ]) for {Î· 1 , Î· 2 , Â· Â· Â· , Î· m } = G (K, F).

F.8

Conditions For Separability

So when is it that a polynomial having coeï¬ƒcients in a ï¬eld F is separable? It turns out
that this is always the case for ï¬elds which are enough like the rational numbers. It involves
considering the derivative of a polynomial. In doing this, there will be no analysis used, just

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

472

FIELDS AND FIELD EXTENSIONS

the rule for diï¬€erentiation which we all learned in calculus. Thus the derivative is deï¬ned
as follows.
(
)â€²
an xn + anâˆ’1 xnâˆ’1 + Â· Â· Â· + a1 x + a0
â‰¡ nan xnâˆ’1 + anâˆ’1 (n âˆ’ 1) xnâˆ’2 + Â· Â· Â· + a1
This kind of formal manipulation is what most students do anyway, never thinking about
where it comes from. Here nan means to add an to itself n times. With this deï¬nition, it
is clear that the usual rules such as the product rule hold. This discussion follows [17].
Deï¬nition F.8.1 A ï¬eld has characteristic 0 if na Ì¸= 0 for all n âˆˆ N and a Ì¸= 0. Otherwise
a ï¬eld F has characteristic p if p Â· 1 = 0 for p Â· 1 deï¬ned as 1 added to itself p times and p
is the smallest positive integer for which this takes place.
Note that with this deï¬nition, some of the terms of the derivative of a polynomial could
vanish in the case that the ï¬eld has characteristic p. I will go ahead and write them anyway.
For example, if the ï¬eld has characteristic p, then
â€²

(xp âˆ’ a) = 0
because formally it equals p Â· 1xpâˆ’1 = 0xpâˆ’1 , the 1 being the 1 in the ï¬eld.
Note that the ï¬eld Zp does not have characteristic 0 because p Â· 1 = 0. Thus not all ï¬elds
have characteristic 0.
How can you tell if a polynomial has no repeated roots? This is the content of the next
theorem.
Theorem F.8.2 Let p (x) be a monic polynomial having coeï¬ƒcients in a ï¬eld F, and let K
be a ï¬eld in which p (x) factors
p (x) =

n
âˆ

(x âˆ’ ri ) , ri âˆˆ K.

i=1

Then the ri are distinct if and only if p (x) and pâ€² (x) are relatively prime over F.
Proof: Suppose ï¬rst that pâ€² (x) and p (x) are relatively prime over F. Since they are not
both zero, there exists polynomials a (x) , b (x) having coeï¬ƒcients in F such that
a (x) p (x) + b (x) pâ€² (x) = 1
Now suppose p (x) has a repeated root r. Then in K [x] ,
2

p (x) = (x âˆ’ r) g (x)
and so pâ€² (x) = 2 (x âˆ’ r) g (x) + (x âˆ’ r) g â€² (x). Then in K [x] ,
(
)
2
2
a (x) (x âˆ’ r) g (x) + b (x) 2 (x âˆ’ r) g (x) + (x âˆ’ r) g â€² (x) = 1
2

Then letting x = r, it follows that 0 = 1. Hence p (x) has no repeated roots.
Next suppose there are no repeated roots of p (x). Then
pâ€² (x) =

n âˆ
âˆ‘

(x âˆ’ rj )

i=1 jÌ¸=i

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.8. CONDITIONS FOR SEPARABILITY

473

pâ€² (x) cannot be zero in this case because
pâ€² (rn ) =

nâˆ’1
âˆ

(rn âˆ’ rj ) Ì¸= 0

j=1

because it is the product of nonzero elements of K. Similarly no term in the sum for pâ€² (x)
can equal zero because
âˆ
(ri âˆ’ rj ) Ì¸= 0.
jÌ¸=i

Then if q (x) is a monic polynomial of degree larger than 1 which divides p (x), then the
roots of q (x) in K are a subset of {r1 , Â· Â· Â· , rn }. Without loss of generality, suppose these
roots of q (x) are {r1 , Â· Â· Â· , rk } , k â‰¤ n âˆ’ 1, since q (x) divides pâ€² (x) which has degree at most
âˆk
n âˆ’ 1. Then q (x) = i=1 (x âˆ’ ri ) but this fails to divide pâ€² (x) as polynomials in K [x] and
so q (x) fails to divide pâ€² (x) as polynomials in F [x] either. Therefore, q (x) = 1 and so the
two are relatively prime. 
The following lemma says that the usual calculus result holds in case you are looking at
polynomials with coeï¬ƒcients in a ï¬eld of characteristic 0.
Lemma F.8.3 Suppose that F has characteristic 0. Then if f â€² (x) = 0, it follows that f (x)
is a constant.
Proof: Suppose
f (x) = an xn + anâˆ’1 xnâˆ’1 + Â· Â· Â· + a1 x + a0
Then take the derivative n âˆ’ 1 times to ï¬nd that an multiplied by a positive integer man
equals 0. Therefore, an = 0 because, by assumption man Ì¸= 0 if an Ì¸= 0. Now repeat the
argument with
f1 (x) = anâˆ’1 xnâˆ’1 + Â· Â· Â· + a1 x + a0
and continue this way to ï¬nd that f (x) = a0 âˆˆ F. 
Now here is a major result which applies to ï¬elds of characteristic 0.
Theorem F.8.4 If F is a ï¬eld of characteristic 0, then every polynomial p (x) , having
coeï¬ƒcients in F is separable.
Proof: It is required to show that the irreducible factors of p (x) have distinct roots in
K a splitting ï¬eld for p (x). So let q (x) be an irreducible monic polynomial. If l (x) is a
monic polynomial of positive degree which divides both q (x) and q â€² (x) , then since q (x) is
irreducible, it must be the case that l (x) = q (x) which forces q (x) to divide q â€² (x) . However,
the degree of q â€² (x) is less than the degree of q (x) so this is impossible. Hence l (x) = 1 and
so q â€² (x) and q (x) are relatively prime which implies that q (x) has distinct roots. 
It follows that the above theory all holds for any ï¬eld of characteristic 0. For example,
if the ï¬eld is Q then everything holds.
Proposition F.8.5 If a ï¬eld F has characteristic p, then p is a prime.
Proof: First note that if n Â· 1 = 0, if and only if for all a Ì¸= 0, n Â· a = 0 also. This just
follows from the distributive law and the deï¬nition of what is meant by n Â· 1, meaning that
you add 1 to itself n times. Suppose then that there are positive integers, each larger than
1 n, m such that nm Â· 1 = 0. Then grouping the terms in the sum associated with nm Â· 1,
it follows that n (m Â· 1) = 0. If the characteristic of the ï¬eld is nm, this is a contradiction
because then m Â· 1 Ì¸= 0 but n times it is, implying that n < nm but n Â· a = 0 for a nonzero
a. Hence n Â· 1 = 0 showing that mn is not the characteristic of the ï¬eld after all. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

474

FIELDS AND FIELD EXTENSIONS

Deï¬nition F.8.6 A ï¬eld F is called perfect if every polynomial p (x) having coeï¬ƒcients in
F is separable.
The above shows that ï¬elds of characteristic 0 are perfect. The above theory about
Galois groups and ï¬xed ï¬elds all works for perfect ï¬elds. What about ï¬elds of characteristic
p where p is a prime? The following interesting lemma has to do with a nonzero a âˆˆ F
having a pth root in F.
Lemma F.8.7 Let F be a ï¬eld of characteristic p. Let a Ì¸= 0 where a âˆˆ F. Then either
p
xp âˆ’ a is irreducible or there exists b âˆˆ F such that xp âˆ’ a = (x âˆ’ b) .
Proof: Suppose that xp âˆ’ a is not irreducible. Then xp âˆ’ a = g (x) f (x) where the
degree of g (x) , k is less than p and at least as large as 1. Then let b be a root of g (x). Then
bp âˆ’ a = 0. Therefore,
p
xp âˆ’ a = xp âˆ’ bp = (x âˆ’ b) .
p

That is right. xp âˆ’ bp = (x âˆ’ b) just like many beginning calculus students believe. It
happens because of the binomial theorem and the fact that the other terms have a factor of
p. Hence
p
xp âˆ’ a = (x âˆ’ b) = g (x) f (x)
p

k

and so g (x) divides (x âˆ’ b) which requires that g (x) = (x âˆ’ b) since g (x) has degree k.
It follows, since g (x) is given to have coeï¬ƒcients in F, that bk âˆˆ F. Also bp âˆˆ F. Since k, p
are relatively prime, due to the fact that k < p with p prime, there are integers m, n such
that
1 = mk + np
Then from what you mean by raising b to an integer power and the usual rules of exponents
for integer powers,
( )m
n
b = bk (bp ) âˆˆ F.

So when is a ï¬eld of characteristic p perfect? As observed above, for a ï¬eld of characteristic p,
p
(a + b) = ap + bp .
Also,
p

(ab) = ap bp
It follows that a â†’ ap is a homomorphism. This is also one to one because, as mentioned
above
p
(a âˆ’ b) = ap âˆ’ bp
Therefore, if ap = bp , it follows that a = b. Therefore, this homomorphism is also one to
one.
Let Fp be the collection of ap where a âˆˆ F. Then clearly Fp is a subï¬eld of F because
it is the image of a one to one homomorphism. What follows is the condition for a ï¬eld of
characteristic p to be perfect.
Theorem F.8.8 Let F be a ï¬eld of characteristic p. Then F is perfect if and only if F = Fp .
Proof: Suppose F = Fp ï¬rst. Let f (x) be an irreducible polynomial over F. By Theorem
F.8.2, if f â€² (x) and f (x) are relatively prime over F then f (x) has no repeated roots. Suppose
then that the two polynomials are not relatively prime. If d (x) divides both f (x) and f â€² (x)
with degree of d (x) â‰¥ 1. Then, since f (x) is irreducible, it follows that d (x) is a multiple

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.9. PERMUTATIONS

475

of f (x) and so f (x) divides f â€² (x) which is impossible unless f â€² (x) = 0. But if f â€² (x) = 0,
then f (x) must be of the form
a0 + a1 xp + a2 x2p + Â· Â· Â· + an xnp
since if it had some other nonzero term with exponent not a multiple of p then f â€² (x) could
not equal zero since you would have something surviving in the expression for the derivative
after taking out multiples of p which is like
kaxkâˆ’1
where a Ì¸= 0 and k < p. Thus ka Ì¸= 0. Hence the form of f (x) is as indicated above.
If ak = bpk for some bk âˆˆ F, then the expression for f (x) is
bp0 + bp1 xp + bp2 x2p + Â· Â· Â· + bpn xnp
(
)p
= b0 + b1 x + bx x2 + Â· Â· Â· + bn xn
because of the fact noted earlier that a â†’ ap is a homomorphism. However, this says that
f (x) is not irreducible after all. It follows that there exists ak such that ak âˆˆ
/ Fp contrary
p
â€²
to the assumption that F = F . Hence the greatest common divisor of f (x) and f (x) must
be 1.
p
Next consider the other direction. Suppose F Ì¸= Fp . Then there exists a âˆˆ F \ F .
p
p
Consider the polynomial x âˆ’ a. As noted above, its derivative equals 0. Therefore, x âˆ’ a
and its derivative cannot be relatively prime. In fact, xp âˆ’ a would divide both. 
Now suppose F is a ï¬nite ï¬eld. If n Â· 1 is never equal to 0 then, since the ï¬eld is ï¬nite,
k Â· 1 = m Â· 1, for some k < m. m > k, and (m âˆ’ k) Â· 1 = 0 which is a contradiction. Hence
F is a ï¬eld of characteristic p for some prime p, by Proposition F.8.5. The mapping a â†’ ap
was shown to be a homomorphism which is also one to one. Therefore, Fp is a subï¬eld of
F. It follows that it has characteristic q for some q a prime. However, this requires q = p
and so Fp = F. Then the following corollary is obtained from the above theorem.
Corollary F.8.9 If F is a ï¬nite ï¬eld, then F is perfect.
With this information, here is a convenient version of the fundamental theorem of Galois
theory.
Theorem F.8.10 Let K be a splitting ï¬eld of any polynomial p (x) âˆˆ F [x] where F is
k
either of characteristic 0 or of characteristic p with Fp = F. Let {Li }i=0 be the increasing
sequence of intermediate ï¬elds between F and K. Then each of these is a normal extension
of F and the Galois group G (Ljâˆ’1 , F) is a normal subgroup of G (Lj , F). In addition to this,
G (Lj , F) â‰ƒ G (K, F) /G (K, Lj )
where the symbol â‰ƒ indicates the two spaces are isomorphic.

F.9

Permutations

Let {a1 , Â· Â· Â· , an } be a set of distinct elements. Then a permutation of these elements is
usually thought of as a list in a particular order. Thus there are exactly n! permutations of
a set having n distinct elements. With this deï¬nition, here is a simple lemma.
Lemma F.9.1 Every permutation can be obtained from every other permutation by a ï¬nite
number of switches.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

476

FIELDS AND FIELD EXTENSIONS

Proof: This is obvious if n = 1 or 2. Suppose then that it is true for sets of nâˆ’1 elements.
Take two permutations of {a1 , Â· Â· Â· , an } , P1 , P2 . To get from P1 to P2 using switches, ï¬rst
make a switch to obtain the last element in the list coinciding with the last element of P2 .
By induction, there are switches which will arrange the ï¬rst n âˆ’ 1 to the right order. 
It is customary to consider permutations in terms of the set In â‰¡ {1, Â· Â· Â· , n} to be more
speciï¬c. Then one can think of a given permutation as a mapping Ïƒ from this set In to
itself which is one to one and onto. In fact, Ïƒ (i) â‰¡ j where j is in the ith position. Often
people write such a Ïƒ in the following form
(
)
1 2 Â·Â·Â· n
(6.21)
i1 i2 Â· Â· Â· in
An easy way to understand the above permutation is through the use of matrix multiplicaT
tion by permutation matrices. The above vector (i1 , Â· Â· Â· , in ) is obtained by
ï£«
ï£¶
1
ï£·
(
)ï£¬
ï£¬ 2 ï£·
ei1 ei2 Â· Â· Â· ein ï£¬ . ï£·
(6.22)
ï£­ .. ï£¸
n
This can be seen right away from looking at a simple example or by using the deï¬nition of
matrix multiplication directly.
Deï¬nition F.9.2 The sign of the permutation (6.21) is deï¬ned as the determinant of the
above matrix in (6.22).
In other words, the sign of the permutation
(
1 2 Â·Â·Â·
i1 i2 Â· Â· Â·

n
in

)

equals sgn (i1 , Â· Â· Â· , in ) deï¬ned earlier in Lemma 3.3.1.
Note that from the fact that the determinant is well deï¬ned and its properties, the sign of
a permutation is 1 if and only if the permutation is produced by an even number of switches
and that the number of switches used to produce a given permutation must be either even
or odd. Of course a switch is a permutation itself and this is called a transposition. Note
also that all these matrices are orthogonal matrices so to take the inverse, it suï¬ƒces to take
a transpose, the inverse also being a permutation matrix.
The resulting group consisting of the permutations of In is called Sn . An important idea
is the notion of a cycle. Let Ïƒ be a permutation, a one to one and onto function deï¬ned on
In . A cycle is of the form
(
)
k, Ïƒ (k) , Ïƒ 2 (k) , Ïƒ 3 (k) , Â· Â· Â· , Ïƒ mâˆ’1 (k) , Ïƒ m (k) = k.
The last condition must hold for some m because In is ï¬nite. Then a cycle can be considered
as a permutation as follows. Let (i1 , i2 , Â· Â· Â· , im ) be a cycle. Then deï¬ne Ïƒ by Ïƒ (i1 ) =
i2 , Ïƒ (i2 ) = i3 , Â· Â· Â· , Ïƒ (im ) = i1 , and if k âˆˆ
/ {i1 , i2 , Â· Â· Â· , im } , then Ïƒ (k) = k.
Note that if you have two cycles, (i1 , i2 , Â· Â· Â· , im ) , (j1 , j2 , Â· Â· Â· , jm ) which are disjoint in
the sense that
{i1 , i2 , Â· Â· Â· , im } âˆ© {j1 , j2 , Â· Â· Â· , jm } = âˆ…,
then they commute. It is then clear that every permutation can be represented in a unique
way by disjoint cycles. Start with 1 and form the cycle determined by 1. Then start with the

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.9. PERMUTATIONS

477

smallest k âˆˆ In which was not included and begin a cycle starting with this. Continue this
way. Use the convention that (k) is just the identity. This representation is unique up to
order of the cycles which does not matter because they commute. Note that a transposition
can be written as (a, b).
A cycle can be written as a product of non disjoint transpositions.
(i1 , i2 , Â· Â· Â· , im ) = (imâˆ’1 , im ) Â· Â· Â· (i2 , im ) (i1 , im )
Thus if m is odd, the permutation has sign 1 and if m is even, the permutation has sign
âˆ’1
âˆ’1. Also, it is clear that the inverse of the above permutation is (i1 , i2 , Â· Â· Â· , im )
=
(im , Â· Â· Â· , i2 , i1 ) .
Deï¬nition F.9.3 An is the subgroup of Sn such that for Ïƒ âˆˆ An , Ïƒ is the product of an
even number of transpositions. It is called the alternating group.
The following important result is useful in describing An .
Proposition F.9.4 Let n â‰¥ 3. Then every permutation in An is the product of 3 cycles
and the identity.
Proof: In case n = 3, you can list all of the permutations in An
(
) (
) (
)
1 2 3
1 2 3
1 2 3
,
,
1 2 3
2 3 1
3 1 2
In terms of cycles, these are
(1, 2, 3) , (1, 3, 2)
You can easily check that they are inverses of each other. Now suppose n â‰¥ 4. The
permutations in An are deï¬ned as the product of an even number of transpositions. There
are two cases. The ï¬rst case is where you have two transpositions which share a number,
(a, c) (c, b) = (a, c, b)
Thus when they share a number, the product is just a 3 cycle. Next suppose you have the
product of two transpositions which are disjoint. This can happen because n â‰¥ 4. First note
that
(a, b) = (c, b) (b, a, c) = (c, b, a) (c, a)
Therefore,
(a, b) (c, d)

= (c, b, a) (c, a) (a, d) (d, c, a)
= (c, b, a) (c, a, d) (d, c, a)

and so every product of disjoint transpositions is the product of 3 cycles. 
Lemma F.9.5 If n â‰¥ 5, then if B is a normal subgroup of An , and B is not the identity,
then B must contain a 3 cycle.
Proof: Let Î± be the permutation in B which is â€œclosestâ€ to the identity without being
the identity. That is, out of all permutations which are not the identity, this is one which
has the most ï¬xed points or equivalently moves the fewest numbers. Then Î± is the product
of disjoint cycles. Suppose that the longest cycle is the ï¬rst one and it has at least four
numbers. Thus
Î± = (i1 , i2 , i3 , i4 , Â· Â· Â· , m) Î³ 1 Â· Â· Â· Î³ p

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

478

FIELDS AND FIELD EXTENSIONS

Since B is normal,
Î±1 â‰¡ (i3 , i2 , i1 ) (i1 , i2 , i3 , i4 , Â· Â· Â· , m) (i1 , i2 , i3 ) Î³ 1 Â· Â· Â· Î³ p âˆˆ Am
Then consider Î±1 Î±âˆ’1 =
(i3 , i2 , i1 ) (i1 , i2 , i3 , i4 , Â· Â· Â· , m) (i1 , i2 , i3 ) (m, Â· Â· Â· i4 , i3 , i2 , i1 )
Then for this permutation, i1 â†’ i3 , i2 â†’ i2 , i3 â†’ i4 , i4 â†’ i1 . The other numbers not in
{i1 , i2 , i3 , i4 } are ï¬xed, and in addition i2 is ï¬xed which did not happen with Î±. Therefore,
this new permutation moves only 3 numbers. Since it is assumed that m â‰¥ 4, this is a
contradiction to Î± ï¬xing the most points. It follows that
Î± = (i1 , i2 , i3 ) Î³ 1 Â· Â· Â· Î³ p

(6.23)

Î± = (i1 , i2 ) Î³ 1 Â· Â· Â· Î³ p

(6.24)

or else
In the ï¬rst case, say Î³ 1 = (i4 , i5 , Â· Â· Â· ) . Multiply as follows Î±1 =
(i4 , i2 , i1 ) (i1 , i2 , i3 ) (i4 , i5 , Â· Â· Â· ) Î³ 2 Â· Â· Â· Î³ p (i1 , i2 , i4 ) âˆˆ B
Then form Î±1 Î±âˆ’1 âˆˆ B given by
âˆ’1
(i4 , i2 , i1 ) (i1 , i2 , i3 ) (i4 , i5 , Â· Â· Â· ) Î³ 2 Â· Â· Â· Î³ p (i1 , i2 , i4 ) Î³ âˆ’1
p Â· Â· Â· Î³ 1 (i3 , i2 , i1 )

= (i4 , i2 , i1 ) (i1 , i2 , i3 ) (i4 , i5 , Â· Â· Â· ) (i1 , i2 , i4 ) (Â· Â· Â· , i5 , i4 ) (i3 , i2 , i1 )
Then i1 â†’ i4 , i2 â†’ i3 , i3 â†’ i5 , i4 â†’ i2 , i5 â†’ i1 and other numbers are ï¬xed. Thus Î±1 Î±âˆ’1
moves 5 points. However, Î± moves more than 5 if Î³ i is not the identity for any i â‰¥ 2. It
follows that
Î± = (i1 , i2 , i3 ) Î³ 1
and Î³ 1 can only be a transposition. However, this cannot happen because then the above
Î± would not even be in An . Therefore, Î³ 1 = Î¹ and so
Î± = (i1 , i2 , i3 )
Thus in this case, B contains a 3 cycle.
Now consider case (6.24). None of the Î³ i can be a cycle of length more than 4 since
the above argument would eliminate this possibility. If any has length 3 then the above
argument implies that Î± equals this 3 cycle. It follows that each Î³ i must be a 2 cycle. Say
Î± = (i1 , i2 ) (i3 , i4 ) Î³ 2 Â· Â· Â· Î³ p
Thus it moves at least four numbers, greater than four if any of Î³ i for i â‰¥ 2 is not the
identity. As before, Î±1 â‰¡
(i4 , i2 , i1 ) (i1 , i2 ) (i3 , i4 ) Î³ 2 Â· Â· Â· Î³ p (i1 , i2 , i4 )
= (i4 , i2 , i1 ) (i1 , i2 ) (i3 , i4 ) (i1 , i2 , i4 ) Î³ 2 Â· Â· Â· Î³ p âˆˆ B
Then Î±1 Î±âˆ’1 =
âˆ’1 âˆ’1
(i4 , i2 , i1 ) (i1 , i2 ) (i3 , i4 ) (i1 , i2 , i4 ) Î³ 2 Â· Â· Â· Î³ p Î³ âˆ’1
p Â· Â· Â· Î³ 2 Î³ 1 (i3 , i4 ) (i1 , i2 )

=

(i4 , i2 , i1 ) (i1 , i2 ) (i3 , i4 ) (i1 , i2 , i4 ) (i3 , i4 ) (i1 , i2 ) âˆˆ B

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.10. SOLVABLE GROUPS

479

Then i1 â†’ i3 , i2 â†’ i4 , i3 â†’ i1 , i4 â†’ i3 so this moves exactly four numbers. Therefore, none
of the Î³ i is diï¬€erent than the identity for i â‰¥ 2. It follows that
Î± = (i1 , i2 ) (i3 , i4 )

(6.25)

and Î± moves exactly four numbers. Then since B is normal, Î±1 â‰¡
(i5 , i4 , i3 ) (i1 , i2 ) (i3 , i4 ) (i3 , i4 , i5 ) âˆˆ B
Then Î±1 Î±âˆ’1 =

(i5 , i4 , i3 ) (i1 , i2 ) (i3 , i4 ) (i3 , i4 , i5 ) (i3 , i4 ) (i1 , i2 ) âˆˆ B

Then i1 â†’ i1 , i2 â†’ i2 , i3 â†’ i4 , i4 â†’ i5 , i5 â†’ i3 . Thus this permutation moves only three
numbers and so Î± cannot be of the form given in (6.25). It follows that case (6.24) does not
occur. 
Deï¬nition F.9.6 A group G is said to be simple if its only normal subgroups are itself and
the identity.
The following major result is due to Galois [17].
Proposition F.9.7 Let n â‰¥ 5. Then An is simple.
Proof: From Lemma F.9.5, if B is a normal subgroup of An , B Ì¸= {Î¹} , then it contains
a 3 cycle Î± = (i1 , i2 , i3 ),
(
)
i1 i2 i3
i2 i3 i1
Now let (j1 , j2 , j3 ) be another 3 cycle.
(

j1
j2

j2
j3

j3
j1

)

Let Ïƒ be a permutation which satisï¬es
Ïƒ (ik ) = jk
Then
ÏƒÎ±Ïƒ âˆ’1 (j1 ) =

ÏƒÎ± (i1 ) = Ïƒ (i2 ) = j2

ÏƒÎ±Ïƒ âˆ’1 (j2 ) =
ÏƒÎ±Ïƒ âˆ’1 (j3 ) =

ÏƒÎ± (i2 ) = Ïƒ (i3 ) = j3
ÏƒÎ± (i3 ) = Ïƒ (i1 ) = j1

while ÏƒÎ±Ïƒ âˆ’1 leaves all other numbers ï¬xed. Thus ÏƒÎ±Ïƒ âˆ’1 is the given 3 cycle. It follows that
B contains every 3 cycle. By Proposition F.9.4, this implies B = An . The only problem is
that it is not know whether Ïƒ is in An . This is where n â‰¥ 5 is used. You can modify Ïƒ on
two numbers not equal to any of the {i1 , i2 , i3 } by multiplying by a transposition so that
the possibly modiï¬ed Ïƒ is expressed as an even number of transpositions. 

F.10

Solvable Groups

Recall the fundamental theorem of Galois theory which established a correspondence between the normal subgroups of G (K, F) and normal ï¬eld extensions. Also recall that if H
is one of these normal subgroups, then there was an isomorphism between G (KH , F) and
the quotient group G (K, F) /H. The general idea of a solvable group is given next.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

480

FIELDS AND FIELD EXTENSIONS

Deï¬nition F.10.1 A group G is solvable if there exists a decreasing sequence of subgroups
m
{Hi }i=0 such that H i is a normal subgroup of H (iâˆ’1) ,
G = H0 âŠ‡ H1 âŠ‡ Â· Â· Â· âŠ‡ Hm = {Î¹} ,
and each quotient group Hiâˆ’1 /Hi is Abelian. That is, for [a] , [b] âˆˆ Hiâˆ’1 /Hi ,
[ab] = [a] [b] = [b] [a] = [ba]
Note that if G is an Abelian group, then it is automatically solvable. In fact you can
just consider H0 = G, H1 = {Î¹}. In this case H0 /H1 is just the group G which is Abelian.
There is another idea which helps in understanding whether a group is solvable. It
involves the commutator subgroup. This is a very good idea because this subgroup is
deï¬ned in terms of the group G.
Deï¬nition F.10.2 Let a, b âˆˆ G a group. Then the commutator is
abaâˆ’1 bâˆ’1
The commutator subgroup, denoted by Gâ€² , is the smallest subgroup which contains all the
commutators.
The nice thing about the commutator subgroup is that it is a normal subgroup. There
are also many other amazing properties.
Theorem F.10.3 Let G be a group and let Gâ€² be the commutator subgroup. Then Gâ€² is a
normal subgroup. Also the quotient group G/Gâ€² is Abelian. If H is any normal subgroup of
G such that G/H is Abelian, then H âŠ‡ Gâ€² . If Gâ€² = {Î¹} , then G must be Abelian.
Proof: The elements of Gâ€² are just ï¬nite products of things like abaâˆ’1 bâˆ’1 . Note that
the inverse of something like this is also one of these.
( âˆ’1 âˆ’1 )âˆ’1
aba b
= babâˆ’1 aâˆ’1 .
Thus the collection of ï¬nite products is indeed a subgroup. Now consider h âˆˆ G. Then
habaâˆ’1 bâˆ’1 hâˆ’1 = hahâˆ’1 hbhâˆ’1 haâˆ’1 hâˆ’1 hbâˆ’1 hâˆ’1
(
)âˆ’1 (
)âˆ’1
= hahâˆ’1 hbhâˆ’1 hahâˆ’1
hbhâˆ’1
which is another one of those commutators. Thus for c a commutator and h âˆˆ G,
hchâˆ’1 = c1
another commutator. If you have a product of commutators c1 c2 Â· Â· Â· cm , then
hc1 c2 Â· Â· Â· cm hâˆ’1 =

m
âˆ

hci hâˆ’1 =

i=1

m
âˆ

di âˆˆ Gâ€²

i=1

â€²

where the di are each commutators. Hence G is a normal subgroup.
Consider now the quotient group. Is [g] [h] = [h] [g]? In other words, is [gh] = [hg]?
âˆ’1
In other words, is gh (hg) = ghg âˆ’1 hâˆ’1 âˆˆ Gâ€² ? Of course. This is a commutator and GÌâ€²
consists of products of these things. Thus the quotient group is Abelian.
Now let H be a normal subgroup of G such that G/H is Abelian. Then if g, h âˆˆ G,
[gh] = [hg] , gh (hg)

Saylor URL: http://www.saylor.org/courses/ma212/

âˆ’1

= ghg âˆ’1 hâˆ’1 âˆˆ H

The Saylor Foundation

F.10. SOLVABLE GROUPS

481

Thus every commutator is in H and so H âŠ‡ G.
The last assertion is obvious because G/ {Î¹} is isomorphic to G. Also, to say that
Gâ€² = {Î¹} is to say that
abaâˆ’1 bâˆ’1 = Î¹
which implies that ab = ba. 
Let G be a group and let Gâ€² be its commutator subgroup. Then the commutator subgroup of Gâ€² is Gâ€²â€² and so forth. To save on notation, denote by G(k) the k th commutator
subgroup. Thus you have the sequence
G(0) âŠ‡ G(1) âŠ‡ G(2) âŠ‡ G(3) Â· Â· Â·
each G(i) being a normal subgroup of G(iâˆ’1) although it is possible that G(i) is not a normal
subgroup of G. Then there is a useful criterion for a group to be solvable.
Theorem F.10.4 Let G be a group. It is solvable if and only if G(k) = {Î¹} for some k.
Proof: If G(k) = {Î¹} then G is clearly solvable because of Theorem F.10.3. The sequence
of commutator subgroups provides the necessary sequence of subgroups.
Next suppose that you have
G = H0 âŠ‡ H1 âŠ‡ Â· Â· Â· âŠ‡ Hm = {Î¹}
where each is normal in the preceding and the quotient groups are Abelian. Then from
Theorem F.10.3, G(1) âŠ† H1 . Thus H1â€² âŠ‡ G(2) . But also, from Theorem F.10.3, since H1 /H2
is Abelian,
H2 âŠ‡ H1â€² âŠ‡ G(2) .
Continuing this way G(k) = {Î¹} for some k â‰¤ m. 
Theorem F.10.5 If G is a solvable group and if H is a homomorphic image of G, then H
is also solvable.
Proof: By the above theorem, it suï¬ƒces to show that H (k) = {Î¹} for some k. Let
â€²
f be the homomorphism. Then (H â€² = f (G
). To see this, consider (a commutator
of
)
)
âˆ’1
âˆ’1
âˆ’1 âˆ’1
(1)
(1)
H, f (a) f (b) f (a) f (b)
= f aba b
. It follows that H
= f G
. Now continue this way, letting G(1) play the role of G and H (1) the role of H. Thus, since G is
solvable, some G(k) = {Î¹} and so H (k) = {Î¹} also. 
Now as an important example, of a group which is not solvable, here is a theorem.
Theorem F.10.6 For n â‰¥ 5, Sn is not solvable.
Proof: It is clear that An is a normal subgroup of Sn because if Ïƒ is a permutation, then
it has the same sign as Ïƒ âˆ’1 . Thus ÏƒÎ±Ïƒ âˆ’1 âˆˆ An if Î± âˆˆ An . If H is a normal subgroup of Sn ,
for which Sn /H is Abelian, then H contains the commutator Gâ€² . However, Î±ÏƒÎ±âˆ’1 Ïƒ âˆ’1 âˆˆ An
obviously so An âŠ‡ Snâ€² . By Proposition F.9.7, this forces Snâ€² = An . So what is Snâ€²â€² ? If it is
(k)
Sn , then Sn Ì¸= {Î¹} for any k and it follows that Sn is not solvable. If Snâ€²â€² = {Î¹} , the only
other possibility, then An / {Î¹} is Abelian and so An is Abelian, but this is obviously false
because the cycles (1, 2, 3) , (2, 1, 4) are both in An . However, (1, 2, 3) (2, 1, 4) is
(
)
1 2 3 4
4 2 1 3
while (2, 1, 4) (1, 2, 3) is

(

1
1

2
3

3
4

4
2

)


Note that the above shows that An is not Abelian for n = 4 also.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

482

F.11

FIELDS AND FIELD EXTENSIONS

Solvability By Radicals

First of all, there exists a ï¬eld which has all the nth roots of 1. You could simply deï¬ne it
to be the smallest sub ï¬eld of C such that it contains these roots. You could also enlarge
it by including some other numbers. For example, you could include Q. Observe that if
Î¾ â‰¡ ei2Ï€/n , then Î¾ n = 1 but Î¾ k Ì¸= 1 if k < n and that if k < l < n, Î¾ k Ì¸= Î¾ l . Such a ï¬eld
has characteristic 0 because for m an integer, m Â· 1 Ì¸= 0. The following is from Herstein [13].
This is the kind of ï¬eld considered here.
Lemma F.11.1 Suppose a ï¬eld F has all the nth roots of 1 for a particular n and suppose
there exists Î¾ such that the nth roots of 1 are of the form Î¾ k for k = 1, Â· Â· Â· , n, the Î¾ k being
distinct. Let a âˆˆ F be nonzero. Let K denote the splitting ï¬eld of xn âˆ’ a over F, thus K is
a normal extension of F. Then K = F [u] where u is any root of xn âˆ’ a. The Galois group
G (K, F) is Abelian.
Proof: Let u be a root of xn âˆ’ a and let K equal F [u] . Then let Î¾ be the nth root of
unity mentioned. Then
(
)n
k
Î¾ k u = (Î¾ n ) un = a
{
}
and so each Î¾ k u is a root of xn âˆ’ a and these are distinct. It follows that u, Î¾u, Â· Â· Â· , Î¾ nâˆ’1 u
are the roots of xn âˆ’ a and all are in F [u] . Thus F [u] = K. Let Ïƒ âˆˆ G (K, F) and observe
that since Ïƒ ï¬xes F,
((
)n
) ( (
))n
0 = Ïƒ Î¾k u âˆ’ a = Ïƒ Î¾k u
âˆ’a
It follows that Ïƒ maps roots of xn âˆ’ a to roots of xn âˆ’ a. Therefore, if Ïƒ, Î± are two elements
of G (K, F) , there exist i, j each no larger than n âˆ’ 1 such that
Ïƒ (u) = Î¾ i u, Î± (u) = Î¾ j u
A typical thing in F [u] is p (u) where p (x) âˆˆ F [x]. Then
(
)
(
)
ÏƒÎ± (p (u)) = p Î¾ j Î¾ i u = p Î¾ i+j u
(
)
(
)
Î±Ïƒ (p (u)) = p Î¾ i Î¾ j u = p Î¾ i+j u
Therefore, G (K, F) is Abelian. 
Deï¬nition F.11.2 For F a ï¬eld, a polynomial p (x) âˆˆ F [x] is solvable by radicals over
F â‰¡ F0 if there is a sequence of ï¬elds F1 = F [a1 ] , F2 = F1 [a2 ] , Â· Â· Â· , Fk = Fkâˆ’1 [ak ] such that
for each i â‰¥ 1, aki i âˆˆ Fiâˆ’1 and Fk contains a splitting ï¬eld K for p (x) over F.
Lemma F.11.3 In the above deï¬nition, you can assume that Fk is a normal extension of
F.
Proof: First note that Fk = F [a1 , a2 , Â· Â· Â· , ak ]. Let G be the normal extension of Fk .
By Lemma F.7.3, G is the smallest ï¬eld which contains the conjugate ï¬elds
[
]
Î· j (F [a1 , a2 , Â· Â· Â· , ak ]) = F Î· j a1 , Î· j a2 , Â· Â· Â· , Î· j ak
( )
(
)ki
for {Î· 1 , Î· 2 , Â· Â· Â· , Î· m } = G (Fk , F). Also, Î· j ai
= Î· j aki i âˆˆ Î· j Fiâˆ’1 , Î· j F = F. Then
G = F [Î· 1 (a1 ) , Î· 1 (a2 ) , Â· Â· Â· , Î· 1 (ak ) , Î· 2 (a1 ) , Î· 2 (a2 ) , Â· Â· Â· , Î· 2 (ak ) Â· Â· Â· ]
and this is a splitting ï¬eld so is a normal extension. Thus G could be the new Fk with
respect to a longer sequence but would now be a splitting ï¬eld. 

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

F.11. SOLVABILITY BY RADICALS

483

At this point, it is a good idea to recall the big fundamental theorem mentioned above
which gives the correspondence between normal subgroups and normal ï¬eld extensions since
it is about to be used again.
F â‰¡ F0
G (F, F) = {Î¹}

âŠ† F1
âŠ† F2
âŠ† G (F1 , F) âŠ† G (F2 , F)

Â·Â·Â·
Â·Â·Â·

âŠ† Fkâˆ’1
âŠ† G (Fkâˆ’1 , F)

âŠ† Fk â‰¡ K
âŠ† G (Fk , F)

(6.26)

Theorem F.11.4 Let K be a splitting ï¬eld of any polynomial p (x) âˆˆ F [x] where F is either
k
of characteristic 0 or of characteristic p with Fp = F. Let {Fi }i=0 be the increasing sequence
of intermediate ï¬elds between F and K. Then each of these is a normal extension of F and
the Galois group G (Fjâˆ’1 , F) is a normal subgroup of G (Fj , F). In addition to this,
G (Fj , F) â‰ƒ G (K, F) /G (K, Fj )
where the symbol â‰ƒ indicates the two spaces are isomorphic.
Theorem F.11.5 Let f (x) be a polynomial in F [x] where F is a ï¬eld of characteristic 0
which contains all nth roots of unity for each n âˆˆ N. Let K be a splitting ï¬eld of f (x) . Then
if f (x) is solvable by radicals over F, then the Galois group G (K, F) is a solvable group.
Proof: Using the deï¬nition given above for f (x) to be solvable by radicals, there is a
sequence of ï¬elds
F0 = F âŠ† F1 âŠ† Â· Â· Â· âŠ† Fk , K âŠ† Fk ,
where Fi = Fiâˆ’1 [ai ], aki i âˆˆ Fiâˆ’1 , and each ï¬eld extension is a normal extension of the preceding one. You can assume that Fk is the splitting ï¬eld of a polynomial having coeï¬ƒcients
in Fjâˆ’1 . This follows from the Lemma F.11.3 above. Then starting the hypotheses of the
theorem at Fjâˆ’1 rather than at F, it follows from Theorem F.11.4 that
G (Fj , Fjâˆ’1 ) â‰ƒ G (Fk , Fjâˆ’1 ) /G (Fk , Fj )
By Lemma F.11.1, the Galois group G (Fj , Fjâˆ’1 ) is Abelian and so this requires that
G (Fk , F) is a solvable group.
Of course K is a normal ï¬eld extension of F because it is a splitting ï¬eld. By Theorem F.10.5, G (Fk , K) is a normal subgroup of G (Fk , F). Also G (K, F) is isomorphic to
G (Fk , F) /G (Fk , K) and so G (K, F) is a homomorphic image of G (Fk , F) which is solvable. Here is why this last assertion is so. Deï¬ne Î¸ : G (Fk , F) /G (Fk , K) â†’ G (K, F) by
Î¸ [Ïƒ] â‰¡ Ïƒ|K . Then this is clearly a homomorphism if it is well deï¬ned. If [Ïƒ] = [Î±] this
means ÏƒÎ±âˆ’1 âˆˆ G (Fk , K) and so ÏƒÎ±âˆ’1 ï¬xes everything in K so that Î¸ is indeed well deï¬ned.
Therefore, by Theorem F.10.5, G (K, F) must also be solvable. 
Now this result implies that you canâ€™t solve the general polynomial equation of degree 5
or more by radicals. Let {a1 , a2 , Â· Â· Â· , an } âŠ† G where G is some ï¬eld which contains a ï¬eld
F0 . Let
F â‰¡ F0 (a1 , a2 , Â· Â· Â· , an )
the ï¬eld of all rational functions in the numbers a1 , a2 , Â· Â· Â· , an . I am using this notation
because I donâ€™t want to assume the ai are algebraic over F. Now consider the equation
p (t) = tn âˆ’ a1 tnâˆ’1 + a2 tnâˆ’2 + Â· Â· Â· Â± an .
and suppose that p (t) has distinct roots, none of them in F. Let K be a splitting ï¬eld for
p (t) over F so that
n
âˆ
p (t) =
(t âˆ’ ri )
k=1

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

484

FIELDS AND FIELD EXTENSIONS

Then it follows that
ai = si (r1 , Â· Â· Â· , rn )
where the si are the elementary symmetric functions deï¬ned in Deï¬nition F.1.2. For Ïƒ âˆˆ
G (K, F) you can deï¬ne ÏƒÌ„ âˆˆ Sn by the rule
ÏƒÌ„ (k) â‰¡ j where Ïƒ (rk ) = rj .
Recall that the automorphisms of G (K, F) take roots of p (t) to roots of p (t). This mapping
Ïƒ â†’ ÏƒÌ„ is onto, a homomorphism, and one to one because the symmetric functions si
are unchanged when the roots are permuted. Thus a rational function in s1 , s2 , Â· Â· Â· , sn is
unaï¬€ected when the roots rk are permuted. It follows that G (K, F) cannot be solvable if
n â‰¥ 5 because Sn is not solvable.
1
3
For example, consider 3x5 âˆ’ 25x3 + 45x + 1 or equivalently x5 âˆ’ 25
3 x + 15x + 3 . It clearly
has no rational roots and a graph will show it has 5 real roots. Let F be the smallest ï¬eld
contained in C which contains the coeï¬ƒcients of the polynomial and all roots of unity. Then
probably none of these roots are in F and they are all distinct. In fact, it appears that the
real numbers which are in F are rational. Therefore, from the above, none of the roots are
solvable by radicals involving numbers from F. Thus none are solvable by radicals using
numbers from the smallest ï¬eld containing the coeï¬ƒcients either.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Bibliography
[1] Apostol T., Calculus Volume II Second edition, Wiley 1969.
[2] Artin M., Algebra, Pearson 2011.
[3] Baker, Roger, Linear Algebra, Rinton Press 2001.
[4] Baker, A. Transcendental Number Theory, Cambridge University Press 1975.
[5] Chahal J.S., Historical Perspective of Mathematics 2000 B.C. - 2000 A.D. Kendrick
Press, Inc. (2007)
[6] Coddington and Levinson, Theory of Ordinary Diï¬€erential Equations McGraw Hill
1955.
[7] Davis H. and Snider A., Vector Analysis Wm. C. Brown 1995.
[8] Edwards C.H., Advanced Calculus of several Variables, Dover 1994.
[9] Friedberg S. Insel A. and Spence L., Linear Algebra, Prentice Hall, 2003.
[10] Golub, G. and Van Loan, C.,Matrix Computations, Johns Hopkins University Press,
1996.
[11] Gurtin M., An introduction to continuum mechanics, Academic press 1981.
[12] Hardy G., A Course Of Pure Mathematics, Tenth edition, Cambridge University Press
1992.
[13] Herstein I. N., Topics In Algebra, Xerox, 1964.
[14] Hofman K. and Kunze R., Linear Algebra, Prentice Hall, 1971.
[15] Householder A. The theory of matrices in numberical analysis , Dover, 1975.
[16] Horn R. and Johnson C., matrix Analysis, Cambridge University Press, 1985.
[17] Jacobsen N. Basic Algebra Freeman 1974.
[18] Karlin S. and Taylor H., A First Course in Stochastic Processes, Academic Press,
1975.
[19] Marcus M., and Minc H., A Survey Of Matrix Theory and Matrix Inequalities,
Allyn and Bacon, INc. Boston, 1964
[20] Nobel B. and Daniel J., Applied Linear Algebra, Prentice Hall, 1977.
[21] E. J. Putzer, American Mathematical Monthly, Vol. 73 (1966), pp. 2-7.
485

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

486

BIBLIOGRAPHY

[22] Rudin W., Principles of Mathematical Analysis, McGraw Hill, 1976.
[23] Rudin W., Functional Analysis, McGraw Hill, 1991.
[24] Salas S. and Hille E., Calculus One and Several Variables, Wiley 1990.
[25] Strang Gilbert, Linear Algebra and its Applications, Harcourt Brace Jovanovich 1980.
[26] Wilkinson, J.H., The Algebraic Eigenvalue Problem, Clarendon Press Oxford 1965.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Answers To Selected Exercises
G.1

Exercises

7 x = 2 âˆ’ 2t, y = âˆ’t, z = t.
9 x = t, y = s + 2, z = âˆ’s, w = s

1.6
âˆ’1

5
9
= 106
âˆ’ 106
i
âˆš
âˆš
3 âˆ’ (1 âˆ’ i) 2, (1 + i) 2.

1 (5 + i9)

G.3

Exercises

1.14
4 This makes no sense at all. You canâ€™t add diï¬€erent
size vectors.

G.4

Exercises

1.17
)1/2
(âˆ‘
âˆ‘n
n
2
Î²
|a
|
Â·
3 | k=1 Î² k ak bk | â‰¤
k=1 k k
(âˆ‘
)1/2
n
2
k=1 Î² k |bk |

4

5 If z Ì¸= 0, let Ï‰ =

z
|z|

4 The inequality still holds. See the proof of the inequality.

7 sin (5x) = 5 cos4 x sin x âˆ’ 10 cos2 x sin3 x + sin5 x
cos (5x) = cos5 x âˆ’ 10 cos3 x sin2 x + 5 cos x sin4 x
âˆš ))
(
(âˆš
)) (
(
9 (x + 2) x âˆ’ i 3 + 1
xâˆ’ 1âˆ’i 3
âˆš )) (
âˆš ))
(
(
(
11 x âˆ’ (1 âˆ’ i) 2
x âˆ’ âˆ’ (1 + i) 2 Â·
âˆš )) (
âˆš ))
(
(
(
x âˆ’ âˆ’ (1 âˆ’ i) 2
x âˆ’ (1 + i) 2
âˆš
15 There is no single âˆ’1.

G.5

Exercises

2.2
2 A=

A+AT
2

+

Aâˆ’AT
2

3 You know that Aij = âˆ’Aji . Let j = i to conclude
that Aii = âˆ’Aii and so Aii = 0.
5 0â€² = 0 + 0â€² = 0.

G.2

Exercises

6 0A = (0 + 0) A = 0A + 0A. Now add the additive
inverse of 0A to both sides.

1.11

7 0 = 0A = (1 + (âˆ’1)) A = A+(âˆ’1) A. Hence, (âˆ’1) A
is the unique additive inverse of A. Thus âˆ’A =
(âˆ’1) A. The additive inverse is unique because if A1
is an additive inverse, then A1 = A1 +(A + (âˆ’A)) =
(A1 + A) + (âˆ’A) = âˆ’A.

1 x = 2 âˆ’ 4t, y = âˆ’8t, z = t.
3 These are invalid row operations.
5 x = 2, y = 0, z = 1.
487

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

488

ANSWERS TO SELECTED EXERCISES

âˆ‘
âˆ‘ âˆ‘
10 (Ax, y) = i (Ax)i yi = i k Aik xk yi
G.6 Exercises
(
)
(
)
âˆ‘
âˆ‘
âˆ‘
âˆ‘
x,AT y = k xk i AT ki yi = k i xk Aik yi ,
2.7
the same as above. Hence the two are equal.
(
)
1 Show the map T : Rn â†’ Rm deï¬ned by T (x) = Ax
T
11 (AB) x, y â‰¡
where A is an mÃ—n matrix and x is an mÃ—1 column
vector is a linear transformation.
(x, (AB) y) =
( T
) ( T T
)
This follows from matrix multiplication rules.
A x,By = B A x, y (
. Since this holds for ev)
T
ery x, y, you have for all y, (AB) x âˆ’ B T AT x, y .
3 Find the matrix for the linear transformation which
rotates every vector in R2 through an angle of Ï€/4.
T
Let y = (AB) x âˆ’ B T AT x. Then since x is arbiâˆš )
(
) ( 1âˆš
cos (Ï€/4) âˆ’ sin (Ï€/4)
2 âˆ’ 21âˆš 2
trary, the result follows.
2
âˆš
=
1
1
sin (Ï€/4) cos (Ï€/4)
2 2
2 2
13 Give an example of matrices, A, B, C such that B Ì¸=
5 Find the matrix for the linear transformation which
C, A Ì¸= 0, and yet AB = AC.
(
)(
) (
)
rotates every vector in R2 through an angle of 2Ï€/3.
1 1
1 âˆ’1
0 0
âˆš )
(
) (
=
2 cos (Ï€/3) âˆ’2 sin (Ï€/3)
1 âˆ’ 3
1 1
âˆ’1 1
0 0
âˆš
=
(
)(
) (
)
2 sin (Ï€/3) 2 cos (Ï€/3)
3
1
1 1
âˆ’1 1
0 0
=
0 0
1 1
1 âˆ’1
7 Find the matrix for the linear transformation which
rotates every vector in R2 through an angle of 2Ï€/3
15 It appears that there are 8 ways to do this.
and then reï¬‚ects across the x axis.
(
)(
)
âˆ’1 âˆ’1
âˆ’1
1 0
cos (2Ï€/3) âˆ’ sin (2Ï€/3)
17 ABB A = AIA = I
0 âˆ’1
sin (2Ï€/3) cos (2Ï€/3)
B âˆ’1 Aâˆ’1 AB = B âˆ’1 IB = I
âˆš )
(
1
âˆ’âˆš
âˆ’ 12 3
2
Then by the deï¬nition of the inverse and its unique=
1
âˆ’1
âˆ’1
âˆ’ 12 3
ness, it follows that (AB) exists and (AB) =
2
B âˆ’1 Aâˆ’1 .
9 Find the matrix for the linear transformation which
19 Multiply both sides on the left by Aâˆ’1 .
(
)(
) (
)
1 1
1 âˆ’1
0 0
21
=
1 1
âˆ’1 1
0 0
23 Almost anything works.
(
)(
) (
)
1 2
1 2
5 2
=
3 4
2 0
11 6
(
)(
) (
)
1 2
1 2
7 10
=
2 0
3 4
2 4
(
)
âˆ’z âˆ’w
25
, z, w arbitrary.
z
w
ï£«

ï£¶âˆ’1 ï£«
3
âˆ’2
4 ï£¸ =ï£­ 0
2
1
ï£«
1
29 Row echelon form: ï£­ 0
0
verse.
1 2
ï£­
2 1
27
1 0

4
1
âˆ’2
0
1
0

5
3
2
3

ï£¶
âˆ’5
âˆ’2 ï£¸
3
ï£¶
ï£¸ . A has no in-

0

Saylor URL: http://www.saylor.org/courses/ma212/

rotates every vector in R2 through an angle of Ï€/4
and then reï¬‚ects across the x axis.
(
)(
)
1 0
cos (Ï€/4) âˆ’ sin (Ï€/4)
0 âˆ’1
sin (Ï€/4) cos (Ï€/4)
âˆš )
( 1âˆš
2 âˆ’ 12 âˆš2
2 âˆš
=
âˆ’ 12 2 âˆ’ 12 2
11 Find the matrix for the linear transformation which
reï¬‚ects every vector in R2 across the x axis and then
rotates every vector through an angle of Ï€/4.
(
)(
)
cos (Ï€/4) âˆ’ sin (Ï€/4)
1 0
sin (Ï€/4) cos (Ï€/4)
0 âˆ’1
âˆš )
( 1âˆš
1
2
2 âˆš2
2 âˆš
=
1
1
2
âˆ’
2
2 2
13 Find the matrix for the linear transformation which
reï¬‚ects every vector in R2 across the x axis and then
rotates every vector through an angle of Ï€/6.
(
)(
)
cos (Ï€/6) âˆ’ sin (Ï€/6)
1 0
sin (Ï€/6) cos (Ï€/6)
0 âˆ’1
)
( 1âˆš
1
2 3
2âˆš
=
1
âˆ’ 12 3
2

The Saylor Foundation

G.7. EXERCISES

489

15 Find the matrix for the linear transformation which
rotates every vector in R2 through an angle of 5Ï€/12.
Hint: Note that 5Ï€/12 = 2Ï€/3 âˆ’ Ï€/4.
(
)
cos (2Ï€/3) âˆ’ sin (2Ï€/3)
sin (2Ï€/3) cos (2Ï€/3)
(
)
cos (âˆ’Ï€/4) âˆ’ sin (âˆ’Ï€/4)
Â·
sin (âˆ’Ï€/4) cos (âˆ’Ï€/4)
âˆš
âˆš âˆš
âˆš )
( 1âˆš âˆš
2âˆš3 âˆ’ 14 âˆš2 âˆ’ 14âˆš 2âˆš 3 âˆ’ 14âˆš 2
4
âˆš
=
1
1
1
1
4 2 3+ 4 2
4 2 3âˆ’ 4 2

41 Obviously not. Because of the Coriolis force experienced by the ï¬red bullet which is not experienced
by the dropped bullet, it will not be as simple as
in the physics books. For example, if the bullet is
ï¬red East, then y â€² sin Ï• > 0 and will contribute to
a force acting on the bullet which has been ï¬red
which will cause it to hit the ground faster than the
one dropped. Of course at the North pole or the
South pole, things should be closer to what is expected in the physics books because there sin Ï• = 0.
Also, if you ï¬re it North or South, there seems to
be no extra force because y â€² = 0.

T

17 Find the matrix for proju (v) where u = (1, 5, 3) .
ï£«
1
1 ï£­
5
35
3

5
25
15

ï£¶
3
15 ï£¸
9

G.7
3.2

(
)
(
)
2 1 = det AAâˆ’1 = det (A) det Aâˆ’1 .
( )
3 det (A) = det AT = det (âˆ’A) = det (âˆ’I) det (A) =
n
(âˆ’1) det (A) = âˆ’ det (A) .

19 Give an example of a 2Ã—2 matrix A which has all its
entries nonzero and satisï¬es A2 = A. Such a matrix
is called idempotent.
You know it canâ€™t be invertible. So try this.
(

a a
b b

)2

(
=

a2 + ba a2 + ba
b2 + ab b2 + ab

Exercises

)

6 Each time you take out an a from a row, you multiply by a the determinant of the matrix which remains. Since there are n rows, you do this n times,
hence you get an .
(
)
(
)
9 det A = det P âˆ’1 BP = det P âˆ’1 det (B) det (P )
(
)
= det (B) det P âˆ’1 P = det (B) .

Let a2 + ab = a, b2 + ab = b. A solution which yields
a nonzero matrix is
(
)
2
2
âˆ’1 âˆ’1

11 If that determinant equals 0 then the matrix Î»I âˆ’ A
has no inverse. It is not one to one and so there
exists x Ì¸= 0 such that (Î»I âˆ’ A) x = 0. Also recall
the process for ï¬nding the inverse.
ï£« âˆ’t
ï£¶
e
0
0
eâˆ’t (cos t + sin t) âˆ’ (sin t) eâˆ’t ï£¸
13 ï£­ 0
0 âˆ’eâˆ’t (cos t âˆ’ sin t) (cos t) eâˆ’t
( )
2
15 You have to have det (Q) det QT = det (Q) = 1
and so det (Q) = Â±1.

21 x2 = âˆ’ 21 t1 âˆ’ 12 t2 âˆ’ t3 , x1 = âˆ’2t1 âˆ’ t2 + t3 where the
ti are arbitrary.
ï£«
ï£¶ ï£«
ï£¶
âˆ’2t1 âˆ’ t2 + t3
4
ï£¬ âˆ’ 1 t1 âˆ’ 1 t2 âˆ’ t3 ï£· ï£¬ 7/2 ï£·
2
ï£¬ 2
ï£· ï£¬
ï£·
ï£· + ï£¬ 0 ï£· , ti âˆˆ F
t1
23 ï£¬
ï£¬
ï£· ï£¬
ï£·
ï£­
ï£¸ ï£­ 0 ï£¸
t2
0
t3
That second vector is a particular solution.
25 Show that the function Tu deï¬ned by Tu (v) â‰¡ v âˆ’
proju (v) is also a linear transformation.

G.8

This is the sum of two linear transformations so it 3.6
is obviously linear.
33 Let a basis for W be {w1 , Â· Â· Â· , wr } Then if there
exists v âˆˆ V \ W, you could add in v to the basis
and obtain a linearly independent set of vectors of
V which implies that the dimension of V is at least
r + 1 contrary to assumption.

Saylor URL: http://www.saylor.org/courses/ma212/

Exercises
ï£«

ï£¶
1 2 3 2
ï£¬ âˆ’6 3 2 3 ï£·
ï£·
5 det ï£¬
ï£­ 5 2 2 3 ï£¸=5
3 4 6 4

The Saylor Foundation

490

ANSWERS TO SELECTED EXERCISES

ï£«

ï£¶
1 âˆ’t
0
2e
1
1
ï£¸
âˆ’ sin t
6 ï£­ 12
2 sin t âˆ’ 2 cos t
1
1
1
cos
t
âˆ’
cos
t
âˆ’
sin
t
2
2
2
(
)
âˆ’1
8 det (Î»I âˆ’ A) = det Î»I âˆ’ S BS
(
)
= det Î»S âˆ’1 S âˆ’ S âˆ’1 BS
(
)
= det S âˆ’1 (Î»I âˆ’ B) S
(
)
= det S âˆ’1 det (Î»I âˆ’ B) det (S)
(
)
= det S âˆ’1 S det (Î»I âˆ’ B) = det (Î»I âˆ’ B)
1 âˆ’t
2e
cos t + 12 sin t
sin t âˆ’ 12 cos t

9 From the Cayley Hamilton theorem,An +anâˆ’1 Anâˆ’1 +
Â· Â· Â· + a1 A + a0 I = 0. Also the characteristic polynomial is det (tI âˆ’ A) and the constant term is
n

(âˆ’1) det (A) . Thus a0 Ì¸= 0 if and only if det (A) Ì¸=
0 if and only if Aâˆ’1 has an inverse. Thus if Aâˆ’1
exists, it follows that
(
)
a0 I = âˆ’ An + anâˆ’1 Anâˆ’1 + Â· Â· Â· + a1 A
(
)
= A âˆ’Anâˆ’1 âˆ’ anâˆ’1 Anâˆ’2 âˆ’ Â· Â· Â· âˆ’ a1 I and also
(
)
a0 I = âˆ’Anâˆ’1 âˆ’ anâˆ’1 Anâˆ’2 âˆ’ Â· Â· Â· âˆ’ a1 I A Therefore, the inverse is
(
)
1
nâˆ’1
âˆ’ anâˆ’1 Anâˆ’2 âˆ’ Â· Â· Â· âˆ’ a1 I
a0 âˆ’A
11 Say the characteristic polynomial is q (t) which is of
degree 3. Then if n â‰¥ 3, tn = q (t) l (t) + r (t) where
the degree of r (t) is either less than 3 or it equals
zero. Thus An = q (A) l (A) + r (A) = r (A) and
so all the terms An for n â‰¥ 3 can be replaced with
some r (A) where the degree of r (t) is no more than
2. Thus, assuming there are no convergence
issues,
âˆ‘2
the inï¬nite sum must be of the form k=0 bk Ak .

G.9

Exercises

4.6
1 A typical thing in {Ax : x âˆˆ P (u1 , Â· Â· Â· , un )} is
âˆ‘n
k=1 tk Auk : tk âˆˆ [0, 1] and so it is just
P (Au1 , Â· Â· Â· , Aun ) .
(
)
1 1
2 E=
0 1

5 Here they
ï£«
1
ï£­ 0
0
ï£«
0
ï£­ 1
0

are.
0
1
0
1
0
0

ï£¶ ï£«
0
0 ï£¸,ï£­
1
ï£¶ ï£«
0
0 ï£¸,ï£­
1

0
1
0

0
0
1

1
0
0

0
0
1

ï£¶ ï£«
1
0 ï£¸,ï£­
0
ï£¶ ï£«
0
1 ï£¸,ï£­
0

ï£¶
0
1 ï£¸
0
ï£¶
0 0 1
0 1 0 ï£¸
1 0 0
0 1
0 0
1 0

So what is the dimension of the span of these? One
way to systematically accomplish this is to unravel
them and then use the row reduced echelon form.
Unraveling these yields the column vectors
ï£¶
ï£« ï£¶ï£« ï£¶ï£« ï£¶ï£« ï£¶ï£« ï£¶ï£«
1
0
0
0
1
0
ï£¬ 0 ï£·ï£¬ 0 ï£·ï£¬ 1 ï£·ï£¬ 1 ï£·ï£¬ 0 ï£·ï£¬ 0 ï£·
ï£·
ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬
ï£¬ 0 ï£·ï£¬ 1 ï£·ï£¬ 0 ï£·ï£¬ 0 ï£·ï£¬ 0 ï£·ï£¬ 1 ï£·
ï£·
ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬
ï£¬ 0 ï£·ï£¬ 1 ï£·ï£¬ 0 ï£·ï£¬ 1 ï£·ï£¬ 0 ï£·ï£¬ 0 ï£·
ï£·
ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬
ï£¬ 1 ï£·ï£¬ 0 ï£·ï£¬ 0 ï£·ï£¬ 0 ï£·ï£¬ 0 ï£·ï£¬ 1 ï£·
ï£·
ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬
ï£¬ 0 ï£·ï£¬ 0 ï£·ï£¬ 1 ï£·ï£¬ 0 ï£·ï£¬ 1 ï£·ï£¬ 0 ï£·
ï£·
ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬
ï£¬ 0 ï£·ï£¬ 0 ï£·ï£¬ 1 ï£·ï£¬ 0 ï£·ï£¬ 0 ï£·ï£¬ 1 ï£·
ï£·
ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬ ï£·ï£¬
ï£­ 0 ï£¸ï£­ 1 ï£¸ï£­ 0 ï£¸ï£­ 0 ï£¸ï£­ 1 ï£¸ï£­ 0 ï£¸
0
0
1
0
0
1
Then arranging these as the columns of a matrix
yields the following along with its row reduced echelon form.
ï£«
ï£¶
1 0 0 0 1 0
ï£¬ 0 0 1 1 0 0 ï£·
ï£¬
ï£·
ï£¬ 0 1 0 0 0 1 ï£·
ï£¬
ï£·
ï£¬ 0 1 0 1 0 0 ï£·
ï£¬
ï£·
ï£¬ 1 0 0 0 0 1 ï£·, row echelon form:
ï£¬
ï£·
ï£¬ 0 0 1 0 1 0 ï£·
ï£¬
ï£·
ï£¬ 0 0 1 0 0 1 ï£·
ï£¬
ï£·
ï£­ 0 1 0 0 1 0 ï£¸
1 0 0 1 0 0
ï£«
ï£¶
1 0 0 0 0 1
ï£¬ 0 1 0 0 0 1 ï£·
ï£¬
ï£·
ï£¬ 0 0 1 0 0 1 ï£·
ï£¬
ï£·
ï£¬ 0 0 0 1 0 âˆ’1 ï£·
ï£¬
ï£·
ï£¬ 0 0 0 0 1 âˆ’1 ï£·
ï£¬
ï£·
ï£¬ 0 0 0 0 0 0 ï£·
ï£¬
ï£·
ï£¬ 0 0 0 0 0 0 ï£·
ï£¬
ï£·
ï£­ 0 0 0 0 0 0 ï£¸
0 0 0 0 0 0
The dimension is 5.

P (e1 , e2 )

E(P (e1 , e2 ))

Saylor URL: http://www.saylor.org/courses/ma212/

10 It is because you cannot have more than min (m, n)
nonzero rows in the row reduced echelon form. Recall that the number of pivot columns is the same
as the number of nonzero rows from the description
of this row reduced echelon form.

The Saylor Foundation

G.10. EXERCISES

491
ï£«

11 It follows from the fact that e1 , Â· Â· Â· , em occur as
columns in row reduced echelon form that the dimension of the column space of A is n and so, since
this column space is A (Rn ) , it follows that it equals
Fm .

1
ï£¬ 0
ï£¬
ï£­ 0
0

2

21 |bâˆ’Ay| = |bâˆ’Ax+Axâˆ’Ay|
2

2

2

2

2

0
0
1
0

ï£¶ ï£«
0
a d
ï£¬ n g
1 ï£·
ï£·=ï£¬
0 ï£¸ ï£­ j m
0
e h

ï£¶
c b
h t ï£·
ï£·
l k ï£¸
z f

More formally, the iith entry of P ij AP ij is
âˆ‘ ij
ij
ij
Pis Asp Ppi
= Pijij Ajj Pji
= Aij

12 Since m > n the dimension of the column space of
A is no more than n and so the columns of A cannot
span Fm .
âˆ‘
15 âˆ‘
If
i ci zi = 0, apply A to both sides to obtain
c
i i wi = 0. By assumption, each ci = 0.
19 There are more columns than rows and at most m
can be pivot columns so it follows at least one column is a linear combination of the others hence A
is not one too one.

0
0
0
1

s,p

31 If A has an inverse, then it is one to one. Hence the
columns are independent. Therefore, they are each
pivot columns. Therefore, the row reduced echelon
form of A is I. This is what was needed for the
procedure to work.

G.10
5.8

= |bâˆ’Ax| + |Ax âˆ’ Ay| + 2 (bâˆ’Ax,A (x âˆ’ y))
(
)
2
2
= |bâˆ’Ax| +|Ax âˆ’ Ay| +2 AT bâˆ’AT Ax, (x âˆ’ y)
= |bâˆ’Ax| + |Ax âˆ’ Ay| and so, Ax is closest to b
out of all vectors Ay.
ï£¶
ï£«
1 0 2 0
ï£¬ 0 1 1 7 ï£·
ï£·
27 No. ï£¬
ï£­ 0 0 0 1 ï£¸
0 0 0 0
29 Let A be an m Ã— n matrix. Then ker (A) is a subspace of Fn . Is it true that every subspace of Fn is
the kernel or null space of some matrix? Prove or
disprove.
Let M be a subspace of Fn . If it equals {0} , consider
the matrix I. Otherwise, it has a basis {m1 , Â· Â· Â· , mk } .
Consider the matrix
(
)
m1 Â· Â· Â· mk 0
where 0 is either not there in case k = n or has
n âˆ’ k columns.
30 This is easy to see when you consider that P ij is
its own inverse and that P ij multiplied on the right
switches the ith and j th columns. Thus you switch
the columns and then you switch the rows. This has
the eï¬€ect of switching Aii and Ajj . For example,
ï£«
ï£¶ï£«
ï£¶
1 0 0 0
a b c d
ï£¬ 0 0 0 1 ï£·ï£¬ e f z h ï£·
ï£¬
ï£·ï£¬
ï£·
ï£­ 0 0 1 0 ï£¸ï£­ j k l m ï£¸Â·
0 1 0 0
n t h g

Saylor URL: http://www.saylor.org/courses/ma212/

Exercises

ï£«

ï£¶
1 2 0
1 ï£­ 2 1 3 ï£¸. =
1 2 3
ï£¶
ï£¶ï£«
ï£«
1 2 0
1 0 0
ï£­ 2 1 0 ï£¸ ï£­ 0 âˆ’3 3 ï£¸
0 0 3
1 0 1
ï£«
ï£¶
ï£«
ï£¶
1 2 1
1 0 0
3 ï£­ 1 2 2 ï£¸. = ï£­ 0 0 1 ï£¸Â·
2 1 1
0 1 0
ï£«
ï£¶ï£«
ï£¶
1 0 0
1 2
1
ï£­ 2 1 0 ï£¸ ï£­ 0 âˆ’3 âˆ’1 ï£¸
1 0 1
0 0
1
ï£«
ï£¶
ï£«
ï£¶
1 2 1
1 0 0 0
ï£¬ 1 2 2 ï£·
ï£¬ 0 0 0 1 ï£·
ï£·
ï£¬
ï£·
5 ï£¬
ï£­ 2 4 1 ï£¸. = ï£­ 0 0 1 0 ï£¸Â·
3 2 1
0 1 0 0
ï£¶ï£«
ï£¶
ï£«
1 0 0 0
1 2
1
ï£¬ 3 1 0 0 ï£· ï£¬ 0 âˆ’4 âˆ’2 ï£·
ï£¬
ï£·ï£¬
ï£·
ï£­ 2 0 1 0 ï£¸ ï£­ 0 0 âˆ’1 ï£¸
1 0 âˆ’1 1
0 0
0
ï£«
ï£¶
1 2 1 0
9 ï£­ 3 0 1 1 ï£¸
1 0 2 1
âˆš âˆš
ï£¶
ï£« 1âˆš
1
10 âˆš
11
0 âˆš
11 âˆš11
11 âˆš
âˆš
3
3
1
ï£¸Â·
= ï£­ 11
âˆš11 âˆ’ 110
âˆš10âˆš11 âˆ’310âˆš 2âˆš 5
1
1
11
âˆ’
10
11
2
5
11
110
10
âˆš
âˆš
âˆš
ï£« âˆš
ï£¶
2
6
4
11
11
11
11
11
11
11
âˆš
âˆš
âˆš
âˆš
âˆš
âˆš
2
1
2
ï£­ 0
ï£¸
11 10 11
22 âˆš10âˆš 11 âˆ’ 55 âˆš 10
âˆš 11
1
1
0
0
2 2 5
5 2 5

The Saylor Foundation

492

ANSWERS TO SELECTED EXERCISES

G.11

ï£«

Exercises

6.6
1 The maximum is 7 and it occurs when x1 = 7, x2 =
0, x3 = 0, x4 = 3, x5 = 5, x6 = 0.
2 Maximize and minimize the following if possible.
All variables are nonnegative.
(a) The minimum is âˆ’7 and it happens when x1 =
0, x2 = 7/2, x3 = 0.
(b) The maximum is 7 and it occurs when x1 =
7, x2 = 0, x3 = 0.
(c) The maximum is 14 and it happens when x1 =
7, x2 = x3 = 0.
(d) The minimum is 0 when x1 = x2 = 0, x3 = 1.
4 Find a solution to the following inequalities for x, y â‰¥
0 if it is possible to do so. If it is not possible, prove
it is not possible.
(a) There is no solution to these inequalities with
x1 , x2 â‰¥ 0.
(b) A solution is x1 = 8/5, x2 = x3 = 0.
(c) There will be no solution to these inequalities
for which all the variables are nonnegative.
(d) There is a solution when x2 = 2, x3 = 0, x1 =
0.
(e) There is no solution to this system of inequalities because the minimum value of x7 is not
0.

G.12

Exercises

7.3
1 Because the vectors which result are not parallel to
the vector you begin with.
3 Î» â†’ Î»âˆ’1 and Î» â†’ Î»m .
5 Let x be the eigenvector. Then Am x = Î»m x,Am x =
Ax = Î»x and so
Î»m = Î»
Hence if Î» Ì¸= 0, then
Î»mâˆ’1 = 1
and so |Î»| = 1.

Saylor URL: http://www.saylor.org/courses/ma212/

ï£¶
âˆ’1 âˆ’1 7
7 ï£­ âˆ’1 0 4 ï£¸, eigenvectors:
âˆ’1 âˆ’1 5
ï£± ï£¼
ï£± ï£¼
ï£² 3 ï£½
ï£² 2 ï£½
1
1
â†” 1,
â†” 2. This is a defective maï£³ ï£¾
ï£³ ï£¾
1
1
trix.
ï£«
ï£¶
âˆ’7 âˆ’12 30
9 ï£­ âˆ’3 âˆ’7 15 ï£¸, eigenvectors:
âˆ’3 âˆ’6 14
ï£±ï£«
ï£±ï£«
ï£¶ ï£« ï£¶ï£¼
ï£¶ï£¼
5 ï£½
ï£² âˆ’2
ï£² 2 ï£½
ï£­ 1 ï£¸ , ï£­ 0 ï£¸ â†” âˆ’1, ï£­ 1 ï£¸ â†” 2
ï£³
ï£¾
ï£³
ï£¾
0
1
1
This matrix is not defective because, even though
Î» = 1 is a repeated eigenvalue, it has a 2 dimensional eigenspace.
ï£¶
ï£«
3 âˆ’2 âˆ’1
1 ï£¸, eigenvectors:
11 ï£­ 0 5
0 2
4
ï£±ï£« ï£¶ ï£«
ï£±ï£«
ï£¶ï£¼
ï£¶ï£¼
0
ï£² 1
ï£² âˆ’1 ï£½
ï£½
ï£­ 0 ï£¸ , ï£­ âˆ’ 1 ï£¸ â†” 3, ï£­ 1 ï£¸ â†” 6
2
ï£³
ï£¾
ï£³
ï£¾
0
1
1
This matrix is not defective.
ï£«
ï£¶
5 2 âˆ’5
13 ï£­ 12 3 âˆ’10 ï£¸, eigenvectors:
12 4 âˆ’11
ï£±ï£« 1 ï£¶ ï£« 5 ï£¶ï£¼
ï£½
ï£² âˆ’3
6
ï£­ 1 ï£¸ , ï£­ 0 ï£¸ â†” âˆ’1
ï£¾
ï£³
0
1
This matrix is defective. In this case, there is only
one eigenvalue, âˆ’1 of multiplicity 3 but the dimension of the eigenspace is only 2.
ï£¶
ï£«
1
26 âˆ’17
âˆ’4
4 ï£¸, eigenvectors:
15 ï£­ 4
âˆ’9 âˆ’18
9
ï£±ï£« 1 ï£¶ï£¼
ï£±ï£«
ï£¶ï£¼
ï£² âˆ’3 ï£½
ï£² âˆ’2 ï£½
ï£­ 2 ï£¸ â†” 0, ï£­ 1 ï£¸ â†” âˆ’12,
3
ï£³
ï£¾
ï£³
ï£¾
0
1
ï£±ï£«
ï£¶ï£¼
ï£² âˆ’1 ï£½
ï£­ 0 ï£¸ â†” 18
ï£¾
ï£³
1
ï£± ï£« 3 ï£¶ï£¼
ï£«
ï£¶
âˆ’2
1 2
ï£² 4 ï£½
17 ï£­ âˆ’11 âˆ’2 9 ï£¸, eigenvectors: ï£­ 14 ï£¸ â†” 1
ï£³
ï£¾
âˆ’8
0 7
1
This is defective.

The Saylor Foundation

G.13. EXERCISES
ï£¶
âˆ’2 âˆ’2
2 âˆ’2 ï£¸, eigenvectors:
0
2
ï£±ï£«
ï£¶ï£¼
ï£¶ï£¼
1
ï£½
ï£² âˆ’i ï£½
âˆ’1 ï£¸ â†” 4, ï£­ âˆ’i ï£¸ â†” 2 âˆ’ 2i,
ï£¾
ï£³
ï£¾
1
1
ï£¼
ï£¶
i ï£½
i ï£¸ â†” 2 + 2i
ï£¾
1
ï£«
ï£¶
4 âˆ’2 âˆ’2
21 ï£­ 0 2 âˆ’2 ï£¸, eigenvectors:
2 0
2
ï£±ï£«
ï£±ï£«
ï£¶ï£¼
ï£¶ï£¼
1
ï£²
ï£½
ï£² âˆ’i ï£½
ï£­ âˆ’1 ï£¸ â†” 4, ï£­ âˆ’i ï£¸ â†” 2 âˆ’ 2i,
ï£³
ï£¾
ï£³
ï£¾
1
1
ï£±ï£« ï£¶ï£¼
ï£² i ï£½
ï£­ i ï£¸ â†” 2 + 2i
ï£³
ï£¾
1
ï£«
ï£¶
1
1 âˆ’6
23 ï£­ 7 âˆ’5 âˆ’6 ï£¸, eigenvectors:
âˆ’1 7
2
ï£±ï£«
ï£±ï£«
ï£¶ï£¼
ï£¶ï£¼
1
ï£² âˆ’i ï£½
ï£½
ï£²
ï£­ âˆ’1 ï£¸ â†” âˆ’6, ï£­ âˆ’i ï£¸ â†” 2 âˆ’ 6i,
ï£¾
ï£³
ï£¾
ï£³
1
1
ï£±ï£« ï£¶ï£¼
ï£² i ï£½
ï£­ i ï£¸ â†” 2 + 6i
ï£³
ï£¾
1

493

ï£«

and so Î» = âˆ’Î»Ì„. Thus a + ib = âˆ’ (a âˆ’ ib) and so
a = 0.

4
19 ï£­ 0
2
ï£±ï£«
ï£²
ï£­
ï£³
ï£±ï£«
ï£²
ï£­
ï£³

31 This follows from the observation that if Ax = Î»x,
then Ax = Î»x
ï£«ï£«
ï£¶ ï£¶ ï£«ï£« 1 ï£¶ ï£¶ ï£«ï£« ï£¶ ï£¶
1
âˆ’2
1
33 ï£­ï£­ âˆ’1 ï£¸ , 1ï£¸ , ï£­ï£­ 12 ï£¸ , 12 ï£¸ , ï£­ï£­ 1 ï£¸ , 13 ï£¸
1
0
1
ï£«
ï£¶
âˆ’1
35 ï£­ 1 ï£¸ (a cos (t) + b sin (t)) ,
1
ï£«
ï£¶
0
(âˆš ))
(
(âˆš )
ï£­ âˆ’1 ï£¸ c sin 2t + d cos 2t ,
1
ï£« ï£¶
2
ï£­ 1 ï£¸ (e cos (2t) + f sin (2t))where a, b, c, d, e, f are
1
scalars.

G.13

Exercises

7.10

This is not defective.
25 First consider the eigenvalue Î» = 1. Then you have
ax2 = 0, bx3 = 0. If neither a nor b = 0 then
Î» = 1 would be a defective eigenvalue and the matrix would be defective. If a = 0, then the dimension of the eigenspace is clearly 2 and so the matrix would be nondefective. If b = 0 but a Ì¸= 0,
then you would have a defective matrix because the
eigenspace would have dimension less than 2. If
c Ì¸= 0, then the matrix is defective. If c = 0 and
a = 0, then it is non defective. Basically, if a, c Ì¸= 0,
then the matrix is defective.
27 A (x + iy) = (a + ib) (x + iy) . Now just take complex conjugates of both sides.
29 Let A be skew symmetric. Then if x is an eigenvector for Î»,
Î»xT xÌ„ = xT AT xÌ„ = âˆ’xT AxÌ„ = âˆ’xT xÌ„Î»Ì„

Saylor URL: http://www.saylor.org/courses/ma212/

1 To get it, you must be able to get the eigenvalues
and this is typically not possible.
(
) (
)(
)
0 âˆ’1
0 âˆ’1
2 0
4
=
2 0
1 0
0 1
(
)(
)
2 0
0 âˆ’1
A1 =
0 1
1 0
(
)
0 âˆ’2
=
1 0
(
) (
)(
)
0 âˆ’2
0 âˆ’1
1 0
=
1 0
1 0
0 2
(
)(
)
(
)
1 0
0 âˆ’1
0 âˆ’1
A2 =
=
. Now
0 2
1 0
2 0
it is back to where you started. Thus the algorithm
merely
between
(
) bounces
(
) the two matrices
0 âˆ’1
0 âˆ’2
and
and so it canâ€™t possi2 0
1 0
bly converge.
15 B (1 + 2i, 6) , B (i, 3) , B (7, 11)
19 Gerschgorinâ€™s theorem shows that there are no zero
eigenvalues and so the matrix is invertible.
21 6xâ€²2 + 12y â€²2 + 18z â€²2 .
âˆš
âˆš
âˆš
23 (xâ€² )2 + 31 3xâ€² âˆ’ 2(y â€² )2 âˆ’ 21 2y â€² âˆ’ 2(z â€² )2 âˆ’ 61 6z â€²

The Saylor Foundation

494

ANSWERS TO SELECTED EXERCISES

âˆ‘n
âˆ‘
âˆ‘
43 âˆ‘
Suppose
âˆ‘ i=1 ai gi = 0. Then 0 =âˆ‘ i ai j Aij fj =
j fj
i Aij ai . It follows that
i Aij ai = 0 for
T
each j. Therefore, since A is invertible, it follows
that each ai = 0. Hence the functions gi are linearly
independent.

25 (0, âˆ’1, 0) (4, âˆ’1, 0) saddle point. (2, âˆ’1, âˆ’12) local
minimum.
27 (1, 1) , (âˆ’1, 1) , (1, âˆ’1) , (âˆ’1, âˆ’1) saddle points.
( 1âˆš âˆš ) (1âˆš âˆš )
âˆ’ 6 5 6, 0 , 6 5 6, 0 Local minimums.
29 Critical points: (0, 1, 0) , Saddle point.

G.15

31 Â±1

Exercises

9.5

G.14

Exercises

1 This is because ABC is one to one.

8.4

7 In the following examples, a linear transformation,
T is given by specifying its action on a basis Î². Find
its matrix with respect to this basis.
(
)
2 0
(a)
1 1
(
)
2 1
(b)
1 0
(
)
1 1
(c)
2 âˆ’1
ï£«
ï£¶
0 1 0 0
ï£¬ 0 0 2 0 ï£·
ï£·
11 A = ï£¬
ï£­ 0 0 0 3 ï£¸
0 0 0 0
ï£«
ï£¶
1 0 2 0 0
ï£¬ 0 1 0 6 0 ï£·
ï£¬
ï£·
ï£·
13 ï£¬
ï£¬ 0 0 1 0 12 ï£·
ï£­ 0 0 0 1 0 ï£¸
0 0 0 0 1

1 The ï¬rst three vectors form a basis and the dimension is 3.
3 No. Not a subspace. Consider (0, 0, 1, 0) and multiply by âˆ’1.
5 NO. Multiply something by âˆ’1.
7 No. Take something nonzero in M where say u1 =
1. Now multiply by 100.
9 Suppose {x1 , Â· Â· Â· , xk } is a set of vectors from Fn .
Show that 0 is in span (x1 , Â· Â· Â· , xk ) .
âˆ‘
0 = i 0xi
11 ï£«
It is aï£¶subspace.
by
ï£« ï£¶ It
ï£«is spanned
ï£¶
3
2
1
ï£¬ 1 ï£· ï£¬ 1 ï£· ï£¬ 0 ï£·
ï£¬ ï£· , ï£¬ ï£· , ï£¬ ï£· . These are also indepenï£­ 1 ï£¸ ï£­ 1 ï£¸ ï£­ 0 ï£¸
0
0
1
dent so they constitute a basis.
13 Pick n points {x1 , Â· Â· Â· , xn } . Then let ei (x) = 0 unn
less x = xi when it equals 1. Then {ei }i=1 is linearly
independent, this for any n.
{
}
15 1, x, x2 , x3 , x4
âˆ‘n
âˆ‘n
17 L ( i=1 ci vi ) â‰¡ i=1 ci wi

15 You can see these are not similar by noticing that
the second has an eigenspace of dimension equal to
1 so it is not similar to any diagonal matrix which
is what the ï¬rst one is.
19 This is because the general solution is yp + y where
Ayp = b and Ay = 0. Now A0 = 0 and so the solution is unique precisely when this is the only solution y to Ay = 0.

19 No. There is a spanning set having 5 vectors and
this would need to be as long as the linearly independent set.
23 No. It canâ€™t. It does not contain 0.

G.16

Exercises

25 No. This would lead to 0 = 1.The last one must 10.6
not be a pivot column and the ones to the left must
(
) (
)
1 1
1 0
each be pivot columns.
2 Consider
,
. These are both in
0 1
0 1
Jordan form.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

G.19. EXERCISES

495
ï£«

ï£¶
0 âˆ’1 âˆ’1 0
ï£¬ âˆ’1 0 âˆ’1 0 ï£·
ï£·
8 ï£¬
ï£­ 1
1
2 0 ï£¸
3
3
3 1
ï£«
ï£¶
1 0 0
9 ï£­ 0 0 1 ï£¸
0 1 0
(
) ( 1
1/2 1/3
âˆ’2
12 Try
,
1/2 2/3
1

8 Î»3 âˆ’ Î»2 + Î» âˆ’ 1
10 Î»2
11 Î»3 âˆ’ 3Î»2 + 14
ï£«
1 1 0 0
ï£¬ 0 1 0 0
16 ï£¬
ï£­ 0 0 2 1
0 0 0 2

G.17

ï£¶
ï£·
ï£·
ï£¸

Exercises

G.19

10.9
4 Î»3 âˆ’ 3Î»2 + 14
ï£«
ï£¶
0 0 âˆ’14
0 ï£¸
5 ï£­ 1 0
0 1
3
ï£«
ï£¶
0 0 0 âˆ’3
ï£¬ 1 0 0 âˆ’1 ï£·
ï£·
6 ï£¬
ï£­ 0 1 0 âˆ’11 ï£¸
0 0 1
8
ï£«
ï£¶
2 0 0
7 ï£­ 0 0 âˆ’7 ï£¸
0 1 âˆ’2
ï£«
ï£¶
ï£«
1 0
0 âˆ’1 0
8 ï£­ 1 0 0 ï£¸ , Q, ï£­ 0 i
0 0
0 0 1

G.18

)

5
3

Exercises

12.7
(
1
ï£«
2 ï£­

)

17
15
1
45

âˆš âˆš ï£¶ ï£« 2âˆš ï£¶
1
5 6
âˆ’ 30
5 5
âˆš
âˆš
ï£¸,ï£­ 1 5 6 ï£¸,ï£­
0âˆš ï£¸
6 âˆš âˆš
1
1
âˆ’5 5
âˆ’ 15 5 6

âˆš ï£¶ ï£«

1
6 âˆš6
1
6 âˆš6
1
3 6

1/2

3 |(Ax, y)| â‰¤ (Ax, x) (Ay, y)
âˆš (
) }
{ âˆš
1, 3 âˆš
(2x(âˆ’ 1) , 6 5 x2 âˆ’ x +)61
9
1
, 20 7 x3 âˆ’ 32 x2 + 53 x âˆ’ 20
ï£¶

0
0 ï£¸ , Q + iQ
âˆ’i

Exercises

11 2x3 âˆ’ 97 x2 + 27 x âˆ’
âˆš
ï£«
ï£¶
9
146
âˆ’ 146
âˆš
ï£¬ 2 146 ï£·
ï£·
73 âˆš
14 ï£¬
ï£­ 7 146 ï£¸
146
0
2

11.4

âˆ’1

1
70

2

16 |x + y| + |x âˆ’ y| = (x + y, x + y) + (x âˆ’ y, x âˆ’ y)
ï£«

2

.6
1 ï£­ .9 ï£¸
1
6 ï£«
The columns are
ï£¶ ï£« 1
n
1
2n âˆ’ (âˆ’1) + 1
2n âˆ’ 1
ï£¬ 2n âˆ’ 3 (âˆ’1)n + 1 ï£· ï£¬ 2n âˆ’ 1
ï£¬ 21
ï£· ï£¬ 2
ï£­ n âˆ’ 2 (âˆ’1)n + 1 ï£¸ , ï£­ 1n âˆ’ 1
2
2
n
1
1
2n âˆ’ 2 (âˆ’1) + 1
2n âˆ’ 1
ï£«
ï£¶ ï£«
ï£¶
n
(âˆ’1) âˆ’ 22n + 1
0
n
ï£¬ 0 ï£· ï£¬ 3 (âˆ’1) âˆ’ 4n + 1 ï£·
2
ï£¬ 1 ï£·,ï£¬
ï£·
ï£­ n ï£¸ ï£­ 2 (âˆ’1)n âˆ’ 3n + 1 ï£¸
2
2
n
0
2 (âˆ’1) âˆ’ 22n + 1

2

2

2

= |x| + |y| + 2 (x, y) + |x| + |y| âˆ’ 2 (x, y) .

ï£¶

ï£¶
ï£·
ï£·,
ï£¸

Saylor URL: http://www.saylor.org/courses/ma212/

21 Give an example of two vectors in R4 x, y and a
subspace V such that x Â· y = 0 but P xÂ·P y Ì¸= 0
where P denotes the projection map which sends x
to its closest point on V .
Try this. V is the span of e1 and e2 and x = e3 +
e1 , y = e4 + e1 .
P x = (e3 + e1 , e1 ) e1 + (e3 + e1 , e2 ) e2 = e1
P y = (e4 + e1 , e1 ) e1 + (e4 + e1 , e2 ) e2 = e1
P xÂ·P y = 1
22 y =

13
5 x

âˆ’

2
5

The Saylor Foundation

496

ANSWERS TO SELECTED EXERCISES

G.20

Exercises

G.23

12.9

15.3

âˆš
1 volume is 218
3 0.

G.21

Exercises

13.12
13 This is easy because you show it preserves distances.
15 (Ax, x) = (U DU âˆ— x, x) = (DU âˆ— x,U âˆ— x) â‰¥ Î´ 2 |U âˆ— x| =
2
Î´ 2 |x|
2

16 0 > ((A + Aâˆ— ) x, x) = (Ax, x) + (Aâˆ— x, x)
= (Ax, x) + (Ax, x) Now let Ax = Î»x. Then you
2
2
2
get 0 > Î» |x| + Î»Ì„ |x| = Re (Î») |x|
19 If Ax = Î»x, then you can take the norm of both
sides and conclude that |Î»| = 1. It follows that the
eigenvalues of A are eiÎ¸ , eâˆ’iÎ¸ and another one which
has magnitude 1 and is real. This can only be 1 or
âˆ’1. Since the determinant is given to be 1, it follows
that it is 1. Therefore, there exists an eigenvector
for the eigenvalue 1.

G.22
14.7

Exercises

ï£«

1
ï£­
2
1
3
ï£±ï£«
ï£²
ï£­
ï£³
ï£±ï£«
ï£²
ï£­
ï£³
ï£±ï£«
ï£²
ï£­
ï£³
ï£«

3
2 ï£­ 2
1
ï£±ï£«
ï£²
ï£­
ï£³
ï£±ï£«
ï£²
ï£­
ï£³
ï£±ï£«
ï£²
ï£­
ï£³
ï£«

ï£«

ï£¶

0.09
1 ï£­ 0.21 ï£¸
0.43
ï£«
ï£¶
4. 237 3 Ã— 10âˆ’2
3 ï£­ 7. 627 1 Ã— 10âˆ’2 ï£¸
0.711 86
28 You have H = U âˆ— DU where U is unitary and D is
a real diagonal matrix. Then you have
ï£« iÎ»
ï£¶
e 1
âˆ
n
âˆ‘
(iD)
ï£·
ï£¬
..
U = Uâˆ— ï£­
eiH = U âˆ—
ï£¸U
.
n!
n=0
eiÎ»n
and this is clearly unitary because each matrix in
the product is.

Saylor URL: http://www.saylor.org/courses/ma212/

3
3 ï£­ 2
1
ï£±ï£«
ï£²
ï£­
ï£³
ï£±ï£«
ï£²
ï£­
ï£³
ï£±ï£«
ï£²
ï£­
ï£³
ï£«

0
4 ï£­ 2
1
ï£±ï£«
ï£²
ï£­
ï£³

Exercises
ï£¶
3
1.0 ï£¸, eigenvectors:
4
ï£¶ï£¼
0.534 91 ï£½
0.390 22 ï£¸ â†” 6. 662,
ï£¾
0.749 4
ï£¶ï£¼
0.130 16
ï£½
0.838 32 ï£¸ â†” 1. 679 0,
ï£¾
âˆ’0.529 42
ï£¶ï£¼
0.834 83
ï£½
âˆ’0.380 73 ï£¸ â†” âˆ’1. 341
ï£¾
âˆ’0.397 63
ï£¶
2 1.0
1 3 ï£¸, eigenvectors:
3 2
ï£¶ï£¼
0.577 35 ï£½
0.577 35 ï£¸ â†” 6.0,
ï£¾
0.577 35
ï£¶ï£¼
0.788 68
ï£½
âˆ’0.211 32 ï£¸ â†” 1. 732 1,
ï£¾
âˆ’0.577 35
ï£¶ï£¼
0.211 32
ï£½
âˆ’0.788 68 ï£¸ â†” âˆ’1. 732 1
ï£¾
0.577 35
ï£¶
2 1.0
5 3 ï£¸, eigenvectors:
3 2
ï£¶ï£¼
0.416 01 ï£½
0.779 18 ï£¸ â†” 7. 873 0,
ï£¾
0.468 85
ï£¶ï£¼
0.904 53
ï£½
âˆ’0.301 51 ï£¸ â†” 2.0,
ï£¾
âˆ’0.301 51
ï£¶ï£¼
9. 356 8 Ã— 10âˆ’2 ï£½
ï£¸ â†” 0.127 02
âˆ’0.549 52
ï£¾
0.830 22
ï£¶
2 1.0
5 3 ï£¸, eigenvectors:
3 2
ï£¶ï£¼
0.284 33 ï£½
0.819 59 ï£¸ â†” 7. 514 6,
ï£¾
0.497 43
2
2
1

The Saylor Foundation

G.23. EXERCISES

497

ï£±ï£«
ï£¶ï£¼
0.209 84
ï£½
ï£²
ï£­ 0.453 06 ï£¸ â†” 0.189 11,
ï£³
ï£¾
âˆ’0.866 43
ï£±ï£«
ï£¶ï£¼
0.935 48
ï£²
ï£½
ï£­
ï£¸ â†” âˆ’0.703 70
âˆ’0.350 73
ï£³
ï£¾
4. 316 8 Ã— 10âˆ’2
ï£«
ï£¶
0 2 1.0
5 ï£­ 2 0 3 ï£¸, eigenvectors:
1 3 2
ï£±ï£«
ï£¶ï£¼
0.379 2
ï£²
ï£½
ï£­ 0.584 81 ï£¸ â†” 4. 975 4,
ï£³
ï£¾
0.717 08
ï£±ï£«
ï£¶ï£¼
0.814 41
ï£²
ï£½
ï£­ 0.156 94 ï£¸ â†” âˆ’0.300 56,
ï£³
ï£¾
âˆ’0.558 66
ï£±ï£«
ï£¶ï£¼
0.439 25
ï£²
ï£½
ï£­ âˆ’0.795 85 ï£¸ â†” âˆ’2. 674 9
ï£³
ï£¾
0.416 76
6 |7. 333 3 âˆ’ Î»q | â‰¤ 0.471 41
7 |7 âˆ’ Î»q | = 2. 449 5
8 |Î»q âˆ’ 8| â‰¤ 3. 266 0
9 âˆ’10 â‰¤ Î» â‰¤ 12
10 x3 + 7x2 + 3x + 7.0 = 0, Solution is:
ï£±
ï£¼
ï£² [x = âˆ’0.145 83 + 1. 011i] , ï£½
[x = âˆ’0.145 83 âˆ’ 1. 011i] ,
ï£³
ï£¾
[x = âˆ’6. 708 3]
11 âˆ’1. 475 5 + 1. 182 7i,
âˆ’1. 475 5 âˆ’ 1. 182 7i, âˆ’0.024 44 + 0.528 23i,
âˆ’0.024 44 âˆ’ 0.528 23i
12 Let QT AQ = H where H is upper Hessenberg.
Then take the transpose of both sides. This will
show that H = H T and so H is zero on the top as
well.

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

Index
âˆ©, 11
âˆª, 11

centripetal acceleration, 66
characteristic and minimal polynomial, 242
characteristic equation, 157
characteristic polynomial, 97, 240
characteristic value, 157
codomain, 12
cofactor, 77, 91
column rank, 94, 110
commutative ring, 445
commutator, 480
commutator subgroup, 480
companion matrix, 267, 383
complete, 360
completeness axiom, 20
complex conjugate, 16
complex numbers
absolute value, 16
ï¬eld, 16
complex numbers, 15
complex roots, 17
composition of linear transformations, 234
comutator, 198
condition number, 347
conformable, 42
conjugate ï¬elds, 471
conjugate linear, 293
converge, 440
convex combination, 244
convex hull, 243
compactness, 244
coordinate axis, 32
coordinates, 32
Coriolis acceleration, 66
Coriolis acceleration
earth, 68
Coriolis force, 66
counting zeros, 187
Courant Fischer theorem, 315
Cramerâ€™s rule, 81, 94
cyclic set, 251

A close to B
eigenvalues, 188
A invariant, 249
Abelâ€™s formula, 103, 264, 265
absolute convergence
convergence, 351
adjugate, 80, 93
algebraic number
minimal polynomial, 216
algebraic numbers, 215
ï¬eld, 217
algebraically complete ï¬eld
countable one, 450
almost linear, 430
almost linear system, 431
alternating group, 477
3 cycles, 477
analytic function of matrix, 413
Archimedean property, 22
assymptotically stable, 430
augmented matrix, 28
automorphism, 459
autonomous, 430
Banach space, 337
basic feasible solution, 136
basic variables, 136
basis, 59, 200
Binet Cauchy
volumes, 306
Binet Cauchy formula, 89
block matrix, 99
multiplication, 100
block multiplication, 98
bounded linear transformations, 340
Cauchy Schwarz inequality, 34, 288, 337
Cauchy sequence, 301, 337, 440
Cayley Hamilton theorem, 97, 263, 274
centrifugal acceleration, 66

damped vibration, 427
498

Saylor URL: http://www.saylor.org/courses/ma212/

The Saylor Foundation

INDEX

499

defective, 162
DeMoivre identity, 17
dense, 22
density of rationals, 22
determinant
block upper triangular matrix, 174
deï¬nition, 86
estimate for Hermitian matrix, 286
expansion along a column, 77
expansion along a row, 77
expansion along row, column, 91
Hadamard inequality, 286
inverse of matrix, 80
matrix inverse, 92
partial derivative, cofactor, 103
permutation of rows, 86
product, 89
product of eigenvalues, 180
product of eigenvalules, 191
row, column operations, 79, 88
summary of properties, 96
symmetric deï¬nition, 87
transpose, 87
diagonalizable, 232, 307
minimal polynomial condition, 266
basis of eigenvectors, 171
diameter, 439
diï¬€erentiable matrix, 62
diï¬€erential equations
ï¬rst order systems, 194
digraph, 44
dimension of vector space, 203
direct sum, 74, 246
directed graph, 44
discrete Fourier transform, 335
distinct roots
polynomial and its derivative, 472
division of real numbers, 23
Dolittleâ€™s method, 124
domain, 12
dot product, 33
dyadics, 226
dynamical system, 171
eigenspace, 159, 248
eigenvalue, 76, 157
eigenvalues, 97, 187, 240
AB and BA, 101
eigenvector, 76, 157
eigenvectors
distinct eigenvalues independence, 162

Saylor URL: http://www.saylor.org/courses/ma212/

elementary matrices, 105
elementary symmetric polynomials, 445
empty set, 11
equality of mixed partial derivatives, 183
equilibrium point, 430
equivalence class, 210, 230
equivalence of norms, 340
equivalence relation, 210, 229
Euclidean algorithm, 23
exchange theorem, 57
existence of a ï¬xed point, 362
ï¬eld
ordered, 14
ï¬eld axioms, 13
ï¬eld extension, 211
dimension, 212
ï¬nite, 212
ï¬eld extensions, 213
ï¬elds
characteristic, 473
perfect, 474
ï¬elds
perfect, 474
ï¬nite dimensional inner product space
closest point, 291
ï¬nite dimensional normed linear space
completeness, 339
equivalence of norms, 339
ï¬xed ï¬eld, 466
ï¬xed ï¬elds and subgroups, 468
Foucalt pendulum, 68
Fourier series, 301
Fredholm alternative, 117, 298
free variable, 30
Frobenius
inner product, 197
Frobenius norm, 329
singular value decomposition, 329
Frobinius norm, 334
functions, 12
fundamental matrix, 366, 423
fundamental theorem of algebra, 443, 450
fundamental theorem of algebra
plausibility argument, 19
fundamental theorem of arithmetic, 26
fundamental theorem of Galois theory, 470
Galois group, 464
size, 464
gamblerâ€™s ruin, 282

The Saylor Foundation

500

INDEX

Gauss Jordan method for inverses, 48
Gauss Seidel method, 356
Gelfand, 349
generalized eigenspace, 75
generalized eigenspaces, 248, 258
generalized eigenvectors, 259
Gerschgorinâ€™s theorem, 186
Gram Schmidt procedure, 134, 173, 290
Gram Schmidt process, 289, 290
Gramm Schmidt process, 173
greatest common divisor, 23, 207
characterization, 23
greatest lower bound, 20
Gronwallâ€™s inequality, 368, 422
group
deï¬nition, 466
group
solvable, 480
Hermitian, 177
orthonormal basis eigenvectors, 313
positive deï¬nite, 318
real eigenvalues, 179
Hermitian matrix
factorization, 286
positive part, 414
positive part, Lipschitz continuous, 414
Hermitian operator, 293
largest, smallest, eigenvalues, 314
spectral representation, 312
Hessian matrix, 184
Hilbert space, 313
Holderâ€™s inequality, 343
homomorphism, 459
Householder
reï¬‚ection, 131
Householder matrix, 130
idempotent, 72, 489
impossibility of solution by radicals, 483
inconsistent, 29
initial value problem
existence, 366, 417
global solutions, 421
linear system, 418
local solutions, existence, uniqueness, 420
uniqueness, 368, 417
injective, 12
inner product, 33, 287
inner product space, 287
adjoint operator, 292

Saylor URL: http://www.saylor.org/courses/ma212/

parallelogram identity, 289
triangle inequality, 289
integers mod a prime, 223
integral
operator valued function, 367
vector valued function, 367
intersection, 11
intervals
notation, 11
invariant, 310
subspace, 249
invariant subspaces
direct sum, block diagonal matrix, 250
inverses and determinants, 92
invertible, 47
invertible matrix
product of elementary matrices, 115
irreducible, 207
relatively prime, 208
isomorphism, 459
extensions, 461
iterative methods
alternate proof of convergence, 365
convergence criterion, 360
diagonally dominant, 365
proof of convergence, 363
Jocobi method, 354
Jordan block, 256, 258
Jordan canonical form
existence and uniqueness, 259
powers of a matrix, 260
ker, 115
kernel, 55
kernel of a product
direct sum decomposition, 246
Krylov sequence, 251
Lagrange form of remainder, 183
Laplace expansion, 91
least squares, 121, 297, 491
least upper bound, 20
Lindemann Weierstrass theorem, 219, 458
linear combination, 39, 56, 88
linear transformation, 53, 225
deï¬ned on a basis, 226
dimension of vector space, 226
existence of eigenvector, 241
kernel, 245
matrix, 54
minimal polynomial, 241

The Saylor Foundation

INDEX

501

rotation, 235
linear transformations
a vector space, 225
composition, matrices, 234
sum, 225, 295
linearly dependent, 56
linearly independent, 56, 200
linearly independent set
extend to basis, 204
Lipschitz condition, 417
LU factorization
justiï¬cation for multiplier method, 127
multiplier method, 123
solutions of linear systems, 125
main diagonal, 78
Markov chain, 279, 280
Markov matrix, 275
limit, 278
regular, 278
steady state, 275, 278
mathematical induction, 21
matrices
commuting, 309
notation, 38
transpose, 46
matrix, 37
diï¬€erentiation operator, 228
injective, 61
inverse, 47
left inverse, 93
lower triangular, 78, 94
Markov, 275
non defective, 177
normal, 177
rank and existence of solutions, 116
rank and nullity, 115
right and left inverse, 61
right inverse, 93
right, left inverse, 93
row, column, determinant rank, 94
self adjoint, 170
stochastic, 275
surjective, 61
symmetric, 169
unitary, 173
upper triangular, 78, 94
matrix exponential, 366
matrix multiplication
deï¬nition, 40
entries of the product, 42

Saylor URL: http://www.saylor.org/courses/ma212/

not commutative, 41
properties, 46
vectors, 39
matrix of linear transformation
orthonormal bases, 231
migration matrix, 279
minimal polynomial, 75, 240, 248
eigenvalues, eigenvectors, 241
ï¬nding it, 263
generalized eigenspaces, 248
minor, 77, 91
mixed partial derivatives, 182
monic, 207
monomorphism, 459
Moore Penrose inverse, 331
least squares, 332
moving coordinate system, 63
acceleration , 66
negative deï¬nite, 318
principle minors, 319
Neuman
series, 370
nilpotent
block diagonal matrix, 256
Jordan form, uniqueness, 257
Jordan normal form, 256
non defective, 266
non solvable group, 481
nonnegative self adjoint
square root, 319
norm, 287
strictly convex, 364
uniformly convex, 364
normal, 324
diagonalizable, 178
non defective, 177
normal closure, 464, 471
normal extension, 463
normal subgroup, 469, 480
normed linear space, 287, 337
normed vector space, 287
norms
equivalent, 338
null and rank, 302
null space, 55
nullity, 115
one to one, 12
onto, 12
operator norm, 340

The Saylor Foundation

502

INDEX

orthogonal matrix, 76, 83, 130, 175
orthogonal projection, 291
orthonormal basis, 289
orthonormal polynomials, 299
p norms, 343
axioms of a norm, 343
parallelepiped
volume, 303
partitioned matrix, 98
Penrose conditions, 332
permutation, 85
even, 107
odd, 107
permutation matrices, 105, 476
permutations
cycle, 476
perp, 117
Perronâ€™s theorem, 404
pivot column, 113
PLU factorization, 126
existence, 130
polar decomposition
left, 324
right, 322
polar form complex number, 16
polynomial, 206
degree, 206
divides, 207
division, 206
equal, 206
Euclidean algorithm, 206
greatest common divisor, 207
greatest common divisor description, 207
greatest common divisor, uniqueness, 207
irreducible, 207
irreducible factorization, 208
relatively prime, 207
root, 206
polynomials
canceling, 208
factorization, 209
positive deï¬nite
postitive eigenvalues, 318
principle minors, 318
postitive deï¬nite, 318
power method, 373
prime number, 23
prime numbers
inï¬nity of primes, 222
principle directions, 165

Saylor URL: http://www.saylor.org/courses/ma212/

principle minors, 318
product rule
matrices, 62
projection map
convex set, 302
Putzerâ€™s method, 424
QR algorithm, 190, 387
convergence, 390
convergence theorem, 390
non convergence, 394
nonconvergence, 191
QR factorization, 131
existence, 133
Gram Schmidt procedure, 134
quadratic form, 181
quotient group, 469
quotient space, 223
quotient vector space, 223
random variables, 279
range, 12
rank, 111
number of pivot columns, 115
rank of a matrix, 94, 110
rank one transformation, 295
rational canonical form, 267
uniqueness, 270
Rayleigh quotient, 383
how close?, 384
real numbers, 12
real Schur form, 175
regression line, 297
regular Sturm Liouville problem, 300
relatively prime, 23
Riesz representation theorem, 292
right Cauchy Green strain tensor, 322
right polar decomposition, 322
row equivalelance
determination, 114
row equivalent, 114
row operations, 28, 105
inverse, 28
linear relations between columns, 111
row rank, 94, 110
row reduced echelon form
deï¬nition, 112
examples, 112
existence, 112
uniqueness, 114
scalar product, 33

The Saylor Foundation

INDEX

503

scalars, 18, 32, 37
Schurâ€™s theorem, 174, 310
inner product space, 310
second derivative test, 185
self adjoint, 177, 293
self adjoint nonnegative
roots, 320
separable
polynomial, 465
sequential compactness, 441
sequentially compact, 441
set notation, 11
sgn, 84
uniqueness, 85
shifted inverse power method, 376
complex eigenvalues, 381
sign of a permutation, 85
similar
matrix and its transpose, 266
similar matrices, 82, 103, 229
similarity transformation, 229
simple ï¬eld extension, 218
simple groups, 479
simplex tableau, 138
simultaneous corrections, 354
simultaneously diagonalizable, 308
commuting family, 310
singular value decomposition, 327
singular values, 327
skew symmetric, 47, 169
slack variables, 136, 138
solvable by radicals, 482
solvable group, 480
space of linear transformations
vector space, 295
span, 56, 88
spanning set
restricting to a basis, 204
spectral mapping theorem, 414
spectral norm, 341
spectral radius, 348, 349
spectrum, 157
splitting ï¬eld, 214
splitting ï¬elds
isomorphic, 462
normal extension, 463
stable, 430
stable manifold, 437
stationary transition probabilities, 280
Stochastic matrix, 280
stochastic matrix, 275

Saylor URL: http://www.saylor.org/courses/ma212/

subsequence, 440
subspace, 56, 200
basis, 60, 205
dimension, 60
invariant, 249
subspaces
direct sum, 246
direct sum, basis, 246
surjective, 12
Sylvester, 74
law of inertia, 196
dimention of kernel of product, 245
Sylvesterâ€™s equation, 306
symmetric, 47, 169
symmetric polynomial theorem, 446
symmetric polynomials, 445
system of linear equations, 30
tensor product, 295
trace, 180
AB and BA, 180
sum of eigenvalues, 191
transpose, 46
properties, 46
transposition, 476
triangle inequality, 35
trivial, 56
union, 11
Unitary matrix
representation, 370
upper Hessenberg matrix, 273, 399
Vandermonde determinant, 104
variation of constants formula, 195, 426
variational inequality, 302
vector
angular velocity, 64
vector space
axioms, 38, 199
basis, 59
dimension, 60
examples, 199
vector space axioms, 33
vectors, 39
volume
parallelepiped, 303
well ordered, 21
Wronskian, 103, 195, 264, 265, 426
Wronskian alternative, 195, 426

The Saylor Foundation

